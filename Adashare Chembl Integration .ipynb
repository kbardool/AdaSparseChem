{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c6bbe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:49.898064Z",
     "start_time": "2021-12-24T19:28:48.733575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "# import tqdm.notebook.trange as tnrange\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader \n",
    "import scipy.sparse\n",
    "from time import sleep\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)\n",
    "torch.set_printoptions( linewidth=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be67b3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:50.785543Z",
     "start_time": "2021-12-24T19:28:49.902186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cuda is available  :  True\n",
      " CUDA device count  :  1\n",
      " CUDA current device:  0\n",
      " GPU Processes :  pynvml module not found, please install pynvml\n",
      "\n",
      " Device :  0\n",
      "   name:        NVIDIA GeForce GTX 970M\n",
      "   capability:  (5, 2)\n",
      "   properties:  _CudaDeviceProperties(name='NVIDIA GeForce GTX 970M', major=5, minor=2, total_memory=3071MB, multi_processor_count=10)\n"
     ]
    }
   ],
   "source": [
    "print(' Cuda is available  : ', torch.cuda.is_available())\n",
    "print(' CUDA device count  : ', torch.cuda.device_count())\n",
    "print(' CUDA current device: ', torch.cuda.current_device())\n",
    "\n",
    "print(' GPU Processes : ', torch.cuda.list_gpu_processes())\n",
    "print()\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(' Device : ', i)\n",
    "    print('   name:       ', torch.cuda.get_device_name())\n",
    "    print('   capability: ', torch.cuda.get_device_capability())\n",
    "    print('   properties: ', torch.cuda.get_device_properties(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bf7159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:52.158549Z",
     "start_time": "2021-12-24T19:28:50.789605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | nan% |  1% |\n",
      " Allocated :  0\n",
      " Reserved  :  0\n",
      "Initial GPU Usage\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | nan% |  1% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | nan% |  1% |\n",
      "CUDA empty cache\n",
      "Close and reopen device\n",
      "Close device\n",
      "Reopen device\n",
      "GPU Usage after closing and reopening\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | nan% |  3% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()                             \n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "print(' Allocated : ', torch.cuda.memory_allocated(\"cuda:0\") ) #returns you the current GPU memory usage by tensors in bytes for a given device\n",
    "print(' Reserved  : ', torch.cuda.memory_reserved(\"cuda:0\") )#returns you the current GPU memory managed by caching allocator in bytes for a given device, in previous PyTorch versions the command was torch.cuda.memory_cached\n",
    " \n",
    "\n",
    "cuda_device = 0 \n",
    "\n",
    "def free_gpu_cache(cuda_device):\n",
    "    print(\"Initial GPU Usage\")    \n",
    "    gpu_usage()                             \n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "    \n",
    "    print(\"CUDA empty cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Close and reopen device\")\n",
    "    cuda.select_device(cuda_device)\n",
    "    print(\"Close device\")    \n",
    "    cuda.close()\n",
    "    print(\"Reopen device\")    \n",
    "    cuda.select_device(cuda_device)\n",
    "\n",
    "    print(\"GPU Usage after closing and reopening\")\n",
    "    gpu_usage()\n",
    "\n",
    "            \n",
    "free_gpu_cache(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49f4dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.120491Z",
     "start_time": "2021-12-24T19:28:52.165632Z"
    }
   },
   "outputs": [],
   "source": [
    "from dev.sparsechem_utils import load_sparse, load_task_weights, class_fold_counts, fold_and_transform_inputs\n",
    "from dev.chembl_dataloader_dev import ClassRegrSparseDataset_v3, ClassRegrSparseDataset\n",
    "from utils.util import (makedir, print_separator, create_path, print_yaml, should, fix_random_seed, \n",
    "                        read_yaml_from_input, timestring, print_heading, print_dbg, print_underline)\n",
    "from dev.sparsechem_env_dev import SparseChemEnv_Dev\n",
    "from dev.train_dev import eval_dev\n",
    "\n",
    "def vprint(s=\"\", verbose = False):\n",
    "    if verbose:\n",
    "        print(s)\n",
    "\n",
    "vprint(f\"\\nArgs : \\n--------------\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd6a7ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.151747Z",
     "start_time": "2021-12-24T19:28:53.123638Z"
    }
   },
   "outputs": [],
   "source": [
    "# def print_separator(text, total_len=50):\n",
    "#     print('#' * total_len)\n",
    "#     left_width = (total_len - len(text))//2\n",
    "#     right_width = total_len - len(text) - left_width\n",
    "#     print(\"#\" * left_width + text + \"#\" * right_width)\n",
    "#     print('#' * total_len)\n",
    "\n",
    "# def print_dbg(text, verbose = False):\n",
    "#     if verbose:\n",
    "#         print(text)\n",
    "\n",
    "# @debug_off\n",
    "# def print_heading(text,  verbose = False):\n",
    "#     len_ttl = max(len(text)+4, 50)\n",
    "#     if verbose:\n",
    "#         print('-' * len_ttl)\n",
    "#         print(f\" {text}\")\n",
    "#         # left_width = (total_len - len(text))//2\n",
    "#         # right_width = total_len - len(text) - left_width\n",
    "#         # print(\"#\" * left_width + text + \"#\" * right_width)\n",
    "#         print('-' * len_ttl,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8ef0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.194699Z",
     "start_time": "2021-12-24T19:28:53.155328Z"
    }
   },
   "outputs": [],
   "source": [
    "print_heading(\"hello_kevin\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05032bf4",
   "metadata": {},
   "source": [
    "## Read yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d33640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.269361Z",
     "start_time": "2021-12-24T19:28:53.198172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "####################READ YAML#####################\n",
      "##################################################\n",
      "{'config': 'yamls/adashare/chembl_2task.yml', 'exp_ids': [0], 'gpus': [0], 'cpu': True}\n"
     ]
    }
   ],
   "source": [
    "input_args = \" --config yamls/adashare/chembl_2task.yml --cpu \".split()\n",
    "\n",
    "print_separator('READ YAML')\n",
    "opt, gpu_ids, _ = read_yaml_from_input(input_args)\n",
    "fix_random_seed(opt[\"seed\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4ae507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.334684Z",
     "start_time": "2021-12-24T19:28:53.281029Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseChem 20211224_112853\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(opt['exp_name'], date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b296e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.386527Z",
     "start_time": "2021-12-24T19:28:53.339506Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['exp_name'] = date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c6bd87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.432700Z",
     "start_time": "2021-12-24T19:28:53.392843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create folder ../experiments/logs/SparseChem/20211224_112853\n",
      " Create folder ../experiments/results/SparseChem/20211224_112853\n",
      " Create folder ../experiments/checkpoints/SparseChem/20211224_112853\n"
     ]
    }
   ],
   "source": [
    "create_path(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8664662e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.501718Z",
     "start_time": "2021-12-24T19:28:53.435579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "exp_name. : 20211224_112853\n",
      "seed. : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "backbone. : SparseChem\n",
      "backbone_orig. : ResNet18\n",
      "orig_tasks. : ['seg', 'sn']\n",
      "tasks. : ['class', 'class', 'class']\n",
      "tasks_num_class. : [5, 5, 5]\n",
      "lambdas. : [1, 1, 1]\n",
      "policy_model. : task-specific\n",
      "verbose. : True\n",
      "paths.\n",
      "paths.log_dir. : ../experiments/logs/SparseChem\n",
      "paths.result_dir. : ../experiments/results/SparseChem\n",
      "paths.checkpoint_dir. : ../experiments/checkpoints/SparseChem\n",
      "dataload.\n",
      "dataload.dataset. : Chembl_23_mini\n",
      "dataload.dataroot. : /home/kbardool/kusanagi/MLDatasets/chembl_23_mini\n",
      "dataload.x. : chembl_23mini_x.npy\n",
      "dataload.folding. : chembl_23mini_folds.npy\n",
      "dataload.weights_class. : None\n",
      "dataload.fold_inputs. : 32000\n",
      "dataload.input_transform. : None\n",
      "dataload.y_tasks. : ['chembl_23_adashare_y1_bin_sparse.npy', 'chembl_23_adashare_y2_bin_sparse.npy', 'chembl_23_adashare_y3_bin_sparse.npy']\n",
      "dataload.y_censor. : None\n",
      "dataload.fold_te. : None\n",
      "dataload.crop_h. : 321\n",
      "dataload.crop_w. : 321\n",
      "dataload.min_samples_class. : 5\n",
      "dataload.fold_va. : 0\n",
      "SC.\n",
      "SC.batch_ratio. : 0.02\n",
      "SC.normalize_loss. : None\n",
      "input_size_freq. : None\n",
      "input_size. : 32000\n",
      "hidden_sizes. : [41, 42, 43, 44]\n",
      "tail_hidden_size. : 50\n",
      "first_non_linearity. : relu\n",
      "middle_non_linearity. : relu\n",
      "middle_dropout. : 0.2\n",
      "last_non_linearity. : relu\n",
      "last_dropout. : 0.2\n",
      "class_output_size. : None\n",
      "regr_output_size. : None\n",
      "policy. : True\n",
      "init_neg_logits. : None\n",
      "is_sparse. : True\n",
      "diff_sparsity_weights. : True\n",
      "is_sharing. : True\n",
      "skip_layer. : 0\n",
      "is_curriculum. : True\n",
      "curriculum_speed. : 3\n",
      "fix_BN. : False\n",
      "retrain_from_pl. : False\n",
      "train.\n",
      "train.batch_size. : 64\n",
      "train.total_iters. : 200\n",
      "train.warm_up_iters. : 4000\n",
      "train.task_lr. : 0.0001\n",
      "train.backbone_lr. : 0.0001\n",
      "train.policy_lr. : 0.0001\n",
      "train.reg_w. : 0.05\n",
      "train.Lambda_sparsity. : 0.05\n",
      "train.reg_w_hamming. : 0.5\n",
      "train.Lambda_sharing. : 0.5\n",
      "train.print_freq. : 100\n",
      "train.val_freq. : 400\n",
      "train.decay_lr_freq. : 4000\n",
      "train.decay_lr_rate. : 0.5\n",
      "train.decay_temp_freq. : 100\n",
      "train.init_temp. : 5\n",
      "train.decay_temp. : 0.965\n",
      "train.resume. : False\n",
      "train.retrain_resume. : False\n",
      "train.policy_iter. : best\n",
      "train.which_iter. : warmup\n",
      "train.init_method. : equal\n",
      "train.hard_sampling. : False\n",
      "test.\n",
      "test.which_iter. : best\n",
      "cpu. : True\n"
     ]
    }
   ],
   "source": [
    "# print yaml on the screen\n",
    "\n",
    "lines = print_yaml(opt)\n",
    "with open(os.path.join(opt['paths']['log_dir'], opt['exp_name'], 'opt.txt'), 'w+') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "for line in lines: print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bfa8d",
   "metadata": {},
   "source": [
    "## Chembl Dataloader V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0a8e4",
   "metadata": {},
   "source": [
    "#### Chembl Data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2604908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.591796Z",
     "start_time": "2021-12-24T19:28:53.505062Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18388       [ 5516 11032 16549 18387]\n",
      "/home/kbardool/kusanagi/MLDatasets/chembl_23_mini/chembl_23mini_x.npy\n",
      "/home/kbardool/kusanagi/MLDatasets/chembl_23_mini/chembl_23_adashare_y1_bin_sparse.npy\n",
      "/home/kbardool/kusanagi/MLDatasets/chembl_23_mini/chembl_23mini_folds.npy\n",
      " weights_class:  None\n",
      " idx_train    len:   5516  - [   0    1    2 ... 5513 5514 5515] \n",
      " idx_train1   len:   5516  - [ 5516  5517  5518 ... 11029 11030 11031] \n",
      " idx_train2   len:   5517  - [11032 11033 11034 ... 16546 16547 16548] \n",
      " val_train    len:   1838  - [16549 16550 16551 ... 18384 18385 18386] \n"
     ]
    }
   ],
   "source": [
    "dataroot = opt['dataload']['dataroot']\n",
    "ecfp     = load_sparse(dataroot, opt['dataload']['x'])\n",
    "\n",
    "total_input = ecfp.shape[0]\n",
    "ranges      = (np.cumsum([0.3, 0.3, 0.3, 0.1])* total_input).astype(np.int32)\n",
    "print(total_input, '     ', ranges)\n",
    "\n",
    "idx_train  = np.arange(ranges[0])\n",
    "idx_train1 = np.arange(ranges[0], ranges[1])\n",
    "idx_train2 = np.arange(ranges[1], ranges[2])\n",
    "idx_val    = np.arange(ranges[2], ranges[-1])\n",
    "\n",
    "\n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['x']))\n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['y_tasks'][0]))\n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['folding']))\n",
    "print(' weights_class: ',opt['dataload']['weights_class'])\n",
    "\n",
    "print(f' idx_train    len: {len(idx_train)  :6d}  - {(idx_train)} ')\n",
    "print(f' idx_train1   len: {len(idx_train1) :6d}  - {(idx_train1)} ')\n",
    "print(f' idx_train2   len: {len(idx_train2) :6d}  - {(idx_train2)} ')\n",
    "print(f' val_train    len: {len(idx_val)    :6d}  - {(idx_val)} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e93f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.634311Z",
     "start_time": "2021-12-24T19:28:53.596068Z"
    }
   },
   "outputs": [],
   "source": [
    "class InfiniteDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize an iterator over the dataset.\n",
    "        self.dataset_iterator = super().__iter__()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch = next(self.dataset_iterator)\n",
    "        except StopIteration:\n",
    "            # Dataset exhausted, use a new fresh iterator.\n",
    "            self.dataset_iterator = super().__iter__()\n",
    "            batch = next(self.dataset_iterator)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874017a7",
   "metadata": {},
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93598b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:53.886782Z",
     "start_time": "2021-12-24T19:28:53.637836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valset = ClassRegrSparseDataset_v3(opt, index = idx_val, verbose = True)\n",
    "val_loader = InfiniteDataLoader(valset, batch_size=opt['train']['batch_size'], num_workers = 1, pin_memory=True, collate_fn=valset.collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ed44b",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7eb382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:54.974702Z",
     "start_time": "2021-12-24T19:28:53.898680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainset = ClassRegrSparseDataset_v3(opt, index = idx_train) \n",
    "train_loader = InfiniteDataLoader(trainset, batch_size=opt['train']['batch_size'], num_workers = 2, pin_memory=True, collate_fn=trainset.collate, shuffle=False)\n",
    "\n",
    "trainset1 = ClassRegrSparseDataset_v3(opt, index = idx_train1)\n",
    "train1_loader = InfiniteDataLoader(trainset1, batch_size=opt['train']['batch_size'], num_workers = 2, pin_memory=True, collate_fn=trainset1.collate, shuffle=False)\n",
    "\n",
    "trainset2 = ClassRegrSparseDataset_v3(opt, index = idx_train2)\n",
    "train2_loader = InfiniteDataLoader(trainset2, batch_size=opt['train']['batch_size'], num_workers = 2, pin_memory=True, collate_fn=trainset2.collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55019de7",
   "metadata": {},
   "source": [
    "#### Test dataloader output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055854ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.106913Z",
     "start_time": "2021-12-24T19:28:54.986617Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_enumerator = enumerate(val_loader)\n",
    "# val_iterator = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb620e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.154382Z",
     "start_time": "2021-12-24T19:28:55.117160Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_batch_idx, val_batch = next(val_enumerator)\n",
    "# print(type(val_batch['row_id']),val_batch['row_id'][0], val_batch['row_id'][-1] )\n",
    "\n",
    "# ctr = 0\n",
    "# for i in range(100):\n",
    "#     val_batch_1 = next(val_loader)\n",
    "#     print(' iteration: ', ctr,' len: ', len(val_batch_1['row_id']),'start: [', val_batch_1['row_id'][0],   val_batch_1['row_id'][-1],']' )\n",
    "#     ctr += 1\n",
    "\n",
    "# ctr = 0    \n",
    "# for val_batch_1 in iter(val_loader):\n",
    "# #     val_batch_1 = next(val_loader)\n",
    "#     print(' iteration: ', ctr,' len: ', len(val_batch_1['row_id']),'start: [', val_batch_1['row_id'][0],   val_batch_1['row_id'][-1],']' )\n",
    "#     ctr += 1    \n",
    "#     if ctr == 105:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a89ae5d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.201653Z",
     "start_time": "2021-12-24T19:28:55.160345Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_batch_1 = next(val_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cafbfe26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.253989Z",
     "start_time": "2021-12-24T19:28:55.207253Z"
    }
   },
   "outputs": [],
   "source": [
    "#  batch_idx, batch = next(batch_enumerator)\n",
    "\n",
    "# print(batch.keys())\n",
    "# print(batch['x_ind'].shape)\n",
    "# print(type(batch['batch_size']))\n",
    "# for i in batch.keys():\n",
    "#     if not isinstance(batch[i], int):\n",
    "#         print(i, batch[i].shape)\n",
    "\n",
    "# task0_Y =  torch.sparse_coo_tensor(\n",
    "#         batch[\"task0_ind\"],\n",
    "#         batch[\"task0_data\"],\n",
    "#         size = [batch[\"batch_size\"], 5]).to(\"cpu\", non_blocking=True).to_dense().numpy()\n",
    "\n",
    "# print(task0_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35bd10ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.300118Z",
     "start_time": "2021-12-24T19:28:55.258629Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(f\" train_loader: dataset input size       :  {train_loader.dataset.input_size}\")\n",
    "# print(f\" train_loader: class output size        :  {train_loader.dataset.class_output_size}\")\n",
    "# print()\n",
    "# print(f\" size of training set 0 (warm up)       :  {len(trainset)}\")\n",
    "# print(f\" size of training set 1 (network parms) :  {len(trainset1)}\")\n",
    "# print(f\" size of training set 2 (policy weights):  {len(trainset2)}\")\n",
    "# print(f\" size of validation set                 :  {len(valset)}\")\n",
    "# print(f\"                                Total   :  {len(trainset)+len(trainset1)+len(trainset2)+len(valset)}\")\n",
    "\n",
    "# print(f\" batch size       : {opt['train']['batch_size']}\")\n",
    "# print(f' len train_loader : {len(train_loader)}')\n",
    "# print(f' len train1_loader: {len(train1_loader)}')\n",
    "# print(f' len train2_loader: {len(train2_loader)}')\n",
    "# print(f' len val_loader   : {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f4f21",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c09986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.350162Z",
     "start_time": "2021-12-24T19:28:55.303987Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate'] = opt['train'].get('alpha_iter_alternate'  , len(train2_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "716c9256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.417665Z",
     "start_time": "2021-12-24T19:28:55.355126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trainset.y_class     :  [(5516, 5), (5516, 5), (5516, 5)]\n",
      " trainset1.y_class    :  [(5516, 5), (5516, 5), (5516, 5)]\n",
      " trainset2.y_class    :  [(5517, 5), (5517, 5), (5517, 5)]\n",
      " valset.y_class       :  [(1838, 5), (1838, 5), (1838, 5)] \n",
      "\n",
      " length train_loader  :  87\n",
      " length train1_loader :  87\n",
      " length train2_loader :  87\n",
      " length val_loader    :  29\n",
      "\n",
      " size of training set 0 (warm up)       :  5516\n",
      " size of training set 1 (network parms) :  5516\n",
      " size of training set 2 (policy weights):  5517\n",
      " size of validation set                 :  1838\n",
      "                               Total    :  18387\n",
      "\n",
      " len train_loader set 0 (warm up)       :  87\n",
      " len train_loader set 1 (network parms) :  87\n",
      " len train_loader set 2 (policy weights):  87\n",
      " len val_loader                         :  29\n",
      "\n",
      " batch size                             : 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" trainset.y_class     :  {[ i.shape  for i in trainset.y_class_list]}\")\n",
    "print(f\" trainset1.y_class    :  {[ i.shape  for i in trainset1.y_class_list]}\")\n",
    "print(f\" trainset2.y_class    :  {[ i.shape  for i in trainset2.y_class_list]}\")\n",
    "print(f\" valset.y_class       :  {[ i.shape  for i in valset.y_class_list  ]} \")\n",
    "print()\n",
    "print(f\" length train_loader  :  {len(train_loader)}\")\n",
    "print(f\" length train1_loader :  {len(train1_loader)}\")\n",
    "print(f\" length train2_loader :  {len(train2_loader)}\")\n",
    "print(f\" length val_loader    :  {len(val_loader)}\")\n",
    "\n",
    "print()\n",
    "print(f' size of training set 0 (warm up)       :  {len(trainset)}')\n",
    "print(f' size of training set 1 (network parms) :  {len(trainset1)}')\n",
    "print(f' size of training set 2 (policy weights):  {len(trainset2)}')\n",
    "print(f' size of validation set                 :  {len(valset)}')\n",
    "print(f'                               Total    :  {len(trainset)+len(trainset1)+len(trainset2)+len(valset)}')\n",
    "print()\n",
    "print(f' len train_loader set 0 (warm up)       :  {len(train_loader)}')\n",
    "print(f' len train_loader set 1 (network parms) :  {len(train1_loader)}')\n",
    "print(f' len train_loader set 2 (policy weights):  {len(train2_loader)}')\n",
    "print(f' len val_loader                         :  {len(val_loader)}')\n",
    "print()\n",
    "# print(f' length of train_loader training set 0 (warm up) : {len(train_loader)}')\n",
    "# print(f' len train1_loader: {len(train1_loader)}')\n",
    "# print(f' len train2_loader: {len(train2_loader)}')\n",
    "\n",
    "\n",
    "print(f\" batch size                             : {opt['train']['batch_size']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edc2b191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.469888Z",
     "start_time": "2021-12-24T19:28:55.421235Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " backbone                : SparseChem \n",
      " paths.log_dir           : ../experiments/logs/SparseChem \n",
      " paths.checkpoint_dir    : ../experiments/checkpoints/SparseChem \n",
      " experiment name         : 20211224_112853 \n",
      " tasks_num_class         : ([5, 5, 5],) \n",
      " Hidden sizes            : [41, 42, 43, 44] \n",
      " init_neg_logits         : (None,) \n",
      " device id               : 0 \n",
      " init temp               : (5,) \n",
      " decay temp              : 0.965 \n",
      " fix BN parms            : False \n",
      " skip_layer              : 0 \n",
      " train.init_method       : equal \n",
      " Total iterations        : 200 \n",
      " Warm-up iterations      : 4000 \n",
      " Print Frequency         : 100 \n",
      " Validation Frequency    : 400 \n",
      " \n",
      " Weight iter alternate   : 87 \n",
      " Alpha  iter alternate   : 87\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    f\"\\n backbone                : {opt['backbone']}\",\n",
    "    f\"\\n paths.log_dir           : {opt['paths']['log_dir']}\", \n",
    "    f\"\\n paths.checkpoint_dir    : {opt['paths']['checkpoint_dir']}\", \n",
    "    f\"\\n experiment name         : {opt['exp_name']}\",\n",
    "    f\"\\n tasks_num_class         : {opt['tasks_num_class'],}\",\n",
    "    f\"\\n Hidden sizes            : {opt['hidden_sizes']}\",     \n",
    "    f\"\\n init_neg_logits         : {opt['init_neg_logits'],}\",\n",
    "    f\"\\n device id               : {gpu_ids[0]}\",\n",
    "    f\"\\n init temp               : {opt['train']['init_temp'],}\",\n",
    "    f\"\\n decay temp              : {opt['train']['decay_temp']}\",\n",
    "    f\"\\n fix BN parms            : {opt['fix_BN']}\",\n",
    "    f\"\\n skip_layer              : {opt['skip_layer']}\",\n",
    "    f\"\\n train.init_method       : {opt['train']['init_method']}\",\n",
    "    f\"\\n Total iterations        : {opt['train']['total_iters']}\",\n",
    "    f\"\\n Warm-up iterations      : {opt['train']['warm_up_iters']}\",\n",
    "    f\"\\n Print Frequency         : {opt['train']['print_freq']}\",\n",
    "    f\"\\n Validation Frequency    : {opt['train']['val_freq']} \\n\",\n",
    "    f\"\\n Weight iter alternate   : {opt['train']['weight_iter_alternate'] }\",\n",
    "    f\"\\n Alpha  iter alternate   : {opt['train']['alpha_iter_alternate'] }\")\n",
    "# print('\\n\\n Opt file \\n ------------ \\n')\n",
    "# pp.pprint(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf868bb",
   "metadata": {},
   "source": [
    "### Create model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d7570ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.519889Z",
     "start_time": "2021-12-24T19:28:55.474234Z"
    }
   },
   "outputs": [],
   "source": [
    "# from dev.MTL2_Dev import MTL2_Dev\n",
    "# from dev.blockdrop_env_dev import BlockDropEnv_Dev\n",
    "# del environ\n",
    "# print(num_int_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3293d4b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.622999Z",
     "start_time": "2021-12-24T19:28:55.524104Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      " * SparseChemEnv_Dev  Initializtion - verbose: False\n",
      "------------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------\n",
      " SparseChemEnv_Dev.super() init()  Start - verbose: False\n",
      "------------------------------------------------------------ \n",
      "\n",
      " log_dir        :  ../experiments/logs/SparseChem/20211224_112853 \n",
      " checkpoint_dir :  ../experiments/checkpoints/SparseChem/20211224_112853 \n",
      " exp_name       :  20211224_112853 \n",
      " tasks_num_class:  [5, 5, 5] \n",
      " device         :  cuda:0 \n",
      " device id      :  0 \n",
      " dataset        :  Chembl_23_mini \n",
      " tasks          :  ['class', 'class', 'class'] \n",
      "\n",
      "--------------------------------------------------\n",
      " SparseChemEnv_Dev.super() init()  end\n",
      "-------------------------------------------------- \n",
      "\n",
      " is_train       :  True \n",
      " init_neg_logits:  None \n",
      " init temp      :  5 \n",
      " decay temp     :  0.965 \n",
      " input_size     :  32000 \n",
      " normalize loss :  None \n",
      " num_tasks      :  3 \n",
      " policys        :  [None, None, None]\n",
      "--------------------------------------------------------\n",
      " * SparseChemEnv_Dev environment successfully created\n",
      "-------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "environ = SparseChemEnv_Dev(log_dir = opt['paths']['log_dir'], \n",
    "                            checkpoint_dir = opt['paths']['checkpoint_dir'], \n",
    "                            exp_name = opt['exp_name'],\n",
    "                            tasks_num_class = opt['tasks_num_class'], \n",
    "                            init_neg_logits = opt['init_neg_logits'], \n",
    "                            device = gpu_ids[0],\n",
    "                            init_temperature = opt['train']['init_temp'], \n",
    "                            temperature_decay= opt['train']['decay_temp'], \n",
    "                            is_train=True,\n",
    "                            opt=opt, \n",
    "                            verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8be8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07462f4d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738062",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9131e9e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.680739Z",
     "start_time": "2021-12-24T19:28:55.627894Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(environ.get_arch_parameters())\n",
    "# print()\n",
    "# print(environ.get_task_specific_parameters())\n",
    "# print()\n",
    "# print(environ.get_backbone_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7498ab15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.741365Z",
     "start_time": "2021-12-24T19:28:55.684077Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " backbone                : SparseChem \n",
      " paths.log_dir           : ../experiments/logs/SparseChem \n",
      " paths.checkpoint_dir    : ../experiments/checkpoints/SparseChem \n",
      " experiment name         : 20211224_112853 \n",
      " tasks_num_class         : ([5, 5, 5],) \n",
      " init_neg_logits         : (None,) \n",
      " device id               : 0 \n",
      " init temp               : (5,) \n",
      " decay temp              : 0.965 \n",
      " fix BN parms            : False \n",
      " skip_layer              : 0 \n",
      " train.init_method       : equal \n",
      " Total iterations        : 200 \n",
      " Warm-up iterations      : 4000 \n",
      " Print Frequency         : 100 \n",
      " Validation Frequency    : 400 \n",
      " Weight iter alternate   : 87 \n",
      " Alpha  iter alternate   : 87\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    f\"\\n backbone                : {opt['backbone']}\",\n",
    "    f\"\\n paths.log_dir           : {opt['paths']['log_dir']}\", \n",
    "    f\"\\n paths.checkpoint_dir    : {opt['paths']['checkpoint_dir']}\", \n",
    "    f\"\\n experiment name         : {opt['exp_name']}\",\n",
    "    f\"\\n tasks_num_class         : {opt['tasks_num_class'],}\",\n",
    "    f\"\\n init_neg_logits         : {opt['init_neg_logits'],}\",\n",
    "    f\"\\n device id               : {gpu_ids[0]}\",\n",
    "    f\"\\n init temp               : {opt['train']['init_temp'],}\",\n",
    "    f\"\\n decay temp              : {opt['train']['decay_temp']}\",\n",
    "    f\"\\n fix BN parms            : {opt['fix_BN']}\",\n",
    "    f\"\\n skip_layer              : {opt['skip_layer']}\",\n",
    "    f\"\\n train.init_method       : {opt['train']['init_method']}\",\n",
    "    f\"\\n Total iterations        : {opt['train']['total_iters']}\",\n",
    "    f\"\\n Warm-up iterations      : {opt['train']['warm_up_iters']}\",\n",
    "    f\"\\n Print Frequency         : {opt['train']['print_freq']}\",\n",
    "    f\"\\n Validation Frequency    : {opt['train']['val_freq']}\",\n",
    "    f\"\\n Weight iter alternate   : {opt['train']['weight_iter_alternate'] }\",\n",
    "    f\"\\n Alpha  iter alternate   : {opt['train']['alpha_iter_alternate'] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fb0d937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.787643Z",
     "start_time": "2021-12-24T19:28:55.751595Z"
    }
   },
   "outputs": [],
   "source": [
    "# val_enumerator = enumerate(val_loader)\n",
    "# batch_enumerator  = enumerate(train_loader,1)\n",
    "# batch_enumerator1 = enumerate(train1_loader,1)\n",
    "# batch_enumerator2 = enumerate(train2_loader,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36910971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.835201Z",
     "start_time": "2021-12-24T19:28:55.792360Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "251f3adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:55.913336Z",
     "start_time": "2021-12-24T19:28:55.839397Z"
    }
   },
   "outputs": [],
   "source": [
    "environ.define_optimizer(policy_learning=False)\n",
    "environ.define_scheduler(policy_learning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b02ee37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:57.674342Z",
     "start_time": "2021-12-24T19:28:55.917337Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate Training \n",
      "cuda available [0]\n",
      "\n",
      "\n",
      "which_iter   : warmup\n",
      "train_resume :  False\n",
      "network[mtl_net].layers: [1, 1, 1, 1]\n",
      "SparseChem num_blocks:  4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_iter   = 0\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0\n",
    "\n",
    "flag         = 'update_w'\n",
    "best_value   = 0 \n",
    "best_iter    = 0\n",
    "p_epoch      = 0\n",
    "best_metrics = None\n",
    "flag_warmup  = True\n",
    "num_blocks = sum(environ.networks['mtl-net'].layers)\n",
    "\n",
    "if opt['train']['resume']:\n",
    "    print('Resume training')\n",
    "    current_iter = environ.load(opt['train']['which_iter'])\n",
    "    environ.networks['mtl-net'].reset_logits()\n",
    "else:\n",
    "    print('Initiate Training ')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available', gpu_ids)   \n",
    "    environ.cuda(gpu_ids)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    environ.cpu()\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(f\"which_iter   : {opt['train']['which_iter']}\\n\"\n",
    "      f\"train_resume :  {opt['train']['resume']}\")\n",
    "\n",
    "print(f\"network[mtl_net].layers: {environ.networks['mtl-net'].layers}\")\n",
    "print(opt['backbone'], 'num_blocks: ', num_blocks)\n",
    "\n",
    "# Fix Alpha - \n",
    "environ.fix_alpha()\n",
    "environ.free_w(opt['fix_BN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc789b12",
   "metadata": {},
   "source": [
    "### Warm-up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47ba6f17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:57.961073Z",
     "start_time": "2021-12-24T19:28:57.691026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_iter        : 0\n",
      "warm_up_iters       : 4000\n",
      "val_freq            : 400\n",
      "batch_size          : 64\n",
      "total_iters         : 200\n",
      "warm_up_iters       : 4000\n",
      "print_freq          : 100\n",
      "val_freq            : 400\n",
      "Length train_loader :  87\n"
     ]
    }
   ],
   "source": [
    "print(f\"current_iter        : {current_iter}\") \n",
    "print(f\"warm_up_iters       : {opt['train']['warm_up_iters']}\")              \n",
    "print(f\"val_freq            : {opt['train']['val_freq']     }\")       \n",
    "print(f\"batch_size          : {opt['train']['batch_size']   }\")         \n",
    "print(f\"total_iters         : {opt['train']['total_iters']}\")  \n",
    "print(f\"warm_up_iters       : {opt['train']['warm_up_iters']}\")\n",
    "print(f\"print_freq          : {opt['train']['print_freq']  }\")\n",
    "print(f\"val_freq            : {opt['train']['val_freq']    }\")\n",
    "print(f\"Length train_loader :  {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30eca97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:58.073846Z",
     "start_time": "2021-12-24T19:28:57.971257Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['train']['warm_up_iters'] = 200\n",
    "opt['train']['total_iters'] = 2000\n",
    "opt['train']['val_freq']   = 50\n",
    "eval_iter = 20\n",
    "batch_idx = 0\n",
    "# opt['train']['print_freq'] =1\n",
    "# opt['train']['weight_iter_alternate'] = 10\n",
    "# opt['train']['alpha_iter_alternate']  = 10\n",
    "\n",
    "\n",
    "# opt['train']['warm_up_iters'] = 200\n",
    "# opt['train']['val_freq']   = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87f349c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:58.117199Z",
     "start_time": "2021-12-24T19:28:58.077613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    " print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c90d0d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:58.152249Z",
     "start_time": "2021-12-24T19:28:58.121510Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current_iter: 0  warm-up iterations:200 - Run  from 1 to 201\n"
     ]
    }
   ],
   "source": [
    "stop_iter  = current_iter +  opt['train']['warm_up_iters']\n",
    "print(f\" Current_iter: {current_iter}  warm-up iterations:{opt['train']['warm_up_iters']} - Run  from {current_iter+1} to {stop_iter+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5beef26",
   "metadata": {},
   "source": [
    "### Warm-Up Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33f151d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:28:58.185154Z",
     "start_time": "2021-12-24T19:28:58.156222Z"
    }
   },
   "outputs": [],
   "source": [
    "# print_heading(f\" {timestring()} - Training iteration {current_iter}  flag: {flag}  p_epoch: {p_epoch} \")    \n",
    "# print(current_iter , , flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8663432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:33:43.072171Z",
     "start_time": "2021-12-24T19:32:11.351535Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current_iter: 200  warm-up iterations:200 - Run  from 201 to 401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457d1725981a4516b4b93aa17e3ab68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:  50%|#####     | 200/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ Validation - eval_iter:20    sum_task_loss: 69.8336   task_loss_avg: 3.4917\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.7340   task_loss_avg: 3.4367\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.7788   task_loss_avg: 3.4389\n",
      " ++ Validation - eval_iter:20    loss_sum : 207.3463  loss_sum_avg:10.3673 \n",
      "** 2021-12-24 11:32:34:600296 - END VALIDATION iteration:  250  validation loss: 10.3673\n",
      "Iteration  300 -  Total Loss: 10.1068     Task Loss: 10.1068  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ Validation - eval_iter:20    sum_task_loss: 69.3143   task_loss_avg: 3.4657\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.3496   task_loss_avg: 3.4175\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.5352   task_loss_avg: 3.4268\n",
      " ++ Validation - eval_iter:20    loss_sum : 206.1991  loss_sum_avg:10.3100 \n",
      "** 2021-12-24 11:32:57:455293 - END VALIDATION iteration:  300  validation loss: 10.3100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ Validation - eval_iter:20    sum_task_loss: 69.2379   task_loss_avg: 3.4619\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.3216   task_loss_avg: 3.4161\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.5635   task_loss_avg: 3.4282\n",
      " ++ Validation - eval_iter:20    loss_sum : 206.1230  loss_sum_avg:10.3061 \n",
      "** 2021-12-24 11:33:20:350911 - END VALIDATION iteration:  350  validation loss: 10.3061\n",
      "Iteration  400 -  Total Loss: 9.5312     Task Loss: 9.5312  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ Validation - eval_iter:20    sum_task_loss: 69.2979   task_loss_avg: 3.4649\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.0641   task_loss_avg: 3.4032\n",
      " ++ Validation - eval_iter:20    sum_task_loss: 68.2750   task_loss_avg: 3.4137\n",
      " ++ Validation - eval_iter:20    loss_sum : 205.6369  loss_sum_avg:10.2818 \n",
      "** 2021-12-24 11:33:43:052799 - END VALIDATION iteration:  400  validation loss: 10.2818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##---------------------------------------------------------------     \n",
    "## part one: warm up\n",
    "##--------------------------------------------------------------- \n",
    "stop_iter  = current_iter + opt['train']['warm_up_iters']\n",
    "print(f\" Current_iter: {current_iter}  warm-up iterations:{opt['train']['warm_up_iters']} - Run  from {current_iter+1} to {stop_iter+1}\")\n",
    "\n",
    "with trange(current_iter+1, stop_iter+1 , initial = current_iter, total = stop_iter, position=0, leave= True, desc=\"training\") as t_warmup :\n",
    "    \n",
    "    for current_iter in t_warmup:\n",
    "        start_time = time.time()\n",
    "        environ.train()\n",
    "        batch_idx += 1\n",
    "        batch = next(train_loader)    \n",
    "#         print_heading(f\" {timestring()} - WARMUP Training iter {current_iter}/{opt['train']['warm_up_iters']}    batch_idx: {batch_idx}\"    \n",
    "#                       f\"    Warm-up iters: {opt['train']['warm_up_iters']}\"\n",
    "#                       f\"    Validation freq:  {opt['train']['val_freq']}\", verbose = False)\n",
    "\n",
    "        environ.set_inputs(batch,train_loader.dataset.input_size)\n",
    "\n",
    "        environ.optimize(opt['lambdas'], is_policy=False, flag='update_w', verbose = False)\n",
    "        \n",
    "        t_warmup.set_postfix({'curr_iter':current_iter, \n",
    "                              'bch_idx': batch_idx, \n",
    "                              'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                              'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "\n",
    "        if should(current_iter, opt['train']['print_freq']):\n",
    "            environ.print_loss(current_iter, start_time, verbose = True)\n",
    "\n",
    "#         print(f\"**  {timestring()}  iteration: {current_iter}  Complete - Loss: {environ.losses['total']['total']:.4f}\" )\n",
    "\n",
    "        ##--------------------------------------------------------------- \n",
    "        # validation\n",
    "        ##--------------------------------------------------------------- \n",
    "        if should(current_iter, opt['train']['val_freq']):\n",
    "            print_dbg(f\"**  {timestring()}  START VALIDATION iteration: {current_iter}    Validation freq {opt['train']['val_freq']}\") \n",
    "\n",
    "            num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "            val_metrics = eval_dev(environ, \n",
    "                                  val_loader, \n",
    "                                  opt['tasks'], \n",
    "                                  policy=False, \n",
    "                                  num_train_layers=None,\n",
    "                                  eval_iter = 20)\n",
    "\n",
    "            environ.print_metrics(current_iter, start_time, val_metrics, title='validation')\n",
    "            environ.save_checkpoint('warmup', current_iter)\n",
    "            \n",
    "            print_dbg(f\"** {timestring()} - END VALIDATION iteration:  {current_iter}  validation loss: {val_metrics['loss']['total']:.4f}\", verbose = True)                \n",
    "print()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29943dd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T18:41:40.504504Z",
     "start_time": "2021-12-24T18:41:40.125683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task1', 'task2', 'task3'])\n",
      "dict_keys(['task1', 'task2', 'task3'])\n",
      "{   'task1': {'total': tensor(3.2914, device='cuda:0', dtype=torch.float64)},\n",
      "    'task2': {'total': tensor(3.2416, device='cuda:0', dtype=torch.float64)},\n",
      "    'task3': {'total': tensor(3.3270, device='cuda:0', dtype=torch.float64)}}\n",
      "tensor(3.2914, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(environ.metrics.keys())\n",
    "# pp.pprint(environ.metrics['task1'])\n",
    "\n",
    "print(environ.losses.keys())\n",
    "\n",
    "pp.pprint(environ.losses)\n",
    "pp.pprint(environ.losses['task1']['total'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2da239bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T18:41:43.514528Z",
     "start_time": "2021-12-24T18:41:43.483091Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'task1', 'task2', 'task3'])\n",
      "{'total': 10.204687159220782}\n",
      "array(3.29143871)\n",
      "array(3.2415543)\n",
      "array(3.32701487)\n"
     ]
    }
   ],
   "source": [
    "print(val_metrics.keys())\n",
    "pp.pprint(val_metrics['loss'])\n",
    "pp.pprint(val_metrics['task1']['classification_agg']['err'].cpu().numpy())\n",
    "pp.pprint(val_metrics['task2']['classification_agg']['err'].cpu().numpy())\n",
    "pp.pprint(val_metrics['task3']['classification_agg']['err'].cpu().numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbf465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T06:50:48.719499Z",
     "start_time": "2021-12-24T06:50:48.693505Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.print_loss(current_iter, start_time, metrics = val_metrics['loss'], verbose=True)\n",
    "# print(opt['lambdas'])\n",
    "# p = (opt['lambdas'][0] * environ.losses['tasks']['task1'])\n",
    "# print(p)\n",
    "\n",
    "# environ.print_metrics(current_iter, start_time, val_metrics , title='validation', verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "443db2fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T21:22:18.633467Z",
     "start_time": "2021-12-24T21:22:18.603277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'alphas': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      "),\n",
      "    'weights': SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      ")}\n",
      "<torch.optim.lr_scheduler.StepLR object at 0x7fc0b4037910>\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(environ.optimizers)\n",
    "pp.pprint(environ.schedulers['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea8b8473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:33:58.267226Z",
     "start_time": "2021-12-24T19:33:58.216610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      " ** 2021-12-24 11:33:58:258425 - Training current iteration 400  flag: update_w \n",
      "----------------------------------------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------\n",
      " ** Set optimizer and scheduler to policy_learning = True\n",
      "------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_heading(f\"** {timestring()} - Training current iteration {current_iter}  flag: {flag} \", verbose = True)    \n",
    "\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0 \n",
    "batch_idx_a = 0 \n",
    "batch_idx_w = 0 \n",
    "\n",
    "\n",
    "if flag_warmup:\n",
    "    print_heading(f\"** Set optimizer and scheduler to policy_learning = True\", verbose = True)\n",
    "    environ.define_optimizer(policy_learning=True)\n",
    "    environ.define_scheduler(policy_learning=True)\n",
    "    flag_warmup = False\n",
    "\n",
    "if current_iter == opt['train']['warm_up_iters']:\n",
    "    print_heading(f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "                  f\"   Take checkpoint and block gradient flow through Policy net\", verbose=True)\n",
    "    environ.save_checkpoint('warmup', current_iter)\n",
    "    environ.fix_alpha()\n",
    "    \n",
    "# batch_enumerator1 = enumerate(train1_loader,1)  \n",
    "# batch_enumerator2 = enumerate(train2_loader,1)  \n",
    "\n",
    "train_total_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcbfcbc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T20:00:10.086985Z",
     "start_time": "2021-12-24T20:00:10.027879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt['train']['print_freq']         100\n",
      "opt['train']['hard_sampling']      False\n",
      "opt['policy']                      True\n",
      "opt['tasks']                       ['class', 'class', 'class']\n",
      "weight_iter_alternate:             87\n",
      "alpha_iter_alternate :             87\n",
      "current_iter                       4750\n",
      "current_iter_w                     87\n",
      "current_iter_a                     87\n",
      "batch_idx_w                        0\n",
      "flag                               update_w\n",
      "curr_epochs                        0\n",
      "train_total_epochs                 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"opt['train']['print_freq']         {opt['train']['print_freq']}\")\n",
    "print(f\"opt['train']['hard_sampling']      {opt['train']['hard_sampling']}\")\n",
    "print(f\"opt['policy']                      {opt['policy']}\")\n",
    "print(f\"opt['tasks']                       {opt['tasks']}\")\n",
    "print(f\"weight_iter_alternate:             {opt['train']['weight_iter_alternate']}\")\n",
    "print(f\"alpha_iter_alternate :             {opt['train']['alpha_iter_alternate']}\")\n",
    "print(f\"current_iter                       {current_iter  }\")\n",
    "print(f\"current_iter_w                     {current_iter_w}\")\n",
    "print(f\"current_iter_a                     {current_iter_a}\")\n",
    "print(f\"batch_idx_w                        {batch_idx_w}\")\n",
    "print(f\"flag                               {flag          }\")\n",
    "curr_epoch = 0\n",
    "train_total_epochs = 30\n",
    "print(f\"curr_epochs                        {curr_epoch}\") \n",
    "print(f\"train_total_epochs                 {train_total_epochs}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67347dd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T20:24:53.501659Z",
     "start_time": "2021-12-24T20:00:11.127259Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d50cb886ba4ca4ac2ac6e14b1d2afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Alternate Weight/Policy training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:1 iteration:  4800 -  Total Loss: 10.3443     Task Loss: 10.3443  \n",
      "Weight training epoch:1 iteration:  4837 -  Total Loss: 10.4821     Task Loss: 10.4821  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:1 iteration:  4900 -  Total Loss: 10.4637     Task Loss: 10.3726  Policy Losses:  Sparsity: 0.0910      Sharing: 5.99474e-05 \n",
      "Policy training epoch:1 iteration:  4924 -  Total Loss: 10.2198     Task Loss: 10.1289  Policy Losses:  Sparsity: 0.0908      Sharing: 8.92505e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:2 iteration:  5000 -  Total Loss: 10.3604     Task Loss: 10.3604  \n",
      "Weight training epoch:2 iteration:  5011 -  Total Loss: 10.3584     Task Loss: 10.3584  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:2 iteration:  5098 -  Total Loss: 10.2184     Task Loss: 10.1280  Policy Losses:  Sparsity: 0.0903      Sharing: 6.05732e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:3 iteration:  5100 -  Total Loss: 10.3918     Task Loss: 10.3918  \n",
      "Weight training epoch:3 iteration:  5185 -  Total Loss: 10.4739     Task Loss: 10.4739  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:3 iteration:  5200 -  Total Loss: 10.4660     Task Loss: 10.3757  Policy Losses:  Sparsity: 0.0902      Sharing: 8.31410e-05 \n",
      "Policy training epoch:3 iteration:  5272 -  Total Loss: 10.2202     Task Loss: 10.1303  Policy Losses:  Sparsity: 0.0898      Sharing: 4.99636e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:4 iteration:  5300 -  Total Loss: 10.2392     Task Loss: 10.2392  \n",
      "Weight training epoch:4 iteration:  5359 -  Total Loss: 10.4066     Task Loss: 10.4066  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:4 iteration:  5400 -  Total Loss: 10.4815     Task Loss: 10.3918  Policy Losses:  Sparsity: 0.0896      Sharing: 3.52263e-05 \n",
      "Policy training epoch:4 iteration:  5446 -  Total Loss: 10.1920     Task Loss: 10.1026  Policy Losses:  Sparsity: 0.0893      Sharing: 8.32677e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:5 iteration:  5500 -  Total Loss: 10.3851     Task Loss: 10.3851  \n",
      "Weight training epoch:5 iteration:  5533 -  Total Loss: 10.3619     Task Loss: 10.3619  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:5 iteration:  5600 -  Total Loss: 10.3649     Task Loss: 10.2759  Policy Losses:  Sparsity: 0.0890      Sharing: 2.87369e-05 \n",
      "Policy training epoch:5 iteration:  5620 -  Total Loss: 10.1988     Task Loss: 10.1099  Policy Losses:  Sparsity: 0.0889      Sharing: 3.63886e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:6 iteration:  5700 -  Total Loss: 10.3132     Task Loss: 10.3132  \n",
      "Weight training epoch:6 iteration:  5707 -  Total Loss: 10.4056     Task Loss: 10.4056  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:6 iteration:  5794 -  Total Loss: 10.2053     Task Loss: 10.1168  Policy Losses:  Sparsity: 0.0884      Sharing: 4.96805e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:7 iteration:  5800 -  Total Loss: 10.3344     Task Loss: 10.3344  \n",
      "Weight training epoch:7 iteration:  5881 -  Total Loss: 10.4343     Task Loss: 10.4343  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:7 iteration:  5900 -  Total Loss: 10.4905     Task Loss: 10.4022  Policy Losses:  Sparsity: 0.0883      Sharing: 4.77433e-05 \n",
      "Policy training epoch:7 iteration:  5968 -  Total Loss: 10.2470     Task Loss: 10.1591  Policy Losses:  Sparsity: 0.0879      Sharing: 4.71324e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:8 iteration:  6000 -  Total Loss: 10.4128     Task Loss: 10.4128  \n",
      "Weight training epoch:8 iteration:  6055 -  Total Loss: 10.4556     Task Loss: 10.4556  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:8 iteration:  6100 -  Total Loss: 10.4868     Task Loss: 10.3991  Policy Losses:  Sparsity: 0.0876      Sharing: 5.10141e-05 \n",
      "Policy training epoch:8 iteration:  6142 -  Total Loss: 10.1770     Task Loss: 10.0895  Policy Losses:  Sparsity: 0.0874      Sharing: 5.91502e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:9 iteration:  6200 -  Total Loss: 10.3505     Task Loss: 10.3505  \n",
      "Weight training epoch:9 iteration:  6229 -  Total Loss: 10.4509     Task Loss: 10.4509  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:9 iteration:  6300 -  Total Loss: 10.4771     Task Loss: 10.3900  Policy Losses:  Sparsity: 0.0870      Sharing: 9.53376e-05 \n",
      "Policy training epoch:9 iteration:  6316 -  Total Loss: 10.1940     Task Loss: 10.1070  Policy Losses:  Sparsity: 0.0870      Sharing: 5.18635e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:10 iteration:  6400 -  Total Loss: 10.4157     Task Loss: 10.4157  \n",
      "Weight training epoch:10 iteration:  6403 -  Total Loss: 10.4430     Task Loss: 10.4430  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:10 iteration:  6490 -  Total Loss: 10.2282     Task Loss: 10.1417  Policy Losses:  Sparsity: 0.0865      Sharing: 4.17605e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:11 iteration:  6500 -  Total Loss: 10.4742     Task Loss: 10.4742  \n",
      "Weight training epoch:11 iteration:  6577 -  Total Loss: 10.4467     Task Loss: 10.4467  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:11 iteration:  6600 -  Total Loss: 10.4873     Task Loss: 10.4009  Policy Losses:  Sparsity: 0.0864      Sharing: 6.02156e-05 \n",
      "Policy training epoch:11 iteration:  6664 -  Total Loss: 10.2219     Task Loss: 10.1359  Policy Losses:  Sparsity: 0.0860      Sharing: 5.37261e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:12 iteration:  6700 -  Total Loss: 10.3150     Task Loss: 10.3150  \n",
      "Weight training epoch:12 iteration:  6751 -  Total Loss: 10.4088     Task Loss: 10.4088  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:12 iteration:  6800 -  Total Loss: 10.4325     Task Loss: 10.3466  Policy Losses:  Sparsity: 0.0858      Sharing: 8.94219e-05 \n",
      "Policy training epoch:12 iteration:  6838 -  Total Loss: 10.1496     Task Loss: 10.0639  Policy Losses:  Sparsity: 0.0856      Sharing: 4.69163e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:13 iteration:  6900 -  Total Loss: 10.3839     Task Loss: 10.3839  \n",
      "Weight training epoch:13 iteration:  6925 -  Total Loss: 10.3225     Task Loss: 10.3225  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:13 iteration:  7000 -  Total Loss: 10.4876     Task Loss: 10.4023  Policy Losses:  Sparsity: 0.0852      Sharing: 5.40465e-05 \n",
      "Policy training epoch:13 iteration:  7012 -  Total Loss: 10.1795     Task Loss: 10.0943  Policy Losses:  Sparsity: 0.0851      Sharing: 5.45904e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:14 iteration:  7099 -  Total Loss: 10.4584     Task Loss: 10.4584  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:14 iteration:  7100 -  Total Loss: 10.5757     Task Loss: 10.4905  Policy Losses:  Sparsity: 0.0851      Sharing: 6.83367e-05 \n",
      "Policy training epoch:14 iteration:  7186 -  Total Loss: 10.1511     Task Loss: 10.0664  Policy Losses:  Sparsity: 0.0847      Sharing: 5.76228e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:15 iteration:  7200 -  Total Loss: 10.4350     Task Loss: 10.4350  \n",
      "Weight training epoch:15 iteration:  7273 -  Total Loss: 10.3049     Task Loss: 10.3049  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:15 iteration:  7300 -  Total Loss: 10.4687     Task Loss: 10.3841  Policy Losses:  Sparsity: 0.0846      Sharing: 6.34938e-05 \n",
      "Policy training epoch:15 iteration:  7360 -  Total Loss: 10.2308     Task Loss: 10.1465  Policy Losses:  Sparsity: 0.0842      Sharing: 8.51154e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:16 iteration:  7400 -  Total Loss: 10.3178     Task Loss: 10.3178  \n",
      "Weight training epoch:16 iteration:  7447 -  Total Loss: 10.4329     Task Loss: 10.4329  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:16 iteration:  7500 -  Total Loss: 10.5069     Task Loss: 10.4229  Policy Losses:  Sparsity: 0.0840      Sharing: 2.59131e-05 \n",
      "Policy training epoch:16 iteration:  7534 -  Total Loss: 10.1672     Task Loss: 10.0833  Policy Losses:  Sparsity: 0.0838      Sharing: 8.43182e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:17 iteration:  7600 -  Total Loss: 10.3373     Task Loss: 10.3373  \n",
      "Weight training epoch:17 iteration:  7621 -  Total Loss: 10.4282     Task Loss: 10.4282  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:17 iteration:  7700 -  Total Loss: 10.4995     Task Loss: 10.4160  Policy Losses:  Sparsity: 0.0834      Sharing: 4.99710e-05 \n",
      "Policy training epoch:17 iteration:  7708 -  Total Loss: 10.1474     Task Loss: 10.0640  Policy Losses:  Sparsity: 0.0834      Sharing: 3.43844e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:18 iteration:  7795 -  Total Loss: 10.4948     Task Loss: 10.4948  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:18 iteration:  7800 -  Total Loss: 10.5333     Task Loss: 10.4499  Policy Losses:  Sparsity: 0.0833      Sharing: 4.71473e-05 \n",
      "Policy training epoch:18 iteration:  7882 -  Total Loss: 10.1764     Task Loss: 10.0934  Policy Losses:  Sparsity: 0.0829      Sharing: 4.29526e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:19 iteration:  7900 -  Total Loss: 10.4252     Task Loss: 10.4252  \n",
      "Weight training epoch:19 iteration:  7969 -  Total Loss: 10.4391     Task Loss: 10.4391  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:19 iteration:  8000 -  Total Loss: 10.5044     Task Loss: 10.4215  Policy Losses:  Sparsity: 0.0828      Sharing: 5.36218e-05 \n",
      "Policy training epoch:19 iteration:  8056 -  Total Loss: 10.2169     Task Loss: 10.1343  Policy Losses:  Sparsity: 0.0825      Sharing: 4.26099e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:20 iteration:  8100 -  Total Loss: 10.3818     Task Loss: 10.3818  \n",
      "Weight training epoch:20 iteration:  8143 -  Total Loss: 10.4374     Task Loss: 10.4374  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:20 iteration:  8200 -  Total Loss: 10.4586     Task Loss: 10.3763  Policy Losses:  Sparsity: 0.0822      Sharing: 6.64741e-05 \n",
      "Policy training epoch:20 iteration:  8230 -  Total Loss: 10.1490     Task Loss: 10.0668  Policy Losses:  Sparsity: 0.0821      Sharing: 6.95288e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:21 iteration:  8300 -  Total Loss: 10.3546     Task Loss: 10.3546  \n",
      "Weight training epoch:21 iteration:  8317 -  Total Loss: 10.3423     Task Loss: 10.3423  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:21 iteration:  8400 -  Total Loss: 10.3516     Task Loss: 10.2698  Policy Losses:  Sparsity: 0.0817      Sharing: 8.78572e-05 \n",
      "Policy training epoch:21 iteration:  8404 -  Total Loss: 10.2236     Task Loss: 10.1418  Policy Losses:  Sparsity: 0.0817      Sharing: 9.13367e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:22 iteration:  8491 -  Total Loss: 10.3814     Task Loss: 10.3814  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:22 iteration:  8500 -  Total Loss: 10.4398     Task Loss: 10.3581  Policy Losses:  Sparsity: 0.0816      Sharing: 7.10785e-05 \n",
      "Policy training epoch:22 iteration:  8578 -  Total Loss: 10.1638     Task Loss: 10.0825  Policy Losses:  Sparsity: 0.0813      Sharing: 4.48525e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:23 iteration:  8600 -  Total Loss: 10.3301     Task Loss: 10.3301  \n",
      "Weight training epoch:23 iteration:  8665 -  Total Loss: 10.4598     Task Loss: 10.4598  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:23 iteration:  8700 -  Total Loss: 10.4922     Task Loss: 10.4110  Policy Losses:  Sparsity: 0.0811      Sharing: 4.86225e-05 \n",
      "Policy training epoch:23 iteration:  8752 -  Total Loss: 10.1523     Task Loss: 10.0714  Policy Losses:  Sparsity: 0.0809      Sharing: 5.57154e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:24 iteration:  8800 -  Total Loss: 10.3752     Task Loss: 10.3752  \n",
      "Weight training epoch:24 iteration:  8839 -  Total Loss: 10.4587     Task Loss: 10.4587  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:24 iteration:  8900 -  Total Loss: 10.5191     Task Loss: 10.4384  Policy Losses:  Sparsity: 0.0806      Sharing: 4.79445e-05 \n",
      "Policy training epoch:24 iteration:  8926 -  Total Loss: 10.1637     Task Loss: 10.0832  Policy Losses:  Sparsity: 0.0805      Sharing: 3.72082e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:25 iteration:  9000 -  Total Loss: 10.3947     Task Loss: 10.3947  \n",
      "Weight training epoch:25 iteration:  9013 -  Total Loss: 10.4423     Task Loss: 10.4423  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:25 iteration:  9100 -  Total Loss: 10.0917     Task Loss: 10.0115  Policy Losses:  Sparsity: 0.0801      Sharing: 8.66130e-05 \n",
      "Policy training epoch:25 iteration:  9100 -  Total Loss: 10.0917     Task Loss: 10.0115  Policy Losses:  Sparsity: 0.0801      Sharing: 8.66130e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:26 iteration:  9187 -  Total Loss: 10.3982     Task Loss: 10.3982  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:26 iteration:  9200 -  Total Loss: 10.5662     Task Loss: 10.4861  Policy Losses:  Sparsity: 0.0800      Sharing: 5.70118e-05 \n",
      "Policy training epoch:26 iteration:  9274 -  Total Loss: 10.1781     Task Loss: 10.0983  Policy Losses:  Sparsity: 0.0797      Sharing: 7.51391e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:27 iteration:  9300 -  Total Loss: 10.3665     Task Loss: 10.3665  \n",
      "Weight training epoch:27 iteration:  9361 -  Total Loss: 10.4524     Task Loss: 10.4524  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:27 iteration:  9400 -  Total Loss: 10.5266     Task Loss: 10.4470  Policy Losses:  Sparsity: 0.0795      Sharing: 5.79357e-05 \n",
      "Policy training epoch:27 iteration:  9448 -  Total Loss: 10.1543     Task Loss: 10.0749  Policy Losses:  Sparsity: 0.0793      Sharing: 6.80909e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:28 iteration:  9500 -  Total Loss: 10.4233     Task Loss: 10.4233  \n",
      "Weight training epoch:28 iteration:  9535 -  Total Loss: 10.5062     Task Loss: 10.5062  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:28 iteration:  9600 -  Total Loss: 10.3582     Task Loss: 10.2792  Policy Losses:  Sparsity: 0.0790      Sharing: 3.50550e-05 \n",
      "Policy training epoch:28 iteration:  9622 -  Total Loss: 10.1589     Task Loss: 10.0799  Policy Losses:  Sparsity: 0.0789      Sharing: 5.70640e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:29 iteration:  9700 -  Total Loss: 10.2570     Task Loss: 10.2570  \n",
      "Weight training epoch:29 iteration:  9709 -  Total Loss: 10.4888     Task Loss: 10.4888  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:29 iteration:  9796 -  Total Loss: 10.1821     Task Loss: 10.1035  Policy Losses:  Sparsity: 0.0785      Sharing: 7.33137e-05 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30 weight training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight training epoch:30 iteration:  9800 -  Total Loss: 10.4237     Task Loss: 10.4237  \n",
      "Weight training epoch:30 iteration:  9883 -  Total Loss: 10.5041     Task Loss: 10.5041  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30 policy training:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy training epoch:30 iteration:  9900 -  Total Loss: 10.4802     Task Loss: 10.4017  Policy Losses:  Sparsity: 0.0785      Sharing: 4.76837e-05 \n",
      "Policy training epoch:30 iteration:  9970 -  Total Loss: 10.1590     Task Loss: 10.0808  Policy Losses:  Sparsity: 0.0781      Sharing: 6.90147e-05 \n"
     ]
    }
   ],
   "source": [
    "curr_epoch = 0\n",
    "main_iter_ctr = 0 \n",
    "verbose = False\n",
    "t = tqdm(total=train_total_epochs, desc=f\" Alternate Weight/Policy training\")\n",
    "\n",
    "while curr_epoch < train_total_epochs:\n",
    "    curr_epoch+=1\n",
    "    t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the network weights\n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_w':\n",
    "        current_iter_w  = 0 \n",
    "        stop_iter_w =   opt['train']['weight_iter_alternate']\n",
    "\n",
    "        with trange(+1, stop_iter_w+1 , initial = current_iter_w, total = stop_iter_w, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} weight training\") as t_weights :\n",
    "            for current_iter_w in t_weights:    \n",
    "                current_iter += 1\n",
    "\n",
    "                start_time = time.time()\n",
    "                environ.train()\n",
    "                \n",
    "                batch = next(train1_loader)\n",
    "                environ.set_inputs(batch, train1_loader.dataset.input_size)\n",
    "\n",
    "                ##----------------------------------------------------------------------\n",
    "                ## Set number of layers to train based on cirriculum_speed \n",
    "                ## and p_epoch (number of epochs of policy training)\n",
    "                ## When curriculum_speed == 3, a num_train_layers is incremented \n",
    "                ## after completion of every 3 policy training epochs\n",
    "                ##----------------------------------------------------------------------\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "\n",
    "                t_weights.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                       'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = f\"Weight training epoch:{curr_epoch} iteration:\", verbose = True)\n",
    "                    environ.resize_results()\n",
    "\n",
    "        #-------------------------------------------------------\n",
    "        # validation process\n",
    "        #------------------------------------------------------- \n",
    "#       if should(current_iter_w, opt['train']['weight_iter_alternate']): \n",
    "        if (current_iter_w >= stop_iter_w):\n",
    "            environ.print_loss(current_iter, start_time, title = f\"Weight training epoch:{curr_epoch} iteration:\", verbose = True)\n",
    "            environ.eval()\n",
    "\n",
    "            val_metrics = eval_dev(environ, \n",
    "                                  val_loader, \n",
    "                                  opt['tasks'], \n",
    "                                  policy=opt['policy'],\n",
    "                                  num_train_layers=num_train_layers, \n",
    "                                  hard_sampling=opt['train']['hard_sampling'],\n",
    "                                  eval_iter = -1)        \n",
    "\n",
    "            if (verbose):\n",
    "                for t_id, task in enumerate(environ.tasks):\n",
    "                    task_key = f\"task{t_id+1}\"    \n",
    "                    environ.print_metrics(current_iter, start_time, val_metrics[task_key][\"classification_agg\"], title='validation', verbose = verbose)        \n",
    "\n",
    "            environ.save_checkpoint('latest', current_iter)\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # if number of iterations completed after the warm up phase is greater than the number of \n",
    "            # (weight/policy alternations) x (cirriculum speed) x (number of layers to be policy trained)\n",
    "            #\n",
    "            # check metrics for improvement, and issue a checkpoint if necessary\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "\n",
    "            if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "                    (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "                new_value = 0\n",
    "#                 print(f\"  {current_iter - opt['train']['warm_up_iters']} IS GREATER THAN \"\n",
    "#                        f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} -- \"\n",
    "#                        f\"  evaluate progress and make checkpoint if necessary.\" )  \n",
    "                \n",
    "                ##------------------------------------------------------\n",
    "                ## compare validation metrics against reference metrics.\n",
    "                ##------------------------------------------------------\n",
    "#                 for k in refer_metrics.keys():\n",
    "#                     if k in val_metrics.keys():\n",
    "#                         for kk in val_metrics[k].keys():\n",
    "#                             if not kk in refer_metrics[k].keys():\n",
    "#                                 continue\n",
    "#                             if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or (\n",
    "#                                     k == 'depth' and not kk.startswith('sigma')) or (kk == 'err'):\n",
    "#                                 value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "#                             else:\n",
    "#                                 value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "#                             value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "#                             new_value += value\n",
    "\n",
    "#                 print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                # if results have improved, save these results and issue a checkpoint\n",
    "\n",
    "#                 if (new_value > best_value):\n",
    "#                     print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)\n",
    "#                     best_value = new_value\n",
    "#                     best_metrics = val_metrics\n",
    "#                     best_iter = current_iter\n",
    "#                     environ.save_checkpoint('best', current_iter)\n",
    "#                     print('New      best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)                         \n",
    "#                     print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                # if results have improved, save these results and issue a checkpoint   \n",
    "\n",
    "            environ.train()\n",
    "            #-------------------------------------------------------\n",
    "            # END validation process\n",
    "            #-------------------------------------------------------       \n",
    "#             print_heading(f\"{timestring()} - SWITCH TO ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "#               f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#               f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']}\",\n",
    "#               verbose = False)       \n",
    "            flag = 'update_alpha'\n",
    "            environ.fix_w()\n",
    "            environ.free_alpha()\n",
    "        #-------------------------------------------------------\n",
    "        # end validation process\n",
    "        #-------------------------------------------------------               \n",
    "\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the  policy \n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_alpha':\n",
    "        current_iter_a = 0\n",
    "        stop_iter_a = opt['train']['alpha_iter_alternate']\n",
    "\n",
    "        with trange( +1, stop_iter_a+1 , initial = 0, total = stop_iter_a, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} policy training\") as t_policy :\n",
    "            for current_iter_a in t_policy:    \n",
    "                current_iter += 1\n",
    "\n",
    "                batch = next(train2_loader)\n",
    "                environ.set_inputs(batch, train2_loader.dataset.input_size)\n",
    "\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = (p_epoch // opt['curriculum_speed']) + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "                print_dbg(f\" num_train_layers  : {num_train_layers}\", verbose = False)\n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "                \n",
    "                t_policy.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                      'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = f\"Policy training epoch:{curr_epoch} iteration:\", verbose=True)\n",
    "                    environ.resize_results()\n",
    "                    # environ.visual_policy(current_iter)\n",
    "\n",
    "        if( current_iter_a >= stop_iter_a):            \n",
    "            environ.print_loss(current_iter, start_time, title = f\"Policy training epoch:{curr_epoch} iteration:\", verbose=True)\n",
    "            flag = 'update_w'\n",
    "            environ.fix_alpha()\n",
    "            environ.free_w(opt['fix_BN'])\n",
    "            environ.decay_temperature()\n",
    "\n",
    "            # print the distribution\n",
    "            print_dbg(np.concatenate(environ.get_policy_prob(), axis=-1), verbose = False)\n",
    "            \n",
    "            p_epoch += 1\n",
    "            print_dbg(f\"** p_epoch incremented: {p_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b84a2b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-24T19:24:59.553721Z",
     "start_time": "2021-12-24T19:24:59.521670Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_train_layers = 2\n",
    "# print(num_train_layers)\n",
    "\n",
    "# logits = torch.rand((16,2)) \n",
    "# print(logits)\n",
    "\n",
    "# if num_policy_layers is None:\n",
    "#     num_policy_layers = logits.shape[0]\n",
    "# else:\n",
    "#     assert (num_policy_layers == logits.shape[0])\n",
    "# print(num_policy_layers)\n",
    "\n",
    "# num_blocks = min(num_train_layers, logits.shape[0])\n",
    "# print(num_blocks)\n",
    "\n",
    "# gt = torch.ones((num_blocks)).long() \n",
    "# print('gt: ', gt)\n",
    "# gt_z = torch.zeros((num_blocks)).long() \n",
    "# print('gt: ', gt_z)\n",
    "\n",
    "# loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "# print('loss_weights: ', loss_weights)\n",
    "\n",
    "# t1 = loss_weights[-num_blocks:] \n",
    "# print(t1)\n",
    "\n",
    "# t2 = logits[-num_blocks:]\n",
    "# print(t2)\n",
    "\n",
    "# ce = environ.cross_entropy_sparsity(logits[-num_blocks:], gt)\n",
    "# print(ce)\n",
    "\n",
    "# ce = environ.cross_entropy_sparsity(logits[-num_blocks:], gt_z)\n",
    "# print(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240104e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Warm-up:  validation - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3eaa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:41:10.772791Z",
     "start_time": "2021-12-15T20:41:05.690320Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# validation\n",
    "if should(current_iter, opt['train']['val_freq']):\n",
    "    print(f\"**  {timestring()}  START VALIDATION iteration: {current_iter} \")    \n",
    "\n",
    "    environ.eval()     # set to evaluation mode (train = False)\n",
    "    num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "    val_metrics = eval_dev(environ, \n",
    "                          val_loader, \n",
    "                          opt['tasks'], \n",
    "                          policy=False, \n",
    "                          num_train_layers=None, \n",
    "                          eval_iter = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac77c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:41:11.489480Z",
     "start_time": "2021-12-15T20:41:11.461278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_metrics.keys()\n",
    "val_metrics['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad14d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:41:55.074366Z",
     "start_time": "2021-12-15T20:41:55.029329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in val_metrics:\n",
    "    print(f'\\n {i} \\n -----------------')\n",
    "    print(val_metrics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d951d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:37:44.423495Z",
     "start_time": "2021-12-15T20:37:44.400109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    for t_id, task in enumerate(environ.tasks):\n",
    "        task_key = f\"task{t_id+1}\"    \n",
    "        environ.print_loss(current_iter, start_time, val_metrics[task_key][\"classification_agg\"], title='validation')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5783f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:36:43.503203Z",
     "start_time": "2021-12-15T20:36:43.371900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    environ.save_checkpoint('latest', current_iter)\n",
    "\n",
    "    print(f\"** {timestring()} - END VALIDATION iteration:  {current_iter} \")                \n",
    "    environ.train()    # set to training mode (train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3074d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T22:26:36.573475Z",
     "start_time": "2021-12-12T22:26:36.455074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for i in val_metrics.keys():\n",
    "#     print(i, type(val_metrics[i]))\n",
    "#     for k in val_metrics[i].keys():\n",
    "#         print(i,k, type(val_metrics[i][k]))\n",
    "#         if isinstance(val_metrics[i][k], pd.core.series.Series):\n",
    "#             print(f\"val_metrics[{i}][{k}] is a series\")\n",
    "#         elif isinstance(val_metrics[i][k], pd.core.frame.DataFrame):\n",
    "#             print(f\"val_metrics[{i}][{k}] is a dataframe\")        \n",
    "\n",
    "# s = val_metrics['task1']['classification_agg']\n",
    "# print(s)\n",
    "# print(s.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047eff66",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4551c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:46:09.100795Z",
     "start_time": "2021-12-20T23:46:08.702295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_separator('READ YAML')\n",
    "opt, gpu_ids, exp_ids =read_yaml_from_input(input_args)\n",
    "print(gpu_ids, exp_ids,  opt['train']['policy_iter'])\n",
    "\n",
    "current_iter = environ.load_checkpoint('latest')\n",
    "\n",
    "print('Evaluating the snapshot saved at %d iter' % current_iter)\n",
    "\n",
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate'] = opt['train'].get('alpha_iter_alternate'  , len(train2_loader))\n",
    "\n",
    "print(opt['train']['weight_iter_alternate'], opt['train']['alpha_iter_alternate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea9c4a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Weight Training Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561e1a9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Weight training - prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454204e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:40:00.014577Z",
     "start_time": "2021-12-20T23:39:59.990525Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# arch_parms = environ.networks['mtl-net'].named_parameters()\n",
    "# print(arch_parms)\n",
    "# for name, parm in arch_parms:\n",
    "#     print(name, '    ',parm.requires_grad)\n",
    "# print_underline('MTL3_Dev Policys', verbose = True)\n",
    "# for i in   environ.networks['mtl-net'].policys:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ad431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:57:34.590953Z",
     "start_time": "2021-12-21T22:57:34.234886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_heading(f\"** {timestring()} - Training current iteration {current_iter}  flag: {flag} \", verbose = True)    \n",
    "\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0 \n",
    "batch_idx_a = 0 \n",
    "batch_idx_w = 0 \n",
    "\n",
    "if flag_warmup:\n",
    "    print_heading(f\"** Set optimizer and scheduler to policy_learning = True\", verbose = True)\n",
    "    environ.define_optimizer(policy_learning=True)\n",
    "    environ.define_scheduler(policy_learning=True)\n",
    "    flag_warmup = False\n",
    "\n",
    "if current_iter == opt['train']['warm_up_iters']:\n",
    "    print_heading(f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "                  f\"   Take checkpoint and block gradient flow through Policy net\", verbose=True)\n",
    "    environ.save_checkpoint('warmup', current_iter)\n",
    "    environ.fix_alpha()\n",
    "    \n",
    "# batch_enumerator1 = enumerate(train1_loader,1)  \n",
    "# batch_enumerator2 = enumerate(train2_loader,1)  \n",
    "\n",
    "train_total_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c0a04",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Weight training - main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc47806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:57:57.069633Z",
     "start_time": "2021-12-21T22:57:57.026843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"opt['train']['print_freq']         {opt['train']['print_freq']}\")\n",
    "print(f\"opt['train']['hard_sampling']      {opt['train']['hard_sampling']}\")\n",
    "print(f\"opt['policy']                      {opt['policy']}\")\n",
    "print(f\"opt['tasks']                       {opt['tasks']}\")\n",
    "print(f\"weight_iter_alternate:             {opt['train']['weight_iter_alternate']}\")\n",
    "print(f\"alpha_iter_alternate :             {opt['train']['alpha_iter_alternate']}\")\n",
    "print(f\"current_iter                       {current_iter  }\")\n",
    "print(f\"current_iter_w                     {current_iter_w}\")\n",
    "print(f\"current_iter_a                     {current_iter_a}\")\n",
    "print(f\"batch_idx_w                        {batch_idx_w}\")\n",
    "print(f\"flag                               {flag          }\")\n",
    "print(f\"train_total_epochs                 {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8a69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:41:10.189643Z",
     "start_time": "2021-12-21T22:41:10.164497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c31fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:58:10.493069Z",
     "start_time": "2021-12-21T22:58:10.469631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------     \n",
    "## Weight / Policy Training\n",
    "##--------------------------------------------------------------- \n",
    "# stop_iter = current_iter_w +  opt['train']['weight_iter_alternate']\n",
    "# print(f\" Current Weight iteration {current_iter_w} - Run  from {current_iter_w+1} to {stop_iter+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee9029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:58:12.219746Z",
     "start_time": "2021-12-21T22:58:12.193318Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f076d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:58:13.757096Z",
     "start_time": "2021-12-21T22:58:13.731942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with tnrange(start_iter_t , stop_iter_t  , initial = start_iter_t , total = stop_iter_t, position=0, leave= True, desc=\"master\") as t :\n",
    "# with tqdm_notebook(total=train_total_epochs) as t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d3730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T18:50:23.014384Z",
     "start_time": "2021-12-21T18:50:22.955398Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534cdee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T23:08:28.021068Z",
     "start_time": "2021-12-21T22:59:25.316087Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curr_epoch = 0\n",
    "main_iter_ctr = 0 \n",
    "verbose = False\n",
    "t = tqdm(total=train_total_epochs, desc=f\" Alternate Weight/Policy training\")\n",
    "\n",
    "while curr_epoch < train_total_epochs:\n",
    "    curr_epoch+=1\n",
    "    t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the network weights\n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_w':\n",
    "        current_iter_w  = 0 \n",
    "        stop_iter_w =   opt['train']['weight_iter_alternate']\n",
    "\n",
    "        with trange(+1, stop_iter_w+1 , initial = current_iter_w, total = stop_iter_w, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} weight training\") as t_weights :\n",
    "            for current_iter_w in t_weights:    \n",
    "                current_iter += 1\n",
    "\n",
    "                start_time = time.time()\n",
    "                environ.train()\n",
    "                \n",
    "#                 if batch_idx_w == len(train1_loader):\n",
    "#                     print_dbg(f\"  Reenumerate train1_loader -  index_w: {batch_idx_w}   len(train1_loader) = {len(train1_loader)} \", verbose)\n",
    "#                     batch_enumerator1 = enumerate(train1_loader,1)    \n",
    "                    \n",
    "                batch = next(train1_loader)\n",
    "                environ.set_inputs(batch, train1_loader.dataset.input_size)\n",
    "\n",
    "                ##----------------------------------------------------------------------\n",
    "                ## Set number of layers to train based on cirriculum_speed \n",
    "                ## and p_epoch (number of epochs of policy training)\n",
    "                ## When curriculum_speed == 3, a num_train_layers is incremented \n",
    "                ## after completion of every 3 policy training epochs\n",
    "                ##----------------------------------------------------------------------\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "\n",
    "#                 print_heading(f\"{timestring()} CALL ENVIRON.OPTIMIZE()    current_iter: {current_iter}     flag: {flag}\\n\"\n",
    "#                       f\"{' ':10s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                       f\"{' ':10s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \\n\"                          \n",
    "#                       f\"{' ':10s} is_policy: {opt['policy']}     p_epoch: {p_epoch}       num_train_layers: {num_train_layers}\", verbose = False) \n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "\n",
    "                t_weights.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                       'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = \"Weight training iteration\", verbose = True)\n",
    "                    environ.resize_results()\n",
    "\n",
    "#                 print_heading(f\"{timestring()} - CONTINUE WEIGHT TRAINING   current_iter: {current_iter}\\n\"\n",
    "#                   f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                   f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']}\",\n",
    "#                   verbose = False)        \n",
    "\n",
    "        #-------------------------------------------------------\n",
    "        # validation process\n",
    "        #------------------------------------------------------- \n",
    "\n",
    "#         if should(current_iter_w, opt['train']['weight_iter_alternate']): \n",
    "\n",
    "        if (current_iter_w >= stop_iter_w):\n",
    "            environ.eval()\n",
    "            print_dbg(\"++ Weight Training Validation  and then Switch to update_alpha\", verbose = False)\n",
    "\n",
    "            val_metrics = eval_dev(environ, \n",
    "                                  val_loader, \n",
    "                                  opt['tasks'], \n",
    "                                  policy=opt['policy'],\n",
    "                                  num_train_layers=num_train_layers, \n",
    "                                  hard_sampling=opt['train']['hard_sampling'],\n",
    "                                  eval_iter = -1)        \n",
    "\n",
    "            if (verbose):\n",
    "                for t_id, task in enumerate(environ.tasks):\n",
    "                    task_key = f\"task{t_id+1}\"    \n",
    "                    environ.print_metrics(current_iter, start_time, val_metrics[task_key][\"classification_agg\"], title='validation', verbose = verbose)        \n",
    "\n",
    "            environ.save_checkpoint('latest', current_iter)\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # if number of iterations completed after the warm up phase is greater than the number of \n",
    "            # (weight/policy alternations) x (cirriculum speed) x (number of layers to be policy trained)\n",
    "            #\n",
    "            # check metrics for improvement, and issue a checkpoint if necessary\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "            if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "                    (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "                new_value = 0\n",
    "#                 print_heading(f\"  evaluate progress and make checkpoint if necessary.\" , verbose = True)\n",
    "#                 print(f\" current iter                                 : {current_iter} \\n\"\n",
    "#                       f\" opt['train']['warm_up_iters']                : {opt['train']['warm_up_iters']} \\n\"\n",
    "#                       f\" num_blocks                                   : {num_blocks} \\n\"\n",
    "#                       f\" opt['curriculum_speed']                      : {opt['curriculum_speed']}\\n\"\n",
    "#                       f\" opt['train']['weight_iter_alternate']        : {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                       f\" opt['train']['alpha_iter_alternate']         : {opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#                       f\" alpha_iter_alternate + weight_iter_alternate : {opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#                       f\" num_blks * curriculum_speed * (alpha_alternate + weight_alternate): \"\n",
    "#                       f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} \\n\"\n",
    "\n",
    "                print(f\"  {current_iter - opt['train']['warm_up_iters']} IS GREATER THAN \"\n",
    "                       f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} -- \"\n",
    "                       f\"  evaluate progress and make checkpoint if necessary.\" )            \n",
    "#               ## compare validation metrics against reference metrics.\n",
    "\n",
    "#                 for k in refer_metrics.keys():\n",
    "#                     if k in val_metrics.keys():\n",
    "#                         for kk in val_metrics[k].keys():\n",
    "#                             if not kk in refer_metrics[k].keys():\n",
    "#                                 continue\n",
    "#                             if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or (\n",
    "#                                     k == 'depth' and not kk.startswith('sigma')) or (kk == 'err'):\n",
    "#                                 value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "#                             else:\n",
    "#                                 value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "#                             value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "#                             new_value += value\n",
    "\n",
    "#                 print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                # if results have improved, save these results and issue a checkpoint\n",
    "\n",
    "#                 if (new_value > best_value):\n",
    "#                     print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)\n",
    "#                     best_value = new_value\n",
    "#                     best_metrics = val_metrics\n",
    "#                     best_iter = current_iter\n",
    "#                     environ.save_checkpoint('best', current_iter)\n",
    "#                     print('New      best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)                         \n",
    "#                     print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                # if results have improved, save these results and issue a checkpoint   \n",
    "\n",
    "            environ.train()\n",
    "            #-------------------------------------------------------\n",
    "            # END validation process\n",
    "            #-------------------------------------------------------       \n",
    "            print_heading(f\"{timestring()} - SWITCH TO ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "              f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "              f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']}\",\n",
    "              verbose = False)       \n",
    "            flag = 'update_alpha'\n",
    "            environ.fix_w()\n",
    "            environ.free_alpha()\n",
    "        #-------------------------------------------------------\n",
    "        # end validation process\n",
    "        #-------------------------------------------------------               \n",
    "\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the  policy \n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_alpha':\n",
    "        current_iter_a = 0\n",
    "        stop_iter_a = opt['train']['alpha_iter_alternate']\n",
    "\n",
    "        with trange( +1, stop_iter_a+1 , initial = 0, total = stop_iter_a, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} policy training\") as t_policy :\n",
    "            for current_iter_a in t_policy:    \n",
    "                current_iter += 1\n",
    "\n",
    "#                 batch_idx_a, batch = next(batch_enumerator2)\n",
    "                batch = next(train2_loader)\n",
    "                environ.set_inputs(batch, train2_loader.dataset.input_size)\n",
    "\n",
    "#                 if batch_idx_a == len(train2_loader):\n",
    "#                     print_dbg(f\" Re-enumerate train2_loader  batch_idx_a: {batch_idx_a}   len(train2_loader) = {len(train2_loader)}\", verbose=False)                \n",
    "#                     batch_enumerator2 = enumerate(train2_loader,1)        \n",
    "\n",
    "#                 print_heading(f\"{timestring()} - ENVIRON.OPTIMIZE()    flag: {flag}    current_iter: {current_iter}   \\n\"\n",
    "#                               f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                               f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \\n\"\n",
    "#                               f\" is_policy: {opt['policy']}   num_train_layers: {num_train_layers}  hard_sampling: {opt['train']['hard_sampling']}\\n\"\n",
    "#                               f\" is_curriculum: {opt['is_curriculum']}     curriculum_speed: {opt['curriculum_speed']}   p_epoch: {p_epoch}\"\n",
    "#                               , verbose = False) \n",
    "\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = (p_epoch // opt['curriculum_speed']) + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "                print_dbg(f\" num_train_layers  : {num_train_layers}\", verbose = False)\n",
    "\n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "                \n",
    "                t_policy.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                      'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = \"Policy training iteration\", verbose=True)\n",
    "                    environ.resize_results()\n",
    "                    # environ.visual_policy(current_iter)\n",
    "\n",
    "#                 print_heading(f\"{timestring()} - CONTINUE ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "#                               f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                               f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \", \n",
    "#                               verbose = False )      \n",
    "\n",
    "        ## if (current_iter_a % alpha_iter_alternate) == 0 \n",
    "#         if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "#         print(f\" policy loop ended - current_iter_a: {current_iter_a}   stop_iter_a: {stop_iter_a}\")\n",
    "        if( current_iter_a >= stop_iter_a):            \n",
    "#             print_heading(f\"{timestring()} - SWITCH TO WEIGHT TRAINING  urrent_iter: {current_iter}\\n\"\n",
    "#                           f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                           f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \",\n",
    "#                           verbose = False )       \n",
    "\n",
    "            flag = 'update_w'\n",
    "            environ.fix_alpha()\n",
    "            environ.free_w(opt['fix_BN'])\n",
    "            environ.decay_temperature()\n",
    "\n",
    "            # print the distribution\n",
    "            print_dbg(np.concatenate(environ.get_policy_prob(), axis=-1), verbose = False)\n",
    "            \n",
    "            p_epoch += 1\n",
    "            print_dbg(f\"** p_epoch incremented: {p_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ae87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T23:20:09.766802Z",
     "start_time": "2021-12-21T23:20:09.716093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"{opt['train']['Lambda_sharing']:.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9f0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:26:17.162189Z",
     "start_time": "2021-12-21T21:26:16.777249Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value))\n",
    "print(best_metrics)\n",
    "best_value = new_value\n",
    "best_metrics = val_metrics\n",
    "best_iter = current_iter\n",
    "environ.save_checkpoint('best', current_iter)\n",
    "print('New best iter : %d, best_value: %.4f \\n' % (best_iter, best_value))                         \n",
    "print(best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f37b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:27:15.488947Z",
     "start_time": "2021-12-21T21:27:15.382938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.losses['tasks'] = {'total' : torch.tensor(0.0, device  = environ.device, dtype=torch.float64)}\n",
    "# environ.device\n",
    "\n",
    "# print(val_metrics)\n",
    "pp.pprint(environ.losses)\n",
    "# environ.print_loss_2(current_iter, start_time, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b334b1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Policy Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97645f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-18T06:36:24.639972Z",
     "start_time": "2021-12-18T06:36:24.620024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(f\" current iter                                 : {current_iter} \\n\"\n",
    "#       f\" opt['train']['warm_up_iters']                : {opt['train']['warm_up_iters']} \\n\"\n",
    "#       f\" num_blocks                                   : {num_blocks} \\n\"\n",
    "#       f\" opt['curriculum_speed']                      : {opt['curriculum_speed']}\\n\"\n",
    "#       f\" opt['train']['weight_iter_alternate']        : {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#       f\" opt['train']['alpha_iter_alternate']         : {opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#       f\" alpha_iter_alternate + weight_iter_alternate : {opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#       f\" num_blocks * curriculum_speed * (alpha_iter_alternate + weight_iter_alternate): \\\n",
    "#           {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} \\n\"\n",
    "#       f\" IF {current_iter - opt['train']['warm_up_iters']} IS GREATER THAN  ??\"\n",
    "#       f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ff6a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-18T06:36:36.364353Z",
     "start_time": "2021-12-18T06:36:36.341535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(f\" task1_logits: {environ.networks['mtl-net'].task1_logits} \\n\")\n",
    "# print(f\" task2_logits: {environ.networks['mtl-net'].task2_logits} \\n\")\n",
    "# print(f\" task3_logits: {environ.networks['mtl-net'].task3_logits} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dcee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T02:07:18.061296Z",
     "start_time": "2021-12-20T02:07:18.038742Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter_a , opt['train']['alpha_iter_alternate'],flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9dc0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T02:07:19.091639Z",
     "start_time": "2021-12-20T02:07:19.060936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------     \n",
    "## part one: warm up\n",
    "##--------------------------------------------------------------- \n",
    "# print(current_iter_a , opt['train']['alpha_iter_alternate'],flag)\n",
    "# stop_iter = current_iter_a +  opt['train']['alpha_iter_alternate']\n",
    "# print(f\" Run iteration {current_iter_a+1} to {stop_iter+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01179d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T02:16:10.956462Z",
     "start_time": "2021-12-20T02:16:10.915819Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter_a, stop_iter, flag)\n",
    "# print(current_iter_a , opt['train']['alpha_iter_alternate'],flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8deabd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if flag == 'update_alpha':\n",
    "\n",
    "    stop_iter = current_iter_a +  opt['train']['alpha_iter_alternate']\n",
    "    print(f\" Current Alpha iteration {current_iter_a} - Run  from {current_iter_a+1} to {stop_iter+1}\")\n",
    "    \n",
    "    with tnrange(current_iter_a+1, stop_iter+1 , initial = current_iter_a+1, total = stop_iter+1, position=0, leave= True, desc=\"weight training\") as t :\n",
    "        for current_iter_a in t:    \n",
    "            current_iter += 1\n",
    " \n",
    "            batch_idx_a, batch = next(batch_enumerator2)\n",
    "            environ.set_inputs(batch, train2_loader.dataset.input_size)\n",
    "\n",
    "            if batch_idx_a == len(train2_loader):\n",
    "                print_dbg(f\" Re-enumerate train2_loader  batch_idx_a: {batch_idx_a}   len(train2_loader) = {len(train2_loader)}\", verbose=False)                \n",
    "                batch_enumerator2 = enumerate(train2_loader,1)        \n",
    "                  \n",
    "            print_heading(f\"{timestring()} - ENVIRON.OPTIMIZE()    flag: {flag}    current_iter: {current_iter}   \\n\"\n",
    "                          f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                          f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \\n\"\n",
    "                          f\" is_policy: {opt['policy']}   num_train_layers: {num_train_layers}  hard_sampling: {opt['train']['hard_sampling']}\\n\"\n",
    "                          f\" is_curriculum: {opt['is_curriculum']}     curriculum_speed: {opt['curriculum_speed']}   p_epoch: {p_epoch}\"\n",
    "                          , verbose = False) \n",
    "    \n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = (p_epoch // opt['curriculum_speed']) + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "\n",
    "            print_dbg(f\" num_train_layers  : {num_train_layers}\", verbose = False)\n",
    "\n",
    "\n",
    "            environ.optimize(opt['lambdas'], \n",
    "                             is_policy=opt['policy'], \n",
    "                             flag=flag, \n",
    "                             num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'],\n",
    "                             verbose = False)\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss_2(current_iter, start_time, verbose=True)\n",
    "                environ.resize_results()\n",
    "                # environ.visual_policy(current_iter)\n",
    "\n",
    "            print_heading(f\"{timestring()} - CONTINUE ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "                          f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                          f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \", \n",
    "                          verbose = False )      \n",
    "    \n",
    "    ## if (current_iter_a % alpha_iter_alternate) == 0 \n",
    "    if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "        print_dbg(f\"** Switch training to update_weight\")                \n",
    "        print_heading(f\"{timestring()} - SWITCH TO WEIGHT TRAINING  urrent_iter: {current_iter}\\n\"\n",
    "                      f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                      f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \",\n",
    "                      verbose = True )       \n",
    "        \n",
    "        flag = 'update_w'\n",
    "        environ.fix_alpha()\n",
    "        environ.free_w(opt['fix_BN'])\n",
    "        environ.decay_temperature()\n",
    "\n",
    "        # print the distribution\n",
    "        dists = environ.get_policy_prob()\n",
    "\n",
    "        print(np.concatenate(dists, axis=-1))\n",
    "        p_epoch += 1\n",
    "        print(f\"** p_epoch incremented: {p_epoch}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efcd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T04:06:20.995174Z",
     "start_time": "2021-12-21T04:06:20.972230Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320533e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:26:28.603565Z",
     "start_time": "2021-12-21T21:26:28.286326Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" task1_logits: \\n {environ.networks['mtl-net'].task1_logits.detach().cpu().numpy()} \\n\")\n",
    "print(f\" task2_logits: \\n {environ.networks['mtl-net'].task2_logits.detach().cpu().numpy()} \\n\")\n",
    "print(f\" task3_logits: \\n {environ.networks['mtl-net'].task3_logits.detach().cpu().numpy()} \\n\")\n",
    "print(f\" task1 softmax: \\n {softmax(environ.networks['mtl-net'].task1_logits.detach().cpu().numpy(), axis = -1)} \\n\")\n",
    "print(f\" task2 softmax: \\n {softmax(environ.networks['mtl-net'].task2_logits.detach().cpu().numpy(), axis = -1)} \\n\")\n",
    "print(f\" task3 softmax: \\n {softmax(environ.networks['mtl-net'].task3_logits.detach().cpu().numpy(), axis = -1)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a32b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:06:36.002509Z",
     "start_time": "2021-12-17T00:06:35.981793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for i in [1,2,3]:\n",
    "#     task_pred = f\"task{i}_pred\"\n",
    "#     task_logits = f\"task{i}_logits\"\n",
    "#     policy_attr = f\"policy{i}\"\n",
    "#     logits_attr = f\"logit{i}\"\n",
    "#     print_heading(f\"{task_pred}\")\n",
    "#     print(getattr(environ, task_pred))\n",
    "#     print(policy_attr)\n",
    "#     print(getattr(environ, policy_attr)) \n",
    "#     print(logits_attr)\n",
    "#     print(getattr(environ, logits_attr)) \n",
    "#     print(task_logits)\n",
    "#     print(getattr(environ.networks['mtl-net'], task_logits)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c1877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T01:06:53.675694Z",
     "start_time": "2021-12-20T01:06:53.648732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fce8c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T01:12:07.284532Z",
     "start_time": "2021-12-20T01:12:07.262680Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current_iter    = 2174\n",
    "current_iter_a  = 348\n",
    "current_iter_w  = 348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9316b4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Softmax & Gumbel Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62dddc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Softmax, LogSoftMax, NegLogLikelihood and Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f6f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:41:36.552551Z",
     "start_time": "2021-12-17T00:41:36.484747Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# print(nn.CrossEntropyLoss.__doc__)\n",
    "loss = nn.CrossEntropyLoss(reduction ='none')\n",
    "# i1 = torch.randn(3, 5, requires_grad=True)\n",
    "# t1 = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "# print(i1)\n",
    "# print(i1)\n",
    "# print(t1)\n",
    "# output = loss(i1, t1)\n",
    "# print(output, output.sum(), output.mean())\n",
    "\n",
    "# i2 = torch.randn(1, 2, requires_grad=True)\n",
    "i0 = torch.tensor([[0.0, 1.0]], dtype=torch.float)\n",
    "i1 = torch.tensor([[1.0, 0.0]], dtype=torch.float)\n",
    "i2 = torch.tensor([[0.5, 0.5]], dtype=torch.float)\n",
    "i3 = torch.tensor([[0.4656, 0.5388]], dtype=torch.float)\n",
    "sm = nn.Softmax(dim =-1)\n",
    "lsm = nn.LogSoftmax(dim = -1)\n",
    "nll = nn.NLLLoss(reduction='none')\n",
    "\n",
    "t1 = torch.tensor([1], dtype=torch.int64)\n",
    "t0 = torch.tensor([0], dtype=torch.int64)\n",
    "t2 = torch.tensor([2], dtype=torch.int64)\n",
    "print('i0     : ', i0)\n",
    "print('sm(i0) : ', sm(i0))\n",
    "print('lsm(i0): ', lsm(i0))\n",
    "print()\n",
    "print('i1     : ', i1)\n",
    "print('sm(i1) : ', sm(i1))\n",
    "print('lsm(i1): ', lsm(i1))\n",
    "print()\n",
    "print('i2     : ', i2)\n",
    "print('sm(i2) : ', sm(i2))\n",
    "print('lsm(i2): ', lsm(i2))\n",
    "print()\n",
    "\n",
    "print('t0: ',t0)\n",
    "print('t1: ',t1)\n",
    "print()\n",
    "output1 = loss(i0, t0)\n",
    "output2 = nll(lsm(i0), t0)\n",
    "print('loss [0,1] and [0] : ', output1)\n",
    "print('nll between lsm(i0): ', output2)\n",
    "print()\n",
    "output1 = loss(i0, t1)\n",
    "output2 = nll(lsm(i0), t1)\n",
    "print('loss [0,1] and [1] : ', output1)\n",
    "print('nll between lsm(i0): ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i1, t0)\n",
    "output2 = nll(lsm(i1), t0)\n",
    "print('loss [1,0] and [0] : ', output1)\n",
    "print('nll between lsm(i1): ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i1, t1)\n",
    "output2 = nll(lsm(i1), t1)\n",
    "print('loss [1,0] and [1] : ', output1)\n",
    "print('nll between lsm(i1): ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i2, t0)\n",
    "output2 = nll(lsm(i2), t0)\n",
    "print('loss [0.5, 0.5] and [0] : ', output1)\n",
    "print('nll between lsm(i1)   and [0] : ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i2, t1)\n",
    "output2 = nll(lsm(i2), t1)\n",
    "print('loss [0.5, 0.5] and [1] : ', output1)\n",
    "print('nll between lsm(i1)   and [1] : ', output2)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b2691",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46139b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T22:21:49.726731Z",
     "start_time": "2021-12-03T22:21:49.391724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.5000, 0.5000],\n",
    "        [0.5000, 0.5000],\n",
    "        [0.5000, 0.5000],\n",
    "        [0.5000, 0.5000]], device='cuda:0', requires_grad=True) \n",
    "\n",
    "b = torch.tensor([0.5000, 0.5000,  0.5000, 0.5000], device='cuda:0', requires_grad=True) \n",
    "print(b.shape)\n",
    "c = torch.tensor([[0.000, 0.000,  0.000, 0.000]], device='cuda:0', requires_grad=True) \n",
    "print(c.shape)\n",
    "d = torch.tensor([[0.5000], [0.5000],  [0.5000], [0.5000]], device='cuda:0', requires_grad=True) \n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70dfbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:41:45.109937Z",
     "start_time": "2021-12-17T00:41:45.086013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(i0)\n",
    "print(i1)\n",
    "print(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd0c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:52:33.490328Z",
     "start_time": "2021-12-17T00:52:33.457768Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp  = 2.5\n",
    "print(F.gumbel_softmax( i1, temp, hard=False))\n",
    "print(F.gumbel_softmax( i1, temp, hard=False))\n",
    "print(F.gumbel_softmax( i1, temp, hard=False))\n",
    "print()\n",
    "\n",
    "print(F.gumbel_softmax( i2, temp, hard=False))\n",
    "print(F.gumbel_softmax( i2, temp, hard=False))\n",
    "print(F.gumbel_softmax( i2, temp, hard=False))\n",
    "print()\n",
    "\n",
    "print(F.gumbel_softmax( i3, temp, hard=False))\n",
    "print(F.gumbel_softmax( i3, temp, hard=False))\n",
    "print(F.gumbel_softmax( i3, temp, hard=False))\n",
    "# print(F.gumbel_softmax( d, 5, hard=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875518ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:54.606453Z",
     "start_time": "2021-12-01T01:34:54.586164Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.randn(20, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0890b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:57.902781Z",
     "start_time": "2021-12-01T01:34:57.880455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(logits[:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85ce6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:44:54.225922Z",
     "start_time": "2021-12-01T01:44:54.204482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format)\n",
    "print(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac484d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:45:28.342309Z",
     "start_time": "2021-12-01T01:45:28.320145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(tmp[0])\n",
    "tmp1 = tmp.exponential_()\n",
    "print(tmp1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b9eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:45:55.855272Z",
     "start_time": "2021-12-01T01:45:55.825274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp2 = tmp1.log()\n",
    "print(tmp2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5db59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:43:07.405165Z",
     "start_time": "2021-12-01T01:43:07.383324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " gumbels = (\n",
    "        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35984a4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:43:29.214582Z",
     "start_time": "2021-12-01T01:43:29.193794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(gumbels.shape)\n",
    "print(gumbels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65270223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:35:04.984426Z",
     "start_time": "2021-12-01T01:35:04.949756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample soft categorical using reparametrization trick:\n",
    "gumbel_soft = F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "\n",
    "# Sample hard categorical using \"Straight-through\" trick:\n",
    "gumbel_hard  = F.gumbel_softmax(logits, tau=1, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89520b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:37:20.518987Z",
     "start_time": "2021-12-01T01:37:20.490071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(logits[0])\n",
    "print(np.argmax(logits[0]))\n",
    "print('\\n')\n",
    "\n",
    "print(gumbel_soft.shape)\n",
    "print(gumbel_soft[0])\n",
    "print(np.argmax(gumbel_soft[0]))\n",
    "print('\\n')\n",
    "\n",
    "print(gumbel_hard.shape)\n",
    "print(gumbel_hard[0])\n",
    "print(np.argmax(gumbel_hard[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b49944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:11.889231Z",
     "start_time": "2021-12-01T01:47:11.865177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gumbel_soft.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8b677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:20.629635Z",
     "start_time": "2021-12-01T01:47:20.607058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tau = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376a731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:21.278402Z",
     "start_time": "2021-12-01T01:47:21.254276Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.empty_like(logits, memory_format=torch.legacy_contiguous_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb14b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:21.920473Z",
     "start_time": "2021-12-01T01:47:21.899432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e5688",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fill tensor `a` with elements drawn from exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636a394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:32.866091Z",
     "start_time": "2021-12-01T01:47:32.838226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e = a.exponential_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e5bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:34.003871Z",
     "start_time": "2021-12-01T01:47:33.978450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682752d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "draw natural log `ln()` on elements of a_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8f568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:47.286259Z",
     "start_time": "2021-12-01T01:47:47.265716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e_l = a_e.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46e180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:47.554155Z",
     "start_time": "2021-12-01T01:47:47.532995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e_l[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86dab1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Neg log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841bf82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:51.788290Z",
     "start_time": "2021-12-01T01:47:51.763038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_el_neg = -a_e_l\n",
    "\n",
    "a_el_neg[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991926d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:08.613360Z",
     "start_time": "2021-12-01T01:48:08.591331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2849ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:09.724747Z",
     "start_time": "2021-12-01T01:48:09.701886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gumbels = (logits + a_el_neg) / tau \n",
    "\n",
    "gumbels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b2b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:14.682363Z",
     "start_time": "2021-12-01T01:48:14.660407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dim = -1\n",
    "gumbels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28f692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:15.779652Z",
     "start_time": "2021-12-01T01:48:15.758743Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " y_soft = gumbels.softmax(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09316403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:17.797567Z",
     "start_time": "2021-12-01T01:48:17.776922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09910a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:19.084522Z",
     "start_time": "2021-12-01T01:48:19.062543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_soft[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a373cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T21:41:56.235154Z",
     "start_time": "2021-09-22T21:41:56.228186Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = y_soft.max(dim, keepdim=True)\n",
    "print(index[0].T)\n",
    "print(index[1].T)\n",
    "y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index[1], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6dc2ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T21:42:54.751798Z",
     "start_time": "2021-09-22T21:42:54.744929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.argmax(y_hard,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3e1f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:43:17.720809Z",
     "start_time": "2021-09-24T03:43:17.662341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp_d= [0,1,0]\n",
    "for i in range(10):\n",
    "    sampled = np.random.choice((2, 1, 0), p=tmp_d)\n",
    "    print(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0baccc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4bc41",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d5a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:42:25.945090Z",
     "start_time": "2021-12-20T22:42:25.917655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49b9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:41:50.353599Z",
     "start_time": "2021-12-20T22:41:50.331414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "curr_iter_t  = 0\n",
    "curr_iter_a  = 0\n",
    "curr_iter_w  = 0\n",
    "stop_iter_t  = 0\n",
    "stop_iter_w  = 0 \n",
    "stop_iter_a  = 0\n",
    "total_weight_epochs = 0\n",
    "total_policy_epochs = 0 \n",
    "train_total_iters = 8\n",
    "weight_iter_alternate = 17\n",
    "alpha_iter_alternate = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7018b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:41:50.588581Z",
     "start_time": "2021-12-20T22:41:50.555942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(curr_iter_t, stop_iter_t, flag, train_total_iters,opt['train']['print_freq'] )\n",
    "start_iter_t = curr_iter_t\n",
    "stop_iter_t = curr_iter_t +  train_total_iters \n",
    "print(f\" Current iteration {curr_iter_t} - Run  from {start_iter_t} to {stop_iter_t}\")\n",
    "\n",
    "print(curr_iter_w, weight_iter_alternate , flag)\n",
    "stop_iter_w = curr_iter_w +  weight_iter_alternate \n",
    "print(f\" Current Weight iteration {curr_iter_w} - Run  from {curr_iter_w+1} to {stop_iter_w}\")\n",
    "\n",
    "\n",
    "print(curr_iter_a ,  alpha_iter_alternate ,flag)\n",
    "stop_iter_a = curr_iter_a +  alpha_iter_alternate \n",
    "print(f\" Current alpha iteration {curr_iter_a} - Run  from {curr_iter_a+1} to {stop_iter_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839ecee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:37:56.218327Z",
     "start_time": "2021-12-20T22:37:10.168700Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# del t, t_w, t_a\n",
    "main_iter_ctr = 0 \n",
    "with tnrange(start_iter_t , stop_iter_t  , initial = start_iter_t , total = stop_iter_t, position=0, leave= True, desc=\"master\") as t :\n",
    "    for curr_t in t:\n",
    "        \n",
    "        with  tnrange(0, weight_iter_alternate , initial = 0, total = weight_iter_alternate, \n",
    "                      position=1, leave= False, desc=f\"epoch {curr_t} weight training\") as t_w :\n",
    "            for curr_w in t_w:    \n",
    "                sleep(0.35)\n",
    "                main_iter_ctr += 1\n",
    "                curr_iter_w  = curr_w\n",
    "                t.set_postfix({'epoch': f\"{curr_t}/{train_total_iters}\", 'main_iter_ctr': main_iter_ctr})\n",
    "                t_w.set_postfix({'weight training epoch': curr_t, 'batch #': curr_iter_w})\n",
    "\n",
    "            print(f\"** Epoch {curr_t}/{train_total_iters} weight training complete - Loss: \"\n",
    "                  f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_t:{curr_t}  main_iter_ctr:{main_iter_ctr}\" )\n",
    "                 \n",
    "        \n",
    "        with  tnrange(0, alpha_iter_alternate  , initial = 0, total = alpha_iter_alternate , \n",
    "                      position=2, leave= False, desc=f\"epoch {curr_t} policy training\") as t_a :\n",
    "            for curr_a in t_a:    \n",
    "                sleep(0.35)\n",
    "                main_iter_ctr += 1                \n",
    "                curr_iter_a = curr_a\n",
    "                t.set_postfix({'epoch': f\"{curr_t}/{train_total_iters}\", 'main_iter_ctr':main_iter_ctr})\n",
    "                t_a.set_postfix({'policy training epoch': curr_t, 'batch #': curr_iter_a})            \n",
    "                \n",
    "            print(f\"** Epoch {curr_t}/{train_total_iters} policy training complete - Loss: \"\n",
    "                  f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_t:{curr_t}  main_iter_ctr:{main_iter_ctr}\" )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2c8ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:04:03.776382Z",
     "start_time": "2021-12-20T23:04:03.746093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "curr_iter_t  = 0\n",
    "curr_iter_a  = 0\n",
    "curr_iter_w  = 0\n",
    "stop_iter_t  = 0\n",
    "stop_iter_w  = 0 \n",
    "stop_iter_a  = 0\n",
    "total_weight_epochs = 0\n",
    "total_policy_epochs = 0 \n",
    "train_total_iters = 100\n",
    "train_total_epochs = 10\n",
    "weight_iter_alternate = 17\n",
    "alpha_iter_alternate = 17\n",
    "\n",
    "print(curr_iter_t, stop_iter_t, flag, train_total_iters,opt['train']['print_freq'] )\n",
    "start_iter_t = curr_iter_t\n",
    "stop_iter_t = curr_iter_t +  train_total_iters \n",
    "print(f\" Current iteration {curr_iter_t} - Run  from {start_iter_t} to {stop_iter_t}\")\n",
    "\n",
    "print(curr_iter_w, weight_iter_alternate , flag)\n",
    "stop_iter_w = curr_iter_w +  weight_iter_alternate \n",
    "print(f\" Current Weight iteration {curr_iter_w} - Run  from {curr_iter_w+1} to {stop_iter_w}\")\n",
    "\n",
    "\n",
    "print(curr_iter_a ,  alpha_iter_alternate ,flag)\n",
    "stop_iter_a = curr_iter_a +  alpha_iter_alternate \n",
    "print(f\" Current alpha iteration {curr_iter_a} - Run  from {curr_iter_a+1} to {stop_iter_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba7fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:08:57.280036Z",
     "start_time": "2021-12-20T23:08:41.214078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del t, t_w, t_a\n",
    "curr_epoch = 0\n",
    "main_iter_ctr = 0 \n",
    "# with tnrange(start_iter_t , stop_iter_t  , initial = start_iter_t , total = stop_iter_t, position=0, leave= True, desc=\"master\") as t :\n",
    "# with tqdm_notebook(total=train_total_epochs) as t:\n",
    "t = tqdm_notebook(total=train_total_epochs)\n",
    "\n",
    "while curr_epoch < train_total_epochs:\n",
    "    curr_epoch+=1\n",
    "    t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the network weights\n",
    "    #-----------------------------------------        \n",
    "    with  tnrange(0, weight_iter_alternate , initial = 0, total = weight_iter_alternate, \n",
    "                  position=1, leave= False, desc=f\"epoch {curr_epoch} weight training\") as t_w :\n",
    "        for curr_w in t_w:    \n",
    "            sleep(0.35)\n",
    "            main_iter_ctr += 1\n",
    "            curr_iter_w  = curr_w\n",
    "\n",
    "            t.set_postfix({'epoch': f\"{curr_epoch}/{train_total_epochs}\", 'main_iter_ctr': main_iter_ctr})\n",
    "            t_w.set_postfix({'weight training epoch': curr_epoch, 'batch #': curr_iter_w})\n",
    "\n",
    "        tqdm.write(f\"** Epoch {curr_epoch}/{train_total_epochs} weight training complete - Loss: \"\n",
    "              f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_epoch:{curr_epoch}  main_iter_ctr:{main_iter_ctr}\" )\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the  policy \n",
    "    #-----------------------------------------        \n",
    "    with  tnrange(0, alpha_iter_alternate  , initial = 0, total = alpha_iter_alternate , \n",
    "                  position=2, leave= False, desc=f\"epoch {curr_epoch} policy training\") as t_a :\n",
    "        for curr_a in t_a:    \n",
    "            sleep(0.35)\n",
    "            main_iter_ctr += 1                \n",
    "            curr_iter_a = curr_a\n",
    "\n",
    "            t.set_postfix({'epoch': f\"{curr_epoch}/{train_total_epochs}\", 'main_iter_ctr':main_iter_ctr})\n",
    "            t_a.set_postfix({'policy training epoch': curr_epoch, 'batch #': curr_iter_a})            \n",
    "\n",
    "        tqdm.write(f\"** Epoch {curr_epoch}/{train_total_epochs} policy training complete - Loss: \"\n",
    "              f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_epoch:{curr_epoch}  main_iter_ctr:{main_iter_ctr}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141aa027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:11:57.104094Z",
     "start_time": "2021-12-20T22:11:57.081272Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with tnrange(start_iter, stop_iter , initial = current_iter_w, total = stop_iter,  position=0, leave= True, desc=\"training\") as t:\n",
    "#     for current_iter_w in t:\n",
    "#         print(current_iter_w)\n",
    "#         current_iter_w += 1\n",
    "#         print(current_iter_w)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3dbaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T21:23:27.031154Z",
     "start_time": "2021-12-15T21:23:27.007124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start = current_iter\n",
    "# end = current_iter + opt['train']['warm_up_iters']\n",
    "# curr_range = range(start,end)\n",
    "# print(start, end)\n",
    "\n",
    "# for i in tqdm.notebook.tnrange(start, end, initial = start, total = end):\n",
    "#     sleep(0.25)\n",
    "#     current_iter += 1\n",
    "# #     print(i)\n",
    "#     pass\n",
    "\n",
    "# print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d53f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T21:23:27.061710Z",
     "start_time": "2021-12-15T21:23:27.035963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start = current_iter\n",
    "# end = current_iter + opt['train']['warm_up_iters']\n",
    "# curr_range = range(start,end)\n",
    "# print(start, end)\n",
    "\n",
    "# for i in tqdm.notebook.tqdm_notebook(cur_range, initial = start, total = end, disable=False, position=0, desc = \"validation\"):\n",
    "#     current_iter += 1\n",
    "#     pass\n",
    "\n",
    "# print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0cd55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T20:40:03.819876Z",
     "start_time": "2021-12-14T20:40:03.768686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import trange\n",
    "# from time import sleep\n",
    "\n",
    "# for i in trange(40, desc='1st loop', position=0, leave = False):\n",
    "#     sleep(1.1)\n",
    "#     for j in trange(5, desc='2nd loop', position =1, leave = False):\n",
    "#         sleep(0.01)\n",
    "#         for k in trange(50, desc='3rd loop', position =0,leave=False):\n",
    "#             sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92f08b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T18:51:59.433086Z",
     "start_time": "2021-12-15T18:51:59.396500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with tqdm(batch_enumerator, leave=False, disable=False) as t:\n",
    "# with tqdm(total=10, bar_format=\"{postfix[0]} {postfix[1][value]:>8.2g}\", postfix=[\"Batch\", dict(value=0)]) as t:\n",
    "# with trange(opt['train']['warm_up_iters'], bar_format=\"{postfix[0]} {postfix[1][value]:>8.2g}\", postfix=[\"Batch\", dict(value=0)]) as t:\n",
    "# with trange(opt['train']['warm_up_iters']) as t:\n",
    "\n",
    "#     for current_iter in t:\n",
    "#         batch_idx, batch = next(batch_enumerator)\n",
    "#         ran = random.randint(1, 100)\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         environ.train()\n",
    "\n",
    "#         print_heading(f\" {timestring()} - WARMUP Training iter {current_iter}/{opt['train']['warm_up_iters']}    batch_idx: {batch_idx}\"    \n",
    "#                       f\"    Warm-up iters: {opt['train']['warm_up_iters']}\"\n",
    "#                       f\"    Validation freq:  {opt['train']['val_freq']}\", verbose = False)\n",
    "\n",
    "#         if batch_idx == len(train_loader) :\n",
    "#     #         print_heading(f\" ******* {timestring()}  re-enumerate train_loader() *******\")\n",
    "#             batch_enumerator = enumerate(train_loader,1)   \n",
    "\n",
    "#         t.set_postfix({'batch_idx': batch_idx, 'num_vowels': ran})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fee04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T18:51:59.469748Z",
     "start_time": "2021-12-15T18:51:59.436522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import tqdm.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868106a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:45.617713Z",
     "start_time": "2021-12-01T01:34:45.588274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19,20]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15829f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:46.630947Z",
     "start_time": "2021-12-01T01:34:46.608486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(a[:,::-1])\n",
    "print()\n",
    "print(a[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9215987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:53.104308Z",
     "start_time": "2021-12-01T01:34:53.081558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = 4\n",
    "b = 4\n",
    "c = 1\n",
    "\n",
    "0 // b + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3f558",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Scipy Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fdde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T20:37:51.105787Z",
     "start_time": "2021-09-24T20:37:51.088465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "row = np.array([5])\n",
    "col = np.array([0])\n",
    "data = np.array([6])\n",
    "y_class = scipy.sparse.csr_matrix((data, (row, col)), shape=(15,1))\n",
    "y_2 = scipy.sparse.csr_matrix((15,0))\n",
    "print(f\"Created y_class # dims: {y_class.ndim}    shape: {y_class.shape}\")\n",
    "print(f\"Created y_2 # dims: {y_2.ndim}    shape: {y_2.shape}\")\n",
    "\n",
    "# y_class[5]= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcabda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T20:34:45.811804Z",
     "start_time": "2021-09-24T20:34:45.795397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(y_class.toarray().T)\n",
    "print(y_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c27bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T20:33:18.869928Z",
     "start_time": "2021-09-24T20:33:18.853474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_class[8,0]= 999\n",
    "y_class.toarray().squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e9591",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### folding step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de820ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:43.426753Z",
     "start_time": "2021-11-03T17:03:43.366179Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_dev = copy.copy(x_file)\n",
    "print(x_dev.shape, type(x_dev))\n",
    "\n",
    "idx = x_dev.nonzero()\n",
    "print(idx[0][:82])\n",
    "print(idx[1][:82])\n",
    "print(x_dev[0].sum(), x_dev[1].sum())\n",
    "print(x_dev[0,0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3fd2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:43.639800Z",
     "start_time": "2021-11-03T17:03:43.592684Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folding_size = 30\n",
    "print(f\" fold - folding_size:{folding_size}\" )\n",
    "\n",
    "## collapse x into folding_size columns\n",
    "idx = x_dev.nonzero()\n",
    "folded = idx[1] % folding_size\n",
    "\n",
    "print(folded[:82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc03a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:43.985453Z",
     "start_time": "2021-11-03T17:03:43.884440Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_fold = scipy.sparse.csr_matrix((x_dev.data, (idx[0], folded)), shape=(x_dev.shape[0], folding_size))\n",
    "print(x_fold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44a0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:44.191775Z",
     "start_time": "2021-11-03T17:03:44.177092Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_fold.sum_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d88c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:44.485347Z",
     "start_time": "2021-11-03T17:03:44.465953Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d2f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:44.739813Z",
     "start_time": "2021-11-03T17:03:44.721549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(type(y_files[0]), type(y_class[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d7163",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Torch tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d3c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:44:12.409875Z",
     "start_time": "2021-09-24T03:44:12.395721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873b312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:44:14.996701Z",
     "start_time": "2021-09-24T03:44:14.978676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459af2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:48:01.639457Z",
     "start_time": "2021-09-24T03:48:01.623099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input.contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392718ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478262b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29ff118",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### using eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd00a86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:14:03.644641Z",
     "start_time": "2021-11-30T21:14:03.622475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(opt['dataload']['y_tasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aa7bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T08:07:39.604778Z",
     "start_time": "2021-10-27T08:07:39.579603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task1 = [1,2,3]\n",
    "task2 = None\n",
    "task3 = {'4': 'Kevin', '5':'Bardool'}\n",
    "\n",
    "for i in [1,2,3]:\n",
    "    print('task{:d}'.format(i))\n",
    "    if eval('task{:d}'.format(i)) is None:\n",
    "        print('task{:d} :  has not been defined '.format(i))\n",
    "#         exec_str = 'task{:d} =  np.random.rand({:d},{:d})'.format(i,3,2)\n",
    "        exec_str = 'y_task{:d} = scipy.sparse.csr_matrix(({:d}, {:d})) '.format(i,3,2)\n",
    "        print(exec_str)\n",
    "        exec(exec_str)\n",
    "        print('task{:d} : '.format(i), eval('task{:d}'.format(i)))\n",
    "        print(eval('type(task{:d})'.format(i)))\n",
    "    else:\n",
    "        print('task{:d} : '.format(i), eval('task{:d}'.format(i)))\n",
    "        \n",
    "print(f\"Created task{i} shape        : {eval('len(task{:d})'.format(i))}\")\n",
    "print(len(task3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce01f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T08:07:44.515337Z",
     "start_time": "2021-10-27T08:07:44.497413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(3,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef347b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T08:07:46.338515Z",
     "start_time": "2021-10-27T08:07:46.319087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(eval('len(task{:d})'.format(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdd594",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load datasets, perform folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:25:56.648521Z",
     "start_time": "2021-12-01T20:25:56.626710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Verify presence of Y label data\n",
    "if (opt['dataload']['y_tasks'] is None) and (opt['dataload']['y_regr'] is None):\n",
    "   raise ValueError(\"No label data specified, please add --y_class and/or --y_regr.\")\n",
    "\n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['x']))  \n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['folding']))\n",
    "for fl in opt['dataload']['y_tasks']:\n",
    "    print(os.path.join(opt['dataload']['dataroot'], fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca3c95",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load X data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31c4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:26:58.627148Z",
     "start_time": "2021-12-01T20:26:58.580407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Load data files \n",
    "##\n",
    "# ecfp     = sc.load_sparse(args.x)\n",
    "# y_class  = sc.load_sparse(args.y_class)\n",
    "# y_regr   = sc.load_sparse(args.y_regr)\n",
    "# y_censor = sc.load_sparse(args.y_censor)\n",
    "\n",
    "dataroot = opt['dataload']['dataroot']\n",
    "\n",
    "ecfp     = load_sparse(dataroot, opt['dataload']['x'])\n",
    "# x_file   = copy.copy(ecfp)\n",
    "\n",
    "print(f\" Input    {opt['dataload']['x']} - type : {type(ecfp)} shape : {ecfp.shape}\")\n",
    "# print(f\" Input    {opt['dataload']['x']} - type : {type(x_file)} shape : {x_file.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe1168",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load Y label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948aed46",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730a3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T20:43:36.174351Z",
     "start_time": "2021-11-30T20:43:36.139816Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,( y_task, y_type )in enumerate(zip(opt['dataload']['y_tasks'],opt['sc_tasks']),1):\n",
    "    print(full_path := os.path.join(dataroot, y_task ))\n",
    "    tmp = load_sparse(full_path)\n",
    "#     np.load(full_path, allow_pickle=True) \n",
    "#     print(type(tmp), tmp.shape)\n",
    "#     tmp_sparse = scipy.sparse.csr_matrix(tmp)\n",
    "    print(type(tmp_sparse), tmp_sparse.shape)\n",
    "    print('indicies: ', len(tmp_sparse.__dict__['indices']), tmp_sparse.__dict__['indices'])\n",
    "    print('indptr  : ', len(tmp_sparse.__dict__['indptr']) , tmp_sparse.__dict__['indptr'])\n",
    "    print('data    : ', len(tmp_sparse.__dict__['data'])   , tmp_sparse.__dict__['data'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e32cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:33.745342Z",
     "start_time": "2021-11-30T21:06:33.703310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_class  = load_sparse(dataroot, opt['dataload']['y_tasks'][0])\n",
    "# print(f\" Input     - type : {type(y_class)} shape : {y_class.shape}\")\n",
    "\n",
    "# y_regr  = load_sparse(dataroot, opt['dataload']['y_tasks'][1])\n",
    "# print(f\" Input     - type : {type(y_regr)} shape : {y_regr.shape}\")\n",
    "\n",
    "##\n",
    "## Load Y label files \n",
    "##\n",
    "y_files=[]\n",
    "\n",
    "for i,( y_task, y_type )in enumerate(zip(opt['dataload']['y_tasks'],opt['sc_tasks']),1):\n",
    "    y_tmp = load_sparse(dataroot,  y_task)\n",
    "    print(f\" y_task:{i}  task type: {y_type:5s}  dataset: {y_task} - type : {type(y_tmp)} shape : {y_tmp.shape}\")\n",
    "    ## Get number of positive / neg and total for each classes\n",
    "    num_pos    = np.array((y_tmp == +1).sum(0)).flatten()\n",
    "    num_neg    = np.array((y_tmp == -1).sum(0)).flatten()\n",
    "    num_class  = np.array((y_tmp !=  0).sum(0)).flatten()\n",
    "    if (num_class != num_pos + num_neg).any():\n",
    "        raise ValueError(\"For classification all y values (--y_class/--y) must be 1 or -1.\")\n",
    "    else:\n",
    "        y_files.append(y_tmp)\n",
    "\n",
    "y_class = copy.copy(y_files[0])\n",
    "# y_regr = copy.copy(y_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31defb05",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load folding file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d2773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:35.307158Z",
     "start_time": "2021-11-30T21:06:35.285431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## load folding file\n",
    "##\n",
    "folding_file = os.path.join(dataroot,opt['dataload']['folding'])\n",
    "folding  = np.load(folding_file)\n",
    "print(f\" Folding  {folding_file} - type : {type(folding)} shape : {folding.shape}\")\n",
    "print(f\"          {folding[:20]}\")\n",
    "\n",
    "assert ecfp.shape[0] == folding.shape[0], \"x and folding must have same number of rows\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773adb77",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load Y censor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00530d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.238543Z",
     "start_time": "2021-11-25T18:42:45.220911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_censor = load_sparse(dataroot, opt['dataload']['y_censor'])\n",
    "# if y_censor is not None:\n",
    "#     print(f\" Input     - type : {type(y_censor)} shape : {y_censor.shape}\") \n",
    "\n",
    "##\n",
    "## Load Y censor file\n",
    "##\n",
    "\n",
    "# y_censor = load_sparse(dataroot, opt['dataload']['y_censor'])\n",
    "# if y_censor is None:\n",
    "#     y_censor = scipy.sparse.csr_matrix(y_regr.shape)\n",
    "#     vprint(f\" y_sensor is {opt['dataload']['y_censor']}   Created y_censor shape       : {y_censor.shape}\")\n",
    "    \n",
    "# y_censor_shape = y_censor.shape if y_censor is not None else \"n/a\"\n",
    "# print(f\" y_censor  - type : {type(y_censor)}  shape: {y_censor_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be4c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:03:53.990463Z",
     "start_time": "2021-11-30T21:03:53.968756Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if (y_regr is None) and (y_censor is not None):\n",
    "#     raise ValueError(\"y_censor provided please also provide --y_regr.\")\n",
    "\n",
    "# # if y_class is None:\n",
    "# #     y_class = scipy.sparse.csr_matrix((ecfp.shape[0], 0))\n",
    "# #     vprint(f\"Created y_class shape        : {y_class.shape}\")\n",
    "\n",
    "# if y_regr is None:\n",
    "#     y_regr  = scipy.sparse.csr_matrix((ecfp.shape[0], 0))\n",
    "#     vprint(f\"Created y_regr shape         : {y_regr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb45a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T12:54:02.548598Z",
     "start_time": "2021-10-22T12:54:02.529641Z"
    },
    "hidden": true
   },
   "source": [
    "#### Input folding & transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c0a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:39.945186Z",
     "start_time": "2021-11-30T21:06:39.922823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"args.fold_inputs : {opt['dataload']['fold_inputs']} \\t\\t  transform: {opt['dataload']['input_transform']}\\n\")\n",
    "print(repr(ecfp))\n",
    "ecfp = fold_and_transform_inputs(ecfp, folding_size=opt['dataload']['fold_inputs'], transform=opt['dataload']['input_transform'])\n",
    "print(repr(ecfp))\n",
    "\n",
    "print(type(ecfp), ecfp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8f8e5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  Loading weights files for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c6e9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:58.845909Z",
     "start_time": "2021-11-30T21:06:58.817727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# num_regr   = np.bincount(y_regr.indices, minlength=y_regr.shape[1])\n",
    "print(' Classification weights: ',opt['dataload']['weights_class'])\n",
    "tasks_class = load_task_weights(opt['dataload']['weights_class'], y=y_class[0], label=\"y_class\")\n",
    "# tasks_regr  = load_task_weights(opt['dataload']['weights_regr'] , y=y_regr , label=\"y_regr\")\n",
    "\n",
    "print(tasks_class)\n",
    "print(tasks_class.training_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0c5b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:07:06.169693Z",
     "start_time": "2021-11-30T21:07:06.149534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869b822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:07:08.157368Z",
     "start_time": "2021-11-30T21:07:08.129096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(y_files[0].shape, y_files[1].shape, y_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1d75d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:09:55.522938Z",
     "start_time": "2021-11-30T21:09:55.500886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if tasks_class.aggregation_weight is None:\n",
    "    '''\n",
    "    fold classes \n",
    "    '''\n",
    "    ## using min_samples rule\n",
    "    fold_pos, fold_neg = class_fold_counts(y_class, folding)\n",
    "    n = opt['dataload']['min_samples_class']\n",
    "    tasks_class.aggregation_weight = ((fold_pos >= n).all(0) & (fold_neg >= n)).all(0).astype(np.float64)\n",
    "    print(f\" tasks_class.aggregation_weight WAS NOT passed \")\n",
    "    print(f\" min_samples_class: opt['dataload']['min_samples_class']\")\n",
    "    print(f\" Class fold counts: \\n  fold_pos:\\n{fold_pos}  \\n\\n  fold_neg:\\n{fold_neg}\") \n",
    "else:\n",
    "    print(f\"  tasks_class.aggregation_weight passed \")\n",
    "    \n",
    "print(f\" tasks_class.aggregation_weight.shape: {tasks_class.aggregation_weight.shape} \\n {tasks_class.aggregation_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04975aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.554463Z",
     "start_time": "2021-11-25T18:42:45.536741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if tasks_regr.aggregation_weight is None:\n",
    "#     if y_censor.nnz == 0:\n",
    "#         y_regr2 = y_regr.copy()\n",
    "#         y_regr2.data[:] = 1\n",
    "#     else:\n",
    "#         ## only counting uncensored data\n",
    "#         y_regr2      = y_censor.copy()\n",
    "#         y_regr2.data = (y_regr2.data == 0).astype(np.int32)\n",
    "  \n",
    "#     fold_regr, _ = sc.class_fold_counts(y_regr2, folding)\n",
    "#     del y_regr2\n",
    "#     tasks_regr.aggregation_weight = (fold_regr >= args.min_samples_regr).all(0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ece76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:10:10.546093Z",
     "start_time": "2021-11-30T21:10:07.896313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display dataset dimensions \n",
    "##\n",
    "print(f\"Input dimension      : {ecfp.shape[1]}\")\n",
    "print(f\"#samples             : {ecfp.shape[0]}\")\n",
    "print(f\"#classification tasks: {y_class[0].shape[1]}\")\n",
    "print(f\"Using {(tasks_class.aggregation_weight > 0).sum()} classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\")\n",
    "\n",
    "# vprint(f\"#regression tasks    : {y_regr.shape[1]}\")\n",
    "# vprint(f\"Using {(tasks_regr.aggregation_weight > 0).sum()} regression tasks for calculating metrics (RMSE, Rsquared, correlation).\")\n",
    "# print(ecfp[18387,:10].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb078d30",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  Compute batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a1fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:10:25.779206Z",
     "start_time": "2021-11-30T21:10:25.742332Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" batch_ratio        : {opt['batch_ratio']}\")\n",
    "print(f\" internal_batch_max : {opt['internal_batch_max']}\")\n",
    "\n",
    "# batch_size  = int(np.ceil(opt['batch_ratio'] * idx_tr.shape[0]))\n",
    "# num_int_batches = 1\n",
    "# print(f\" batch_ratio * # idx_tr:   {opt['batch_ratio']} * {idx_tr.shape[0]} = {opt['batch_ratio'] * idx_tr.shape[0]}\")\n",
    "\n",
    "\n",
    "# if opt['internal_batch_max'] is not None:\n",
    "#     if opt['internal_batch_max'] < batch_size:\n",
    "#         num_int_batches = int(np.ceil(batch_size / opt['internal_batch_max']))\n",
    "#         print(f\"\\n\\n internal_batch_max: {opt['internal_batch_max']}   batch_size: {batch_size}\")\n",
    "#         print(f\" batch_size / internal_batch_max: {batch_size / opt['internal_batch_max']}   num_int_batches: {num_int_batches}\")\n",
    "#         batch_size      = int(np.ceil(batch_size / num_int_batches))\n",
    "#         print(f\" batch_size / num_int_batches: {batch_size / num_int_batches}   modified batch_size: {batch_size}\")\n",
    "        \n",
    "\n",
    "batch_size = 320 \n",
    "print(f\" batch size:   {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2645c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Separate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0704f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:10:38.737304Z",
     "start_time": "2021-11-30T21:10:38.713730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"opt['dataload']['fold_te'] : {opt['dataload']['fold_te'] }\")\n",
    "print(f\"opt['dataload']['fold_va'] : {opt['dataload']['fold_va'] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0718e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:11:17.886147Z",
     "start_time": "2021-11-30T21:11:17.861334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if opt['dataload']['fold_te'] is not None and opt['dataload']['fold_te'] >= 0:\n",
    "    ## removing test data\n",
    "    print(f\" Remove test data\")\n",
    "    assert opt['dataload']['fold_te'] != opt['dataload']['fold_va'], \"fold_va and fold_te must not be equal.\"\n",
    "    keep    = (folding != args.fold_te)\n",
    "    ecfp    = ecfp[keep]\n",
    "    y_class = y_class[keep]\n",
    "    y_regr  = y_regr[keep]\n",
    "    y_censor= y_censor[keep]\n",
    "    folding = folding[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056298c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:11:23.295720Z",
     "start_time": "2021-11-30T21:11:23.267229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prop = (np.cumsum([0.3, 0.3, 0.3, 0.1])* ecfp.shape[0]+1).astype(np.int32)\n",
    "print(prop, prop.astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21f1d3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Separate train, train1, train2, and validation  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ed184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:12:32.402571Z",
     "start_time": "2021-11-30T21:12:32.372192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fold_va = opt['dataload']['fold_va']\n",
    "fold_va = 0\n",
    "fold_train1 = 1\n",
    "fold_train2 = 2\n",
    "\n",
    "idx_val    = np.where(folding == fold_va)[0]\n",
    "idx_train  = np.where(folding == fold_train1)[0]\n",
    "idx_train1 = np.where(folding == fold_train2)[0]\n",
    "idx_train2 = np.where(folding >  fold_train2)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5f106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:27:23.242326Z",
     "start_time": "2021-12-01T20:27:23.206716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataroot = opt['dataload']['dataroot']\n",
    "ecfp     = load_sparse(dataroot, opt['dataload']['x'])\n",
    "\n",
    "total_input = ecfp.shape[0]\n",
    "ranges      = (np.cumsum([0.3, 0.3, 0.1, 0.3])* total_input).astype(np.int32)\n",
    "print(total_input, '     ', ranges)\n",
    "\n",
    "idx_train  = np.arange(ranges[0])\n",
    "idx_train1 = np.arange(ranges[0], ranges[1])\n",
    "idx_train2 = np.arange(ranges[1], ranges[2])\n",
    "idx_val    = np.arange(ranges[2], ranges[-1])\n",
    "\n",
    "print( f' idx_train   len: {len(idx_train) :6d}  - {(idx_train)} ')\n",
    "print( f' idx_train1  len: {len(idx_train1):6d}  - {(idx_train1)}')\n",
    "print( f' idx_train2  len: {len(idx_train2):6d}  - {(idx_train2)}')\n",
    "print( f' idx_val     len: {len(idx_val)   :6d}  - {(idx_val)}   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118c6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.746087Z",
     "start_time": "2021-11-25T18:42:45.726692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_class_tr = y_class[idx_train]\n",
    "# y_class_va = y_class[idx_va]\n",
    "\n",
    "# y_regr_tr  = y_regr[idx_tr]\n",
    "# y_regr_va  = y_regr[idx_va]\n",
    "\n",
    "# y_censor_tr = y_censor[idx_tr]\n",
    "# y_censor_va = y_censor[idx_va]\n",
    "\n",
    "# num_pos_va  = np.array((y_class_va == +1).sum(0)).flatten()\n",
    "# num_neg_va  = np.array((y_class_va == -1).sum(0)).flatten()\n",
    "# num_regr_va = np.bincount(y_regr_va.indices, minlength=y_regr.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ce213",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Chembl Dataloader V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f34348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.782963Z",
     "start_time": "2021-11-25T18:42:45.750141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Instantiate datasets\n",
    "##\n",
    "trainset  = ClassRegrSparseDataset(x=ecfp, y_class=y_class, y_regr=None, y_censor=None, indicies = idx_train)\n",
    "trainset1 = ClassRegrSparseDataset(x=ecfp, y_class=y_class, y_regr=None, y_censor=None, indicies = idx_train1)\n",
    "trainset2 = ClassRegrSparseDataset(x=ecfp, y_class=y_class, y_regr=None, y_censor=None, indicies = idx_train2)\n",
    "valset    = ClassRegrSparseDataset(x=ecfp, y_class=y_class, y_regr=None, y_censor=None, indicies = idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5685573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.811341Z",
     "start_time": "2021-11-25T18:42:45.784980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_size  = trainset.input_size\n",
    "output_size = trainset.output_size\n",
    "\n",
    "class_output_size = trainset.class_output_size\n",
    "regr_output_size  = trainset.regr_output_size\n",
    "\n",
    "print(f' trainset - input size       : {input_size:6d}       output_size     : {output_size:6d}')\n",
    "print(f' trainset - class_output_size: {class_output_size:6d}       regr_output_size: {regr_output_size:6d}')\n",
    "\n",
    "for i in [trainset, trainset1, trainset2, valset]:\n",
    "    print(f' trainset - input size       : {i.input_size:6d}       output_size     : {i.output_size:6d}')\n",
    "    print(f' trainset - class_output_size: {i.class_output_size:6d}       regr_output_size: {i.regr_output_size:6d}')\n",
    "# dataset_tr.y_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2115e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.834395Z",
     "start_time": "2021-11-25T18:42:45.815015Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# trainset.__dict__\n",
    "batch_size = 320"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-gpu",
   "language": "python",
   "name": "pyt-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
