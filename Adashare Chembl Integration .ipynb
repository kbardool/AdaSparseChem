{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d574cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:05.709952Z",
     "start_time": "2022-01-26T15:34:03.594422Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:08.233990Z",
     "iopub.status.busy": "2022-01-07T22:44:08.233053Z",
     "iopub.status.idle": "2022-01-07T22:44:08.273284Z",
     "shell.execute_reply": "2022-01-07T22:44:08.271908Z",
     "shell.execute_reply.started": "2022-01-07T22:44:08.233943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "# import tqdm.notebook.trange as tnrange\n",
    "import copy, pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader \n",
    "import scipy.sparse\n",
    "from time import sleep\n",
    "from scipy.special import softmax\n",
    " \n",
    "from datetime import datetime\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    " # from tqdm import trange, tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from dev.sparsechem_utils_dev import load_sparse, load_task_weights, class_fold_counts, fold_and_transform_inputs\n",
    "from dev.sparsechem_utils_dev import print_metrics_cr\n",
    "from dev.chembl_dataloader_dev import ClassRegrSparseDataset_v3, ClassRegrSparseDataset, InfiniteDataLoader\n",
    "from utils.util import ( makedir, print_separator, create_path, print_yaml, print_yaml2, should, \n",
    "                         fix_random_seed, read_yaml_from_input, timestring, print_heading, print_dbg, \n",
    "                         print_underline, write_parms_report, get_command_line_args)\n",
    "from dev.sparsechem_env_dev import SparseChemEnv_Dev\n",
    "from dev.train_dev import evaluate\n",
    "\n",
    "# torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions( linewidth=132)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe56f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:07.094111Z",
     "start_time": "2022-01-26T15:34:05.714877Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:10.627259Z",
     "iopub.status.busy": "2022-01-07T22:44:10.626952Z",
     "iopub.status.idle": "2022-01-07T22:44:11.043381Z",
     "shell.execute_reply": "2022-01-07T22:44:11.042421Z",
     "shell.execute_reply.started": "2022-01-07T22:44:10.627221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cuda is available  :  True\n",
      " CUDA device count  :  1\n",
      " CUDA current device:  0\n",
      " GPU Processes      :  GPU:0\n",
      "no processes are running\n",
      "\n",
      " Device : cuda:0\n",
      "   name:        NVIDIA GeForce GTX 970M\n",
      "   capability:  (5, 2)\n",
      "   properties:  _CudaDeviceProperties(name='NVIDIA GeForce GTX 970M', major=5, minor=2, total_memory=3071MB, multi_processor_count=10)\n",
      "   Allocated :  0\n",
      "   Reserved  :  0\n",
      "\n",
      "| ID | GPU  | MEM |\n",
      "-------------------\n",
      "|  0 | nan% |  1% |\n"
     ]
    }
   ],
   "source": [
    "print(' Cuda is available  : ', torch.cuda.is_available())\n",
    "print(' CUDA device count  : ', torch.cuda.device_count())\n",
    "print(' CUDA current device: ', torch.cuda.current_device())\n",
    "print(' GPU Processes      : ', torch.cuda.list_gpu_processes())\n",
    "print()\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\" Device : cuda:{i}\")\n",
    "    print('   name:       ', torch.cuda.get_device_name())\n",
    "    print('   capability: ', torch.cuda.get_device_capability())\n",
    "    print('   properties: ', torch.cuda.get_device_properties(i))\n",
    "    ## current GPU memory usage by tensors in bytes for a given device\n",
    "    print('   Allocated : ', torch.cuda.memory_allocated(i) ) \n",
    "    ## current GPU memory managed by caching allocator in bytes for a given device, in previous PyTorch versions the command was torch.cuda.memory_cached\n",
    "    print('   Reserved  : ', torch.cuda.memory_reserved(i) )   \n",
    "    print()\n",
    "\n",
    "gpu_usage()                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05032bf4",
   "metadata": {},
   "source": [
    "## Read yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7bb1dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:07.141533Z",
     "start_time": "2022-01-26T15:34:07.098476Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " command line parms :  {'config': 'yamls/adashare/chembl_2task.yml', 'exp_instance': None, 'exp_ids': [0], 'batch_size': 9999, 'backbone_lr': None, 'task_lr': None, 'decay_lr_rate': None, 'decay_lr_freq': None, 'gpus': [0], 'cpu': True}\n",
      "Namespace(config='yamls/adashare/chembl_2task.yml', exp_instance=None, exp_ids=[0], batch_size=9999, backbone_lr=None, task_lr=None, decay_lr_rate=None, decay_lr_freq=None, gpus=[0], cpu=True)\n",
      "\n",
      "0126_0734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_args = \" --config yamls/adashare/chembl_2task.yml --cpu --batch_size 09999\".split()\n",
    "# get command line arguments\n",
    "args = get_command_line_args(input_args)\n",
    "print(args)\n",
    "\n",
    "print()\n",
    "\n",
    "if args.exp_instance is None:\n",
    "    args.exp_instance = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    \n",
    "print(args.exp_instance)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13447e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:07.199218Z",
     "start_time": "2022-01-26T15:34:07.146480Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "####################READ YAML#####################\n",
      "##################################################\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " experiment name       : SparseChem \n",
      " experiment instance   : 0126_0734 \n",
      " folder_name           : 0126_0734_bs128_lr0.01_dr0.50_df2000 \n",
      " experiment description: Run network warmup  for 100 iters, then alternating policy/weights modify curriculum speed from 20 to 3  \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_separator('READ YAML')\n",
    "\n",
    "opt, gpu_ids, _ = read_yaml_from_input(args)\n",
    "\n",
    "fix_random_seed(opt[\"seed\"][0])\n",
    "\n",
    "opt['exp_description'] = f\"Run network warmup  for 100 iters, then alternating policy/weights modify curriculum speed from 20 to 3  \\n\"\n",
    "\n",
    "# folder_name=  f\"{opt['exp_instance']}_bs{opt['train']['batch_size']:03d}_{opt['train']['decay_lr_rate']:3.2f}_{opt['train']['decay_lr_freq']}\"\n",
    "\n",
    "print_heading(f\" experiment name       : {opt['exp_name']} \\n\"\n",
    "              f\" experiment instance   : {opt['exp_instance']} \\n\"\n",
    "              f\" folder_name           : {opt['paths']['exp_folder']} \\n\"\n",
    "              f\" experiment description: {opt['exp_description']}\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a6a63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:07.232556Z",
     "start_time": "2022-01-26T15:34:07.201957Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(opt['exp_instance'])\n",
    "# print(opt['paths']['exp_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d33640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:07.298347Z",
     "start_time": "2022-01-26T15:34:07.236406Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create folder ../experiments/AdaSparseChem/0126_0734_bs128_lr0.01_dr0.50_df2000\n",
      "            exp_name : SparseChem\n",
      "        exp_instance : 0126_0734\n",
      "     exp_description : Run network warmup  for 100 iters, then alternating policy/weights modify curriculum speed from 20 to 3  \n",
      "\n",
      "                seed : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "            backbone : SparseChem\n",
      "       backbone_orig : ResNet18\n",
      "          orig_tasks : ['seg', 'sn']\n",
      "               tasks : ['class', 'class', 'class']\n",
      "     tasks_num_class : [5, 5, 5]\n",
      "             lambdas : [1, 1, 1]\n",
      "        policy_model : task-specific\n",
      "             verbose : False\n",
      "     input_size_freq : None\n",
      "          input_size : 32000\n",
      "        hidden_sizes : [25, 25, 25, 25, 25, 25]\n",
      "    tail_hidden_size : 25\n",
      " first_non_linearity : relu\n",
      "middle_non_linearity : relu\n",
      "      middle_dropout : 0.2\n",
      "  last_non_linearity : relu\n",
      "        last_dropout : 0.2\n",
      "   class_output_size : None\n",
      "    regr_output_size : None\n",
      "              policy : True\n",
      "     init_neg_logits : None\n",
      "           is_sparse : True\n",
      "diff_sparsity_weights : True\n",
      "          is_sharing : True\n",
      "          skip_layer : 0\n",
      "       is_curriculum : True\n",
      "    curriculum_speed : 3\n",
      "              fix_BN : False\n",
      "     retrain_from_pl : False\n",
      "\n",
      "paths\n",
      "-----\n",
      "      experiment_dir : ../experiments/AdaSparseChem\n",
      "             log_dir : ../experiments/AdaSparseChem\n",
      "          result_dir : ../experiments/AdaSparseChem\n",
      "      checkpoint_dir : ../experiments/AdaSparseChem\n",
      "          exp_folder : 0126_0734_bs128_lr0.01_dr0.50_df2000\n",
      "\n",
      "dataload\n",
      "--------\n",
      "             dataset : Chembl_23_mini\n",
      "            dataroot : /home/kbardool/kusanagi/MLDatasets/chembl_23mini_synthetic\n",
      "                   x : chembl_23mini_x.npy\n",
      "      x_split_ratios : [0.75, 0.001, 0.001, 0.248]\n",
      "             folding : chembl_23mini_folds.npy\n",
      "       weights_class : None\n",
      "         fold_inputs : 32000\n",
      "     input_transform : None\n",
      "             y_tasks : ['chembl_23mini_adashare_y1_bin_sparse.npy', 'chembl_23mini_adashare_y2_bin_sparse.npy', 'chembl_23mini_adashare_y3_bin_sparse.npy']\n",
      "            y_censor : None\n",
      "             fold_te : None\n",
      "              crop_h : 321\n",
      "              crop_w : 321\n",
      "   min_samples_class : 5\n",
      "             fold_va : 0\n",
      "\n",
      "train\n",
      "-----\n",
      "          batch_size : 128\n",
      "             task_lr : 0.01\n",
      "         backbone_lr : 0.01\n",
      "       decay_lr_rate : 0.5\n",
      "       decay_lr_freq : 2000\n",
      "         total_iters : 25000\n",
      "       warm_up_iters : 25000\n",
      "           policy_lr : 0.0001\n",
      "               reg_w : 0.05\n",
      "     Lambda_sparsity : 0.05\n",
      "       reg_w_hamming : 0.1\n",
      "      Lambda_sharing : 0.1\n",
      "          print_freq : -1\n",
      "            val_freq : 500\n",
      "           init_temp : 5\n",
      "          decay_temp : 0.965\n",
      "     decay_temp_freq : 2\n",
      "              resume : False\n",
      "      retrain_resume : False\n",
      "         policy_iter : best\n",
      "          which_iter : warmup\n",
      "         init_method : equal\n",
      "       hard_sampling : False\n",
      "\n",
      "SC\n",
      "--\n",
      "         batch_ratio : 0.02\n",
      "      normalize_loss : None\n",
      "\n",
      "test\n",
      "----\n",
      "          which_iter : best\n",
      "                 cpu : True\n"
     ]
    }
   ],
   "source": [
    "# for line in lines: \n",
    "create_path(opt)    \n",
    "\n",
    "# print yaml on the screen\n",
    "for line in print_yaml2(opt):\n",
    "    print(line)\n",
    "\n",
    "write_parms_report(opt)    \n",
    "\n",
    "log_dir        =  os.path.join(opt['paths']['log_dir'], opt['paths']['exp_folder'])\n",
    "checkpoint_dir =  os.path.join(opt['paths']['checkpoint_dir'], opt['paths']['exp_folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bfa8d",
   "metadata": {},
   "source": [
    "## Chembl Dataloader V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c17b578f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:08.103483Z",
     "start_time": "2022-01-26T15:34:07.300940Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:16.229028Z",
     "iopub.status.busy": "2022-01-07T22:44:16.227544Z",
     "iopub.status.idle": "2022-01-07T22:44:16.659397Z",
     "shell.execute_reply": "2022-01-07T22:44:16.658348Z",
     "shell.execute_reply.started": "2022-01-07T22:44:16.228966Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset  = ClassRegrSparseDataset_v3(opt, split_ratios = opt['dataload']['x_split_ratios'], ratio_index = 0, verbose = False)\n",
    "trainset1 = ClassRegrSparseDataset_v3(opt, split_ratios = opt['dataload']['x_split_ratios'], ratio_index = 1)\n",
    "trainset2 = ClassRegrSparseDataset_v3(opt, split_ratios = opt['dataload']['x_split_ratios'], ratio_index = 2)\n",
    "valset    = ClassRegrSparseDataset_v3(opt, split_ratios = opt['dataload']['x_split_ratios'], ratio_index = 3)\n",
    "\n",
    "\n",
    "train_loader  = InfiniteDataLoader(trainset , batch_size=opt['train']['batch_size'], num_workers = 2, pin_memory=True, collate_fn=trainset.collate, shuffle=False)\n",
    "val_loader    = InfiniteDataLoader(valset   , batch_size=opt['train']['batch_size'], num_workers = 1, pin_memory=True, collate_fn=valset.collate  , shuffle=False)\n",
    "train1_loader = InfiniteDataLoader(trainset1, batch_size=opt['train']['batch_size'], num_workers = 2, pin_memory=True, collate_fn=trainset1.collate, shuffle=False)\n",
    "train2_loader = InfiniteDataLoader(trainset2, batch_size=opt['train']['batch_size'], num_workers = 2, pin_memory=True, collate_fn=trainset2.collate, shuffle=False)\n",
    "\n",
    "\n",
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate']  = opt['train'].get('alpha_iter_alternate'  , len(train2_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f4f21",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c09986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:08.366158Z",
     "start_time": "2022-01-26T15:34:08.112684Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:18.146907Z",
     "iopub.status.busy": "2022-01-07T22:44:18.145721Z",
     "iopub.status.idle": "2022-01-07T22:44:18.191126Z",
     "shell.execute_reply": "2022-01-07T22:44:18.189994Z",
     "shell.execute_reply.started": "2022-01-07T22:44:18.146867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trainset.y_class                       :  [(13791, 5), (13791, 5), (13791, 5)]\n",
      " trainset1.y_class                      :  [(18, 5), (18, 5), (18, 5)]\n",
      " trainset2.y_class                      :  [(18, 5), (18, 5), (18, 5)]\n",
      " valset.y_class                         :  [(4561, 5), (4561, 5), (4561, 5)] \n",
      "\n",
      " size of training set 0 (warm up)       :  13791\n",
      " size of training set 1 (network parms) :  18\n",
      " size of training set 2 (policy weights):  18\n",
      " size of validation set                 :  4561\n",
      "                               Total    :  18388\n",
      "\n",
      " batch size                             :  128\n",
      "\n",
      " # batches training 0 (warm up)         :  108\n",
      " # batches training 1 (network parms)   :  1\n",
      " # batches training 2 (policy weights)  :  1\n",
      " # batches validation dataset           :  36\n",
      "\n",
      "\n",
      " batch size                             : 128 \n",
      " backbone                               : SparseChem \n",
      " paths.log_dir                          : ../experiments/AdaSparseChem \n",
      " paths.checkpoint_dir                   : ../experiments/AdaSparseChem \n",
      " experiment name                        : SparseChem \n",
      " tasks_num_class                        : ([5, 5, 5],) \n",
      " Hidden sizes                           : [25, 25, 25, 25, 25, 25] \n",
      " init_neg_logits                        : (None,) \n",
      " device id                              : 0 \n",
      " init temp                              : (5,) \n",
      " decay temp                             : 0.965 \n",
      " fix BN parms                           : False \n",
      " skip_layer                             : 0 \n",
      " train.init_method                      : equal \n",
      " Total iterations                       : 25000 \n",
      " Warm-up iterations                     : 25000 \n",
      " Print Frequency                        : -1 \n",
      " Validation Frequency                   : 500 \n",
      " \n",
      " Weight iter alternate                  : 1 \n",
      " Alpha  iter alternate                  : 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\" trainset.y_class                       :  {[ i.shape  for i in trainset.y_class_list]}\")\n",
    "print(f\" trainset1.y_class                      :  {[ i.shape  for i in trainset1.y_class_list]}\")\n",
    "print(f\" trainset2.y_class                      :  {[ i.shape  for i in trainset2.y_class_list]}\")\n",
    "print(f\" valset.y_class                         :  {[ i.shape  for i in valset.y_class_list  ]} \")\n",
    "print()\n",
    "\n",
    "print(f' size of training set 0 (warm up)       :  {len(trainset)}')\n",
    "print(f' size of training set 1 (network parms) :  {len(trainset1)}')\n",
    "print(f' size of training set 2 (policy weights):  {len(trainset2)}')\n",
    "print(f' size of validation set                 :  {len(valset)}')\n",
    "print(f'                               Total    :  {len(trainset)+len(trainset1)+len(trainset2)+len(valset)}')\n",
    "print()\n",
    "print(f\" batch size                             :  {opt['train']['batch_size']}\")\n",
    "print()\n",
    "print(f\" # batches training 0 (warm up)         :  {len(train_loader)}\")\n",
    "print(f\" # batches training 1 (network parms)   :  {len(train1_loader)}\")\n",
    "print(f\" # batches training 2 (policy weights)  :  {len(train2_loader)}\")\n",
    "print(f\" # batches validation dataset           :  {len(val_loader)}\")\n",
    "print()\n",
    "print(\n",
    "    f\"\\n batch size                             : {opt['train']['batch_size']}\", \n",
    "    f\"\\n backbone                               : {opt['backbone']}\",\n",
    "    f\"\\n paths.log_dir                          : {opt['paths']['log_dir']}\", \n",
    "    f\"\\n paths.checkpoint_dir                   : {opt['paths']['checkpoint_dir']}\", \n",
    "    f\"\\n experiment name                        : {opt['exp_name']}\",\n",
    "    f\"\\n tasks_num_class                        : {opt['tasks_num_class'],}\",\n",
    "    f\"\\n Hidden sizes                           : {opt['hidden_sizes']}\",     \n",
    "    f\"\\n init_neg_logits                        : {opt['init_neg_logits'],}\",\n",
    "    f\"\\n device id                              : {gpu_ids[0]}\",\n",
    "    f\"\\n init temp                              : {opt['train']['init_temp'],}\",\n",
    "    f\"\\n decay temp                             : {opt['train']['decay_temp']}\",\n",
    "    f\"\\n fix BN parms                           : {opt['fix_BN']}\",\n",
    "    f\"\\n skip_layer                             : {opt['skip_layer']}\",\n",
    "    f\"\\n train.init_method                      : {opt['train']['init_method']}\",\n",
    "    f\"\\n Total iterations                       : {opt['train']['total_iters']}\",\n",
    "    f\"\\n Warm-up iterations                     : {opt['train']['warm_up_iters']}\",\n",
    "    f\"\\n Print Frequency                        : {opt['train']['print_freq']}\",\n",
    "    f\"\\n Validation Frequency                   : {opt['train']['val_freq']} \\n\",\n",
    "    f\"\\n Weight iter alternate                  : {opt['train']['weight_iter_alternate'] }\",\n",
    "    f\"\\n Alpha  iter alternate                  : {opt['train']['alpha_iter_alternate'] }\")\n",
    "# print('\\n\\n Opt file \\n ------------ \\n')\n",
    "# pp.pprint(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf868bb",
   "metadata": {},
   "source": [
    "### Create model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3293d4b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:08.488910Z",
     "start_time": "2022-01-26T15:34:08.370415Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:21.499081Z",
     "iopub.status.busy": "2022-01-07T22:44:21.498215Z",
     "iopub.status.idle": "2022-01-07T22:44:21.604241Z",
     "shell.execute_reply": "2022-01-07T22:44:21.602711Z",
     "shell.execute_reply.started": "2022-01-07T22:44:21.499039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "* SparseChemEnv_Dev  Initializtion - verbose: False\n",
      "------------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------\n",
      "SparseChemEnv_Dev.super() init()  Start - verbose: False\n",
      "------------------------------------------------------------ \n",
      "\n",
      " log_dir        :  ../experiments/AdaSparseChem/0126_0734_bs128_lr0.01_dr0.50_df2000 \n",
      " checkpoint_dir :  ../experiments/AdaSparseChem/0126_0734_bs128_lr0.01_dr0.50_df2000 \n",
      " exp_name       :  SparseChem \n",
      " tasks_num_class:  [5, 5, 5] \n",
      " device         :  cuda:0 \n",
      " device id      :  0 \n",
      " dataset        :  Chembl_23_mini \n",
      " tasks          :  ['class', 'class', 'class'] \n",
      "\n",
      "--------------------------------------------------\n",
      "SparseChemEnv_Dev.super() init()  end\n",
      "-------------------------------------------------- \n",
      "\n",
      " is_train       :  True \n",
      " init_neg_logits:  None \n",
      " init temp      :  5 \n",
      " decay temp     :  0.965 \n",
      " input_size     :  32000 \n",
      " normalize loss :  None \n",
      " num_tasks      :  3 \n",
      " policys        :  [None, None, None]\n",
      "--------------------------------------------------------\n",
      "* SparseChemEnv_Dev environment successfully created\n",
      "-------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "environ = SparseChemEnv_Dev(log_dir = log_dir, \n",
    "                            checkpoint_dir = checkpoint_dir, \n",
    "                            exp_name = opt['exp_name'],\n",
    "                            tasks_num_class = opt['tasks_num_class'], \n",
    "                            init_neg_logits = opt['init_neg_logits'], \n",
    "                            device = gpu_ids[0],\n",
    "                            init_temperature = opt['train']['init_temp'], \n",
    "                            temperature_decay= opt['train']['decay_temp'], \n",
    "                            is_train=True,\n",
    "                            opt=opt, \n",
    "                            verbose = False)\n",
    "\n",
    "cfg = environ.print_configuration()\n",
    "write_parms_report(opt, cfg, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "100f7046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:08.560381Z",
     "start_time": "2022-01-26T15:34:08.500579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTL3_Dev(\n",
       "  (backbone): SparseChem_Backbone(\n",
       "    (Input_linear): SparseLinear(in_features=32000, out_features=25, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): SparseChemBlock(\n",
       "          (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "          (non_linear): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): SparseChemBlock(\n",
       "          (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "          (non_linear): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): SparseChemBlock(\n",
       "          (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "          (non_linear): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): SparseChemBlock(\n",
       "          (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "          (non_linear): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): SparseChemBlock(\n",
       "          (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "          (non_linear): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): SparseChemBlock(\n",
       "          (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "          (non_linear): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (task1_fc1_c0): SparseChem_Classification_Module(\n",
       "    (linear): Linear(in_features=25, out_features=5, bias=True)\n",
       "  )\n",
       "  (task2_fc1_c0): SparseChem_Classification_Module(\n",
       "    (linear): Linear(in_features=25, out_features=5, bias=True)\n",
       "  )\n",
       "  (task3_fc1_c0): SparseChem_Classification_Module(\n",
       "    (linear): Linear(in_features=25, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.networks['mtl-net']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07462f4d",
   "metadata": {},
   "source": [
    "## Warmup Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738062",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7498ab15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:08.610001Z",
     "start_time": "2022-01-26T15:34:08.565427Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:25.814333Z",
     "iopub.status.busy": "2022-01-07T22:44:25.813544Z",
     "iopub.status.idle": "2022-01-07T22:44:25.847331Z",
     "shell.execute_reply": "2022-01-07T22:44:25.845489Z",
     "shell.execute_reply.started": "2022-01-07T22:44:25.814290Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tasks_num_class         : ([5, 5, 5],) \n",
      " init_neg_logits         : (None,) \n",
      " device id               : 0 \n",
      " init temp               : (5,) \n",
      " decay temp              : 0.965 \n",
      " fix BN parms            : False \n",
      " skip_layer              : 0 \n",
      "\n",
      " train.init_method       : equal \n",
      " Total iterations        : 25000 \n",
      " Warm-up iterations      : 25000 \n",
      " Print Frequency         : -1 \n",
      " Validation Frequency    : 500 \n",
      " Weight iter alternate   : 1 \n",
      " Alpha  iter alternate   : 1 \n",
      " Network[mtl_net].layers : [1, 1, 1, 1, 1, 1] \n",
      " Num_blocks              : 6\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    f\"\\n tasks_num_class         : {opt['tasks_num_class'],}\",\n",
    "    f\"\\n init_neg_logits         : {opt['init_neg_logits'],}\",\n",
    "    f\"\\n device id               : {gpu_ids[0]}\",\n",
    "    f\"\\n init temp               : {opt['train']['init_temp'],}\",\n",
    "    f\"\\n decay temp              : {opt['train']['decay_temp']}\",\n",
    "    f\"\\n fix BN parms            : {opt['fix_BN']}\",\n",
    "    f\"\\n skip_layer              : {opt['skip_layer']}\",\n",
    "    f\"\\n\"\n",
    "    f\"\\n train.init_method       : {opt['train']['init_method']}\",\n",
    "    f\"\\n Total iterations        : {opt['train']['total_iters']}\",\n",
    "    f\"\\n Warm-up iterations      : {opt['train']['warm_up_iters']}\",\n",
    "    f\"\\n Print Frequency         : {opt['train']['print_freq']}\",\n",
    "    f\"\\n Validation Frequency    : {opt['train']['val_freq']}\",\n",
    "    f\"\\n Weight iter alternate   : {opt['train']['weight_iter_alternate'] }\",\n",
    "    f\"\\n Alpha  iter alternate   : {opt['train']['alpha_iter_alternate'] }\",\n",
    "    f\"\\n Network[mtl_net].layers : {environ.networks['mtl-net'].layers}\",\n",
    "    f\"\\n Num_blocks              : {sum(environ.networks['mtl-net'].layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251f3adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:13.160010Z",
     "start_time": "2022-01-26T15:34:08.614202Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:27.458746Z",
     "iopub.status.busy": "2022-01-07T22:44:27.457640Z",
     "iopub.status.idle": "2022-01-07T22:44:27.491358Z",
     "shell.execute_reply": "2022-01-07T22:44:27.490019Z",
     "shell.execute_reply.started": "2022-01-07T22:44:27.458686Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate Training \n",
      "cuda available [0]\n",
      "\n",
      "\n",
      " set print_freq to length of train loader: 108\n"
     ]
    }
   ],
   "source": [
    "environ.define_optimizer(policy_learning=False)\n",
    "environ.define_scheduler(policy_learning=False)\n",
    "# Fix Alpha - \n",
    "environ.fix_alpha()\n",
    "environ.free_weights(opt['fix_BN'])\n",
    "\n",
    "if opt['train']['resume']:\n",
    "    print('Resume training')\n",
    "    current_iter = environ.load(opt['train']['which_iter'])\n",
    "    environ.networks['mtl-net'].reset_logits()\n",
    "else:\n",
    "    print('Initiate Training ')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available', gpu_ids)   \n",
    "    environ.cuda(gpu_ids)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    environ.cpu()\n",
    "print('\\n')\n",
    "\n",
    "current_iter   = 0\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0\n",
    "flag         = 'update_w'\n",
    "best_value   = 0 \n",
    "best_iter    = 0\n",
    "p_epoch      = 0\n",
    "best_metrics = None\n",
    "flag_warmup  = True\n",
    "eval_iter    = -1\n",
    "num_prints   = 0\n",
    "\n",
    "num_blocks = sum(environ.networks['mtl-net'].layers)\n",
    "\n",
    "if opt['train']['print_freq'] == -1:\n",
    "    print(f\" set print_freq to length of train loader: {len(train_loader)}\")\n",
    "    opt['train']['print_freq']    = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdcfd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d6aac59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:13.387260Z",
     "start_time": "2022-01-26T15:34:13.187901Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:31.240717Z",
     "iopub.status.busy": "2022-01-07T22:44:31.240386Z",
     "iopub.status.idle": "2022-01-07T22:44:31.264367Z",
     "shell.execute_reply": "2022-01-07T22:44:31.262797Z",
     "shell.execute_reply.started": "2022-01-07T22:44:31.240692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which_iter          : warmup\n",
      "train_resume        : False\n",
      "\n",
      "Length train_loader : 108\n",
      "Length val_loader   : 36\n",
      "\n",
      "total_iters         : 25000\n",
      "warm_up_iters       : 100\n",
      "\n",
      "val_freq            : 500\n",
      "print_freq          : 108\n",
      "\n",
      "batch_size          : 128\n",
      "Backbone LR         : 0.01\n",
      "LR decay rate       : 0.5\n",
      "LR decay frequency  : 2000\n",
      "\n",
      " output folder      : 0126_0734_bs128_lr0.01_dr0.50_df2000\n"
     ]
    }
   ],
   "source": [
    "opt['train']['warm_up_iters'] = 100\n",
    "# opt['train']['print_freq'] =1\n",
    "# opt['train']['weight_iter_alternate'] = 10R1\n",
    "# opt['train']['alpha_iter_alternate']  = 10\n",
    "# opt['train']['warm_up_iters'] = 200\n",
    "# opt['train']['val_freq']   = 50\n",
    "# opt['train']['val_freq']      = 500\n",
    "\n",
    "print(f\"which_iter          : {opt['train']['which_iter']}\")\n",
    "print(f\"train_resume        : {opt['train']['resume']}\")\n",
    "print()\n",
    "print(f\"Length train_loader : {len(train_loader)}\")\n",
    "print(f\"Length val_loader   : {len(val_loader)}\")\n",
    "print()\n",
    "print(f\"total_iters         : {opt['train']['total_iters']}\")  \n",
    "print(f\"warm_up_iters       : {opt['train']['warm_up_iters']}\")   \n",
    "print()\n",
    "print(f\"val_freq            : {opt['train']['val_freq']     }\")      \n",
    "print(f\"print_freq          : {opt['train']['print_freq']  }\")\n",
    "print()\n",
    "print(f\"batch_size          : {opt['train']['batch_size']   }\")         \n",
    "print(f\"Backbone LR         : {opt['train']['backbone_lr']   }\")        \n",
    "print(f\"LR decay rate       : {opt['train']['decay_lr_rate']   }\")        \n",
    "print(f\"LR decay frequency  : {opt['train']['decay_lr_freq']   }\")        \n",
    "print()\n",
    "print(f\" output folder      : {opt['paths']['exp_folder']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e42d110",
   "metadata": {},
   "source": [
    "### Warm-up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2dd41aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:37:01.818683Z",
     "start_time": "2022-01-26T00:37:01.781307Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:32.955710Z",
     "iopub.status.busy": "2022-01-07T22:44:32.954619Z",
     "iopub.status.idle": "2022-01-07T22:44:32.980934Z",
     "shell.execute_reply": "2022-01-07T22:44:32.979443Z",
     "shell.execute_reply.started": "2022-01-07T22:44:32.955675Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(current_iter)\n",
    "#     stop_iter  = current_iter + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6cc691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:18.151999Z",
     "start_time": "2022-01-26T15:34:18.103074Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:35.007879Z",
     "iopub.status.busy": "2022-01-07T22:44:35.007527Z",
     "iopub.status.idle": "2022-01-07T22:44:35.034650Z",
     "shell.execute_reply": "2022-01-07T22:44:35.033033Z",
     "shell.execute_reply.started": "2022-01-07T22:44:35.007855Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Last iteration: 0  # of warm-up iterations to do:100 - Run  from 1 to 100\n"
     ]
    }
   ],
   "source": [
    "stop_iter  = current_iter + opt['train']['warm_up_iters']\n",
    "print(f\" Last iteration: {current_iter}  # of warm-up iterations to do:{opt['train']['warm_up_iters']} - Run  from {current_iter+1} to {stop_iter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8663432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:31.846499Z",
     "start_time": "2022-01-26T15:34:22.204584Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:38.657578Z",
     "iopub.status.busy": "2022-01-07T22:44:38.657285Z",
     "iopub.status.idle": "2022-01-07T22:45:02.893150Z",
     "shell.execute_reply": "2022-01-07T22:45:02.891381Z",
     "shell.execute_reply.started": "2022-01-07T22:44:38.657539Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Last iteration: 0  # of warm-up iterations to do:100 - Run  from 1 to 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cb3d308262479bb03841d0e0637f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:  20.0159     mean:   4.0032 \n",
      " Task losses:  16.2607     mean:   3.2521 \n",
      " Task losses:  12.4230     mean:   2.4846 \n",
      " Task losses:  11.1655     mean:   2.2331 \n",
      " Task losses:  11.4116     mean:   2.2823 \n",
      " Task losses:  11.3344     mean:   2.2669 \n",
      " Task losses:  11.0627     mean:   2.2125 \n",
      " Task losses:  11.1395     mean:   2.2279 \n",
      " Task losses:  10.8323     mean:   2.1665 \n",
      " Task losses:  10.3662     mean:   2.0732 \n",
      " Task losses:  10.2680     mean:   2.0536 \n",
      " Task losses:  11.2267     mean:   2.2453 \n",
      " Task losses:  11.6958     mean:   2.3392 \n",
      " Task losses:  11.0142     mean:   2.2028 \n",
      " Task losses:  10.6298     mean:   2.1260 \n",
      " Task losses:  11.5142     mean:   2.3028 \n",
      " Task losses:  10.8070     mean:   2.1614 \n",
      " Task losses:  10.5107     mean:   2.1021 \n",
      " Task losses:  10.7061     mean:   2.1412 \n",
      " Task losses:  10.5138     mean:   2.1028 \n",
      " Task losses:   9.9856     mean:   1.9971 \n",
      " Task losses:   9.7149     mean:   1.9430 \n",
      " Task losses:  11.3587     mean:   2.2717 \n",
      " Task losses:  10.9147     mean:   2.1829 \n",
      " Task losses:  10.4468     mean:   2.0894 \n",
      " Task losses:  11.0432     mean:   2.2086 \n",
      " Task losses:  25.2946     mean:   5.0589 \n",
      " Task losses:  10.5677     mean:   2.1135 \n",
      " Task losses:  10.3308     mean:   2.0662 \n",
      " Task losses:  10.9592     mean:   2.1918 \n",
      " Task losses:  11.0908     mean:   2.2182 \n",
      " Task losses:  10.8339     mean:   2.1668 \n",
      " Task losses:  11.3940     mean:   2.2788 \n",
      " Task losses:  11.5350     mean:   2.3070 \n",
      " Task losses:  11.6135     mean:   2.3227 \n",
      " Task losses:  10.8046     mean:   2.1609 \n",
      " Task losses:  10.4471     mean:   2.0894 \n",
      " Task losses:  10.4825     mean:   2.0965 \n",
      " Task losses:  10.3735     mean:   2.0747 \n",
      " Task losses:  10.4739     mean:   2.0948 \n",
      " Task losses:  10.3791     mean:   2.0758 \n",
      " Task losses:  11.2135     mean:   2.2427 \n",
      " Task losses:  10.3813     mean:   2.0763 \n",
      " Task losses:  11.3743     mean:   2.2749 \n",
      " Task losses:  10.8422     mean:   2.1684 \n",
      " Task losses:  10.4805     mean:   2.0961 \n",
      " Task losses:  11.4620     mean:   2.2924 \n",
      " Task losses:  10.1406     mean:   2.0281 \n",
      " Task losses:  10.4559     mean:   2.0912 \n",
      " Task losses:  10.4142     mean:   2.0828 \n",
      " Task losses:  10.3025     mean:   2.0605 \n",
      " Task losses:  10.3053     mean:   2.0611 \n",
      " Task losses:  10.4149     mean:   2.0830 \n",
      " Task losses:  10.3923     mean:   2.0785 \n",
      " Task losses:  10.2979     mean:   2.0596 \n",
      " Task losses:  10.3851     mean:   2.0770 \n",
      " Task losses:  10.7961     mean:   2.1592 \n",
      " Task losses:  10.3418     mean:   2.0684 \n",
      " Task losses:  10.3595     mean:   2.0719 \n",
      " Task losses:  10.3838     mean:   2.0768 \n",
      " Task losses:  10.2600     mean:   2.0520 \n",
      " Task losses:  10.0701     mean:   2.0140 \n",
      " Task losses:  10.2448     mean:   2.0490 \n",
      " Task losses:   9.8729     mean:   1.9746 \n",
      " Task losses:  10.3670     mean:   2.0734 \n",
      " Task losses:  10.3302     mean:   2.0660 \n",
      " Task losses:  10.3073     mean:   2.0615 \n",
      " Task losses:  10.2292     mean:   2.0458 \n",
      " Task losses:  10.2646     mean:   2.0529 \n",
      " Task losses:  10.4509     mean:   2.0902 \n",
      " Task losses:  10.5556     mean:   2.1111 \n",
      " Task losses:  10.1418     mean:   2.0284 \n",
      " Task losses:  10.2119     mean:   2.0424 \n",
      " Task losses:  10.2162     mean:   2.0432 \n",
      " Task losses:  10.2735     mean:   2.0547 \n",
      " Task losses:  10.1667     mean:   2.0333 \n",
      " Task losses:  10.1648     mean:   2.0330 \n",
      " Task losses:  10.4644     mean:   2.0929 \n",
      " Task losses:  10.1947     mean:   2.0389 \n",
      " Task losses:  10.2318     mean:   2.0464 \n",
      " Task losses:  10.3834     mean:   2.0767 \n",
      " Task losses:  10.1798     mean:   2.0360 \n",
      " Task losses:  10.3596     mean:   2.0719 \n",
      " Task losses:  10.5529     mean:   2.1106 \n",
      " Task losses:  10.2770     mean:   2.0554 \n",
      " Task losses:  10.4045     mean:   2.0809 \n",
      " Task losses:  10.4675     mean:   2.0935 \n",
      " Task losses:   9.7833     mean:   1.9567 \n",
      " Task losses:  10.1686     mean:   2.0337 \n",
      " Task losses:  10.1724     mean:   2.0345 \n",
      " Task losses:  10.1953     mean:   2.0391 \n",
      " Task losses:  10.1350     mean:   2.0270 \n",
      " Task losses:  10.4421     mean:   2.0884 \n",
      " Task losses:  10.3896     mean:   2.0779 \n",
      " Task losses:  10.1641     mean:   2.0328 \n",
      " Task losses:  10.6459     mean:   2.1292 \n",
      " Task losses:  10.0196     mean:   2.0039 \n",
      " Task losses:  10.2056     mean:   2.0411 \n",
      " Task losses:   9.6871     mean:   1.9374 \n",
      " Task losses:  10.1068     mean:   2.0214 \n"
     ]
    }
   ],
   "source": [
    "##---------------------------------------------------------------     \n",
    "## part one: warm up\n",
    "##---------------------------------------------------------------\n",
    "num_prints = 0\n",
    "print(f\" Last iteration: {current_iter}  # of warm-up iterations to do:{opt['train']['warm_up_iters']} - Run  from {current_iter+1} to {stop_iter}\")\n",
    "t0 = time.time()\n",
    "\n",
    "with trange(current_iter+1, stop_iter+1 , initial = current_iter, total = stop_iter, position=0, leave= True, desc=\"training\") as t_warmup :\n",
    "    \n",
    "    for current_iter in t_warmup:\n",
    "        start_time = time.time()\n",
    "        environ.train()\n",
    "        batch = next(train_loader)    \n",
    "    \n",
    "#         print_heading(f\" {timestring()} - WARMUP Training iter {current_iter}/{opt['train']['warm_up_iters']} \", verbose = False)\n",
    "\n",
    "        environ.set_inputs(batch, train_loader.dataset.input_size)\n",
    "\n",
    "        environ.optimize(opt['lambdas'], \n",
    "                         is_policy=False, \n",
    "                         flag='update_w', \n",
    "                         verbose = False)\n",
    "        \n",
    "        t_warmup.set_postfix({'curr_iter':current_iter, \n",
    "                              'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                              'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "\n",
    "        if should(current_iter, opt['train']['print_freq']):\n",
    "#             environ.print_loss(current_iter, start_time, verbose = False)\n",
    "            environ.print_loss(current_iter, start_time, title = f\"[c]Warmup training : {curr_epoch} iteration:\", verbose = True)\n",
    "        ##--------------------------------------------------------------- \n",
    "        # validation\n",
    "        ##--------------------------------------------------------------- \n",
    "        if should(current_iter, opt['train']['val_freq']):\n",
    "            environ.print_loss(current_iter, start_time, title = f\"[e]Weight training epoch:{curr_epoch} iteration:\", verbose = True)\n",
    "#             print_dbg(f\"**  {timestring()}  START VALIDATION iteration: {current_iter}    Validation freq {opt['train']['val_freq']}\") \n",
    "\n",
    "            num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "            val_metrics = environ.evaluate(\n",
    "                                   val_loader, \n",
    "                                   opt['tasks'], \n",
    "                                   is_policy=False, \n",
    "                                   num_train_layers=None,\n",
    "                                   eval_iter = eval_iter, \n",
    "                                   progress=True,\n",
    "                                   leave = False,\n",
    "                                   verbose = False)\n",
    "\n",
    "            environ.print_metrics(current_iter, start_time, title='validation')\n",
    "            environ.save_checkpoint('warmup', current_iter)\n",
    "            \n",
    "            print_metrics_cr(current_iter, time.time() - t0, None, environ.val_metrics, num_prints)\n",
    "            num_prints += 1            \n",
    "            t0 = time.time()\n",
    "            print()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ceb75",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Post Warm-up Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6605cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:09:28.974620Z",
     "start_time": "2022-01-25T23:09:28.950076Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "[0.01, 0.01]\n"
     ]
    }
   ],
   "source": [
    "print(environ.optimizers['weights'])\n",
    "print(environ.schedulers['weights'].get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3c7b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T22:56:40.881032Z",
     "start_time": "2022-01-15T22:56:40.524189Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses.keys      :  dict_keys(['parms', 'task1', 'task2', 'task3'])\n",
      "losses[task]keys :  dict_keys(['cls_loss', 'cls_loss_mean'])\n",
      "{   'parms': {'gumbel_temp': 5, 'lr_0': 0.0001, 'lr_1': 0.0001},\n",
      "    'task1': {   'cls_loss': tensor(3.5276, device='cuda:0', dtype=torch.float64),\n",
      "                 'cls_loss_mean': tensor(0.7055, device='cuda:0', dtype=torch.float64)},\n",
      "    'task2': {   'cls_loss': tensor(2.6970, device='cuda:0', dtype=torch.float64),\n",
      "                 'cls_loss_mean': tensor(0.5394, device='cuda:0', dtype=torch.float64)},\n",
      "    'task3': {   'cls_loss': tensor(4.1007, device='cuda:0', dtype=torch.float64),\n",
      "                 'cls_loss_mean': tensor(0.8201, device='cuda:0', dtype=torch.float64)}}\n"
     ]
    }
   ],
   "source": [
    "print('losses.keys      : ', environ.losses.keys())\n",
    "print('losses[task]keys : ', environ.losses['task1'].keys())\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d4b8e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T22:57:02.151169Z",
     "start_time": "2022-01-15T22:57:02.056562Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'loss_mean', 'task1', 'task2', 'task3', 'aggregated', 'train_time', 'epoch'])\n",
      "<class 'dict'>\n",
      "\n",
      "<class 'dict'>\n",
      "\n",
      "{   'aggregated': {   'auc_pr': 0.8163426647572675,\n",
      "                      'avg_prec_score': 0.8164138615123665,\n",
      "                      'bceloss': 0.7729340473810831,\n",
      "                      'f1_max': 0.7574111413660235,\n",
      "                      'kappa': 0.4593877004350045,\n",
      "                      'kappa_max': 0.4722816344851592,\n",
      "                      'logloss': tensor(0.0002, device='cuda:0', dtype=torch.float64),\n",
      "                      'p_f1_max': 0.1918067594369252,\n",
      "                      'p_kappa_max': 0.545021508137385,\n",
      "                      'roc_auc_score': 0.8101998985724014,\n",
      "                      'sc_loss': tensor(0.3217, device='cuda:0', dtype=torch.float64)},\n",
      "    'epoch': 4000,\n",
      "    'loss': {   'task1': 3.990690022259455,\n",
      "                'task2': 3.7726694706664947,\n",
      "                'task3': 3.817710672775008,\n",
      "                'total': 11.581070165700957},\n",
      "    'loss_mean': {   'task1': 0.7981380044518911,\n",
      "                     'task2': 0.7545338941332989,\n",
      "                     'task3': 0.7635421345550015,\n",
      "                     'total': 2.316214033140191},\n",
      "    'task1': {   'classification':       roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max     kappa  \\\n",
      "task                                                                          \n",
      "0          0.840926  0.819320        0.819383  0.744605  0.188466  0.505203   \n",
      "1          0.817256  0.785624        0.785697  0.706325  0.234966  0.461667   \n",
      "2          0.821726  0.819394        0.819462  0.768309  0.103616  0.482997   \n",
      "3          0.772370  0.766114        0.766202  0.713072  0.197119  0.414875   \n",
      "4          0.785095  0.803825        0.803897  0.735945  0.241081  0.416901   \n",
      "\n",
      "      kappa_max  p_kappa_max   bceloss  \n",
      "task                                    \n",
      "0      0.524762     0.693089  0.702767  \n",
      "1      0.474705     0.729373  0.757761  \n",
      "2      0.496770     0.312012  0.779951  \n",
      "3      0.423936     0.581922  0.935489  \n",
      "4      0.427422     0.811243  0.819495  ,\n",
      "                 'classification_agg': {   'auc_pr': 0.798855508429197,\n",
      "                                           'avg_prec_score': 0.7989281568935325,\n",
      "                                           'bceloss': 0.799092447757721,\n",
      "                                           'f1_max': 0.7336512000971758,\n",
      "                                           'kappa': 0.4563288316147501,\n",
      "                                           'kappa_max': 0.46951913050242283,\n",
      "                                           'logloss': 0.006299707993919771,\n",
      "                                           'p_f1_max': 0.19304971992969516,\n",
      "                                           'p_kappa_max': 0.6255278527736664,\n",
      "                                           'roc_auc_score': 0.807474721566956,\n",
      "                                           'sc_loss': 3.990690022259455}},\n",
      "    'task2': {   'classification':       roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max     kappa  \\\n",
      "task                                                                          \n",
      "0          0.833828  0.843551        0.843636  0.786776  0.286642  0.512757   \n",
      "1          0.784282  0.798552        0.798620  0.756201  0.065474  0.405454   \n",
      "2          0.794011  0.798233        0.798299  0.733455  0.167343  0.431454   \n",
      "3          0.839064  0.860586        0.860625  0.781065  0.199097  0.508344   \n",
      "4          0.841909  0.844915        0.844966  0.774130  0.124443  0.519265   \n",
      "\n",
      "      kappa_max  p_kappa_max   bceloss  \n",
      "task                                    \n",
      "0      0.518114     0.426390  0.753670  \n",
      "1      0.422608     0.354472  0.840536  \n",
      "2      0.436441     0.481076  0.776165  \n",
      "3      0.518856     0.430450  0.718333  \n",
      "4      0.526225     0.478729  0.695051  ,\n",
      "                 'classification_agg': {   'auc_pr': 0.8291675538480338,\n",
      "                                           'avg_prec_score': 0.8292292868467086,\n",
      "                                           'bceloss': 0.7567508101463318,\n",
      "                                           'f1_max': 0.7663254180716083,\n",
      "                                           'kappa': 0.4754547147763093,\n",
      "                                           'kappa_max': 0.48444883865167504,\n",
      "                                           'logloss': 0.005955540493049498,\n",
      "                                           'p_f1_max': 0.16859976947307587,\n",
      "                                           'p_kappa_max': 0.4342232346534729,\n",
      "                                           'roc_auc_score': 0.8186188467896192,\n",
      "                                           'sc_loss': 3.7726694706664947}},\n",
      "    'task3': {   'classification':       roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max     kappa  \\\n",
      "task                                                                          \n",
      "0          0.789143  0.818832        0.818915  0.795256  0.131201  0.430588   \n",
      "1          0.773360  0.795295        0.795367  0.746918  0.344915  0.380806   \n",
      "2          0.863028  0.864169        0.864209  0.780428  0.294887  0.545351   \n",
      "3          0.807607  0.821005        0.821136  0.791405  0.165901  0.457213   \n",
      "4          0.789393  0.805724        0.805793  0.747277  0.131950  0.417939   \n",
      "\n",
      "      kappa_max  p_kappa_max   bceloss  \n",
      "task                                    \n",
      "0      0.437650     0.549203  0.781711  \n",
      "1      0.414754     0.814326  0.928911  \n",
      "2      0.565829     0.637907  0.605613  \n",
      "3      0.469116     0.303002  0.703193  \n",
      "4      0.427037     0.572130  0.795366  ,\n",
      "                 'classification_agg': {   'auc_pr': 0.8210049319945718,\n",
      "                                           'avg_prec_score': 0.8210841407968588,\n",
      "                                           'bceloss': 0.7629588842391969,\n",
      "                                           'f1_max': 0.7722568059292869,\n",
      "                                           'kappa': 0.4463795549139541,\n",
      "                                           'kappa_max': 0.46287693430137955,\n",
      "                                           'logloss': 0.006026642588024569,\n",
      "                                           'p_f1_max': 0.21377078890800477,\n",
      "                                           'p_kappa_max': 0.5753134369850159,\n",
      "                                           'roc_auc_score': 0.8045061273606293,\n",
      "                                           'sc_loss': 3.817710672775008}},\n",
      "    'train_time': 47.46593999862671}\n"
     ]
    }
   ],
   "source": [
    "print( environ.val_metrics.keys())\n",
    "# pp.pprint(val_metrics)\n",
    "print(type(environ.val_metrics['aggregated']))\n",
    "print()\n",
    "print(type(environ.val_metrics['task1']['classification_agg']))\n",
    "print()\n",
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce42477c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.266303Z",
     "start_time": "2022-01-14T18:57:26.166878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"val_metrics.pkl\", mode= 'wb') as f:\n",
    "#         pickle.dump(val_metrics, f)\n",
    "    \n",
    "# with open('val_metrics.pkl', 'rb') as f:    \n",
    "#     tst_val_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "797b6487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.297444Z",
     "start_time": "2022-01-14T18:57:26.269323Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(environ.input.shape) \n",
    "# a = getattr(environ, 'task1_pred')\n",
    "# yc_data = environ.batch['task1_data']\n",
    "# print(yc_data.shape)\n",
    "# yc_ind = environ.batch['task1_ind']\n",
    "# print(yc_ind.shape)\n",
    "# yc_hat_all = getattr(environ, 'task1_pred')\n",
    "# print(yc_hat_all.shape)\n",
    "# yc_hat  = yc_hat_all[yc_ind[0], yc_ind[1]]\n",
    "# print(yc_hat_all.shape, yc_hat.shape)\n",
    "\n",
    "# \n",
    "# environ.losses\n",
    "# loss = {}\n",
    "# for key in environ.losses.keys():\n",
    "#     loss[key] = {}\n",
    "#     for subkey, v in environ.losses[key].items():\n",
    "#         print(f\" key:  {key}   subkey: {subkey} \")\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             loss[key][subkey] = v.data\n",
    "#             print(f\" Tensor  -  key:  {key}   subkey: {subkey}           value type: {type(v)}  value: {v:.4f}\")\n",
    "#         else:\n",
    "#             loss[key][subkey] = v\n",
    "#             print(f\" integer -  key:  {key}   subkey: {subkey}           value type: {type(v)}  value: {v:.4f}\")\n",
    "# pp.pprint(tst_val_metrics)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2d6ab42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.340792Z",
     "start_time": "2022-01-14T18:57:26.302528Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print('metrics.keys: ', environ.metrics.keys())\n",
    "# print('metrics[task].keys: ', environ.metrics['task1'].keys())\n",
    "# pp.pprint(environ.metrics['task1'])\n",
    "# pp.pprint(environ.losses['task1']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ddcaed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.373399Z",
     "start_time": "2022-01-14T18:57:26.345065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# title='Iteration'\n",
    "# for t_id, _ in enumerate(environ.tasks):\n",
    "#     task_key = f\"task{t_id+1}\"\n",
    "# #     print_heading(f\"{title}  {current_iter}  {task_key} : {val_metrics[task_key]['classification_agg']}\", verbose = True)\n",
    "\n",
    "#     for key, _  in val_metrics[task_key]['classification_agg'].items():\n",
    "#         print('%s/%-20s'%(task_key, key), val_metrics[task_key]['classification_agg'][key], current_iter)\n",
    "#         print(f\"{task_key:s}/{key:20s}\", val_metrics[task_key]['classification_agg'][key], current_iter)\n",
    "#         print()\n",
    "#             # print_current_errors(os.path.join(self.log_dir, 'loss.txt'), current_iter,key, loss[key], time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a669b0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:52:48.162261Z",
     "start_time": "2022-01-07T22:52:48.140423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.print_loss(current_iter, start_time, metrics = val_metrics['loss'], verbose=True)\n",
    "# print(opt['lambdas'])\n",
    "# p = (opt['lambdas'][0] * environ.losses['tasks']['task1'])\n",
    "# print(p)\n",
    "\n",
    "# environ.print_metrics(current_iter, start_time, val_metrics , title='validation', verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e3de0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:52:47.661019Z",
     "start_time": "2022-01-07T22:52:47.639094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter)\n",
    "# print_metrics_cr(current_iter, t1 - t0, None, val_metrics , True)\n",
    "# environ.print_metrics(current_iter, start_time, val_metrics, title='validation', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da239bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T23:14:58.034384Z",
     "start_time": "2022-01-07T23:14:58.004850Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17988/3181484359.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" val_metric keys               : {val_metrics.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" loss keys                     : {val_metrics['loss'].keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" task1 keys                    : {val_metrics['task1'].keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" task1 classification keys     : {val_metrics['task1']['classification'].keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" task1 classification_agg keys : {val_metrics['task1']['classification_agg'].keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "print(f\" val_metric keys               : {val_metrics.keys()}\")\n",
    "print(f\" loss keys                     : {val_metrics['loss'].keys()}\")\n",
    "print(f\" task1 keys                    : {val_metrics['task1'].keys()}\")\n",
    "print(f\" task1 classification keys     : {val_metrics['task1']['classification'].keys()}\")\n",
    "print(f\" task1 classification_agg keys : {val_metrics['task1']['classification_agg'].keys()}\")\n",
    "print()\n",
    "print(f\" task1                       : {val_metrics['task1']['classification_agg']['loss']:5f}\")\n",
    "print(f\" task2                       : {val_metrics['task2']['classification_agg']['loss']:5f}\")\n",
    "print(f\" task3                       : {val_metrics['task3']['classification_agg']['loss']:5f}\")\n",
    "print(f\" loss                        : {val_metrics['loss']['total']:5f}\")\n",
    "print(f\" train_time                  : {val_metrics['train_time']:2f}\")\n",
    "print(f\" epoch                       : {val_metrics['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49645c",
   "metadata": {},
   "source": [
    "## Weight & Policy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "443db2fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:34:49.652631Z",
     "start_time": "2022-01-26T15:34:49.298707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'alphas': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      "),\n",
      "    'weights': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0001\n",
      ")}\n",
      "<torch.optim.lr_scheduler.StepLR object at 0x7fa368890ac0>\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "** 2022-01-26 07:34:49:324696 - Training current iteration 100  warmup iters: 100   flag: update_w \n",
      "------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------\n",
      "** Set optimizer and scheduler to policy_learning = True\n",
      "------------------------------------------------------------ \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "** Switch from Warm Up training to Alternate training Weights & Policy \n",
      "   Take checkpoint and block gradient flow through Policy net\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(environ.optimizers)\n",
    "# pp.pprint(environ.schedulers['weights'])\n",
    "\n",
    "print_heading(f\"** {timestring()} - Training current iteration {current_iter}  warmup iters: {opt['train']['warm_up_iters']}   flag: {flag} \", verbose = True)    \n",
    "\n",
    "if flag_warmup:\n",
    "    print_heading(f\"** Set optimizer and scheduler to policy_learning = True\", verbose = True)\n",
    "    environ.define_optimizer(policy_learning=True)\n",
    "    environ.define_scheduler(policy_learning=True)\n",
    "    flag_warmup = False\n",
    "\n",
    "if current_iter == opt['train']['warm_up_iters']:\n",
    "    print_heading(f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "                  f\"   Take checkpoint and block gradient flow through Policy net\", verbose=True)\n",
    "    environ.save_checkpoint('warmup', current_iter)\n",
    "    environ.fix_alpha()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea8b8473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:35:38.115760Z",
     "start_time": "2022-01-26T15:35:38.067560Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:16.084885Z",
     "iopub.status.busy": "2022-01-07T22:49:16.084628Z",
     "iopub.status.idle": "2022-01-07T22:49:16.119033Z",
     "shell.execute_reply": "2022-01-07T22:49:16.117523Z",
     "shell.execute_reply.started": "2022-01-07T22:49:16.084860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_iter_w :                      108\n",
      "stop_iter_a :                      108\n",
      "\n",
      "weight_iter_alternate:             1\n",
      "alpha_iter_alternate :             1\n",
      "\n",
      "opt['train']['print_freq']         108\n",
      "opt['train']['hard_sampling']      False\n",
      "opt['policy']                      True\n",
      "opt['tasks']                       ['class', 'class', 'class']\n",
      "opt['fix_BN']                      False\n",
      "\n",
      "total_iters                        25000\n",
      "current_iter                       100\n",
      "\n",
      "current_iter_w                     0\n",
      "current_iter_a                     0\n",
      "batch_idx_w                        0\n",
      "\n",
      "curriculum_speed                   3\n",
      "curr_epochs                        0\n",
      "train_total_epochs                 10\n",
      "\n",
      "flag                               update_w\n"
     ]
    }
   ],
   "source": [
    "# batch_enumerator1 = enumerate(train1_loader,1)  \n",
    "# batch_enumerator2 = enumerate(train2_loader,1)  \n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0 \n",
    "batch_idx_a = 0 \n",
    "batch_idx_w = 0 \n",
    "curr_epoch  = 0 \n",
    "num_prints  = 0\n",
    "train_total_epochs = 10\n",
    "curriculum_speed = opt['curriculum_speed']  \n",
    "\n",
    "stop_iter_w = len(train_loader)\n",
    "stop_iter_a = len(train_loader)\n",
    "\n",
    "print(f\"stop_iter_w :                      {stop_iter_w}\")\n",
    "print(f\"stop_iter_a :                      {stop_iter_a}\")\n",
    "print()         \n",
    "print(f\"weight_iter_alternate:             {opt['train']['weight_iter_alternate']}\")\n",
    "print(f\"alpha_iter_alternate :             {opt['train']['alpha_iter_alternate']}\")\n",
    "print()\n",
    "print(f\"opt['train']['print_freq']         {opt['train']['print_freq']}\")\n",
    "print(f\"opt['train']['hard_sampling']      {opt['train']['hard_sampling']}\")\n",
    "print(f\"opt['policy']                      {opt['policy']}\")\n",
    "print(f\"opt['tasks']                       {opt['tasks']}\")\n",
    "print(f\"opt['fix_BN']                      {opt['fix_BN']}\" )\n",
    "print()\n",
    "print(f\"total_iters                        {opt['train']['total_iters']}\")  \n",
    "print(f\"current_iter                       {current_iter  }\")\n",
    "print()\n",
    "print(f\"current_iter_w                     {current_iter_w}\")\n",
    "print(f\"current_iter_a                     {current_iter_a}\")\n",
    "print(f\"batch_idx_w                        {batch_idx_w}\")\n",
    "print()\n",
    "print(f\"curriculum_speed                   {curriculum_speed}\")\n",
    "print(f\"curr_epochs                        {curr_epoch}\") \n",
    "print(f\"train_total_epochs                 {train_total_epochs}\") \n",
    "print()\n",
    "print(f\"flag                               {flag          }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd50bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:37:25.204838Z",
     "start_time": "2022-01-26T00:37:25.183257Z"
    }
   },
   "outputs": [],
   "source": [
    "# curr_epoch = 0\n",
    "# train_total_epochs = 55\n",
    "# train_total_epochs = 40\n",
    "\n",
    "# curr_epoch = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaef0d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T15:35:42.353615Z",
     "start_time": "2022-01-26T15:35:42.329293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 108\n",
      "0 108\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(current_iter_a, stop_iter_a)\n",
    "print(current_iter_w, stop_iter_w)\n",
    "print(opt['policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8c7bb5e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:00:55.811193Z",
     "start_time": "2022-01-27T01:00:55.784547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_iters         : 19540\n",
      "curr_epochs           : 90\n",
      "train_total_epochs    : 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"curr_epochs           : {curr_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "05e7df82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:00:57.237838Z",
     "start_time": "2022-01-27T01:00:57.213194Z"
    }
   },
   "outputs": [],
   "source": [
    "train_total_epochs += 30\n",
    "# train_total_epochs += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dcbfcbc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:00:58.173163Z",
     "start_time": "2022-01-27T01:00:58.146139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_iters         : 19540\n",
      "curr_epochs           : 90\n",
      "train_total_epochs    : 120\n"
     ]
    }
   ],
   "source": [
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"curr_epochs           : {curr_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2bca2972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:01:00.270284Z",
     "start_time": "2022-01-27T01:01:00.203007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59292cf83d0b44918bed85bc415a4c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Alternate Weight/Policy training:  75%|#######5  | 90/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_prints = 0 \n",
    "verbose = False\n",
    "t = tqdm(initial = curr_epoch, total=train_total_epochs, desc=f\" Alternate Weight/Policy training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dcd6a699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:11:08.763361Z",
     "start_time": "2022-01-27T01:01:02.124747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 91 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:91 iteration:  19648 -  Total Loss: 4.0531     Task Loss: 4.0531  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | logloss bceloss  aucroc   aucpr  f1_max| t1 loss t2 loss t3 lossttl loss|tr_time|\n",
      "91    | 0.00018 0.81026 0.78620 0.78616 0.74446|  3.7568  3.8431  4.5338 12.1337|   17.9|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 91 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.4930     mean:   0.6986     Sharing: 5.28842e-05     Sparsity: 8.59965e-02     Total:   3.5790     mean:   0.7846\n",
      " Task losses:   8.1971     mean:   1.6394     Sharing: 6.96386e-03     Sparsity: 8.59097e-02     Total:   8.2900     mean:   1.7323\n",
      " Task losses:  10.8667     mean:   2.1733     Sharing: 9.13756e-03     Sparsity: 8.58560e-02     Total:  10.9617     mean:   2.2683\n",
      " Task losses:  12.5174     mean:   2.5035     Sharing: 7.44592e-03     Sparsity: 8.58332e-02     Total:  12.6107     mean:   2.5968\n",
      " Task losses:  14.6937     mean:   2.9387     Sharing: 7.15319e-03     Sparsity: 8.58324e-02     Total:  14.7867     mean:   3.0317\n",
      " Task losses:   9.9347     mean:   1.9869     Sharing: 4.90936e-03     Sparsity: 8.58311e-02     Total:  10.0254     mean:   2.0777\n",
      " Task losses:   4.2904     mean:   0.8581     Sharing: 4.70061e-03     Sparsity: 8.58262e-02     Total:   4.3810     mean:   0.9486\n",
      " Task losses:   4.0279     mean:   0.8056     Sharing: 5.64806e-03     Sparsity: 8.58212e-02     Total:   4.1194     mean:   0.8971\n",
      " Task losses:   4.6846     mean:   0.9369     Sharing: 6.18102e-03     Sparsity: 8.58151e-02     Total:   4.7766     mean:   1.0289\n",
      " Task losses:   7.6017     mean:   1.5203     Sharing: 6.09926e-03     Sparsity: 8.58073e-02     Total:   7.6936     mean:   1.6123\n",
      " Task losses:   4.6767     mean:   0.9353     Sharing: 5.90588e-03     Sparsity: 8.58025e-02     Total:   4.7684     mean:   1.0270\n",
      " Task losses:   6.3100     mean:   1.2620     Sharing: 4.54632e-03     Sparsity: 8.57996e-02     Total:   6.4004     mean:   1.3524\n",
      " Task losses:   4.6053     mean:   0.9211     Sharing: 2.43454e-03     Sparsity: 8.57958e-02     Total:   4.6935     mean:   1.0093\n",
      " Task losses:   3.3887     mean:   0.6777     Sharing: 2.08416e-03     Sparsity: 8.57906e-02     Total:   3.4766     mean:   0.7656\n",
      " Task losses:   3.1434     mean:   0.6287     Sharing: 2.97469e-03     Sparsity: 8.57846e-02     Total:   3.2322     mean:   0.7174\n",
      " Task losses:   2.9115     mean:   0.5823     Sharing: 3.56257e-03     Sparsity: 8.57773e-02     Total:   3.0008     mean:   0.6716\n",
      " Task losses:   4.5081     mean:   0.9016     Sharing: 3.28253e-03     Sparsity: 8.57688e-02     Total:   4.5971     mean:   0.9907\n",
      " Task losses:  11.1392     mean:   2.2278     Sharing: 2.54023e-03     Sparsity: 8.57613e-02     Total:  11.2275     mean:   2.3161\n",
      " Task losses:  10.9861     mean:   2.1972     Sharing: 1.52176e-03     Sparsity: 8.57580e-02     Total:  11.0733     mean:   2.2845\n",
      " Task losses:   3.4201     mean:   0.6840     Sharing: 1.77861e-03     Sparsity: 8.57520e-02     Total:   3.5076     mean:   0.7716\n",
      " Task losses:   2.7746     mean:   0.5549     Sharing: 2.16520e-03     Sparsity: 8.57456e-02     Total:   2.8625     mean:   0.6428\n",
      " Task losses:   3.2168     mean:   0.6434     Sharing: 2.14338e-03     Sparsity: 8.57406e-02     Total:   3.3047     mean:   0.7312\n",
      " Task losses:   3.2977     mean:   0.6595     Sharing: 1.71276e-03     Sparsity: 8.57348e-02     Total:   3.3852     mean:   0.7470\n",
      " Task losses:   4.5832     mean:   0.9166     Sharing: 1.62208e-03     Sparsity: 8.57285e-02     Total:   4.6706     mean:   1.0040\n",
      " Task losses:   5.1328     mean:   1.0266     Sharing: 1.73626e-03     Sparsity: 8.57220e-02     Total:   5.2202     mean:   1.1140\n",
      " Task losses:   3.6312     mean:   0.7262     Sharing: 1.91051e-03     Sparsity: 8.57132e-02     Total:   3.7188     mean:   0.8139\n",
      " Task losses:   7.1446     mean:   1.4289     Sharing: 1.69081e-03     Sparsity: 8.57024e-02     Total:   7.2320     mean:   1.5163\n",
      " Task losses:   9.5146     mean:   1.9029     Sharing: 1.36552e-03     Sparsity: 8.56872e-02     Total:   9.6017     mean:   1.9900\n",
      " Task losses:  11.9460     mean:   2.3892     Sharing: 1.03576e-03     Sparsity: 8.56681e-02     Total:  12.0327     mean:   2.4759\n",
      " Task losses:   7.9199     mean:   1.5840     Sharing: 9.22769e-04     Sparsity: 8.56475e-02     Total:   8.0064     mean:   1.6705\n",
      " Task losses:   4.9157     mean:   0.9831     Sharing: 1.09554e-03     Sparsity: 8.56259e-02     Total:   5.0024     mean:   1.0699\n",
      " Task losses:   4.4505     mean:   0.8901     Sharing: 9.73493e-04     Sparsity: 8.56049e-02     Total:   4.5370     mean:   0.9767\n",
      " Task losses:   4.7572     mean:   0.9514     Sharing: 8.89440e-04     Sparsity: 8.55827e-02     Total:   4.8436     mean:   1.0379\n",
      " Task losses:   7.4441     mean:   1.4888     Sharing: 1.13331e-03     Sparsity: 8.55607e-02     Total:   7.5308     mean:   1.5755\n",
      " Task losses:  13.3528     mean:   2.6706     Sharing: 1.21641e-03     Sparsity: 8.55355e-02     Total:  13.4396     mean:   2.7573\n",
      " Task losses:   4.0601     mean:   0.8120     Sharing: 1.20947e-03     Sparsity: 8.55192e-02     Total:   4.1468     mean:   0.8988\n",
      " Task losses:   4.5524     mean:   0.9105     Sharing: 1.20191e-03     Sparsity: 8.55034e-02     Total:   4.6391     mean:   0.9972\n",
      " Task losses:   6.4048     mean:   1.2810     Sharing: 1.00863e-03     Sparsity: 8.54878e-02     Total:   6.4913     mean:   1.3675\n",
      " Task losses:   6.9028     mean:   1.3806     Sharing: 9.73831e-04     Sparsity: 8.54744e-02     Total:   6.9892     mean:   1.4670\n",
      " Task losses:   3.0126     mean:   0.6025     Sharing: 1.18495e-03     Sparsity: 8.54642e-02     Total:   3.0993     mean:   0.6892\n",
      " Task losses:   4.0673     mean:   0.8135     Sharing: 1.09447e-03     Sparsity: 8.54539e-02     Total:   4.1539     mean:   0.9000\n",
      " Task losses:   6.0961     mean:   1.2192     Sharing: 1.01993e-03     Sparsity: 8.54440e-02     Total:   6.1826     mean:   1.3057\n",
      " Task losses:   5.5279     mean:   1.1056     Sharing: 8.86410e-04     Sparsity: 8.54337e-02     Total:   5.6142     mean:   1.1919\n",
      " Task losses:   4.4455     mean:   0.8891     Sharing: 9.68546e-04     Sparsity: 8.54228e-02     Total:   4.5319     mean:   0.9755\n",
      " Task losses:   4.3721     mean:   0.8744     Sharing: 1.27167e-03     Sparsity: 8.54123e-02     Total:   4.4588     mean:   0.9611\n",
      " Task losses:   5.5010     mean:   1.1002     Sharing: 1.15238e-03     Sparsity: 8.54026e-02     Total:   5.5875     mean:   1.1868\n",
      " Task losses:   4.7961     mean:   0.9592     Sharing: 8.62663e-04     Sparsity: 8.53932e-02     Total:   4.8824     mean:   1.0455\n",
      " Task losses:   5.6103     mean:   1.1221     Sharing: 8.76114e-04     Sparsity: 8.53838e-02     Total:   5.6966     mean:   1.2083\n",
      " Task losses:   4.4190     mean:   0.8838     Sharing: 1.11398e-03     Sparsity: 8.53754e-02     Total:   4.5055     mean:   0.9703\n",
      " Task losses:   4.4891     mean:   0.8978     Sharing: 1.42699e-03     Sparsity: 8.53669e-02     Total:   4.5759     mean:   0.9846\n",
      " Task losses:   4.9584     mean:   0.9917     Sharing: 1.24205e-03     Sparsity: 8.53587e-02     Total:   5.0450     mean:   1.0783\n",
      " Task losses:   5.3217     mean:   1.0643     Sharing: 7.11511e-04     Sparsity: 8.53525e-02     Total:   5.4077     mean:   1.1504\n",
      " Task losses:   4.3561     mean:   0.8712     Sharing: 7.89603e-04     Sparsity: 8.53454e-02     Total:   4.4422     mean:   0.9573\n",
      " Task losses:   4.2135     mean:   0.8427     Sharing: 1.34771e-03     Sparsity: 8.53368e-02     Total:   4.3001     mean:   0.9294\n",
      " Task losses:   8.0061     mean:   1.6012     Sharing: 1.16488e-03     Sparsity: 8.53278e-02     Total:   8.0926     mean:   1.6877\n",
      " Task losses:   5.7181     mean:   1.1436     Sharing: 1.05872e-03     Sparsity: 8.53179e-02     Total:   5.8045     mean:   1.2300\n",
      " Task losses:   3.3008     mean:   0.6602     Sharing: 1.21594e-03     Sparsity: 8.53082e-02     Total:   3.3873     mean:   0.7467\n",
      " Task losses:   3.8344     mean:   0.7669     Sharing: 1.10662e-03     Sparsity: 8.52985e-02     Total:   3.9208     mean:   0.8533\n",
      " Task losses:   3.7969     mean:   0.7594     Sharing: 1.10290e-03     Sparsity: 8.52877e-02     Total:   3.8833     mean:   0.8458\n",
      " Task losses:   3.1458     mean:   0.6292     Sharing: 1.12774e-03     Sparsity: 8.52762e-02     Total:   3.2322     mean:   0.7156\n",
      " Task losses:   3.4822     mean:   0.6964     Sharing: 9.65690e-04     Sparsity: 8.52651e-02     Total:   3.5684     mean:   0.7827\n",
      " Task losses:   3.1607     mean:   0.6321     Sharing: 1.04906e-03     Sparsity: 8.52549e-02     Total:   3.2470     mean:   0.7184\n",
      " Task losses:   3.2073     mean:   0.6415     Sharing: 8.72989e-04     Sparsity: 8.52449e-02     Total:   3.2934     mean:   0.7276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.3976     mean:   0.6795     Sharing: 8.25192e-04     Sparsity: 8.52355e-02     Total:   3.4836     mean:   0.7656\n",
      " Task losses:   3.9599     mean:   0.7920     Sharing: 9.42578e-04     Sparsity: 8.52253e-02     Total:   4.0460     mean:   0.8781\n",
      " Task losses:   3.0342     mean:   0.6068     Sharing: 7.77841e-04     Sparsity: 8.52170e-02     Total:   3.1202     mean:   0.6928\n",
      " Task losses:   3.4731     mean:   0.6946     Sharing: 4.68254e-04     Sparsity: 8.52085e-02     Total:   3.5587     mean:   0.7803\n",
      " Task losses:   3.5824     mean:   0.7165     Sharing: 7.29750e-04     Sparsity: 8.52003e-02     Total:   3.6683     mean:   0.8024\n",
      " Task losses:   3.6943     mean:   0.7389     Sharing: 1.16270e-03     Sparsity: 8.51955e-02     Total:   3.7807     mean:   0.8252\n",
      " Task losses:   3.8926     mean:   0.7785     Sharing: 9.03770e-04     Sparsity: 8.51912e-02     Total:   3.9787     mean:   0.8646\n",
      " Task losses:   4.3986     mean:   0.8797     Sharing: 6.52651e-04     Sparsity: 8.51864e-02     Total:   4.4845     mean:   0.9656\n",
      " Task losses:   4.1803     mean:   0.8361     Sharing: 1.24598e-03     Sparsity: 8.51817e-02     Total:   4.2667     mean:   0.9225\n",
      " Task losses:   4.0859     mean:   0.8172     Sharing: 1.49173e-03     Sparsity: 8.51768e-02     Total:   4.1726     mean:   0.9038\n",
      " Task losses:   4.4564     mean:   0.8913     Sharing: 1.28997e-03     Sparsity: 8.51717e-02     Total:   4.5429     mean:   0.9777\n",
      " Task losses:   3.1511     mean:   0.6302     Sharing: 9.98393e-04     Sparsity: 8.51691e-02     Total:   3.2373     mean:   0.7164\n",
      " Task losses:   2.6760     mean:   0.5352     Sharing: 7.25051e-04     Sparsity: 8.51656e-02     Total:   2.7619     mean:   0.6211\n",
      " Task losses:   2.6287     mean:   0.5257     Sharing: 1.18536e-03     Sparsity: 8.51616e-02     Total:   2.7151     mean:   0.6121\n",
      " Task losses:   2.9438     mean:   0.5888     Sharing: 1.57170e-03     Sparsity: 8.51570e-02     Total:   3.0305     mean:   0.6755\n",
      " Task losses:   3.2917     mean:   0.6583     Sharing: 1.40284e-03     Sparsity: 8.51517e-02     Total:   3.3783     mean:   0.7449\n",
      " Task losses:   3.1982     mean:   0.6396     Sharing: 9.95765e-04     Sparsity: 8.51494e-02     Total:   3.2843     mean:   0.7258\n",
      " Task losses:   3.3935     mean:   0.6787     Sharing: 1.01610e-03     Sparsity: 8.51462e-02     Total:   3.4797     mean:   0.7649\n",
      " Task losses:   3.4476     mean:   0.6895     Sharing: 1.15817e-03     Sparsity: 8.51428e-02     Total:   3.5339     mean:   0.7758\n",
      " Task losses:   3.7213     mean:   0.7443     Sharing: 1.24515e-03     Sparsity: 8.51390e-02     Total:   3.8077     mean:   0.8306\n",
      " Task losses:   3.6845     mean:   0.7369     Sharing: 8.88348e-04     Sparsity: 8.51347e-02     Total:   3.7705     mean:   0.8229\n",
      " Task losses:   4.1314     mean:   0.8263     Sharing: 9.40780e-04     Sparsity: 8.51300e-02     Total:   4.2175     mean:   0.9123\n",
      " Task losses:   3.3383     mean:   0.6677     Sharing: 1.05838e-03     Sparsity: 8.51255e-02     Total:   3.4245     mean:   0.7538\n",
      " Task losses:   3.6322     mean:   0.7264     Sharing: 6.80988e-04     Sparsity: 8.51233e-02     Total:   3.7180     mean:   0.8122\n",
      " Task losses:   4.1498     mean:   0.8300     Sharing: 8.33049e-04     Sparsity: 8.51204e-02     Total:   4.2358     mean:   0.9159\n",
      " Task losses:   3.7369     mean:   0.7474     Sharing: 9.82404e-04     Sparsity: 8.51209e-02     Total:   3.8230     mean:   0.8335\n",
      " Task losses:   4.1213     mean:   0.8243     Sharing: 1.17839e-03     Sparsity: 8.51201e-02     Total:   4.2076     mean:   0.9106\n",
      " Task losses:   3.5657     mean:   0.7131     Sharing: 1.02162e-03     Sparsity: 8.51193e-02     Total:   3.6519     mean:   0.7993\n",
      " Task losses:   3.3554     mean:   0.6711     Sharing: 9.05931e-04     Sparsity: 8.51171e-02     Total:   3.4415     mean:   0.7571\n",
      " Task losses:   3.9104     mean:   0.7821     Sharing: 1.14868e-03     Sparsity: 8.51163e-02     Total:   3.9967     mean:   0.8684\n",
      " Task losses:   3.0564     mean:   0.6113     Sharing: 1.28236e-03     Sparsity: 8.51150e-02     Total:   3.1428     mean:   0.6977\n",
      " Task losses:   4.0353     mean:   0.8071     Sharing: 1.04076e-03     Sparsity: 8.51142e-02     Total:   4.1214     mean:   0.8932\n",
      " Task losses:   3.9912     mean:   0.7982     Sharing: 8.81111e-04     Sparsity: 8.51146e-02     Total:   4.0772     mean:   0.8842\n",
      " Task losses:   3.1934     mean:   0.6387     Sharing: 1.18203e-03     Sparsity: 8.51185e-02     Total:   3.2797     mean:   0.7250\n",
      " Task losses:   2.9470     mean:   0.5894     Sharing: 1.17590e-03     Sparsity: 8.51209e-02     Total:   3.0333     mean:   0.6757\n",
      " Task losses:   2.7119     mean:   0.5424     Sharing: 9.10337e-04     Sparsity: 8.51218e-02     Total:   2.7979     mean:   0.6284\n",
      " Task losses:   2.6835     mean:   0.5367     Sharing: 1.04807e-03     Sparsity: 8.51215e-02     Total:   2.7697     mean:   0.6229\n",
      " Task losses:   2.3858     mean:   0.4772     Sharing: 1.26573e-03     Sparsity: 8.51201e-02     Total:   2.4722     mean:   0.5635\n",
      " Task losses:   3.7822     mean:   0.7564     Sharing: 1.10719e-03     Sparsity: 8.51177e-02     Total:   3.8685     mean:   0.8427\n",
      " Task losses:   3.2621     mean:   0.6524     Sharing: 9.07232e-04     Sparsity: 8.51148e-02     Total:   3.3481     mean:   0.7384\n",
      " Task losses:   2.9886     mean:   0.5977     Sharing: 7.88033e-04     Sparsity: 8.51112e-02     Total:   3.0745     mean:   0.6836\n",
      " Task losses:   2.4950     mean:   0.4990     Sharing: 9.12711e-04     Sparsity: 8.51071e-02     Total:   2.5811     mean:   0.5850\n",
      " Task losses:   2.6700     mean:   0.5340     Sharing: 1.24556e-03     Sparsity: 8.51020e-02     Total:   2.7563     mean:   0.6203\n",
      " Task losses:   2.5749     mean:   0.5150     Sharing: 1.19564e-03     Sparsity: 8.50962e-02     Total:   2.6612     mean:   0.6013\n",
      " Task losses:   3.4001     mean:   0.6800     Sharing: 8.87886e-04     Sparsity: 8.50897e-02     Total:   3.4861     mean:   0.7660\n",
      "[e]Policy training epoch:91 iteration:  19756 -  Total Loss: 3.4861     Task Loss: 3.4001  Policy Losses:  Sparsity: 0.0851      Sharing: 8.87886e-04 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 92 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:92 iteration:  19864 -  Total Loss: 4.4422     Task Loss: 4.4422  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92    | 0.00020 0.91082 0.78427 0.78384 0.74331|  4.3326  4.0616  5.2774 13.6716|   42.9|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 92 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.9582     mean:   0.7916     Sharing: 9.46611e-04     Sparsity: 8.50822e-02     Total:   4.0442     mean:   0.8777\n",
      " Task losses:   8.9895     mean:   1.7979     Sharing: 1.18768e-03     Sparsity: 8.50743e-02     Total:   9.0758     mean:   1.8842\n",
      " Task losses:  11.5034     mean:   2.3007     Sharing: 1.44217e-03     Sparsity: 8.50693e-02     Total:  11.5899     mean:   2.3872\n",
      " Task losses:  12.5421     mean:   2.5084     Sharing: 1.39930e-03     Sparsity: 8.50643e-02     Total:  12.6286     mean:   2.5949\n",
      " Task losses:  13.9752     mean:   2.7950     Sharing: 1.20323e-03     Sparsity: 8.50573e-02     Total:  14.0615     mean:   2.8813\n",
      " Task losses:  11.9963     mean:   2.3993     Sharing: 1.04434e-03     Sparsity: 8.50630e-02     Total:  12.0824     mean:   2.4854\n",
      " Task losses:   5.7401     mean:   1.1480     Sharing: 9.94851e-04     Sparsity: 8.50784e-02     Total:   5.8262     mean:   1.2341\n",
      " Task losses:   5.3169     mean:   1.0634     Sharing: 1.10982e-03     Sparsity: 8.50913e-02     Total:   5.4031     mean:   1.1496\n",
      " Task losses:   6.4390     mean:   1.2878     Sharing: 9.31437e-04     Sparsity: 8.51020e-02     Total:   6.5251     mean:   1.3738\n",
      " Task losses:   9.0132     mean:   1.8026     Sharing: 9.28491e-04     Sparsity: 8.51116e-02     Total:   9.0993     mean:   1.8887\n",
      " Task losses:   6.4696     mean:   1.2939     Sharing: 1.04379e-03     Sparsity: 8.51182e-02     Total:   6.5558     mean:   1.3801\n",
      " Task losses:   7.6496     mean:   1.5299     Sharing: 1.00498e-03     Sparsity: 8.51232e-02     Total:   7.7357     mean:   1.6160\n",
      " Task losses:   6.8086     mean:   1.3617     Sharing: 8.53250e-04     Sparsity: 8.51274e-02     Total:   6.8945     mean:   1.4477\n",
      " Task losses:   5.1535     mean:   1.0307     Sharing: 9.04376e-04     Sparsity: 8.51308e-02     Total:   5.2395     mean:   1.1167\n",
      " Task losses:   5.2739     mean:   1.0548     Sharing: 8.74033e-04     Sparsity: 8.51333e-02     Total:   5.3599     mean:   1.1408\n",
      " Task losses:   4.4600     mean:   0.8920     Sharing: 8.90369e-04     Sparsity: 8.51347e-02     Total:   4.5460     mean:   0.9780\n",
      " Task losses:   8.6310     mean:   1.7262     Sharing: 9.55805e-04     Sparsity: 8.51369e-02     Total:   8.7171     mean:   1.8123\n",
      " Task losses:  19.4482     mean:   3.8896     Sharing: 1.08752e-03     Sparsity: 8.51420e-02     Total:  19.5344     mean:   3.9759\n",
      " Task losses:  17.8473     mean:   3.5695     Sharing: 1.23838e-03     Sparsity: 8.51608e-02     Total:  17.9337     mean:   3.6559\n",
      " Task losses:   5.1232     mean:   1.0246     Sharing: 1.21358e-03     Sparsity: 8.51794e-02     Total:   5.2095     mean:   1.1110\n",
      " Task losses:   4.2813     mean:   0.8563     Sharing: 1.04423e-03     Sparsity: 8.51954e-02     Total:   4.3675     mean:   0.9425\n",
      " Task losses:   4.3487     mean:   0.8697     Sharing: 7.53934e-04     Sparsity: 8.52090e-02     Total:   4.4347     mean:   0.9557\n",
      " Task losses:   4.2764     mean:   0.8553     Sharing: 8.08150e-04     Sparsity: 8.52207e-02     Total:   4.3625     mean:   0.9413\n",
      " Task losses:   5.4546     mean:   1.0909     Sharing: 7.57893e-04     Sparsity: 8.52306e-02     Total:   5.5406     mean:   1.1769\n",
      " Task losses:   5.7743     mean:   1.1549     Sharing: 7.68339e-04     Sparsity: 8.52388e-02     Total:   5.8603     mean:   1.2409\n",
      " Task losses:   4.2342     mean:   0.8468     Sharing: 6.17251e-04     Sparsity: 8.52454e-02     Total:   4.3201     mean:   0.9327\n",
      " Task losses:   6.4931     mean:   1.2986     Sharing: 8.25817e-04     Sparsity: 8.52502e-02     Total:   6.5791     mean:   1.3847\n",
      " Task losses:   8.5889     mean:   1.7178     Sharing: 7.60491e-04     Sparsity: 8.52531e-02     Total:   8.6749     mean:   1.8038\n",
      " Task losses:   9.7996     mean:   1.9599     Sharing: 6.86342e-04     Sparsity: 8.52552e-02     Total:   9.8856     mean:   2.0459\n",
      " Task losses:   6.6927     mean:   1.3385     Sharing: 8.13171e-04     Sparsity: 8.52543e-02     Total:   6.7788     mean:   1.4246\n",
      " Task losses:   4.9523     mean:   0.9905     Sharing: 1.06134e-03     Sparsity: 8.52526e-02     Total:   5.0386     mean:   1.0768\n",
      " Task losses:   5.2694     mean:   1.0539     Sharing: 7.75581e-04     Sparsity: 8.52496e-02     Total:   5.3555     mean:   1.1399\n",
      " Task losses:   5.5457     mean:   1.1091     Sharing: 6.49000e-04     Sparsity: 8.52452e-02     Total:   5.6316     mean:   1.1950\n",
      " Task losses:   7.7856     mean:   1.5571     Sharing: 1.02280e-03     Sparsity: 8.52403e-02     Total:   7.8719     mean:   1.6434\n",
      " Task losses:  15.3219     mean:   3.0644     Sharing: 8.99320e-04     Sparsity: 8.52332e-02     Total:  15.4080     mean:   3.1505\n",
      " Task losses:   4.7985     mean:   0.9597     Sharing: 8.50146e-04     Sparsity: 8.52332e-02     Total:   4.8846     mean:   1.0458\n",
      " Task losses:   5.7840     mean:   1.1568     Sharing: 9.94186e-04     Sparsity: 8.52325e-02     Total:   5.8703     mean:   1.2430\n",
      " Task losses:   7.2851     mean:   1.4570     Sharing: 1.03431e-03     Sparsity: 8.52305e-02     Total:   7.3714     mean:   1.5433\n",
      " Task losses:   5.9703     mean:   1.1941     Sharing: 8.49103e-04     Sparsity: 8.52292e-02     Total:   6.0563     mean:   1.2801\n",
      " Task losses:   4.4506     mean:   0.8901     Sharing: 1.04830e-03     Sparsity: 8.52271e-02     Total:   4.5369     mean:   0.9764\n",
      " Task losses:   5.2115     mean:   1.0423     Sharing: 9.82965e-04     Sparsity: 8.52254e-02     Total:   5.2977     mean:   1.1285\n",
      " Task losses:   6.0552     mean:   1.2110     Sharing: 9.64185e-04     Sparsity: 8.52224e-02     Total:   6.1414     mean:   1.2972\n",
      " Task losses:   6.2482     mean:   1.2496     Sharing: 7.53000e-04     Sparsity: 8.52192e-02     Total:   6.3342     mean:   1.3356\n",
      " Task losses:   5.9905     mean:   1.1981     Sharing: 9.61696e-04     Sparsity: 8.52163e-02     Total:   6.0767     mean:   1.2843\n",
      " Task losses:   6.8250     mean:   1.3650     Sharing: 1.15791e-03     Sparsity: 8.52133e-02     Total:   6.9114     mean:   1.4514\n",
      " Task losses:   6.6183     mean:   1.3237     Sharing: 1.22629e-03     Sparsity: 8.52103e-02     Total:   6.7047     mean:   1.4101\n",
      " Task losses:   6.8544     mean:   1.3709     Sharing: 1.10217e-03     Sparsity: 8.52088e-02     Total:   6.9407     mean:   1.4572\n",
      " Task losses:   7.1069     mean:   1.4214     Sharing: 7.91967e-04     Sparsity: 8.52073e-02     Total:   7.1929     mean:   1.5074\n",
      " Task losses:   6.4841     mean:   1.2968     Sharing: 8.59231e-04     Sparsity: 8.52063e-02     Total:   6.5701     mean:   1.3829\n",
      " Task losses:   6.3680     mean:   1.2736     Sharing: 9.70637e-04     Sparsity: 8.52058e-02     Total:   6.4542     mean:   1.3598\n",
      " Task losses:   8.3855     mean:   1.6771     Sharing: 9.17872e-04     Sparsity: 8.52047e-02     Total:   8.4716     mean:   1.7632\n",
      " Task losses:   6.0757     mean:   1.2151     Sharing: 1.03177e-03     Sparsity: 8.52037e-02     Total:   6.1619     mean:   1.3014\n",
      " Task losses:   5.8709     mean:   1.1742     Sharing: 7.76634e-04     Sparsity: 8.52030e-02     Total:   5.9568     mean:   1.2602\n",
      " Task losses:   5.5351     mean:   1.1070     Sharing: 1.04559e-03     Sparsity: 8.52017e-02     Total:   5.6214     mean:   1.1933\n",
      " Task losses:   7.8272     mean:   1.5654     Sharing: 1.15398e-03     Sparsity: 8.51993e-02     Total:   7.9135     mean:   1.6518\n",
      " Task losses:   6.5647     mean:   1.3129     Sharing: 1.10672e-03     Sparsity: 8.52011e-02     Total:   6.6510     mean:   1.3993\n",
      " Task losses:   4.5308     mean:   0.9062     Sharing: 1.02481e-03     Sparsity: 8.52020e-02     Total:   4.6171     mean:   0.9924\n",
      " Task losses:   5.0371     mean:   1.0074     Sharing: 8.22117e-04     Sparsity: 8.52021e-02     Total:   5.1231     mean:   1.0934\n",
      " Task losses:   4.8514     mean:   0.9703     Sharing: 9.32266e-04     Sparsity: 8.52014e-02     Total:   4.9376     mean:   1.0564\n",
      " Task losses:   4.4790     mean:   0.8958     Sharing: 7.98583e-04     Sparsity: 8.52000e-02     Total:   4.5650     mean:   0.9818\n",
      " Task losses:   4.7768     mean:   0.9554     Sharing: 6.15194e-04     Sparsity: 8.51983e-02     Total:   4.8626     mean:   1.0412\n",
      " Task losses:   5.4385     mean:   1.0877     Sharing: 7.17814e-04     Sparsity: 8.51962e-02     Total:   5.5244     mean:   1.1736\n",
      " Task losses:   4.9340     mean:   0.9868     Sharing: 7.01110e-04     Sparsity: 8.51987e-02     Total:   5.0199     mean:   1.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   5.0089     mean:   1.0018     Sharing: 5.51775e-04     Sparsity: 8.52005e-02     Total:   5.0947     mean:   1.0875\n",
      " Task losses:   5.6864     mean:   1.1373     Sharing: 7.72059e-04     Sparsity: 8.52015e-02     Total:   5.7724     mean:   1.2233\n",
      " Task losses:   4.5102     mean:   0.9020     Sharing: 7.91614e-04     Sparsity: 8.52038e-02     Total:   4.5962     mean:   0.9880\n",
      " Task losses:   5.3126     mean:   1.0625     Sharing: 6.45543e-04     Sparsity: 8.52052e-02     Total:   5.3984     mean:   1.1484\n",
      " Task losses:   5.0537     mean:   1.0107     Sharing: 6.39975e-04     Sparsity: 8.52057e-02     Total:   5.1395     mean:   1.0966\n",
      " Task losses:   4.9952     mean:   0.9990     Sharing: 5.08696e-04     Sparsity: 8.52063e-02     Total:   5.0809     mean:   1.0848\n",
      " Task losses:   5.2975     mean:   1.0595     Sharing: 7.66019e-04     Sparsity: 8.52062e-02     Total:   5.3834     mean:   1.1455\n",
      " Task losses:   5.1592     mean:   1.0318     Sharing: 8.29279e-04     Sparsity: 8.52054e-02     Total:   5.2453     mean:   1.1179\n",
      " Task losses:   5.0865     mean:   1.0173     Sharing: 5.36462e-04     Sparsity: 8.52048e-02     Total:   5.1723     mean:   1.1030\n",
      " Task losses:   5.2359     mean:   1.0472     Sharing: 5.38295e-04     Sparsity: 8.52033e-02     Total:   5.3216     mean:   1.1329\n",
      " Task losses:   5.0064     mean:   1.0013     Sharing: 7.65994e-04     Sparsity: 8.52017e-02     Total:   5.0923     mean:   1.0872\n",
      " Task losses:   4.3307     mean:   0.8661     Sharing: 8.47240e-04     Sparsity: 8.52000e-02     Total:   4.4168     mean:   0.9522\n",
      " Task losses:   3.5885     mean:   0.7177     Sharing: 7.68726e-04     Sparsity: 8.51978e-02     Total:   3.6744     mean:   0.8037\n",
      " Task losses:   3.3300     mean:   0.6660     Sharing: 7.03101e-04     Sparsity: 8.51952e-02     Total:   3.4159     mean:   0.7519\n",
      " Task losses:   4.3356     mean:   0.8671     Sharing: 7.90661e-04     Sparsity: 8.51926e-02     Total:   4.4216     mean:   0.9531\n",
      " Task losses:   5.0528     mean:   1.0106     Sharing: 8.24551e-04     Sparsity: 8.51895e-02     Total:   5.1388     mean:   1.0966\n",
      " Task losses:   4.3172     mean:   0.8634     Sharing: 8.16251e-04     Sparsity: 8.51862e-02     Total:   4.4032     mean:   0.9494\n",
      " Task losses:   4.6198     mean:   0.9240     Sharing: 5.73317e-04     Sparsity: 8.51828e-02     Total:   4.7056     mean:   1.0097\n",
      " Task losses:   4.5472     mean:   0.9094     Sharing: 7.09216e-04     Sparsity: 8.51796e-02     Total:   4.6331     mean:   0.9953\n",
      " Task losses:   5.3776     mean:   1.0755     Sharing: 1.00829e-03     Sparsity: 8.51768e-02     Total:   5.4638     mean:   1.1617\n",
      " Task losses:   4.9321     mean:   0.9864     Sharing: 9.18617e-04     Sparsity: 8.51740e-02     Total:   5.0182     mean:   1.0725\n",
      " Task losses:   4.7496     mean:   0.9499     Sharing: 8.58059e-04     Sparsity: 8.51708e-02     Total:   4.8356     mean:   1.0359\n",
      " Task losses:   4.6040     mean:   0.9208     Sharing: 9.39464e-04     Sparsity: 8.51680e-02     Total:   4.6901     mean:   1.0069\n",
      " Task losses:   5.0365     mean:   1.0073     Sharing: 7.39450e-04     Sparsity: 8.51648e-02     Total:   5.1224     mean:   1.0932\n",
      " Task losses:   5.2151     mean:   1.0430     Sharing: 6.98760e-04     Sparsity: 8.51621e-02     Total:   5.3010     mean:   1.1289\n",
      " Task losses:   4.9912     mean:   0.9982     Sharing: 8.19623e-04     Sparsity: 8.51600e-02     Total:   5.0772     mean:   1.0842\n",
      " Task losses:   6.4188     mean:   1.2838     Sharing: 6.59555e-04     Sparsity: 8.51580e-02     Total:   6.5046     mean:   1.3696\n",
      " Task losses:   5.3983     mean:   1.0797     Sharing: 8.34117e-04     Sparsity: 8.51550e-02     Total:   5.4843     mean:   1.1656\n",
      " Task losses:   5.5866     mean:   1.1173     Sharing: 8.21009e-04     Sparsity: 8.51513e-02     Total:   5.6726     mean:   1.2033\n",
      " Task losses:   5.8905     mean:   1.1781     Sharing: 7.97629e-04     Sparsity: 8.51473e-02     Total:   5.9764     mean:   1.2640\n",
      " Task losses:   4.7497     mean:   0.9499     Sharing: 7.53234e-04     Sparsity: 8.51450e-02     Total:   4.8356     mean:   1.0358\n",
      " Task losses:   7.2396     mean:   1.4479     Sharing: 7.80518e-04     Sparsity: 8.51427e-02     Total:   7.3255     mean:   1.5338\n",
      " Task losses:   6.9643     mean:   1.3929     Sharing: 7.49374e-04     Sparsity: 8.51401e-02     Total:   7.0501     mean:   1.4787\n",
      " Task losses:   6.5716     mean:   1.3143     Sharing: 7.50780e-04     Sparsity: 8.51361e-02     Total:   6.6574     mean:   1.4002\n",
      " Task losses:   5.3742     mean:   1.0748     Sharing: 7.09648e-04     Sparsity: 8.51320e-02     Total:   5.4600     mean:   1.1607\n",
      " Task losses:   3.7578     mean:   0.7516     Sharing: 9.17335e-04     Sparsity: 8.51286e-02     Total:   3.8439     mean:   0.8376\n",
      " Task losses:   3.8649     mean:   0.7730     Sharing: 1.04591e-03     Sparsity: 8.51248e-02     Total:   3.9511     mean:   0.8592\n",
      " Task losses:   3.4360     mean:   0.6872     Sharing: 1.04340e-03     Sparsity: 8.51210e-02     Total:   3.5221     mean:   0.7734\n",
      " Task losses:   4.7677     mean:   0.9535     Sharing: 6.75490e-04     Sparsity: 8.51167e-02     Total:   4.8535     mean:   1.0393\n",
      " Task losses:   3.5830     mean:   0.7166     Sharing: 8.74023e-04     Sparsity: 8.51129e-02     Total:   3.6689     mean:   0.8026\n",
      " Task losses:   3.2840     mean:   0.6568     Sharing: 1.05520e-03     Sparsity: 8.51089e-02     Total:   3.3702     mean:   0.7430\n",
      " Task losses:   2.9006     mean:   0.5801     Sharing: 8.99245e-04     Sparsity: 8.51047e-02     Total:   2.9866     mean:   0.6661\n",
      " Task losses:   2.8446     mean:   0.5689     Sharing: 8.16847e-04     Sparsity: 8.50999e-02     Total:   2.9305     mean:   0.6548\n",
      " Task losses:   2.6687     mean:   0.5337     Sharing: 7.50726e-04     Sparsity: 8.50948e-02     Total:   2.7546     mean:   0.6196\n",
      " Task losses:   3.8409     mean:   0.7682     Sharing: 9.28223e-04     Sparsity: 8.50910e-02     Total:   3.9269     mean:   0.8542\n",
      "[e]Policy training epoch:92 iteration:  19972 -  Total Loss: 3.9269     Task Loss: 3.8409  Policy Losses:  Sparsity: 0.0851      Sharing: 9.28223e-04 \n",
      "[e]Policy training epoch:92 decay gumbel temp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 93 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:93 iteration:  20080 -  Total Loss: 4.0194     Task Loss: 4.0194  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93    | 0.00019 0.84544 0.79107 0.78947 0.74750|  4.3039  3.6779  4.6830 12.6647|   44.8|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 93 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.7680     mean:   0.7536     Sharing: 9.95532e-04     Sparsity: 8.50866e-02     Total:   3.8541     mean:   0.8397\n",
      " Task losses:   9.2225     mean:   1.8445     Sharing: 1.00714e-03     Sparsity: 8.50817e-02     Total:   9.3086     mean:   1.9306\n",
      " Task losses:  11.3085     mean:   2.2617     Sharing: 1.17184e-03     Sparsity: 8.50745e-02     Total:  11.3948     mean:   2.3480\n",
      " Task losses:  14.1282     mean:   2.8256     Sharing: 1.09108e-03     Sparsity: 8.50669e-02     Total:  14.2143     mean:   2.9118\n",
      " Task losses:  14.9331     mean:   2.9866     Sharing: 8.20979e-04     Sparsity: 8.50641e-02     Total:  15.0190     mean:   3.0725\n",
      " Task losses:  11.9365     mean:   2.3873     Sharing: 8.76546e-04     Sparsity: 8.50605e-02     Total:  12.0225     mean:   2.4732\n",
      " Task losses:   5.2326     mean:   1.0465     Sharing: 1.20133e-03     Sparsity: 8.50613e-02     Total:   5.3189     mean:   1.1328\n",
      " Task losses:   4.0462     mean:   0.8092     Sharing: 1.21717e-03     Sparsity: 8.50615e-02     Total:   4.1325     mean:   0.8955\n",
      " Task losses:   5.1243     mean:   1.0249     Sharing: 7.59934e-04     Sparsity: 8.50601e-02     Total:   5.2101     mean:   1.1107\n",
      " Task losses:   7.8512     mean:   1.5702     Sharing: 1.08851e-03     Sparsity: 8.50568e-02     Total:   7.9374     mean:   1.6564\n",
      " Task losses:   5.2920     mean:   1.0584     Sharing: 1.43212e-03     Sparsity: 8.50557e-02     Total:   5.3785     mean:   1.1449\n",
      " Task losses:   5.8319     mean:   1.1664     Sharing: 1.43132e-03     Sparsity: 8.50530e-02     Total:   5.9184     mean:   1.2529\n",
      " Task losses:   4.5928     mean:   0.9186     Sharing: 1.17675e-03     Sparsity: 8.50476e-02     Total:   4.6790     mean:   1.0048\n",
      " Task losses:   4.6079     mean:   0.9216     Sharing: 1.19708e-03     Sparsity: 8.50413e-02     Total:   4.6941     mean:   1.0078\n",
      " Task losses:   4.6298     mean:   0.9260     Sharing: 1.23950e-03     Sparsity: 8.50343e-02     Total:   4.7160     mean:   1.0122\n",
      " Task losses:   3.8515     mean:   0.7703     Sharing: 1.17682e-03     Sparsity: 8.50265e-02     Total:   3.9377     mean:   0.8565\n",
      " Task losses:   6.0475     mean:   1.2095     Sharing: 1.34348e-03     Sparsity: 8.50188e-02     Total:   6.1339     mean:   1.2959\n",
      " Task losses:  16.4699     mean:   3.2940     Sharing: 1.30118e-03     Sparsity: 8.50090e-02     Total:  16.5562     mean:   3.3803\n",
      " Task losses:  16.7515     mean:   3.3503     Sharing: 1.31893e-03     Sparsity: 8.50093e-02     Total:  16.8379     mean:   3.4366\n",
      " Task losses:   4.9777     mean:   0.9955     Sharing: 1.37345e-03     Sparsity: 8.50197e-02     Total:   5.0641     mean:   1.0819\n",
      " Task losses:   3.6761     mean:   0.7352     Sharing: 1.29922e-03     Sparsity: 8.50281e-02     Total:   3.7624     mean:   0.8215\n",
      " Task losses:   4.1947     mean:   0.8389     Sharing: 1.19789e-03     Sparsity: 8.50339e-02     Total:   4.2810     mean:   0.9252\n",
      " Task losses:   4.0648     mean:   0.8130     Sharing: 1.15820e-03     Sparsity: 8.50380e-02     Total:   4.1510     mean:   0.8992\n",
      " Task losses:   5.5769     mean:   1.1154     Sharing: 1.05144e-03     Sparsity: 8.50409e-02     Total:   5.6630     mean:   1.2015\n",
      " Task losses:   7.6498     mean:   1.5300     Sharing: 1.11502e-03     Sparsity: 8.50429e-02     Total:   7.7359     mean:   1.6161\n",
      " Task losses:   4.6509     mean:   0.9302     Sharing: 8.58366e-04     Sparsity: 8.50435e-02     Total:   4.7368     mean:   1.0161\n",
      " Task losses:   8.9094     mean:   1.7819     Sharing: 8.15774e-04     Sparsity: 8.50432e-02     Total:   8.9952     mean:   1.8677\n",
      " Task losses:  11.9902     mean:   2.3980     Sharing: 9.60633e-04     Sparsity: 8.50408e-02     Total:  12.0762     mean:   2.4840\n",
      " Task losses:  15.0668     mean:   3.0134     Sharing: 1.06981e-03     Sparsity: 8.50366e-02     Total:  15.1529     mean:   3.0995\n",
      " Task losses:   7.3980     mean:   1.4796     Sharing: 8.45253e-04     Sparsity: 8.50316e-02     Total:   7.4839     mean:   1.5655\n",
      " Task losses:   4.6485     mean:   0.9297     Sharing: 6.51608e-04     Sparsity: 8.50266e-02     Total:   4.7342     mean:   1.0154\n",
      " Task losses:   5.5105     mean:   1.1021     Sharing: 9.68347e-04     Sparsity: 8.50207e-02     Total:   5.5965     mean:   1.1881\n",
      " Task losses:   5.6058     mean:   1.1212     Sharing: 1.08203e-03     Sparsity: 8.50136e-02     Total:   5.6919     mean:   1.2072\n",
      " Task losses:   4.5615     mean:   0.9123     Sharing: 1.13412e-03     Sparsity: 8.50047e-02     Total:   4.6476     mean:   0.9984\n",
      " Task losses:  27.1935     mean:   5.4387     Sharing: 8.52113e-04     Sparsity: 8.49972e-02     Total:  27.2793     mean:   5.5245\n",
      " Task losses:   4.1397     mean:   0.8279     Sharing: 1.59972e-03     Sparsity: 8.50222e-02     Total:   4.2263     mean:   0.9146\n",
      " Task losses:   5.1917     mean:   1.0383     Sharing: 2.57215e-03     Sparsity: 8.50438e-02     Total:   5.2793     mean:   1.1260\n",
      " Task losses:   5.8646     mean:   1.1729     Sharing: 3.02298e-03     Sparsity: 8.50625e-02     Total:   5.9527     mean:   1.2610\n",
      " Task losses:   6.9895     mean:   1.3979     Sharing: 2.98211e-03     Sparsity: 8.50782e-02     Total:   7.0775     mean:   1.4860\n",
      " Task losses:   3.6941     mean:   0.7388     Sharing: 3.46424e-03     Sparsity: 8.50921e-02     Total:   3.7827     mean:   0.8274\n",
      " Task losses:   4.5398     mean:   0.9080     Sharing: 3.63470e-03     Sparsity: 8.51031e-02     Total:   4.6285     mean:   0.9967\n",
      " Task losses:   5.7649     mean:   1.1530     Sharing: 3.38662e-03     Sparsity: 8.51110e-02     Total:   5.8534     mean:   1.2415\n",
      " Task losses:   5.3981     mean:   1.0796     Sharing: 3.07752e-03     Sparsity: 8.51173e-02     Total:   5.4862     mean:   1.1678\n",
      " Task losses:   5.5853     mean:   1.1171     Sharing: 2.87790e-03     Sparsity: 8.51207e-02     Total:   5.6733     mean:   1.2051\n",
      " Task losses:   5.2406     mean:   1.0481     Sharing: 2.86139e-03     Sparsity: 8.51223e-02     Total:   5.3285     mean:   1.1361\n",
      " Task losses:   5.9623     mean:   1.1925     Sharing: 2.67144e-03     Sparsity: 8.51208e-02     Total:   6.0501     mean:   1.2802\n",
      " Task losses:   5.9680     mean:   1.1936     Sharing: 2.56742e-03     Sparsity: 8.51185e-02     Total:   6.0557     mean:   1.2813\n",
      " Task losses:   5.8665     mean:   1.1733     Sharing: 2.06033e-03     Sparsity: 8.51160e-02     Total:   5.9536     mean:   1.2605\n",
      " Task losses:   5.1292     mean:   1.0258     Sharing: 2.05412e-03     Sparsity: 8.51127e-02     Total:   5.2163     mean:   1.1130\n",
      " Task losses:   4.9506     mean:   0.9901     Sharing: 1.80667e-03     Sparsity: 8.51087e-02     Total:   5.0375     mean:   1.0770\n",
      " Task losses:   6.0365     mean:   1.2073     Sharing: 1.10714e-03     Sparsity: 8.51042e-02     Total:   6.1227     mean:   1.2935\n",
      " Task losses:   9.3235     mean:   1.8647     Sharing: 1.11308e-03     Sparsity: 8.51001e-02     Total:   9.4097     mean:   1.9509\n",
      " Task losses:   7.2584     mean:   1.4517     Sharing: 1.21972e-03     Sparsity: 8.50962e-02     Total:   7.3447     mean:   1.5380\n",
      " Task losses:   4.4725     mean:   0.8945     Sharing: 1.61121e-03     Sparsity: 8.50917e-02     Total:   4.5593     mean:   0.9812\n",
      " Task losses:   4.8680     mean:   0.9736     Sharing: 1.80771e-03     Sparsity: 8.50873e-02     Total:   4.9549     mean:   1.0605\n",
      " Task losses:   5.7453     mean:   1.1491     Sharing: 1.83870e-03     Sparsity: 8.50827e-02     Total:   5.8322     mean:   1.2360\n",
      " Task losses:   4.2065     mean:   0.8413     Sharing: 1.59366e-03     Sparsity: 8.50778e-02     Total:   4.2932     mean:   0.9280\n",
      " Task losses:   4.8266     mean:   0.9653     Sharing: 1.70407e-03     Sparsity: 8.50720e-02     Total:   4.9134     mean:   1.0521\n",
      " Task losses:   5.1089     mean:   1.0218     Sharing: 1.57930e-03     Sparsity: 8.50651e-02     Total:   5.1956     mean:   1.1084\n",
      " Task losses:   4.5925     mean:   0.9185     Sharing: 1.33332e-03     Sparsity: 8.50574e-02     Total:   4.6788     mean:   1.0049\n",
      " Task losses:   4.3019     mean:   0.8604     Sharing: 1.00728e-03     Sparsity: 8.50495e-02     Total:   4.3879     mean:   0.9464\n",
      " Task losses:   4.3143     mean:   0.8629     Sharing: 8.83838e-04     Sparsity: 8.50412e-02     Total:   4.4002     mean:   0.9488\n",
      " Task losses:   4.5462     mean:   0.9092     Sharing: 1.09936e-03     Sparsity: 8.50330e-02     Total:   4.6323     mean:   0.9954\n",
      " Task losses:   4.2408     mean:   0.8482     Sharing: 1.33731e-03     Sparsity: 8.50251e-02     Total:   4.3272     mean:   0.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.4062     mean:   0.8812     Sharing: 1.40105e-03     Sparsity: 8.50174e-02     Total:   4.4926     mean:   0.9677\n",
      " Task losses:   4.0680     mean:   0.8136     Sharing: 1.33399e-03     Sparsity: 8.50098e-02     Total:   4.1544     mean:   0.8999\n",
      " Task losses:   4.5665     mean:   0.9133     Sharing: 1.14398e-03     Sparsity: 8.50045e-02     Total:   4.6526     mean:   0.9994\n",
      " Task losses:   4.6400     mean:   0.9280     Sharing: 1.01873e-03     Sparsity: 8.49992e-02     Total:   4.7260     mean:   1.0140\n",
      " Task losses:   4.9214     mean:   0.9843     Sharing: 6.98149e-04     Sparsity: 8.49932e-02     Total:   5.0071     mean:   1.0700\n",
      " Task losses:   5.1073     mean:   1.0215     Sharing: 6.71158e-04     Sparsity: 8.49861e-02     Total:   5.1929     mean:   1.1071\n",
      " Task losses:   5.2369     mean:   1.0474     Sharing: 7.74408e-04     Sparsity: 8.49784e-02     Total:   5.3227     mean:   1.1331\n",
      " Task losses:   5.1274     mean:   1.0255     Sharing: 1.07644e-03     Sparsity: 8.49701e-02     Total:   5.2135     mean:   1.1115\n",
      " Task losses:   3.9304     mean:   0.7861     Sharing: 1.18344e-03     Sparsity: 8.49612e-02     Total:   4.0166     mean:   0.8722\n",
      " Task losses:   4.0058     mean:   0.8012     Sharing: 8.84657e-04     Sparsity: 8.49524e-02     Total:   4.0916     mean:   0.8870\n",
      " Task losses:   3.8669     mean:   0.7734     Sharing: 1.10433e-03     Sparsity: 8.49433e-02     Total:   3.9530     mean:   0.8594\n",
      " Task losses:   3.5079     mean:   0.7016     Sharing: 9.95929e-04     Sparsity: 8.49342e-02     Total:   3.5938     mean:   0.7875\n",
      " Task losses:   3.1576     mean:   0.6315     Sharing: 7.14461e-04     Sparsity: 8.49247e-02     Total:   3.2432     mean:   0.7172\n",
      " Task losses:   3.2779     mean:   0.6556     Sharing: 1.00749e-03     Sparsity: 8.49152e-02     Total:   3.3638     mean:   0.7415\n",
      " Task losses:   3.7681     mean:   0.7536     Sharing: 1.17204e-03     Sparsity: 8.49062e-02     Total:   3.8542     mean:   0.8397\n",
      " Task losses:   3.9297     mean:   0.7859     Sharing: 7.59542e-04     Sparsity: 8.48976e-02     Total:   4.0153     mean:   0.8716\n",
      " Task losses:   4.0494     mean:   0.8099     Sharing: 8.39815e-04     Sparsity: 8.48891e-02     Total:   4.1351     mean:   0.8956\n",
      " Task losses:   4.3227     mean:   0.8645     Sharing: 1.02871e-03     Sparsity: 8.48810e-02     Total:   4.4086     mean:   0.9504\n",
      " Task losses:   4.7671     mean:   0.9534     Sharing: 6.44282e-04     Sparsity: 8.48726e-02     Total:   4.8526     mean:   1.0389\n",
      " Task losses:   4.8129     mean:   0.9626     Sharing: 7.81099e-04     Sparsity: 8.48644e-02     Total:   4.8986     mean:   1.0482\n",
      " Task losses:   5.5642     mean:   1.1128     Sharing: 9.99391e-04     Sparsity: 8.48550e-02     Total:   5.6501     mean:   1.1987\n",
      " Task losses:   3.9184     mean:   0.7837     Sharing: 7.70499e-04     Sparsity: 8.48446e-02     Total:   4.0040     mean:   0.8693\n",
      " Task losses:   3.9844     mean:   0.7969     Sharing: 5.12525e-04     Sparsity: 8.48345e-02     Total:   4.0698     mean:   0.8822\n",
      " Task losses:   4.6191     mean:   0.9238     Sharing: 6.86338e-04     Sparsity: 8.48251e-02     Total:   4.7046     mean:   1.0093\n",
      " Task losses:   4.1590     mean:   0.8318     Sharing: 5.77534e-04     Sparsity: 8.48160e-02     Total:   4.2444     mean:   0.9172\n",
      " Task losses:   3.9087     mean:   0.7817     Sharing: 6.45459e-04     Sparsity: 8.48070e-02     Total:   3.9942     mean:   0.8672\n",
      " Task losses:   4.5401     mean:   0.9080     Sharing: 7.96919e-04     Sparsity: 8.47986e-02     Total:   4.6257     mean:   0.9936\n",
      " Task losses:   4.1353     mean:   0.8271     Sharing: 5.70128e-04     Sparsity: 8.47914e-02     Total:   4.2207     mean:   0.9124\n",
      " Task losses:   4.6884     mean:   0.9377     Sharing: 6.56540e-04     Sparsity: 8.47851e-02     Total:   4.7738     mean:   1.0231\n",
      " Task losses:   3.7378     mean:   0.7476     Sharing: 8.96384e-04     Sparsity: 8.47787e-02     Total:   3.8235     mean:   0.8332\n",
      " Task losses:   6.0131     mean:   1.2026     Sharing: 7.35561e-04     Sparsity: 8.47731e-02     Total:   6.0986     mean:   1.2881\n",
      " Task losses:   5.3937     mean:   1.0787     Sharing: 6.90068e-04     Sparsity: 8.47676e-02     Total:   5.4792     mean:   1.1642\n",
      " Task losses:   8.0440     mean:   1.6088     Sharing: 8.45169e-04     Sparsity: 8.47619e-02     Total:   8.1296     mean:   1.6944\n",
      " Task losses:   6.9805     mean:   1.3961     Sharing: 7.04959e-04     Sparsity: 8.47557e-02     Total:   7.0660     mean:   1.4816\n",
      " Task losses:   4.8722     mean:   0.9744     Sharing: 6.94628e-04     Sparsity: 8.47480e-02     Total:   4.9577     mean:   1.0599\n",
      " Task losses:   4.1563     mean:   0.8313     Sharing: 8.21754e-04     Sparsity: 8.47393e-02     Total:   4.2419     mean:   0.9168\n",
      " Task losses:   3.9844     mean:   0.7969     Sharing: 6.01292e-04     Sparsity: 8.47304e-02     Total:   4.0697     mean:   0.8822\n",
      " Task losses:   5.0025     mean:   1.0005     Sharing: 6.43025e-04     Sparsity: 8.47208e-02     Total:   5.0879     mean:   1.0859\n",
      " Task losses:   3.7462     mean:   0.7492     Sharing: 7.59199e-04     Sparsity: 8.47114e-02     Total:   3.8317     mean:   0.8347\n",
      " Task losses:   3.4655     mean:   0.6931     Sharing: 6.46005e-04     Sparsity: 8.47019e-02     Total:   3.5508     mean:   0.7784\n",
      " Task losses:   2.8722     mean:   0.5744     Sharing: 6.50649e-04     Sparsity: 8.46924e-02     Total:   2.9575     mean:   0.6598\n",
      " Task losses:   2.7632     mean:   0.5526     Sharing: 8.03794e-04     Sparsity: 8.46830e-02     Total:   2.8487     mean:   0.6381\n",
      " Task losses:   2.7617     mean:   0.5523     Sharing: 9.88891e-04     Sparsity: 8.46733e-02     Total:   2.8474     mean:   0.6380\n",
      " Task losses:   3.4933     mean:   0.6987     Sharing: 9.25039e-04     Sparsity: 8.46641e-02     Total:   3.5789     mean:   0.7842\n",
      "[e]Policy training epoch:93 iteration:  20188 -  Total Loss: 3.5789     Task Loss: 3.4933  Policy Losses:  Sparsity: 0.0847      Sharing: 9.25039e-04 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 94 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:94 iteration:  20296 -  Total Loss: 3.7114     Task Loss: 3.7114  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94    | 0.00019 0.84785 0.78638 0.78732 0.74435|  3.9070  4.4191  4.3834 12.7095|   43.0|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 94 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.0687     mean:   0.6137     Sharing: 5.85988e-04     Sparsity: 8.46551e-02     Total:   3.1539     mean:   0.6990\n",
      " Task losses:   5.8543     mean:   1.1709     Sharing: 8.37058e-04     Sparsity: 8.46464e-02     Total:   5.9398     mean:   1.2563\n",
      " Task losses:   6.7002     mean:   1.3400     Sharing: 8.35871e-04     Sparsity: 8.46375e-02     Total:   6.7856     mean:   1.4255\n",
      " Task losses:   7.4760     mean:   1.4952     Sharing: 5.12972e-04     Sparsity: 8.46263e-02     Total:   7.5611     mean:   1.5803\n",
      " Task losses:   8.0323     mean:   1.6065     Sharing: 9.73612e-04     Sparsity: 8.46156e-02     Total:   8.1179     mean:   1.6920\n",
      " Task losses:   5.9114     mean:   1.1823     Sharing: 1.06111e-03     Sparsity: 8.46049e-02     Total:   5.9971     mean:   1.2679\n",
      " Task losses:   4.1043     mean:   0.8209     Sharing: 9.25745e-04     Sparsity: 8.45932e-02     Total:   4.1898     mean:   0.9064\n",
      " Task losses:   3.7366     mean:   0.7473     Sharing: 6.19814e-04     Sparsity: 8.45821e-02     Total:   3.8218     mean:   0.8325\n",
      " Task losses:   4.8931     mean:   0.9786     Sharing: 6.25074e-04     Sparsity: 8.45714e-02     Total:   4.9783     mean:   1.0638\n",
      " Task losses:   7.1932     mean:   1.4386     Sharing: 7.68448e-04     Sparsity: 8.45610e-02     Total:   7.2786     mean:   1.5240\n",
      " Task losses:   4.9551     mean:   0.9910     Sharing: 7.20009e-04     Sparsity: 8.45505e-02     Total:   5.0403     mean:   1.0763\n",
      " Task losses:   6.7487     mean:   1.3497     Sharing: 4.89766e-04     Sparsity: 8.45397e-02     Total:   6.8337     mean:   1.4348\n",
      " Task losses:   5.3645     mean:   1.0729     Sharing: 6.17916e-04     Sparsity: 8.45303e-02     Total:   5.4496     mean:   1.1580\n",
      " Task losses:   5.0389     mean:   1.0078     Sharing: 7.79574e-04     Sparsity: 8.45212e-02     Total:   5.1242     mean:   1.0931\n",
      " Task losses:   4.2776     mean:   0.8555     Sharing: 8.68271e-04     Sparsity: 8.45127e-02     Total:   4.3630     mean:   0.9409\n",
      " Task losses:   3.9717     mean:   0.7943     Sharing: 5.69294e-04     Sparsity: 8.45045e-02     Total:   4.0568     mean:   0.8794\n",
      " Task losses:   5.5113     mean:   1.1023     Sharing: 7.36584e-04     Sparsity: 8.44965e-02     Total:   5.5966     mean:   1.1875\n",
      " Task losses:  10.4980     mean:   2.0996     Sharing: 1.22995e-03     Sparsity: 8.44901e-02     Total:  10.5837     mean:   2.1853\n",
      " Task losses:  12.3059     mean:   2.4612     Sharing: 1.08667e-03     Sparsity: 8.44785e-02     Total:  12.3915     mean:   2.5468\n",
      " Task losses:   4.4134     mean:   0.8827     Sharing: 6.14131e-04     Sparsity: 8.44653e-02     Total:   4.4984     mean:   0.9678\n",
      " Task losses:   3.3267     mean:   0.6653     Sharing: 8.07891e-04     Sparsity: 8.44536e-02     Total:   3.4120     mean:   0.7506\n",
      " Task losses:   3.7423     mean:   0.7485     Sharing: 1.02092e-03     Sparsity: 8.44442e-02     Total:   3.8278     mean:   0.8339\n",
      " Task losses:   4.0600     mean:   0.8120     Sharing: 1.06002e-03     Sparsity: 8.44350e-02     Total:   4.1455     mean:   0.8975\n",
      " Task losses:   4.5039     mean:   0.9008     Sharing: 8.75642e-04     Sparsity: 8.44259e-02     Total:   4.5892     mean:   0.9861\n",
      " Task losses:   4.3077     mean:   0.8615     Sharing: 8.29369e-04     Sparsity: 8.44176e-02     Total:   4.3930     mean:   0.9468\n",
      " Task losses:   3.4695     mean:   0.6939     Sharing: 1.09007e-03     Sparsity: 8.44095e-02     Total:   3.5550     mean:   0.7794\n",
      " Task losses:   6.1010     mean:   1.2202     Sharing: 1.18664e-03     Sparsity: 8.44019e-02     Total:   6.1866     mean:   1.3058\n",
      " Task losses:   7.5663     mean:   1.5133     Sharing: 7.76763e-04     Sparsity: 8.43941e-02     Total:   7.6515     mean:   1.5984\n",
      " Task losses:   9.3990     mean:   1.8798     Sharing: 6.87217e-04     Sparsity: 8.43875e-02     Total:   9.4841     mean:   1.9649\n",
      " Task losses:   6.1489     mean:   1.2298     Sharing: 1.02214e-03     Sparsity: 8.43817e-02     Total:   6.2343     mean:   1.3152\n",
      " Task losses:   4.5484     mean:   0.9097     Sharing: 7.59914e-04     Sparsity: 8.43776e-02     Total:   4.6335     mean:   0.9948\n",
      " Task losses:   3.7624     mean:   0.7525     Sharing: 7.00846e-04     Sparsity: 8.43730e-02     Total:   3.8475     mean:   0.8376\n",
      " Task losses:   4.0050     mean:   0.8010     Sharing: 7.67728e-04     Sparsity: 8.43705e-02     Total:   4.0901     mean:   0.8861\n",
      " Task losses:   4.2161     mean:   0.8432     Sharing: 7.28274e-04     Sparsity: 8.43673e-02     Total:   4.3012     mean:   0.9283\n",
      " Task losses:   6.7878     mean:   1.3576     Sharing: 8.36501e-04     Sparsity: 8.43650e-02     Total:   6.8730     mean:   1.4428\n",
      " Task losses:   3.8508     mean:   0.7702     Sharing: 1.14222e-03     Sparsity: 8.43646e-02     Total:   3.9363     mean:   0.8557\n",
      " Task losses:   4.2958     mean:   0.8592     Sharing: 1.04115e-03     Sparsity: 8.43640e-02     Total:   4.3812     mean:   0.9446\n",
      " Task losses:   5.6630     mean:   1.1326     Sharing: 6.97846e-04     Sparsity: 8.43661e-02     Total:   5.7481     mean:   1.2177\n",
      " Task losses:   5.7874     mean:   1.1575     Sharing: 1.04134e-03     Sparsity: 8.43677e-02     Total:   5.8728     mean:   1.2429\n",
      " Task losses:   3.2667     mean:   0.6533     Sharing: 1.20403e-03     Sparsity: 8.43685e-02     Total:   3.3523     mean:   0.7389\n",
      " Task losses:   4.4818     mean:   0.8964     Sharing: 9.53501e-04     Sparsity: 8.43684e-02     Total:   4.5671     mean:   0.9817\n",
      " Task losses:   7.2622     mean:   1.4524     Sharing: 8.01673e-04     Sparsity: 8.43683e-02     Total:   7.3474     mean:   1.5376\n",
      " Task losses:   7.2413     mean:   1.4483     Sharing: 8.86574e-04     Sparsity: 8.43675e-02     Total:   7.3266     mean:   1.5335\n",
      " Task losses:   4.4803     mean:   0.8961     Sharing: 1.27618e-03     Sparsity: 8.43658e-02     Total:   4.5660     mean:   0.9817\n",
      " Task losses:   4.2914     mean:   0.8583     Sharing: 1.18666e-03     Sparsity: 8.43635e-02     Total:   4.3770     mean:   0.9438\n",
      " Task losses:   5.1980     mean:   1.0396     Sharing: 1.05595e-03     Sparsity: 8.43610e-02     Total:   5.2834     mean:   1.1250\n",
      " Task losses:   5.5003     mean:   1.1001     Sharing: 1.04691e-03     Sparsity: 8.43585e-02     Total:   5.5857     mean:   1.1855\n",
      " Task losses:   5.6210     mean:   1.1242     Sharing: 1.24277e-03     Sparsity: 8.43592e-02     Total:   5.7066     mean:   1.2098\n",
      " Task losses:   4.8217     mean:   0.9643     Sharing: 9.50490e-04     Sparsity: 8.43599e-02     Total:   4.9070     mean:   1.0497\n",
      " Task losses:   5.0039     mean:   1.0008     Sharing: 1.07179e-03     Sparsity: 8.43599e-02     Total:   5.0893     mean:   1.0862\n",
      " Task losses:   6.6477     mean:   1.3295     Sharing: 1.33430e-03     Sparsity: 8.43594e-02     Total:   6.7334     mean:   1.4152\n",
      " Task losses:   8.3829     mean:   1.6766     Sharing: 1.43122e-03     Sparsity: 8.43585e-02     Total:   8.4687     mean:   1.7624\n",
      " Task losses:   6.5423     mean:   1.3085     Sharing: 1.11527e-03     Sparsity: 8.43573e-02     Total:   6.6277     mean:   1.3939\n",
      " Task losses:   4.8639     mean:   0.9728     Sharing: 7.87045e-04     Sparsity: 8.43557e-02     Total:   4.9490     mean:   1.0579\n",
      " Task losses:   8.4673     mean:   1.6935     Sharing: 9.78236e-04     Sparsity: 8.43538e-02     Total:   8.5526     mean:   1.7788\n",
      " Task losses:   6.6477     mean:   1.3295     Sharing: 7.46826e-04     Sparsity: 8.43526e-02     Total:   6.7328     mean:   1.4146\n",
      " Task losses:   4.2140     mean:   0.8428     Sharing: 5.97378e-04     Sparsity: 8.43516e-02     Total:   4.2990     mean:   0.9278\n",
      " Task losses:   4.9813     mean:   0.9963     Sharing: 6.04267e-04     Sparsity: 8.43510e-02     Total:   5.0663     mean:   1.0812\n",
      " Task losses:   5.1620     mean:   1.0324     Sharing: 5.89520e-04     Sparsity: 8.43498e-02     Total:   5.2470     mean:   1.1173\n",
      " Task losses:   3.9484     mean:   0.7897     Sharing: 4.79504e-04     Sparsity: 8.43488e-02     Total:   4.0332     mean:   0.8745\n",
      " Task losses:   3.9517     mean:   0.7903     Sharing: 7.25766e-04     Sparsity: 8.43473e-02     Total:   4.0368     mean:   0.8754\n",
      " Task losses:   3.6990     mean:   0.7398     Sharing: 8.58545e-04     Sparsity: 8.43459e-02     Total:   3.7842     mean:   0.8250\n",
      " Task losses:   3.6890     mean:   0.7378     Sharing: 7.57108e-04     Sparsity: 8.43441e-02     Total:   3.7741     mean:   0.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.8281     mean:   0.7656     Sharing: 6.99247e-04     Sparsity: 8.43425e-02     Total:   3.9131     mean:   0.8507\n",
      " Task losses:   3.6487     mean:   0.7297     Sharing: 7.51808e-04     Sparsity: 8.43405e-02     Total:   3.7338     mean:   0.8148\n",
      " Task losses:   3.5857     mean:   0.7171     Sharing: 9.05360e-04     Sparsity: 8.43381e-02     Total:   3.6709     mean:   0.8024\n",
      " Task losses:   4.1016     mean:   0.8203     Sharing: 1.05065e-03     Sparsity: 8.43354e-02     Total:   4.1870     mean:   0.9057\n",
      " Task losses:   3.8492     mean:   0.7698     Sharing: 8.70456e-04     Sparsity: 8.43326e-02     Total:   3.9344     mean:   0.8551\n",
      " Task losses:   4.3238     mean:   0.8648     Sharing: 8.17880e-04     Sparsity: 8.43300e-02     Total:   4.4090     mean:   0.9499\n",
      " Task losses:   4.4298     mean:   0.8860     Sharing: 1.04327e-03     Sparsity: 8.43269e-02     Total:   4.5152     mean:   0.9713\n",
      " Task losses:   5.2976     mean:   1.0595     Sharing: 1.08108e-03     Sparsity: 8.43236e-02     Total:   5.3830     mean:   1.1449\n",
      " Task losses:   5.0570     mean:   1.0114     Sharing: 8.23632e-04     Sparsity: 8.43201e-02     Total:   5.1422     mean:   1.0965\n",
      " Task losses:   5.4038     mean:   1.0808     Sharing: 1.16249e-03     Sparsity: 8.43166e-02     Total:   5.4893     mean:   1.1662\n",
      " Task losses:   6.4043     mean:   1.2809     Sharing: 1.53619e-03     Sparsity: 8.43131e-02     Total:   6.4902     mean:   1.3667\n",
      " Task losses:   4.3126     mean:   0.8625     Sharing: 1.45363e-03     Sparsity: 8.43095e-02     Total:   4.3983     mean:   0.9483\n",
      " Task losses:   3.1709     mean:   0.6342     Sharing: 1.21790e-03     Sparsity: 8.43057e-02     Total:   3.2564     mean:   0.7197\n",
      " Task losses:   3.1598     mean:   0.6320     Sharing: 7.88862e-04     Sparsity: 8.43018e-02     Total:   3.2449     mean:   0.7170\n",
      " Task losses:   3.6284     mean:   0.7257     Sharing: 1.03132e-03     Sparsity: 8.42981e-02     Total:   3.7138     mean:   0.8110\n",
      " Task losses:   3.6904     mean:   0.7381     Sharing: 7.29710e-04     Sparsity: 8.42955e-02     Total:   3.7754     mean:   0.8231\n",
      " Task losses:   3.3705     mean:   0.6741     Sharing: 7.08674e-04     Sparsity: 8.42930e-02     Total:   3.4555     mean:   0.7591\n",
      " Task losses:   3.6847     mean:   0.7369     Sharing: 9.82374e-04     Sparsity: 8.42927e-02     Total:   3.7700     mean:   0.8222\n",
      " Task losses:   3.3199     mean:   0.6640     Sharing: 9.01555e-04     Sparsity: 8.42923e-02     Total:   3.4051     mean:   0.7492\n",
      " Task losses:   4.0263     mean:   0.8053     Sharing: 7.28716e-04     Sparsity: 8.42910e-02     Total:   4.1113     mean:   0.8903\n",
      " Task losses:   3.6089     mean:   0.7218     Sharing: 6.87634e-04     Sparsity: 8.42897e-02     Total:   3.6939     mean:   0.8068\n",
      " Task losses:   3.5696     mean:   0.7139     Sharing: 7.36247e-04     Sparsity: 8.42867e-02     Total:   3.6546     mean:   0.7989\n",
      " Task losses:   3.5475     mean:   0.7095     Sharing: 6.60688e-04     Sparsity: 8.42828e-02     Total:   3.6324     mean:   0.7944\n",
      " Task losses:   3.6696     mean:   0.7339     Sharing: 4.61727e-04     Sparsity: 8.42799e-02     Total:   3.7543     mean:   0.8187\n",
      " Task losses:   3.8574     mean:   0.7715     Sharing: 7.01681e-04     Sparsity: 8.42765e-02     Total:   3.9423     mean:   0.8564\n",
      " Task losses:   3.7228     mean:   0.7446     Sharing: 1.06489e-03     Sparsity: 8.42721e-02     Total:   3.8081     mean:   0.8299\n",
      " Task losses:   3.5915     mean:   0.7183     Sharing: 9.87773e-04     Sparsity: 8.42677e-02     Total:   3.6767     mean:   0.8036\n",
      " Task losses:   3.7308     mean:   0.7462     Sharing: 6.05901e-04     Sparsity: 8.42631e-02     Total:   3.8157     mean:   0.8310\n",
      " Task losses:   3.8464     mean:   0.7693     Sharing: 6.73562e-04     Sparsity: 8.42589e-02     Total:   3.9313     mean:   0.8542\n",
      " Task losses:   4.4417     mean:   0.8883     Sharing: 1.11180e-03     Sparsity: 8.42545e-02     Total:   4.5271     mean:   0.9737\n",
      " Task losses:   4.2234     mean:   0.8447     Sharing: 1.10222e-03     Sparsity: 8.42502e-02     Total:   4.3087     mean:   0.9300\n",
      " Task losses:   8.0349     mean:   1.6070     Sharing: 1.02839e-03     Sparsity: 8.42456e-02     Total:   8.1201     mean:   1.6922\n",
      " Task losses:   7.5012     mean:   1.5002     Sharing: 8.51388e-04     Sparsity: 8.42426e-02     Total:   7.5863     mean:   1.5853\n",
      " Task losses:   5.4665     mean:   1.0933     Sharing: 9.47098e-04     Sparsity: 8.42393e-02     Total:   5.5517     mean:   1.1785\n",
      " Task losses:   4.7859     mean:   0.9572     Sharing: 1.00048e-03     Sparsity: 8.42362e-02     Total:   4.8711     mean:   1.0424\n",
      " Task losses:   3.7794     mean:   0.7559     Sharing: 9.97484e-04     Sparsity: 8.42333e-02     Total:   3.8646     mean:   0.8411\n",
      " Task losses:   3.7218     mean:   0.7444     Sharing: 1.07795e-03     Sparsity: 8.42302e-02     Total:   3.8071     mean:   0.8297\n",
      " Task losses:   3.1747     mean:   0.6349     Sharing: 9.29177e-04     Sparsity: 8.42267e-02     Total:   3.2599     mean:   0.7201\n",
      " Task losses:   4.7492     mean:   0.9498     Sharing: 5.16633e-04     Sparsity: 8.42228e-02     Total:   4.8339     mean:   1.0346\n",
      " Task losses:   3.8854     mean:   0.7771     Sharing: 1.03212e-03     Sparsity: 8.42186e-02     Total:   3.9706     mean:   0.8623\n",
      " Task losses:   3.2054     mean:   0.6411     Sharing: 1.33127e-03     Sparsity: 8.42139e-02     Total:   3.2910     mean:   0.7266\n",
      " Task losses:   2.6840     mean:   0.5368     Sharing: 1.27843e-03     Sparsity: 8.42091e-02     Total:   2.7695     mean:   0.6223\n",
      " Task losses:   2.6074     mean:   0.5215     Sharing: 1.04303e-03     Sparsity: 8.42043e-02     Total:   2.6926     mean:   0.6067\n",
      " Task losses:   2.5644     mean:   0.5129     Sharing: 1.01824e-03     Sparsity: 8.41997e-02     Total:   2.6496     mean:   0.5981\n",
      " Task losses:   3.2788     mean:   0.6558     Sharing: 1.10406e-03     Sparsity: 8.41950e-02     Total:   3.3641     mean:   0.7411\n",
      "[e]Policy training epoch:94 iteration:  20404 -  Total Loss: 3.3641     Task Loss: 3.2788  Policy Losses:  Sparsity: 0.0842      Sharing: 1.10406e-03 \n",
      "[e]Policy training epoch:94 decay gumbel temp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 95 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:95 iteration:  20512 -  Total Loss: 4.2876     Task Loss: 4.2876  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95    | 0.00020 0.89327 0.78751 0.78998 0.74533|  3.7913  4.4640  5.1188 13.3741|   43.6|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 95 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.7746     mean:   0.7549     Sharing: 1.20285e-03     Sparsity: 8.41903e-02     Total:   3.8600     mean:   0.8403\n",
      " Task losses:   5.8392     mean:   1.1678     Sharing: 1.33109e-03     Sparsity: 8.41851e-02     Total:   5.9247     mean:   1.2534\n",
      " Task losses:   6.0047     mean:   1.2009     Sharing: 1.27830e-03     Sparsity: 8.41781e-02     Total:   6.0901     mean:   1.2864\n",
      " Task losses:   6.6186     mean:   1.3237     Sharing: 1.09242e-03     Sparsity: 8.41666e-02     Total:   6.7039     mean:   1.4090\n",
      " Task losses:   7.5628     mean:   1.5126     Sharing: 7.85589e-04     Sparsity: 8.41550e-02     Total:   7.6478     mean:   1.5975\n",
      " Task losses:   6.4685     mean:   1.2937     Sharing: 6.33642e-04     Sparsity: 8.41432e-02     Total:   6.5533     mean:   1.3785\n",
      " Task losses:   4.4994     mean:   0.8999     Sharing: 1.02503e-03     Sparsity: 8.41314e-02     Total:   4.5845     mean:   0.9850\n",
      " Task losses:   3.9303     mean:   0.7861     Sharing: 1.27233e-03     Sparsity: 8.41195e-02     Total:   4.0157     mean:   0.8714\n",
      " Task losses:   5.4953     mean:   1.0991     Sharing: 1.00079e-03     Sparsity: 8.41083e-02     Total:   5.5805     mean:   1.1842\n",
      " Task losses:   8.0476     mean:   1.6095     Sharing: 8.90523e-04     Sparsity: 8.40967e-02     Total:   8.1325     mean:   1.6945\n",
      " Task losses:   4.7971     mean:   0.9594     Sharing: 9.43695e-04     Sparsity: 8.40828e-02     Total:   4.8821     mean:   1.0444\n",
      " Task losses:   4.3322     mean:   0.8664     Sharing: 1.04076e-03     Sparsity: 8.40691e-02     Total:   4.4173     mean:   0.9515\n",
      " Task losses:   3.8054     mean:   0.7611     Sharing: 1.18852e-03     Sparsity: 8.40570e-02     Total:   3.8906     mean:   0.8463\n",
      " Task losses:   4.3989     mean:   0.8798     Sharing: 1.00516e-03     Sparsity: 8.40455e-02     Total:   4.4840     mean:   0.9648\n",
      " Task losses:   4.8874     mean:   0.9775     Sharing: 7.38834e-04     Sparsity: 8.40349e-02     Total:   4.9722     mean:   1.0623\n",
      " Task losses:   3.6761     mean:   0.7352     Sharing: 1.16417e-03     Sparsity: 8.40239e-02     Total:   3.7613     mean:   0.8204\n",
      " Task losses:   6.9259     mean:   1.3852     Sharing: 1.44673e-03     Sparsity: 8.40133e-02     Total:   7.0113     mean:   1.4706\n",
      " Task losses:  17.8859     mean:   3.5772     Sharing: 1.15900e-03     Sparsity: 8.40071e-02     Total:  17.9710     mean:   3.6623\n",
      " Task losses:  19.2333     mean:   3.8467     Sharing: 7.86002e-04     Sparsity: 8.40009e-02     Total:  19.3181     mean:   3.9314\n",
      " Task losses:   4.8943     mean:   0.9789     Sharing: 5.88149e-04     Sparsity: 8.40202e-02     Total:   4.9789     mean:   1.0635\n",
      " Task losses:   3.6189     mean:   0.7238     Sharing: 7.29740e-04     Sparsity: 8.40370e-02     Total:   3.7037     mean:   0.8086\n",
      " Task losses:   3.9775     mean:   0.7955     Sharing: 1.16364e-03     Sparsity: 8.40516e-02     Total:   4.0628     mean:   0.8807\n",
      " Task losses:   3.7399     mean:   0.7480     Sharing: 1.04816e-03     Sparsity: 8.40642e-02     Total:   3.8250     mean:   0.8331\n",
      " Task losses:   5.2341     mean:   1.0468     Sharing: 6.51538e-04     Sparsity: 8.40752e-02     Total:   5.3188     mean:   1.1315\n",
      " Task losses:   6.8228     mean:   1.3646     Sharing: 8.07439e-04     Sparsity: 8.40840e-02     Total:   6.9077     mean:   1.4495\n",
      " Task losses:   4.4805     mean:   0.8961     Sharing: 9.41644e-04     Sparsity: 8.40928e-02     Total:   4.5656     mean:   0.9811\n",
      " Task losses:   7.0309     mean:   1.4062     Sharing: 1.04760e-03     Sparsity: 8.41003e-02     Total:   7.1161     mean:   1.4913\n",
      " Task losses:   8.9637     mean:   1.7927     Sharing: 9.13193e-04     Sparsity: 8.41052e-02     Total:   9.0487     mean:   1.8777\n",
      " Task losses:  10.6206     mean:   2.1241     Sharing: 4.71984e-04     Sparsity: 8.41101e-02     Total:  10.7051     mean:   2.2087\n",
      " Task losses:   6.6505     mean:   1.3301     Sharing: 1.17939e-03     Sparsity: 8.41144e-02     Total:   6.7358     mean:   1.4154\n",
      " Task losses:   4.4618     mean:   0.8924     Sharing: 1.46846e-03     Sparsity: 8.41154e-02     Total:   4.5473     mean:   0.9779\n",
      " Task losses:   4.4174     mean:   0.8835     Sharing: 1.39125e-03     Sparsity: 8.41153e-02     Total:   4.5029     mean:   0.9690\n",
      " Task losses:   4.4004     mean:   0.8801     Sharing: 1.19138e-03     Sparsity: 8.41143e-02     Total:   4.4857     mean:   0.9654\n",
      " Task losses:   4.3297     mean:   0.8659     Sharing: 9.31005e-04     Sparsity: 8.41122e-02     Total:   4.4147     mean:   0.9510\n",
      " Task losses:   4.3847     mean:   0.8769     Sharing: 8.81627e-04     Sparsity: 8.41137e-02     Total:   4.4697     mean:   0.9619\n",
      " Task losses:   3.3245     mean:   0.6649     Sharing: 1.23454e-03     Sparsity: 8.41120e-02     Total:   3.4099     mean:   0.7503\n",
      " Task losses:   4.0296     mean:   0.8059     Sharing: 1.35582e-03     Sparsity: 8.41099e-02     Total:   4.1151     mean:   0.8914\n",
      " Task losses:   5.4480     mean:   1.0896     Sharing: 1.05675e-03     Sparsity: 8.41068e-02     Total:   5.5332     mean:   1.1748\n",
      " Task losses:   5.9266     mean:   1.1853     Sharing: 6.95214e-04     Sparsity: 8.41019e-02     Total:   6.0114     mean:   1.2701\n",
      " Task losses:   2.9186     mean:   0.5837     Sharing: 7.45177e-04     Sparsity: 8.40969e-02     Total:   3.0034     mean:   0.6686\n",
      " Task losses:   4.9013     mean:   0.9803     Sharing: 9.22794e-04     Sparsity: 8.40916e-02     Total:   4.9863     mean:   1.0653\n",
      " Task losses:   7.5223     mean:   1.5045     Sharing: 8.20418e-04     Sparsity: 8.40856e-02     Total:   7.6072     mean:   1.5894\n",
      " Task losses:   7.6750     mean:   1.5350     Sharing: 4.69466e-04     Sparsity: 8.40794e-02     Total:   7.7596     mean:   1.6196\n",
      " Task losses:   5.3302     mean:   1.0660     Sharing: 8.19797e-04     Sparsity: 8.40730e-02     Total:   5.4151     mean:   1.1509\n",
      " Task losses:   5.3156     mean:   1.0631     Sharing: 1.04175e-03     Sparsity: 8.40656e-02     Total:   5.4007     mean:   1.1482\n",
      " Task losses:   5.2143     mean:   1.0429     Sharing: 6.92561e-04     Sparsity: 8.40584e-02     Total:   5.2991     mean:   1.1276\n",
      " Task losses:   5.6502     mean:   1.1300     Sharing: 7.82500e-04     Sparsity: 8.40511e-02     Total:   5.7350     mean:   1.2149\n",
      " Task losses:   5.4432     mean:   1.0886     Sharing: 7.99939e-04     Sparsity: 8.40450e-02     Total:   5.5281     mean:   1.1735\n",
      " Task losses:   4.6937     mean:   0.9387     Sharing: 5.75155e-04     Sparsity: 8.40394e-02     Total:   4.7783     mean:   1.0234\n",
      " Task losses:   4.7211     mean:   0.9442     Sharing: 7.18921e-04     Sparsity: 8.40342e-02     Total:   4.8058     mean:   1.0290\n",
      " Task losses:   5.3115     mean:   1.0623     Sharing: 9.53784e-04     Sparsity: 8.40295e-02     Total:   5.3965     mean:   1.1473\n",
      " Task losses:   7.2734     mean:   1.4547     Sharing: 8.76635e-04     Sparsity: 8.40250e-02     Total:   7.3583     mean:   1.5396\n",
      " Task losses:   7.0420     mean:   1.4084     Sharing: 5.09168e-04     Sparsity: 8.40196e-02     Total:   7.1265     mean:   1.4929\n",
      " Task losses:   7.4180     mean:   1.4836     Sharing: 1.05851e-03     Sparsity: 8.40139e-02     Total:   7.5031     mean:   1.5687\n",
      " Task losses:  11.1841     mean:   2.2368     Sharing: 1.42139e-03     Sparsity: 8.40088e-02     Total:  11.2695     mean:   2.3222\n",
      " Task losses:  10.2074     mean:   2.0415     Sharing: 1.24297e-03     Sparsity: 8.40047e-02     Total:  10.2927     mean:   2.1267\n",
      " Task losses:   4.6733     mean:   0.9347     Sharing: 8.72965e-04     Sparsity: 8.40024e-02     Total:   4.7582     mean:   1.0195\n",
      " Task losses:   5.1190     mean:   1.0238     Sharing: 7.79932e-04     Sparsity: 8.39995e-02     Total:   5.2038     mean:   1.1086\n",
      " Task losses:   5.4154     mean:   1.0831     Sharing: 7.50368e-04     Sparsity: 8.39961e-02     Total:   5.5001     mean:   1.1678\n",
      " Task losses:   4.4848     mean:   0.8970     Sharing: 9.56580e-04     Sparsity: 8.39933e-02     Total:   4.5698     mean:   0.9819\n",
      " Task losses:   4.1087     mean:   0.8217     Sharing: 1.14518e-03     Sparsity: 8.39899e-02     Total:   4.1938     mean:   0.9069\n",
      " Task losses:   4.3681     mean:   0.8736     Sharing: 8.88432e-04     Sparsity: 8.39858e-02     Total:   4.4530     mean:   0.9585\n",
      " Task losses:   4.1768     mean:   0.8354     Sharing: 1.15836e-03     Sparsity: 8.39814e-02     Total:   4.2620     mean:   0.9205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.8732     mean:   0.7746     Sharing: 1.12573e-03     Sparsity: 8.39768e-02     Total:   3.9583     mean:   0.8597\n",
      " Task losses:   3.6350     mean:   0.7270     Sharing: 1.10237e-03     Sparsity: 8.39719e-02     Total:   3.7200     mean:   0.8121\n",
      " Task losses:   3.5055     mean:   0.7011     Sharing: 1.32541e-03     Sparsity: 8.39668e-02     Total:   3.5908     mean:   0.7864\n",
      " Task losses:   4.0102     mean:   0.8020     Sharing: 1.09463e-03     Sparsity: 8.39620e-02     Total:   4.0953     mean:   0.8871\n",
      " Task losses:   3.6051     mean:   0.7210     Sharing: 9.72425e-04     Sparsity: 8.39571e-02     Total:   3.6900     mean:   0.8059\n",
      " Task losses:   4.2718     mean:   0.8544     Sharing: 9.36757e-04     Sparsity: 8.39522e-02     Total:   4.3567     mean:   0.9392\n",
      " Task losses:   4.4117     mean:   0.8823     Sharing: 1.12059e-03     Sparsity: 8.39463e-02     Total:   4.4968     mean:   0.9674\n",
      " Task losses:   4.1778     mean:   0.8356     Sharing: 9.23251e-04     Sparsity: 8.39407e-02     Total:   4.2626     mean:   0.9204\n",
      " Task losses:   4.1739     mean:   0.8348     Sharing: 9.50108e-04     Sparsity: 8.39351e-02     Total:   4.2588     mean:   0.9197\n",
      " Task losses:   3.6425     mean:   0.7285     Sharing: 1.17769e-03     Sparsity: 8.39304e-02     Total:   3.7276     mean:   0.8136\n",
      " Task losses:   3.6313     mean:   0.7263     Sharing: 1.15210e-03     Sparsity: 8.39260e-02     Total:   3.7164     mean:   0.8113\n",
      " Task losses:   3.2021     mean:   0.6404     Sharing: 7.52861e-04     Sparsity: 8.39213e-02     Total:   3.2868     mean:   0.7251\n",
      " Task losses:   3.2643     mean:   0.6529     Sharing: 8.25400e-04     Sparsity: 8.39162e-02     Total:   3.3491     mean:   0.7376\n",
      " Task losses:   3.1821     mean:   0.6364     Sharing: 1.37598e-03     Sparsity: 8.39104e-02     Total:   3.2674     mean:   0.7217\n",
      " Task losses:   3.0914     mean:   0.6183     Sharing: 1.41189e-03     Sparsity: 8.39046e-02     Total:   3.1767     mean:   0.7036\n",
      " Task losses:   3.6566     mean:   0.7313     Sharing: 1.00321e-03     Sparsity: 8.38986e-02     Total:   3.7415     mean:   0.8162\n",
      " Task losses:   3.7840     mean:   0.7568     Sharing: 7.52196e-04     Sparsity: 8.38931e-02     Total:   3.8686     mean:   0.8414\n",
      " Task losses:   3.6984     mean:   0.7397     Sharing: 1.01598e-03     Sparsity: 8.38875e-02     Total:   3.7833     mean:   0.8246\n",
      " Task losses:   3.7407     mean:   0.7481     Sharing: 1.06803e-03     Sparsity: 8.38820e-02     Total:   3.8256     mean:   0.8331\n",
      " Task losses:   4.1186     mean:   0.8237     Sharing: 9.90699e-04     Sparsity: 8.38766e-02     Total:   4.2035     mean:   0.9086\n",
      " Task losses:   3.7705     mean:   0.7541     Sharing: 1.05763e-03     Sparsity: 8.38711e-02     Total:   3.8555     mean:   0.8390\n",
      " Task losses:   3.7733     mean:   0.7547     Sharing: 1.01051e-03     Sparsity: 8.38638e-02     Total:   3.8581     mean:   0.8395\n",
      " Task losses:   3.5478     mean:   0.7096     Sharing: 8.59305e-04     Sparsity: 8.38564e-02     Total:   3.6325     mean:   0.7943\n",
      " Task losses:   3.7314     mean:   0.7463     Sharing: 9.50024e-04     Sparsity: 8.38490e-02     Total:   3.8162     mean:   0.8311\n",
      " Task losses:   4.1038     mean:   0.8208     Sharing: 1.00983e-03     Sparsity: 8.38414e-02     Total:   4.1887     mean:   0.9056\n",
      " Task losses:   4.2933     mean:   0.8587     Sharing: 9.90595e-04     Sparsity: 8.38335e-02     Total:   4.3781     mean:   0.9435\n",
      " Task losses:   3.9536     mean:   0.7907     Sharing: 7.20685e-04     Sparsity: 8.38280e-02     Total:   4.0382     mean:   0.8753\n",
      " Task losses:   4.0207     mean:   0.8041     Sharing: 9.24458e-04     Sparsity: 8.38226e-02     Total:   4.1054     mean:   0.8889\n",
      " Task losses:   3.8588     mean:   0.7718     Sharing: 1.11281e-03     Sparsity: 8.38173e-02     Total:   3.9437     mean:   0.8567\n",
      " Task losses:   3.7573     mean:   0.7515     Sharing: 9.93242e-04     Sparsity: 8.38120e-02     Total:   3.8421     mean:   0.8363\n",
      " Task losses:   3.2649     mean:   0.6530     Sharing: 7.14426e-04     Sparsity: 8.38059e-02     Total:   3.3495     mean:   0.7375\n",
      " Task losses:   5.1139     mean:   1.0228     Sharing: 6.63787e-04     Sparsity: 8.37995e-02     Total:   5.1983     mean:   1.1072\n",
      " Task losses:   4.6090     mean:   0.9218     Sharing: 7.63173e-04     Sparsity: 8.37930e-02     Total:   4.6936     mean:   1.0064\n",
      " Task losses:   6.6476     mean:   1.3295     Sharing: 7.68021e-04     Sparsity: 8.37871e-02     Total:   6.7321     mean:   1.4141\n",
      " Task losses:   5.6320     mean:   1.1264     Sharing: 7.73430e-04     Sparsity: 8.37802e-02     Total:   5.7165     mean:   1.2109\n",
      " Task losses:   4.3371     mean:   0.8674     Sharing: 7.44681e-04     Sparsity: 8.37734e-02     Total:   4.4216     mean:   0.9519\n",
      " Task losses:   3.8306     mean:   0.7661     Sharing: 7.23397e-04     Sparsity: 8.37665e-02     Total:   3.9151     mean:   0.8506\n",
      " Task losses:   4.5952     mean:   0.9190     Sharing: 9.57271e-04     Sparsity: 8.37596e-02     Total:   4.6799     mean:   1.0038\n",
      " Task losses:   5.2908     mean:   1.0582     Sharing: 8.96215e-04     Sparsity: 8.37522e-02     Total:   5.3754     mean:   1.1428\n",
      " Task losses:   4.1526     mean:   0.8305     Sharing: 8.77663e-04     Sparsity: 8.37446e-02     Total:   4.2373     mean:   0.9152\n",
      " Task losses:   3.2370     mean:   0.6474     Sharing: 1.04696e-03     Sparsity: 8.37366e-02     Total:   3.3218     mean:   0.7322\n",
      " Task losses:   2.8520     mean:   0.5704     Sharing: 8.42686e-04     Sparsity: 8.37283e-02     Total:   2.9366     mean:   0.6550\n",
      " Task losses:   2.7386     mean:   0.5477     Sharing: 1.31708e-03     Sparsity: 8.37202e-02     Total:   2.8236     mean:   0.6328\n",
      " Task losses:   2.6257     mean:   0.5251     Sharing: 1.32437e-03     Sparsity: 8.37124e-02     Total:   2.7108     mean:   0.6102\n",
      " Task losses:   3.5293     mean:   0.7059     Sharing: 7.35571e-04     Sparsity: 8.37049e-02     Total:   3.6137     mean:   0.7903\n",
      "[e]Policy training epoch:95 iteration:  20620 -  Total Loss: 3.6137     Task Loss: 3.5293  Policy Losses:  Sparsity: 0.0837      Sharing: 7.35571e-04 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 96 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:96 iteration:  20728 -  Total Loss: 4.5063     Task Loss: 4.5063  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96    | 0.00018 0.83624 0.78492 0.78517 0.74291|  3.6511  4.0906  4.7815 12.5232|   42.5|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 96 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.3224     mean:   0.6645     Sharing: 8.09977e-04     Sparsity: 8.36984e-02     Total:   3.4069     mean:   0.7490\n",
      " Task losses:   5.8843     mean:   1.1769     Sharing: 1.25872e-03     Sparsity: 8.36920e-02     Total:   5.9692     mean:   1.2618\n",
      " Task losses:   6.3538     mean:   1.2708     Sharing: 1.00536e-03     Sparsity: 8.36848e-02     Total:   6.4385     mean:   1.3555\n",
      " Task losses:   7.1709     mean:   1.4342     Sharing: 1.06171e-03     Sparsity: 8.36765e-02     Total:   7.2556     mean:   1.5189\n",
      " Task losses:   7.6255     mean:   1.5251     Sharing: 1.22727e-03     Sparsity: 8.36679e-02     Total:   7.7104     mean:   1.6100\n",
      " Task losses:   6.1162     mean:   1.2232     Sharing: 1.01544e-03     Sparsity: 8.36578e-02     Total:   6.2009     mean:   1.3079\n",
      " Task losses:   4.2099     mean:   0.8420     Sharing: 6.90048e-04     Sparsity: 8.36478e-02     Total:   4.2943     mean:   0.9263\n",
      " Task losses:   3.6145     mean:   0.7229     Sharing: 9.60668e-04     Sparsity: 8.36371e-02     Total:   3.6991     mean:   0.8075\n",
      " Task losses:   4.6403     mean:   0.9281     Sharing: 1.10774e-03     Sparsity: 8.36258e-02     Total:   4.7250     mean:   1.0128\n",
      " Task losses:   8.4580     mean:   1.6916     Sharing: 1.05553e-03     Sparsity: 8.36147e-02     Total:   8.5427     mean:   1.7763\n",
      " Task losses:   4.5601     mean:   0.9120     Sharing: 9.37422e-04     Sparsity: 8.36017e-02     Total:   4.6446     mean:   0.9966\n",
      " Task losses:   4.5253     mean:   0.9051     Sharing: 6.02822e-04     Sparsity: 8.35885e-02     Total:   4.6095     mean:   0.9893\n",
      " Task losses:   3.7119     mean:   0.7424     Sharing: 1.19620e-03     Sparsity: 8.35760e-02     Total:   3.7967     mean:   0.8272\n",
      " Task losses:   3.2157     mean:   0.6431     Sharing: 1.30842e-03     Sparsity: 8.35638e-02     Total:   3.3006     mean:   0.7280\n",
      " Task losses:   3.0500     mean:   0.6100     Sharing: 9.92715e-04     Sparsity: 8.35522e-02     Total:   3.1346     mean:   0.6946\n",
      " Task losses:   3.1640     mean:   0.6328     Sharing: 1.04332e-03     Sparsity: 8.35411e-02     Total:   3.2486     mean:   0.7174\n",
      " Task losses:   4.5610     mean:   0.9122     Sharing: 1.01650e-03     Sparsity: 8.35305e-02     Total:   4.6455     mean:   0.9967\n",
      " Task losses:  14.2834     mean:   2.8567     Sharing: 1.07566e-03     Sparsity: 8.35202e-02     Total:  14.3680     mean:   2.9413\n",
      " Task losses:  14.1771     mean:   2.8354     Sharing: 1.40549e-03     Sparsity: 8.35117e-02     Total:  14.2620     mean:   2.9203\n",
      " Task losses:   3.3060     mean:   0.6612     Sharing: 1.63289e-03     Sparsity: 8.35031e-02     Total:   3.3911     mean:   0.7463\n",
      " Task losses:   2.9052     mean:   0.5810     Sharing: 2.04570e-03     Sparsity: 8.34948e-02     Total:   2.9907     mean:   0.6666\n",
      " Task losses:   3.2139     mean:   0.6428     Sharing: 2.24744e-03     Sparsity: 8.34867e-02     Total:   3.2997     mean:   0.7285\n",
      " Task losses:   3.3050     mean:   0.6610     Sharing: 1.84528e-03     Sparsity: 8.34785e-02     Total:   3.3903     mean:   0.7463\n",
      " Task losses:   5.2060     mean:   1.0412     Sharing: 1.66244e-03     Sparsity: 8.34692e-02     Total:   5.2912     mean:   1.1263\n",
      " Task losses:   5.1818     mean:   1.0364     Sharing: 1.40038e-03     Sparsity: 8.34601e-02     Total:   5.2667     mean:   1.1212\n",
      " Task losses:   4.1924     mean:   0.8385     Sharing: 1.07731e-03     Sparsity: 8.34511e-02     Total:   4.2770     mean:   0.9230\n",
      " Task losses:   6.4002     mean:   1.2800     Sharing: 1.19344e-03     Sparsity: 8.34422e-02     Total:   6.4848     mean:   1.3647\n",
      " Task losses:   7.1812     mean:   1.4362     Sharing: 1.49945e-03     Sparsity: 8.34331e-02     Total:   7.2661     mean:   1.5212\n",
      " Task losses:   7.8967     mean:   1.5793     Sharing: 1.64872e-03     Sparsity: 8.34225e-02     Total:   7.9818     mean:   1.6644\n",
      " Task losses:   6.4482     mean:   1.2896     Sharing: 1.20848e-03     Sparsity: 8.34088e-02     Total:   6.5329     mean:   1.3743\n",
      " Task losses:   4.8522     mean:   0.9704     Sharing: 6.62327e-04     Sparsity: 8.33944e-02     Total:   4.9362     mean:   1.0545\n",
      " Task losses:   4.6801     mean:   0.9360     Sharing: 9.40015e-04     Sparsity: 8.33806e-02     Total:   4.7644     mean:   1.0203\n",
      " Task losses:   4.4934     mean:   0.8987     Sharing: 1.23074e-03     Sparsity: 8.33654e-02     Total:   4.5780     mean:   0.9833\n",
      " Task losses:   4.0920     mean:   0.8184     Sharing: 1.08123e-03     Sparsity: 8.33506e-02     Total:   4.1764     mean:   0.9028\n",
      " Task losses:   4.7305     mean:   0.9461     Sharing: 9.94946e-04     Sparsity: 8.33452e-02     Total:   4.8148     mean:   1.0304\n",
      " Task losses:   3.8774     mean:   0.7755     Sharing: 1.20621e-03     Sparsity: 8.33437e-02     Total:   3.9619     mean:   0.8600\n",
      " Task losses:   5.5185     mean:   1.1037     Sharing: 1.29906e-03     Sparsity: 8.33414e-02     Total:   5.6032     mean:   1.1883\n",
      " Task losses:   5.1650     mean:   1.0330     Sharing: 9.44803e-04     Sparsity: 8.33380e-02     Total:   5.2492     mean:   1.1173\n",
      " Task losses:   5.8747     mean:   1.1749     Sharing: 8.18993e-04     Sparsity: 8.33338e-02     Total:   5.9589     mean:   1.2591\n",
      " Task losses:   2.9411     mean:   0.5882     Sharing: 9.84823e-04     Sparsity: 8.33283e-02     Total:   3.0254     mean:   0.6725\n",
      " Task losses:   3.6007     mean:   0.7201     Sharing: 9.63772e-04     Sparsity: 8.33226e-02     Total:   3.6850     mean:   0.8044\n",
      " Task losses:   4.9024     mean:   0.9805     Sharing: 1.00068e-03     Sparsity: 8.33169e-02     Total:   4.9867     mean:   1.0648\n",
      " Task losses:   4.5845     mean:   0.9169     Sharing: 9.23246e-04     Sparsity: 8.33113e-02     Total:   4.6687     mean:   1.0011\n",
      " Task losses:   4.2065     mean:   0.8413     Sharing: 1.01939e-03     Sparsity: 8.33050e-02     Total:   4.2909     mean:   0.9256\n",
      " Task losses:   4.3665     mean:   0.8733     Sharing: 1.00216e-03     Sparsity: 8.32979e-02     Total:   4.4508     mean:   0.9576\n",
      " Task losses:   4.3386     mean:   0.8677     Sharing: 1.03646e-03     Sparsity: 8.32900e-02     Total:   4.4229     mean:   0.9520\n",
      " Task losses:   4.1068     mean:   0.8214     Sharing: 8.71986e-04     Sparsity: 8.32815e-02     Total:   4.1910     mean:   0.9055\n",
      " Task losses:   4.6984     mean:   0.9397     Sharing: 1.03826e-03     Sparsity: 8.32728e-02     Total:   4.7827     mean:   1.0240\n",
      " Task losses:   4.0547     mean:   0.8109     Sharing: 1.22951e-03     Sparsity: 8.32637e-02     Total:   4.1392     mean:   0.8954\n",
      " Task losses:   4.0486     mean:   0.8097     Sharing: 1.12601e-03     Sparsity: 8.32548e-02     Total:   4.1330     mean:   0.8941\n",
      " Task losses:   4.3646     mean:   0.8729     Sharing: 1.02624e-03     Sparsity: 8.32461e-02     Total:   4.4489     mean:   0.9572\n",
      " Task losses:   6.5696     mean:   1.3139     Sharing: 1.09962e-03     Sparsity: 8.32376e-02     Total:   6.6539     mean:   1.3983\n",
      " Task losses:   5.2265     mean:   1.0453     Sharing: 1.10105e-03     Sparsity: 8.32292e-02     Total:   5.3108     mean:   1.1296\n",
      " Task losses:   4.3320     mean:   0.8664     Sharing: 9.99669e-04     Sparsity: 8.32195e-02     Total:   4.4162     mean:   0.9506\n",
      " Task losses:   6.1419     mean:   1.2284     Sharing: 7.84387e-04     Sparsity: 8.32100e-02     Total:   6.2259     mean:   1.3124\n",
      " Task losses:   5.5348     mean:   1.1070     Sharing: 8.42080e-04     Sparsity: 8.32020e-02     Total:   5.6188     mean:   1.1910\n",
      " Task losses:   3.6710     mean:   0.7342     Sharing: 1.13418e-03     Sparsity: 8.31938e-02     Total:   3.7553     mean:   0.8185\n",
      " Task losses:   3.9430     mean:   0.7886     Sharing: 1.15381e-03     Sparsity: 8.31857e-02     Total:   4.0274     mean:   0.8729\n",
      " Task losses:   4.0746     mean:   0.8149     Sharing: 6.38555e-04     Sparsity: 8.31772e-02     Total:   4.1584     mean:   0.8987\n",
      " Task losses:   3.3587     mean:   0.6717     Sharing: 8.49987e-04     Sparsity: 8.31691e-02     Total:   3.4427     mean:   0.7558\n",
      " Task losses:   3.3346     mean:   0.6669     Sharing: 1.06516e-03     Sparsity: 8.31609e-02     Total:   3.4188     mean:   0.7511\n",
      " Task losses:   3.5025     mean:   0.7005     Sharing: 7.36232e-04     Sparsity: 8.31530e-02     Total:   3.5864     mean:   0.7844\n",
      " Task losses:   3.3623     mean:   0.6725     Sharing: 7.75422e-04     Sparsity: 8.31452e-02     Total:   3.4462     mean:   0.7564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.5285     mean:   0.7057     Sharing: 1.26601e-03     Sparsity: 8.31375e-02     Total:   3.6129     mean:   0.7901\n",
      " Task losses:   4.4005     mean:   0.8801     Sharing: 1.16292e-03     Sparsity: 8.31299e-02     Total:   4.4848     mean:   0.9644\n",
      " Task losses:   2.8890     mean:   0.5778     Sharing: 7.22090e-04     Sparsity: 8.31222e-02     Total:   2.9728     mean:   0.6616\n",
      " Task losses:   3.5187     mean:   0.7037     Sharing: 8.03386e-04     Sparsity: 8.31146e-02     Total:   3.6026     mean:   0.7876\n",
      " Task losses:   3.4588     mean:   0.6918     Sharing: 7.33187e-04     Sparsity: 8.31063e-02     Total:   3.5426     mean:   0.7756\n",
      " Task losses:   3.4281     mean:   0.6856     Sharing: 7.13617e-04     Sparsity: 8.30981e-02     Total:   3.5119     mean:   0.7694\n",
      " Task losses:   3.5305     mean:   0.7061     Sharing: 7.93278e-04     Sparsity: 8.30896e-02     Total:   3.6143     mean:   0.7900\n",
      " Task losses:   3.2962     mean:   0.6592     Sharing: 7.85366e-04     Sparsity: 8.30812e-02     Total:   3.3801     mean:   0.7431\n",
      " Task losses:   3.1602     mean:   0.6320     Sharing: 8.91546e-04     Sparsity: 8.30731e-02     Total:   3.2442     mean:   0.7160\n",
      " Task losses:   3.4798     mean:   0.6960     Sharing: 8.36020e-04     Sparsity: 8.30648e-02     Total:   3.5637     mean:   0.7799\n",
      " Task losses:   3.8935     mean:   0.7787     Sharing: 8.63180e-04     Sparsity: 8.30565e-02     Total:   3.9774     mean:   0.8626\n",
      " Task losses:   2.8675     mean:   0.5735     Sharing: 8.76149e-04     Sparsity: 8.30483e-02     Total:   2.9514     mean:   0.6574\n",
      " Task losses:   2.8138     mean:   0.5628     Sharing: 8.12233e-04     Sparsity: 8.30401e-02     Total:   2.8976     mean:   0.6466\n",
      " Task losses:   2.8131     mean:   0.5626     Sharing: 8.54010e-04     Sparsity: 8.30319e-02     Total:   2.8970     mean:   0.6465\n",
      " Task losses:   2.7712     mean:   0.5542     Sharing: 9.59148e-04     Sparsity: 8.30238e-02     Total:   2.8552     mean:   0.6382\n",
      " Task losses:   3.4407     mean:   0.6881     Sharing: 8.45621e-04     Sparsity: 8.30161e-02     Total:   3.5245     mean:   0.7720\n",
      " Task losses:   3.1449     mean:   0.6290     Sharing: 8.40416e-04     Sparsity: 8.30081e-02     Total:   3.2287     mean:   0.7128\n",
      " Task losses:   3.5078     mean:   0.7016     Sharing: 1.26931e-03     Sparsity: 8.30000e-02     Total:   3.5920     mean:   0.7858\n",
      " Task losses:   3.5165     mean:   0.7033     Sharing: 1.28925e-03     Sparsity: 8.29923e-02     Total:   3.6008     mean:   0.7876\n",
      " Task losses:   3.9742     mean:   0.7948     Sharing: 1.26472e-03     Sparsity: 8.29837e-02     Total:   4.0584     mean:   0.8791\n",
      " Task losses:   3.7175     mean:   0.7435     Sharing: 1.09474e-03     Sparsity: 8.29756e-02     Total:   3.8015     mean:   0.8276\n",
      " Task losses:   3.7456     mean:   0.7491     Sharing: 9.01371e-04     Sparsity: 8.29675e-02     Total:   3.8295     mean:   0.8330\n",
      " Task losses:   3.2163     mean:   0.6433     Sharing: 1.00648e-03     Sparsity: 8.29588e-02     Total:   3.3003     mean:   0.7272\n",
      " Task losses:   3.4450     mean:   0.6890     Sharing: 1.06788e-03     Sparsity: 8.29504e-02     Total:   3.5291     mean:   0.7730\n",
      " Task losses:   3.6857     mean:   0.7371     Sharing: 1.07640e-03     Sparsity: 8.29421e-02     Total:   3.7697     mean:   0.8212\n",
      " Task losses:   3.8664     mean:   0.7733     Sharing: 8.63334e-04     Sparsity: 8.29340e-02     Total:   3.9502     mean:   0.8571\n",
      " Task losses:   3.6680     mean:   0.7336     Sharing: 1.05071e-03     Sparsity: 8.29261e-02     Total:   3.7520     mean:   0.8176\n",
      " Task losses:   3.5897     mean:   0.7179     Sharing: 1.27675e-03     Sparsity: 8.29181e-02     Total:   3.6739     mean:   0.8021\n",
      " Task losses:   3.7352     mean:   0.7470     Sharing: 1.01334e-03     Sparsity: 8.29098e-02     Total:   3.8192     mean:   0.8310\n",
      " Task losses:   3.6858     mean:   0.7372     Sharing: 5.65936e-04     Sparsity: 8.29019e-02     Total:   3.7693     mean:   0.8206\n",
      " Task losses:   2.8998     mean:   0.5800     Sharing: 7.48023e-04     Sparsity: 8.28937e-02     Total:   2.9835     mean:   0.6636\n",
      " Task losses:   3.9815     mean:   0.7963     Sharing: 9.89969e-04     Sparsity: 8.28860e-02     Total:   4.0654     mean:   0.8802\n",
      " Task losses:   4.0058     mean:   0.8012     Sharing: 1.00450e-03     Sparsity: 8.28786e-02     Total:   4.0897     mean:   0.8850\n",
      " Task losses:   4.1027     mean:   0.8205     Sharing: 7.44268e-04     Sparsity: 8.28712e-02     Total:   4.1863     mean:   0.9042\n",
      " Task losses:   4.0777     mean:   0.8155     Sharing: 7.33023e-04     Sparsity: 8.28649e-02     Total:   4.1613     mean:   0.8991\n",
      " Task losses:   3.0903     mean:   0.6181     Sharing: 7.43161e-04     Sparsity: 8.28587e-02     Total:   3.1739     mean:   0.7017\n",
      " Task losses:   3.1723     mean:   0.6345     Sharing: 5.89306e-04     Sparsity: 8.28526e-02     Total:   3.2558     mean:   0.7179\n",
      " Task losses:   2.5236     mean:   0.5047     Sharing: 4.89364e-04     Sparsity: 8.28461e-02     Total:   2.6070     mean:   0.5881\n",
      " Task losses:   4.2144     mean:   0.8429     Sharing: 7.59214e-04     Sparsity: 8.28406e-02     Total:   4.2980     mean:   0.9265\n",
      " Task losses:   3.6231     mean:   0.7246     Sharing: 8.96990e-04     Sparsity: 8.28355e-02     Total:   3.7068     mean:   0.8084\n",
      " Task losses:   2.9594     mean:   0.5919     Sharing: 7.32889e-04     Sparsity: 8.28303e-02     Total:   3.0430     mean:   0.6755\n",
      " Task losses:   2.5223     mean:   0.5045     Sharing: 8.69279e-04     Sparsity: 8.28248e-02     Total:   2.6060     mean:   0.5882\n",
      " Task losses:   2.5977     mean:   0.5195     Sharing: 8.76134e-04     Sparsity: 8.28191e-02     Total:   2.6814     mean:   0.6032\n",
      " Task losses:   2.7701     mean:   0.5540     Sharing: 7.53333e-04     Sparsity: 8.28133e-02     Total:   2.8537     mean:   0.6376\n",
      " Task losses:   3.8286     mean:   0.7657     Sharing: 6.69564e-04     Sparsity: 8.28073e-02     Total:   3.9121     mean:   0.8492\n",
      "[e]Policy training epoch:96 iteration:  20836 -  Total Loss: 3.9121     Task Loss: 3.8286  Policy Losses:  Sparsity: 0.0828      Sharing: 6.69564e-04 \n",
      "[e]Policy training epoch:96 decay gumbel temp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 97 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:97 iteration:  20944 -  Total Loss: 4.8378     Task Loss: 4.8378  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97    | 0.00018 0.80685 0.79127 0.79292 0.74530|  3.7145  4.2813  4.0950 12.0908|   46.3|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 97 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.7509     mean:   0.7502     Sharing: 1.07665e-03     Sparsity: 8.28006e-02     Total:   3.8348     mean:   0.8341\n",
      " Task losses:   6.9258     mean:   1.3852     Sharing: 1.15331e-03     Sparsity: 8.27941e-02     Total:   7.0097     mean:   1.4691\n",
      " Task losses:   7.6414     mean:   1.5283     Sharing: 7.95851e-04     Sparsity: 8.27875e-02     Total:   7.7250     mean:   1.6119\n",
      " Task losses:   9.3072     mean:   1.8614     Sharing: 9.75276e-04     Sparsity: 8.27861e-02     Total:   9.3910     mean:   1.9452\n",
      " Task losses:   8.3766     mean:   1.6753     Sharing: 9.59272e-04     Sparsity: 8.27845e-02     Total:   8.4603     mean:   1.7591\n",
      " Task losses:   8.5163     mean:   1.7033     Sharing: 9.78177e-04     Sparsity: 8.27909e-02     Total:   8.6001     mean:   1.7870\n",
      " Task losses:   5.0745     mean:   1.0149     Sharing: 7.00861e-04     Sparsity: 8.27961e-02     Total:   5.1580     mean:   1.0984\n",
      " Task losses:   4.1417     mean:   0.8283     Sharing: 7.28061e-04     Sparsity: 8.28007e-02     Total:   4.2252     mean:   0.9119\n",
      " Task losses:   5.7875     mean:   1.1575     Sharing: 8.43311e-04     Sparsity: 8.28034e-02     Total:   5.8712     mean:   1.2411\n",
      " Task losses:  11.1591     mean:   2.2318     Sharing: 8.21571e-04     Sparsity: 8.28062e-02     Total:  11.2427     mean:   2.3154\n",
      " Task losses:   5.7519     mean:   1.1504     Sharing: 9.06408e-04     Sparsity: 8.28072e-02     Total:   5.8356     mean:   1.2341\n",
      " Task losses:   5.0227     mean:   1.0045     Sharing: 8.09620e-04     Sparsity: 8.28064e-02     Total:   5.1064     mean:   1.0882\n",
      " Task losses:   3.5088     mean:   0.7018     Sharing: 9.64945e-04     Sparsity: 8.28047e-02     Total:   3.5926     mean:   0.7855\n",
      " Task losses:   3.2637     mean:   0.6527     Sharing: 6.91990e-04     Sparsity: 8.28018e-02     Total:   3.3472     mean:   0.7362\n",
      " Task losses:   2.9155     mean:   0.5831     Sharing: 9.69216e-04     Sparsity: 8.27981e-02     Total:   2.9992     mean:   0.6669\n",
      " Task losses:   2.8352     mean:   0.5670     Sharing: 1.18311e-03     Sparsity: 8.27940e-02     Total:   2.9192     mean:   0.6510\n",
      " Task losses:   4.5761     mean:   0.9152     Sharing: 1.27465e-03     Sparsity: 8.27894e-02     Total:   4.6602     mean:   0.9993\n",
      " Task losses:  16.9162     mean:   3.3832     Sharing: 1.01669e-03     Sparsity: 8.27860e-02     Total:  17.0000     mean:   3.4671\n",
      " Task losses:  15.6531     mean:   3.1306     Sharing: 9.16729e-04     Sparsity: 8.27791e-02     Total:  15.7368     mean:   3.2143\n",
      " Task losses:   3.1448     mean:   0.6290     Sharing: 9.92959e-04     Sparsity: 8.27763e-02     Total:   3.2286     mean:   0.7127\n",
      " Task losses:   2.9006     mean:   0.5801     Sharing: 7.75382e-04     Sparsity: 8.27733e-02     Total:   2.9842     mean:   0.6637\n",
      " Task losses:   3.2563     mean:   0.6513     Sharing: 8.18114e-04     Sparsity: 8.27696e-02     Total:   3.3399     mean:   0.7349\n",
      " Task losses:   3.1040     mean:   0.6208     Sharing: 1.04870e-03     Sparsity: 8.27656e-02     Total:   3.1878     mean:   0.7046\n",
      " Task losses:   4.0950     mean:   0.8190     Sharing: 8.84136e-04     Sparsity: 8.27608e-02     Total:   4.1786     mean:   0.9026\n",
      " Task losses:   5.4976     mean:   1.0995     Sharing: 5.88631e-04     Sparsity: 8.27555e-02     Total:   5.5809     mean:   1.1829\n",
      " Task losses:   3.7796     mean:   0.7559     Sharing: 8.57045e-04     Sparsity: 8.27503e-02     Total:   3.8633     mean:   0.8395\n",
      " Task losses:   7.4331     mean:   1.4866     Sharing: 9.55855e-04     Sparsity: 8.27452e-02     Total:   7.5168     mean:   1.5703\n",
      " Task losses:   9.1001     mean:   1.8200     Sharing: 1.03355e-03     Sparsity: 8.27398e-02     Total:   9.1838     mean:   1.9038\n",
      " Task losses:   9.5615     mean:   1.9123     Sharing: 9.46929e-04     Sparsity: 8.27354e-02     Total:   9.6452     mean:   1.9960\n",
      " Task losses:   6.4188     mean:   1.2838     Sharing: 8.83490e-04     Sparsity: 8.27341e-02     Total:   6.5025     mean:   1.3674\n",
      " Task losses:   4.3432     mean:   0.8686     Sharing: 7.08977e-04     Sparsity: 8.27319e-02     Total:   4.4266     mean:   0.9521\n",
      " Task losses:   4.1827     mean:   0.8365     Sharing: 9.21374e-04     Sparsity: 8.27285e-02     Total:   4.2663     mean:   0.9202\n",
      " Task losses:   4.2956     mean:   0.8591     Sharing: 1.08733e-03     Sparsity: 8.27242e-02     Total:   4.3794     mean:   0.9429\n",
      " Task losses:   7.1228     mean:   1.4246     Sharing: 8.61545e-04     Sparsity: 8.27203e-02     Total:   7.2064     mean:   1.5081\n",
      " Task losses:  12.9153     mean:   2.5831     Sharing: 5.88223e-04     Sparsity: 8.27168e-02     Total:  12.9986     mean:   2.6664\n",
      " Task losses:   4.4493     mean:   0.8899     Sharing: 1.09775e-03     Sparsity: 8.27104e-02     Total:   4.5331     mean:   0.9737\n",
      " Task losses:   6.1368     mean:   1.2274     Sharing: 1.18060e-03     Sparsity: 8.27038e-02     Total:   6.2207     mean:   1.3113\n",
      " Task losses:   7.0974     mean:   1.4195     Sharing: 1.17399e-03     Sparsity: 8.26964e-02     Total:   7.1813     mean:   1.5033\n",
      " Task losses:   5.8078     mean:   1.1616     Sharing: 8.25147e-04     Sparsity: 8.26883e-02     Total:   5.8913     mean:   1.2451\n",
      " Task losses:   3.6570     mean:   0.7314     Sharing: 5.76695e-04     Sparsity: 8.26782e-02     Total:   3.7402     mean:   0.8146\n",
      " Task losses:   4.9284     mean:   0.9857     Sharing: 9.44148e-04     Sparsity: 8.26678e-02     Total:   5.0120     mean:   1.0693\n",
      " Task losses:   6.9218     mean:   1.3844     Sharing: 9.34015e-04     Sparsity: 8.26572e-02     Total:   7.0054     mean:   1.4679\n",
      " Task losses:   6.1547     mean:   1.2309     Sharing: 6.10143e-04     Sparsity: 8.26454e-02     Total:   6.2379     mean:   1.3142\n",
      " Task losses:   5.7234     mean:   1.1447     Sharing: 7.57699e-04     Sparsity: 8.26324e-02     Total:   5.8068     mean:   1.2281\n",
      " Task losses:   5.3897     mean:   1.0779     Sharing: 7.86458e-04     Sparsity: 8.26196e-02     Total:   5.4731     mean:   1.1614\n",
      " Task losses:   5.4224     mean:   1.0845     Sharing: 6.44207e-04     Sparsity: 8.26068e-02     Total:   5.5057     mean:   1.1677\n",
      " Task losses:   5.0119     mean:   1.0024     Sharing: 7.83111e-04     Sparsity: 8.25943e-02     Total:   5.0953     mean:   1.0858\n",
      " Task losses:   5.4172     mean:   1.0834     Sharing: 7.60019e-04     Sparsity: 8.25824e-02     Total:   5.5005     mean:   1.1668\n",
      " Task losses:   4.0367     mean:   0.8073     Sharing: 4.86528e-04     Sparsity: 8.25725e-02     Total:   4.1198     mean:   0.8904\n",
      " Task losses:   4.1616     mean:   0.8323     Sharing: 7.81765e-04     Sparsity: 8.25628e-02     Total:   4.2449     mean:   0.9157\n",
      " Task losses:   4.1637     mean:   0.8327     Sharing: 9.03338e-04     Sparsity: 8.25534e-02     Total:   4.2471     mean:   0.9162\n",
      " Task losses:   6.1578     mean:   1.2316     Sharing: 5.27332e-04     Sparsity: 8.25444e-02     Total:   6.2409     mean:   1.3146\n",
      " Task losses:   4.5659     mean:   0.9132     Sharing: 7.01870e-04     Sparsity: 8.25356e-02     Total:   4.6492     mean:   0.9964\n",
      " Task losses:   3.5957     mean:   0.7191     Sharing: 1.12822e-03     Sparsity: 8.25269e-02     Total:   3.6794     mean:   0.8028\n",
      " Task losses:   4.2833     mean:   0.8567     Sharing: 1.03002e-03     Sparsity: 8.25172e-02     Total:   4.3669     mean:   0.9402\n",
      " Task losses:   4.7259     mean:   0.9452     Sharing: 7.52722e-04     Sparsity: 8.25067e-02     Total:   4.8092     mean:   1.0284\n",
      " Task losses:   3.3712     mean:   0.6742     Sharing: 7.19105e-04     Sparsity: 8.24961e-02     Total:   3.4544     mean:   0.7575\n",
      " Task losses:   4.0144     mean:   0.8029     Sharing: 9.24478e-04     Sparsity: 8.24859e-02     Total:   4.0978     mean:   0.8863\n",
      " Task losses:   3.5038     mean:   0.7008     Sharing: 8.38175e-04     Sparsity: 8.24759e-02     Total:   3.5871     mean:   0.7841\n",
      " Task losses:   3.2638     mean:   0.6528     Sharing: 7.46538e-04     Sparsity: 8.24663e-02     Total:   3.3470     mean:   0.7360\n",
      " Task losses:   3.7074     mean:   0.7415     Sharing: 8.04603e-04     Sparsity: 8.24566e-02     Total:   3.7906     mean:   0.8247\n",
      " Task losses:   3.3454     mean:   0.6691     Sharing: 6.65923e-04     Sparsity: 8.24462e-02     Total:   3.4285     mean:   0.7522\n",
      " Task losses:   3.2372     mean:   0.6474     Sharing: 5.68827e-04     Sparsity: 8.24372e-02     Total:   3.3202     mean:   0.7304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.4193     mean:   0.6839     Sharing: 6.32798e-04     Sparsity: 8.24285e-02     Total:   3.5024     mean:   0.7669\n",
      " Task losses:   4.0929     mean:   0.8186     Sharing: 5.32776e-04     Sparsity: 8.24197e-02     Total:   4.1759     mean:   0.9015\n",
      " Task losses:   2.9908     mean:   0.5982     Sharing: 4.91117e-04     Sparsity: 8.24105e-02     Total:   3.0737     mean:   0.6811\n",
      " Task losses:   3.5965     mean:   0.7193     Sharing: 6.61766e-04     Sparsity: 8.24018e-02     Total:   3.6796     mean:   0.8024\n",
      " Task losses:   4.0730     mean:   0.8146     Sharing: 6.74392e-04     Sparsity: 8.23929e-02     Total:   4.1560     mean:   0.8977\n",
      " Task losses:   4.4032     mean:   0.8806     Sharing: 5.46381e-04     Sparsity: 8.23844e-02     Total:   4.4862     mean:   0.9636\n",
      " Task losses:   4.2797     mean:   0.8559     Sharing: 8.94000e-04     Sparsity: 8.23765e-02     Total:   4.3630     mean:   0.9392\n",
      " Task losses:   3.6841     mean:   0.7368     Sharing: 9.94508e-04     Sparsity: 8.23698e-02     Total:   3.7675     mean:   0.8202\n",
      " Task losses:   3.7776     mean:   0.7555     Sharing: 7.92866e-04     Sparsity: 8.23648e-02     Total:   3.8607     mean:   0.8387\n",
      " Task losses:   3.7774     mean:   0.7555     Sharing: 9.74596e-04     Sparsity: 8.23595e-02     Total:   3.8608     mean:   0.8388\n",
      " Task losses:   4.8016     mean:   0.9603     Sharing: 1.01863e-03     Sparsity: 8.23570e-02     Total:   4.8849     mean:   1.0437\n",
      " Task losses:   3.6956     mean:   0.7391     Sharing: 6.57822e-04     Sparsity: 8.23540e-02     Total:   3.7786     mean:   0.8221\n",
      " Task losses:   2.7544     mean:   0.5509     Sharing: 1.18508e-03     Sparsity: 8.23505e-02     Total:   2.8380     mean:   0.6344\n",
      " Task losses:   2.5457     mean:   0.5091     Sharing: 1.45295e-03     Sparsity: 8.23469e-02     Total:   2.6295     mean:   0.5929\n",
      " Task losses:   3.0639     mean:   0.6128     Sharing: 1.11899e-03     Sparsity: 8.23436e-02     Total:   3.1473     mean:   0.6962\n",
      " Task losses:   3.4650     mean:   0.6930     Sharing: 8.30755e-04     Sparsity: 8.23396e-02     Total:   3.5482     mean:   0.7762\n",
      " Task losses:   3.3005     mean:   0.6601     Sharing: 1.06819e-03     Sparsity: 8.23355e-02     Total:   3.3839     mean:   0.7435\n",
      " Task losses:   3.4684     mean:   0.6937     Sharing: 1.17101e-03     Sparsity: 8.23307e-02     Total:   3.5519     mean:   0.7772\n",
      " Task losses:   3.1652     mean:   0.6330     Sharing: 9.23316e-04     Sparsity: 8.23256e-02     Total:   3.2485     mean:   0.7163\n",
      " Task losses:   3.4754     mean:   0.6951     Sharing: 9.83189e-04     Sparsity: 8.23204e-02     Total:   3.5587     mean:   0.7784\n",
      " Task losses:   3.2437     mean:   0.6487     Sharing: 1.03613e-03     Sparsity: 8.23165e-02     Total:   3.3271     mean:   0.7321\n",
      " Task losses:   3.3873     mean:   0.6775     Sharing: 9.59570e-04     Sparsity: 8.23123e-02     Total:   3.4706     mean:   0.7607\n",
      " Task losses:   3.3487     mean:   0.6697     Sharing: 1.00787e-03     Sparsity: 8.23081e-02     Total:   3.4320     mean:   0.7530\n",
      " Task losses:   3.8299     mean:   0.7660     Sharing: 1.29778e-03     Sparsity: 8.23040e-02     Total:   3.9135     mean:   0.8496\n",
      " Task losses:   3.9309     mean:   0.7862     Sharing: 1.14877e-03     Sparsity: 8.22998e-02     Total:   4.0144     mean:   0.8696\n",
      " Task losses:   4.4534     mean:   0.8907     Sharing: 6.85165e-04     Sparsity: 8.22951e-02     Total:   4.5364     mean:   0.9737\n",
      " Task losses:   4.8307     mean:   0.9661     Sharing: 9.42394e-04     Sparsity: 8.22901e-02     Total:   4.9139     mean:   1.0494\n",
      " Task losses:   3.9067     mean:   0.7813     Sharing: 1.29112e-03     Sparsity: 8.22841e-02     Total:   3.9903     mean:   0.8649\n",
      " Task losses:   4.7510     mean:   0.9502     Sharing: 1.27610e-03     Sparsity: 8.22782e-02     Total:   4.8345     mean:   1.0337\n",
      " Task losses:   3.9805     mean:   0.7961     Sharing: 1.04353e-03     Sparsity: 8.22722e-02     Total:   4.0639     mean:   0.8794\n",
      " Task losses:   4.0677     mean:   0.8135     Sharing: 8.80097e-04     Sparsity: 8.22656e-02     Total:   4.1509     mean:   0.8967\n",
      " Task losses:   6.5198     mean:   1.3040     Sharing: 1.00936e-03     Sparsity: 8.22591e-02     Total:   6.6030     mean:   1.3872\n",
      " Task losses:   6.2414     mean:   1.2483     Sharing: 1.35232e-03     Sparsity: 8.22553e-02     Total:   6.3250     mean:   1.3319\n",
      " Task losses:   5.9176     mean:   1.1835     Sharing: 1.16746e-03     Sparsity: 8.22515e-02     Total:   6.0010     mean:   1.2669\n",
      " Task losses:   5.8553     mean:   1.1711     Sharing: 7.95742e-04     Sparsity: 8.22477e-02     Total:   5.9384     mean:   1.2541\n",
      " Task losses:   3.3924     mean:   0.6785     Sharing: 6.64920e-04     Sparsity: 8.22438e-02     Total:   3.4753     mean:   0.7614\n",
      " Task losses:   3.3120     mean:   0.6624     Sharing: 7.93244e-04     Sparsity: 8.22393e-02     Total:   3.3950     mean:   0.7454\n",
      " Task losses:   3.2769     mean:   0.6554     Sharing: 1.01123e-03     Sparsity: 8.22344e-02     Total:   3.3602     mean:   0.7386\n",
      " Task losses:   3.8931     mean:   0.7786     Sharing: 9.96674e-04     Sparsity: 8.22275e-02     Total:   3.9763     mean:   0.8618\n",
      " Task losses:   3.7179     mean:   0.7436     Sharing: 8.22360e-04     Sparsity: 8.22206e-02     Total:   3.8010     mean:   0.8266\n",
      " Task losses:   3.0582     mean:   0.6116     Sharing: 7.10050e-04     Sparsity: 8.22137e-02     Total:   3.1411     mean:   0.6946\n",
      " Task losses:   2.4740     mean:   0.4948     Sharing: 6.75773e-04     Sparsity: 8.22068e-02     Total:   2.5569     mean:   0.5777\n",
      " Task losses:   2.5690     mean:   0.5138     Sharing: 7.38730e-04     Sparsity: 8.21997e-02     Total:   2.6519     mean:   0.5967\n",
      " Task losses:   2.7285     mean:   0.5457     Sharing: 8.61173e-04     Sparsity: 8.21927e-02     Total:   2.8116     mean:   0.6288\n",
      " Task losses:   3.9082     mean:   0.7816     Sharing: 8.23175e-04     Sparsity: 8.21860e-02     Total:   3.9912     mean:   0.8647\n",
      "[e]Policy training epoch:97 iteration:  21052 -  Total Loss: 3.9912     Task Loss: 3.9082  Policy Losses:  Sparsity: 0.0822      Sharing: 8.23175e-04 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 98 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:98 iteration:  21160 -  Total Loss: 3.7264     Task Loss: 3.7264  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98    | 0.00019 0.86661 0.78437 0.78895 0.74265|  3.6955  4.9663  4.3294 12.9913|   44.2|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 98 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.0419     mean:   0.8084     Sharing: 7.18882e-04     Sparsity: 8.21789e-02     Total:   4.1248     mean:   0.8913\n",
      " Task losses:   8.3626     mean:   1.6725     Sharing: 7.88113e-04     Sparsity: 8.21719e-02     Total:   8.4456     mean:   1.7555\n",
      " Task losses:  10.8716     mean:   2.1743     Sharing: 6.78360e-04     Sparsity: 8.21642e-02     Total:  10.9545     mean:   2.2572\n",
      " Task losses:  11.7638     mean:   2.3528     Sharing: 1.00910e-03     Sparsity: 8.21584e-02     Total:  11.8470     mean:   2.4359\n",
      " Task losses:  12.3059     mean:   2.4612     Sharing: 9.82597e-04     Sparsity: 8.21551e-02     Total:  12.3891     mean:   2.5443\n",
      " Task losses:   9.3325     mean:   1.8665     Sharing: 8.64094e-04     Sparsity: 8.21543e-02     Total:   9.4155     mean:   1.9495\n",
      " Task losses:   5.2485     mean:   1.0497     Sharing: 9.01967e-04     Sparsity: 8.21523e-02     Total:   5.3315     mean:   1.1328\n",
      " Task losses:   5.0151     mean:   1.0030     Sharing: 9.05171e-04     Sparsity: 8.21500e-02     Total:   5.0981     mean:   1.0861\n",
      " Task losses:   5.9209     mean:   1.1842     Sharing: 7.97873e-04     Sparsity: 8.21470e-02     Total:   6.0039     mean:   1.2671\n",
      " Task losses:  10.5061     mean:   2.1012     Sharing: 1.04569e-03     Sparsity: 8.21440e-02     Total:  10.5893     mean:   2.1844\n",
      " Task losses:   6.1683     mean:   1.2337     Sharing: 1.00341e-03     Sparsity: 8.21408e-02     Total:   6.2514     mean:   1.3168\n",
      " Task losses:   5.9602     mean:   1.1920     Sharing: 7.24311e-04     Sparsity: 8.21375e-02     Total:   6.0430     mean:   1.2749\n",
      " Task losses:   4.9588     mean:   0.9918     Sharing: 1.03549e-03     Sparsity: 8.21341e-02     Total:   5.0420     mean:   1.0749\n",
      " Task losses:   4.0249     mean:   0.8050     Sharing: 1.18205e-03     Sparsity: 8.21307e-02     Total:   4.1082     mean:   0.8883\n",
      " Task losses:   4.7925     mean:   0.9585     Sharing: 1.06795e-03     Sparsity: 8.21272e-02     Total:   4.8756     mean:   1.0417\n",
      " Task losses:   3.8882     mean:   0.7776     Sharing: 1.03077e-03     Sparsity: 8.21229e-02     Total:   3.9713     mean:   0.8608\n",
      " Task losses:   7.4040     mean:   1.4808     Sharing: 1.13701e-03     Sparsity: 8.21175e-02     Total:   7.4872     mean:   1.5640\n",
      " Task losses:  27.1701     mean:   5.4340     Sharing: 9.76384e-04     Sparsity: 8.21125e-02     Total:  27.2532     mean:   5.5171\n",
      " Task losses:  23.8252     mean:   4.7650     Sharing: 1.28330e-03     Sparsity: 8.21494e-02     Total:  23.9087     mean:   4.8485\n",
      " Task losses:   3.9303     mean:   0.7861     Sharing: 1.63714e-03     Sparsity: 8.21946e-02     Total:   4.0141     mean:   0.8699\n",
      " Task losses:   3.7115     mean:   0.7423     Sharing: 1.76138e-03     Sparsity: 8.22349e-02     Total:   3.7955     mean:   0.8263\n",
      " Task losses:   3.6675     mean:   0.7335     Sharing: 1.87607e-03     Sparsity: 8.22699e-02     Total:   3.7517     mean:   0.8177\n",
      " Task losses:   3.4866     mean:   0.6973     Sharing: 1.97921e-03     Sparsity: 8.23009e-02     Total:   3.5709     mean:   0.7816\n",
      " Task losses:   5.0651     mean:   1.0130     Sharing: 2.08667e-03     Sparsity: 8.23281e-02     Total:   5.1496     mean:   1.0974\n",
      " Task losses:   5.8015     mean:   1.1603     Sharing: 1.87656e-03     Sparsity: 8.23519e-02     Total:   5.8857     mean:   1.2445\n",
      " Task losses:   4.3861     mean:   0.8772     Sharing: 1.74098e-03     Sparsity: 8.23724e-02     Total:   4.4702     mean:   0.9613\n",
      " Task losses:   6.8943     mean:   1.3789     Sharing: 1.74019e-03     Sparsity: 8.23909e-02     Total:   6.9785     mean:   1.4630\n",
      " Task losses:  10.6314     mean:   2.1263     Sharing: 1.72628e-03     Sparsity: 8.24058e-02     Total:  10.7155     mean:   2.2104\n",
      " Task losses:  13.2421     mean:   2.6484     Sharing: 1.59202e-03     Sparsity: 8.24180e-02     Total:  13.3261     mean:   2.7324\n",
      " Task losses:   9.0265     mean:   1.8053     Sharing: 1.25728e-03     Sparsity: 8.24296e-02     Total:   9.1101     mean:   1.8890\n",
      " Task losses:   5.6474     mean:   1.1295     Sharing: 1.06671e-03     Sparsity: 8.24407e-02     Total:   5.7309     mean:   1.2130\n",
      " Task losses:   4.8366     mean:   0.9673     Sharing: 1.02693e-03     Sparsity: 8.24501e-02     Total:   4.9201     mean:   1.0508\n",
      " Task losses:   4.8792     mean:   0.9758     Sharing: 1.03805e-03     Sparsity: 8.24574e-02     Total:   4.9627     mean:   1.0593\n",
      " Task losses:   7.0123     mean:   1.4025     Sharing: 1.01782e-03     Sparsity: 8.24632e-02     Total:   7.0958     mean:   1.4859\n",
      " Task losses:  12.0222     mean:   2.4044     Sharing: 8.92738e-04     Sparsity: 8.24777e-02     Total:  12.1055     mean:   2.4878\n",
      " Task losses:   4.4560     mean:   0.8912     Sharing: 1.18238e-03     Sparsity: 8.25174e-02     Total:   4.5397     mean:   0.9749\n",
      " Task losses:   5.9829     mean:   1.1966     Sharing: 1.41635e-03     Sparsity: 8.25521e-02     Total:   6.0669     mean:   1.2806\n",
      " Task losses:   6.6157     mean:   1.3231     Sharing: 1.66819e-03     Sparsity: 8.25826e-02     Total:   6.6999     mean:   1.4074\n",
      " Task losses:   9.1602     mean:   1.8320     Sharing: 1.49636e-03     Sparsity: 8.26029e-02     Total:   9.2443     mean:   1.9161\n",
      " Task losses:   3.7878     mean:   0.7576     Sharing: 1.26709e-03     Sparsity: 8.26162e-02     Total:   3.8716     mean:   0.8414\n",
      " Task losses:   4.8872     mean:   0.9774     Sharing: 1.47755e-03     Sparsity: 8.26274e-02     Total:   4.9714     mean:   1.0616\n",
      " Task losses:   7.1868     mean:   1.4374     Sharing: 1.68599e-03     Sparsity: 8.26375e-02     Total:   7.2711     mean:   1.5217\n",
      " Task losses:   7.1858     mean:   1.4372     Sharing: 1.64693e-03     Sparsity: 8.26463e-02     Total:   7.2701     mean:   1.5214\n",
      " Task losses:   5.3879     mean:   1.0776     Sharing: 1.41178e-03     Sparsity: 8.26534e-02     Total:   5.4720     mean:   1.1617\n",
      " Task losses:   4.6791     mean:   0.9358     Sharing: 1.24218e-03     Sparsity: 8.26593e-02     Total:   4.7630     mean:   1.0197\n",
      " Task losses:   5.9985     mean:   1.1997     Sharing: 1.10734e-03     Sparsity: 8.26639e-02     Total:   6.0822     mean:   1.2835\n",
      " Task losses:   5.5715     mean:   1.1143     Sharing: 1.23257e-03     Sparsity: 8.26669e-02     Total:   5.6554     mean:   1.1982\n",
      " Task losses:   5.1498     mean:   1.0300     Sharing: 1.09519e-03     Sparsity: 8.26690e-02     Total:   5.2336     mean:   1.1137\n",
      " Task losses:   4.7895     mean:   0.9579     Sharing: 1.12333e-03     Sparsity: 8.26700e-02     Total:   4.8733     mean:   1.0417\n",
      " Task losses:   4.5460     mean:   0.9092     Sharing: 1.28779e-03     Sparsity: 8.26703e-02     Total:   4.6300     mean:   0.9932\n",
      " Task losses:   6.2092     mean:   1.2418     Sharing: 1.29599e-03     Sparsity: 8.26697e-02     Total:   6.2932     mean:   1.3258\n",
      " Task losses:   6.9586     mean:   1.3917     Sharing: 9.17564e-04     Sparsity: 8.26686e-02     Total:   7.0422     mean:   1.4753\n",
      " Task losses:   6.3516     mean:   1.2703     Sharing: 6.93868e-04     Sparsity: 8.26672e-02     Total:   6.4349     mean:   1.3537\n",
      " Task losses:   4.7716     mean:   0.9543     Sharing: 1.10695e-03     Sparsity: 8.26635e-02     Total:   4.8554     mean:   1.0381\n",
      " Task losses:   6.1058     mean:   1.2212     Sharing: 1.06319e-03     Sparsity: 8.26591e-02     Total:   6.1895     mean:   1.3049\n",
      " Task losses:   6.1318     mean:   1.2264     Sharing: 6.06716e-04     Sparsity: 8.26547e-02     Total:   6.2150     mean:   1.3096\n",
      " Task losses:   4.8663     mean:   0.9733     Sharing: 9.69753e-04     Sparsity: 8.26507e-02     Total:   4.9499     mean:   1.0569\n",
      " Task losses:   6.0485     mean:   1.2097     Sharing: 9.70428e-04     Sparsity: 8.26462e-02     Total:   6.1321     mean:   1.2933\n",
      " Task losses:   4.9078     mean:   0.9816     Sharing: 1.18573e-03     Sparsity: 8.26391e-02     Total:   4.9917     mean:   1.0654\n",
      " Task losses:   4.4534     mean:   0.8907     Sharing: 7.71970e-04     Sparsity: 8.26313e-02     Total:   4.5368     mean:   0.9741\n",
      " Task losses:   4.7207     mean:   0.9441     Sharing: 9.00830e-04     Sparsity: 8.26236e-02     Total:   4.8042     mean:   1.0277\n",
      " Task losses:   4.0876     mean:   0.8175     Sharing: 1.05660e-03     Sparsity: 8.26156e-02     Total:   4.1713     mean:   0.9012\n",
      " Task losses:   4.2946     mean:   0.8589     Sharing: 7.72357e-04     Sparsity: 8.26075e-02     Total:   4.3779     mean:   0.9423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.4813     mean:   0.8963     Sharing: 9.36811e-04     Sparsity: 8.26008e-02     Total:   4.5648     mean:   0.9798\n",
      " Task losses:   5.5044     mean:   1.1009     Sharing: 9.67577e-04     Sparsity: 8.25938e-02     Total:   5.5880     mean:   1.1845\n",
      " Task losses:   3.6626     mean:   0.7325     Sharing: 8.23552e-04     Sparsity: 8.25879e-02     Total:   3.7460     mean:   0.8159\n",
      " Task losses:   4.1104     mean:   0.8221     Sharing: 7.17928e-04     Sparsity: 8.25819e-02     Total:   4.1937     mean:   0.9054\n",
      " Task losses:   3.9909     mean:   0.7982     Sharing: 7.63993e-04     Sparsity: 8.25772e-02     Total:   4.0742     mean:   0.8815\n",
      " Task losses:   4.8579     mean:   0.9716     Sharing: 7.55524e-04     Sparsity: 8.25721e-02     Total:   4.9412     mean:   1.0549\n",
      " Task losses:   5.4040     mean:   1.0808     Sharing: 8.22489e-04     Sparsity: 8.25660e-02     Total:   5.4874     mean:   1.1642\n",
      " Task losses:   3.9320     mean:   0.7864     Sharing: 8.10425e-04     Sparsity: 8.25597e-02     Total:   4.0154     mean:   0.8698\n",
      " Task losses:   4.4520     mean:   0.8904     Sharing: 5.97944e-04     Sparsity: 8.25536e-02     Total:   4.5351     mean:   0.9736\n",
      " Task losses:   4.7836     mean:   0.9567     Sharing: 5.39010e-04     Sparsity: 8.25467e-02     Total:   4.8666     mean:   1.0398\n",
      " Task losses:   5.1345     mean:   1.0269     Sharing: 9.01133e-04     Sparsity: 8.25389e-02     Total:   5.2180     mean:   1.1103\n",
      " Task losses:   3.7080     mean:   0.7416     Sharing: 7.36848e-04     Sparsity: 8.25314e-02     Total:   3.7912     mean:   0.8249\n",
      " Task losses:   3.3910     mean:   0.6782     Sharing: 5.45015e-04     Sparsity: 8.25244e-02     Total:   3.4741     mean:   0.7613\n",
      " Task losses:   3.5298     mean:   0.7060     Sharing: 9.94896e-04     Sparsity: 8.25174e-02     Total:   3.6133     mean:   0.7895\n",
      " Task losses:   3.7631     mean:   0.7526     Sharing: 1.01644e-03     Sparsity: 8.25103e-02     Total:   3.8466     mean:   0.8361\n",
      " Task losses:   4.0106     mean:   0.8021     Sharing: 9.91891e-04     Sparsity: 8.25032e-02     Total:   4.0941     mean:   0.8856\n",
      " Task losses:   4.2819     mean:   0.8564     Sharing: 7.00881e-04     Sparsity: 8.24957e-02     Total:   4.3651     mean:   0.9396\n",
      " Task losses:   4.7347     mean:   0.9469     Sharing: 6.99262e-04     Sparsity: 8.24888e-02     Total:   4.8179     mean:   1.0301\n",
      " Task losses:   4.6218     mean:   0.9244     Sharing: 1.00457e-03     Sparsity: 8.24820e-02     Total:   4.7053     mean:   1.0079\n",
      " Task losses:   5.3064     mean:   1.0613     Sharing: 8.14994e-04     Sparsity: 8.24749e-02     Total:   5.3897     mean:   1.1446\n",
      " Task losses:   5.3557     mean:   1.0711     Sharing: 6.64617e-04     Sparsity: 8.24678e-02     Total:   5.4388     mean:   1.1543\n",
      " Task losses:   5.3225     mean:   1.0645     Sharing: 8.40833e-04     Sparsity: 8.24611e-02     Total:   5.4058     mean:   1.1478\n",
      " Task losses:   4.1729     mean:   0.8346     Sharing: 8.64456e-04     Sparsity: 8.24533e-02     Total:   4.2562     mean:   0.9179\n",
      " Task losses:   4.6484     mean:   0.9297     Sharing: 1.08140e-03     Sparsity: 8.24457e-02     Total:   4.7319     mean:   1.0132\n",
      " Task losses:   4.7954     mean:   0.9591     Sharing: 8.51313e-04     Sparsity: 8.24382e-02     Total:   4.8787     mean:   1.0424\n",
      " Task losses:   4.5731     mean:   0.9146     Sharing: 4.83508e-04     Sparsity: 8.24306e-02     Total:   4.6560     mean:   0.9975\n",
      " Task losses:   4.6496     mean:   0.9299     Sharing: 1.03750e-03     Sparsity: 8.24229e-02     Total:   4.7331     mean:   1.0134\n",
      " Task losses:   4.9602     mean:   0.9920     Sharing: 1.27551e-03     Sparsity: 8.24155e-02     Total:   5.0439     mean:   1.0757\n",
      " Task losses:   4.6879     mean:   0.9376     Sharing: 9.60996e-04     Sparsity: 8.24082e-02     Total:   4.7713     mean:   1.0210\n",
      " Task losses:   4.3497     mean:   0.8699     Sharing: 6.34447e-04     Sparsity: 8.24011e-02     Total:   4.4327     mean:   0.9530\n",
      " Task losses:   4.0861     mean:   0.8172     Sharing: 1.10670e-03     Sparsity: 8.23938e-02     Total:   4.1696     mean:   0.9007\n",
      " Task losses:   6.2215     mean:   1.2443     Sharing: 1.23932e-03     Sparsity: 8.23867e-02     Total:   6.3051     mean:   1.3279\n",
      " Task losses:   5.8492     mean:   1.1698     Sharing: 9.09353e-04     Sparsity: 8.23793e-02     Total:   5.9324     mean:   1.2531\n",
      " Task losses:   6.7859     mean:   1.3572     Sharing: 7.95235e-04     Sparsity: 8.23710e-02     Total:   6.8691     mean:   1.4403\n",
      " Task losses:   6.2894     mean:   1.2579     Sharing: 7.94902e-04     Sparsity: 8.23632e-02     Total:   6.3725     mean:   1.3410\n",
      " Task losses:   5.0267     mean:   1.0053     Sharing: 1.06090e-03     Sparsity: 8.23565e-02     Total:   5.1101     mean:   1.0888\n",
      " Task losses:   3.9641     mean:   0.7928     Sharing: 7.84834e-04     Sparsity: 8.23520e-02     Total:   4.0472     mean:   0.8760\n",
      " Task losses:   3.9362     mean:   0.7872     Sharing: 1.04946e-03     Sparsity: 8.23471e-02     Total:   4.0196     mean:   0.8706\n",
      " Task losses:   5.0465     mean:   1.0093     Sharing: 1.09227e-03     Sparsity: 8.23402e-02     Total:   5.1300     mean:   1.0927\n",
      " Task losses:   4.3998     mean:   0.8800     Sharing: 9.03641e-04     Sparsity: 8.23328e-02     Total:   4.4830     mean:   0.9632\n",
      " Task losses:   3.5338     mean:   0.7068     Sharing: 8.07822e-04     Sparsity: 8.23263e-02     Total:   3.6169     mean:   0.7899\n",
      " Task losses:   2.9325     mean:   0.5865     Sharing: 9.18006e-04     Sparsity: 8.23198e-02     Total:   3.0158     mean:   0.6697\n",
      " Task losses:   2.9030     mean:   0.5806     Sharing: 1.11701e-03     Sparsity: 8.23134e-02     Total:   2.9864     mean:   0.6640\n",
      " Task losses:   2.7103     mean:   0.5421     Sharing: 1.18992e-03     Sparsity: 8.23069e-02     Total:   2.7938     mean:   0.6256\n",
      " Task losses:   3.3798     mean:   0.6760     Sharing: 8.26056e-04     Sparsity: 8.23006e-02     Total:   3.4629     mean:   0.7591\n",
      "[e]Policy training epoch:98 iteration:  21268 -  Total Loss: 3.4629     Task Loss: 3.3798  Policy Losses:  Sparsity: 0.0823      Sharing: 8.26056e-04 \n",
      "[e]Policy training epoch:98 decay gumbel temp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 99 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:99 iteration:  21376 -  Total Loss: 4.6281     Task Loss: 4.6281  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99    | 0.00019 0.86339 0.79073 0.79221 0.74475|  4.7722  4.3071  3.8614 12.9407|   43.0|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 99 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.6270     mean:   0.9254     Sharing: 8.21814e-04     Sparsity: 8.22942e-02     Total:   4.7101     mean:   1.0085\n",
      " Task losses:   7.7048     mean:   1.5410     Sharing: 1.18157e-03     Sparsity: 8.22877e-02     Total:   7.7882     mean:   1.6244\n",
      " Task losses:   7.9556     mean:   1.5911     Sharing: 1.20430e-03     Sparsity: 8.22818e-02     Total:   8.0391     mean:   1.6746\n",
      " Task losses:   9.1295     mean:   1.8259     Sharing: 8.41598e-04     Sparsity: 8.22742e-02     Total:   9.2126     mean:   1.9090\n",
      " Task losses:   8.3333     mean:   1.6667     Sharing: 7.13517e-04     Sparsity: 8.22656e-02     Total:   8.4163     mean:   1.7496\n",
      " Task losses:   9.0973     mean:   1.8195     Sharing: 9.35555e-04     Sparsity: 8.22580e-02     Total:   9.1805     mean:   1.9027\n",
      " Task losses:   4.8888     mean:   0.9778     Sharing: 7.97714e-04     Sparsity: 8.22510e-02     Total:   4.9719     mean:   1.0608\n",
      " Task losses:   4.2293     mean:   0.8459     Sharing: 6.19397e-04     Sparsity: 8.22445e-02     Total:   4.3121     mean:   0.9287\n",
      " Task losses:   4.8215     mean:   0.9643     Sharing: 6.39766e-04     Sparsity: 8.22382e-02     Total:   4.9044     mean:   1.0472\n",
      " Task losses:   7.3827     mean:   1.4765     Sharing: 7.73365e-04     Sparsity: 8.22312e-02     Total:   7.4658     mean:   1.5596\n",
      " Task losses:   4.4568     mean:   0.8914     Sharing: 7.11406e-04     Sparsity: 8.22254e-02     Total:   4.5397     mean:   0.9743\n",
      " Task losses:   4.7769     mean:   0.9554     Sharing: 6.60206e-04     Sparsity: 8.22191e-02     Total:   4.8598     mean:   1.0383\n",
      " Task losses:   4.0530     mean:   0.8106     Sharing: 9.64940e-04     Sparsity: 8.22123e-02     Total:   4.1362     mean:   0.8938\n",
      " Task losses:   3.9861     mean:   0.7972     Sharing: 9.99803e-04     Sparsity: 8.22057e-02     Total:   4.0693     mean:   0.8804\n",
      " Task losses:   3.9242     mean:   0.7848     Sharing: 7.48048e-04     Sparsity: 8.21993e-02     Total:   4.0071     mean:   0.8678\n",
      " Task losses:   3.9320     mean:   0.7864     Sharing: 6.05171e-04     Sparsity: 8.21928e-02     Total:   4.0148     mean:   0.8692\n",
      " Task losses:   5.2450     mean:   1.0490     Sharing: 8.63487e-04     Sparsity: 8.21863e-02     Total:   5.3280     mean:   1.1320\n",
      " Task losses:  10.3562     mean:   2.0712     Sharing: 6.82881e-04     Sparsity: 8.21804e-02     Total:  10.4390     mean:   2.1541\n",
      " Task losses:  14.2737     mean:   2.8547     Sharing: 8.96816e-04     Sparsity: 8.21720e-02     Total:  14.3567     mean:   2.9378\n",
      " Task losses:   4.2969     mean:   0.8594     Sharing: 1.61161e-03     Sparsity: 8.21890e-02     Total:   4.3807     mean:   0.9432\n",
      " Task losses:   3.4510     mean:   0.6902     Sharing: 2.11146e-03     Sparsity: 8.22039e-02     Total:   3.5353     mean:   0.7745\n",
      " Task losses:   4.0270     mean:   0.8054     Sharing: 1.96888e-03     Sparsity: 8.22170e-02     Total:   4.1112     mean:   0.8896\n",
      " Task losses:   4.0646     mean:   0.8129     Sharing: 2.25442e-03     Sparsity: 8.22281e-02     Total:   4.1490     mean:   0.8974\n",
      " Task losses:   4.9580     mean:   0.9916     Sharing: 2.48898e-03     Sparsity: 8.22380e-02     Total:   5.0427     mean:   1.0763\n",
      " Task losses:   6.6771     mean:   1.3354     Sharing: 2.19674e-03     Sparsity: 8.22466e-02     Total:   6.7616     mean:   1.4199\n",
      " Task losses:   4.6118     mean:   0.9224     Sharing: 1.83187e-03     Sparsity: 8.22541e-02     Total:   4.6959     mean:   1.0064\n",
      " Task losses:   6.1830     mean:   1.2366     Sharing: 1.62092e-03     Sparsity: 8.22604e-02     Total:   6.2668     mean:   1.3205\n",
      " Task losses:   7.9760     mean:   1.5952     Sharing: 1.54780e-03     Sparsity: 8.22651e-02     Total:   8.0598     mean:   1.6790\n",
      " Task losses:   8.5334     mean:   1.7067     Sharing: 1.28254e-03     Sparsity: 8.22683e-02     Total:   8.6169     mean:   1.7902\n",
      " Task losses:   5.5179     mean:   1.1036     Sharing: 1.00987e-03     Sparsity: 8.22703e-02     Total:   5.6012     mean:   1.1869\n",
      " Task losses:   5.9588     mean:   1.1918     Sharing: 1.17974e-03     Sparsity: 8.22691e-02     Total:   6.0423     mean:   1.2752\n",
      " Task losses:   4.9433     mean:   0.9887     Sharing: 1.01444e-03     Sparsity: 8.22673e-02     Total:   5.0266     mean:   1.0719\n",
      " Task losses:   4.9955     mean:   0.9991     Sharing: 1.11368e-03     Sparsity: 8.22650e-02     Total:   5.0789     mean:   1.0825\n",
      " Task losses:   5.2031     mean:   1.0406     Sharing: 1.51503e-03     Sparsity: 8.22625e-02     Total:   5.2869     mean:   1.1244\n",
      " Task losses:   6.3376     mean:   1.2675     Sharing: 1.65441e-03     Sparsity: 8.22602e-02     Total:   6.4215     mean:   1.3514\n",
      " Task losses:   5.1646     mean:   1.0329     Sharing: 1.06710e-03     Sparsity: 8.22583e-02     Total:   5.2479     mean:   1.1162\n",
      " Task losses:   6.9033     mean:   1.3807     Sharing: 1.34681e-03     Sparsity: 8.22561e-02     Total:   6.9869     mean:   1.4643\n",
      " Task losses:   7.5593     mean:   1.5119     Sharing: 1.66743e-03     Sparsity: 8.22539e-02     Total:   7.6432     mean:   1.5958\n",
      " Task losses:   8.2456     mean:   1.6491     Sharing: 1.27336e-03     Sparsity: 8.22507e-02     Total:   8.3291     mean:   1.7326\n",
      " Task losses:   3.4085     mean:   0.6817     Sharing: 1.05754e-03     Sparsity: 8.22461e-02     Total:   3.4918     mean:   0.7650\n",
      " Task losses:   5.4354     mean:   1.0871     Sharing: 1.14527e-03     Sparsity: 8.22417e-02     Total:   5.5188     mean:   1.1705\n",
      " Task losses:   8.2845     mean:   1.6569     Sharing: 1.09349e-03     Sparsity: 8.22381e-02     Total:   8.3679     mean:   1.7402\n",
      " Task losses:   7.2929     mean:   1.4586     Sharing: 1.13176e-03     Sparsity: 8.22346e-02     Total:   7.3763     mean:   1.5420\n",
      " Task losses:   5.2355     mean:   1.0471     Sharing: 1.21332e-03     Sparsity: 8.22310e-02     Total:   5.3189     mean:   1.1305\n",
      " Task losses:   4.8784     mean:   0.9757     Sharing: 9.07863e-04     Sparsity: 8.22267e-02     Total:   4.9615     mean:   1.0588\n",
      " Task losses:   6.4231     mean:   1.2846     Sharing: 8.77082e-04     Sparsity: 8.22221e-02     Total:   6.5062     mean:   1.3677\n",
      " Task losses:   5.7419     mean:   1.1484     Sharing: 1.02699e-03     Sparsity: 8.22172e-02     Total:   5.8251     mean:   1.2316\n",
      " Task losses:   6.0896     mean:   1.2179     Sharing: 9.31000e-04     Sparsity: 8.22127e-02     Total:   6.1728     mean:   1.3011\n",
      " Task losses:   5.2730     mean:   1.0546     Sharing: 1.02628e-03     Sparsity: 8.22079e-02     Total:   5.3562     mean:   1.1378\n",
      " Task losses:   5.2753     mean:   1.0551     Sharing: 1.04939e-03     Sparsity: 8.22030e-02     Total:   5.3585     mean:   1.1383\n",
      " Task losses:   5.7894     mean:   1.1579     Sharing: 6.76036e-04     Sparsity: 8.21987e-02     Total:   5.8722     mean:   1.2407\n",
      " Task losses:   5.7456     mean:   1.1491     Sharing: 5.50429e-04     Sparsity: 8.21945e-02     Total:   5.8284     mean:   1.2319\n",
      " Task losses:   5.8915     mean:   1.1783     Sharing: 6.93202e-04     Sparsity: 8.21900e-02     Total:   5.9744     mean:   1.2612\n",
      " Task losses:   5.0960     mean:   1.0192     Sharing: 7.83349e-04     Sparsity: 8.21852e-02     Total:   5.1790     mean:   1.1022\n",
      " Task losses:   7.3978     mean:   1.4796     Sharing: 7.26968e-04     Sparsity: 8.21806e-02     Total:   7.4807     mean:   1.5625\n",
      " Task losses:   6.8482     mean:   1.3696     Sharing: 4.33331e-04     Sparsity: 8.21751e-02     Total:   6.9308     mean:   1.4522\n",
      " Task losses:   4.4669     mean:   0.8934     Sharing: 6.71402e-04     Sparsity: 8.21700e-02     Total:   4.5498     mean:   0.9762\n",
      " Task losses:   6.0369     mean:   1.2074     Sharing: 1.01249e-03     Sparsity: 8.21647e-02     Total:   6.1201     mean:   1.2906\n",
      " Task losses:   5.5921     mean:   1.1184     Sharing: 8.74713e-04     Sparsity: 8.21597e-02     Total:   5.6751     mean:   1.2014\n",
      " Task losses:   4.3494     mean:   0.8699     Sharing: 8.92371e-04     Sparsity: 8.21543e-02     Total:   4.4325     mean:   0.9529\n",
      " Task losses:   4.3517     mean:   0.8703     Sharing: 6.46432e-04     Sparsity: 8.21504e-02     Total:   4.4345     mean:   0.9531\n",
      " Task losses:   4.5848     mean:   0.9170     Sharing: 6.74198e-04     Sparsity: 8.21465e-02     Total:   4.6677     mean:   0.9998\n",
      " Task losses:   4.4558     mean:   0.8912     Sharing: 9.41803e-04     Sparsity: 8.21428e-02     Total:   4.5389     mean:   0.9742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.4612     mean:   0.8922     Sharing: 9.83362e-04     Sparsity: 8.21393e-02     Total:   4.5444     mean:   0.9754\n",
      " Task losses:   5.5442     mean:   1.1088     Sharing: 6.80392e-04     Sparsity: 8.21354e-02     Total:   5.6270     mean:   1.1917\n",
      " Task losses:   3.7352     mean:   0.7470     Sharing: 6.23062e-04     Sparsity: 8.21315e-02     Total:   3.8180     mean:   0.8298\n",
      " Task losses:   4.8199     mean:   0.9640     Sharing: 8.51884e-04     Sparsity: 8.21268e-02     Total:   4.9029     mean:   1.0470\n",
      " Task losses:   3.8497     mean:   0.7699     Sharing: 1.08212e-03     Sparsity: 8.21218e-02     Total:   3.9329     mean:   0.8531\n",
      " Task losses:   4.1854     mean:   0.8371     Sharing: 8.52172e-04     Sparsity: 8.21165e-02     Total:   4.2683     mean:   0.9200\n",
      " Task losses:   4.0929     mean:   0.8186     Sharing: 7.41656e-04     Sparsity: 8.21109e-02     Total:   4.1758     mean:   0.9014\n",
      " Task losses:   4.0369     mean:   0.8074     Sharing: 8.84026e-04     Sparsity: 8.21054e-02     Total:   4.1199     mean:   0.8904\n",
      " Task losses:   4.1428     mean:   0.8286     Sharing: 7.47646e-04     Sparsity: 8.21001e-02     Total:   4.2257     mean:   0.9114\n",
      " Task losses:   4.1721     mean:   0.8344     Sharing: 7.40151e-04     Sparsity: 8.20948e-02     Total:   4.2550     mean:   0.9173\n",
      " Task losses:   4.2882     mean:   0.8576     Sharing: 6.95149e-04     Sparsity: 8.20910e-02     Total:   4.3710     mean:   0.9404\n",
      " Task losses:   3.5940     mean:   0.7188     Sharing: 6.13610e-04     Sparsity: 8.20868e-02     Total:   3.6767     mean:   0.8015\n",
      " Task losses:   3.0835     mean:   0.6167     Sharing: 5.44886e-04     Sparsity: 8.20847e-02     Total:   3.1661     mean:   0.6993\n",
      " Task losses:   2.8445     mean:   0.5689     Sharing: 5.86679e-04     Sparsity: 8.20820e-02     Total:   2.9272     mean:   0.6516\n",
      " Task losses:   3.4513     mean:   0.6903     Sharing: 4.50646e-04     Sparsity: 8.20791e-02     Total:   3.5339     mean:   0.7728\n",
      " Task losses:   4.0225     mean:   0.8045     Sharing: 6.94866e-04     Sparsity: 8.20761e-02     Total:   4.1053     mean:   0.8873\n",
      " Task losses:   3.7880     mean:   0.7576     Sharing: 9.76503e-04     Sparsity: 8.20728e-02     Total:   3.8711     mean:   0.8407\n",
      " Task losses:   4.2012     mean:   0.8402     Sharing: 7.33078e-04     Sparsity: 8.20690e-02     Total:   4.2840     mean:   0.9230\n",
      " Task losses:   4.0643     mean:   0.8129     Sharing: 6.35033e-04     Sparsity: 8.20653e-02     Total:   4.1470     mean:   0.8956\n",
      " Task losses:   4.8831     mean:   0.9766     Sharing: 6.47301e-04     Sparsity: 8.20615e-02     Total:   4.9658     mean:   1.0593\n",
      " Task losses:   4.1758     mean:   0.8352     Sharing: 5.96394e-04     Sparsity: 8.20582e-02     Total:   4.2585     mean:   0.9178\n",
      " Task losses:   4.4940     mean:   0.8988     Sharing: 6.85230e-04     Sparsity: 8.20632e-02     Total:   4.5767     mean:   0.9815\n",
      " Task losses:   4.1348     mean:   0.8270     Sharing: 9.38137e-04     Sparsity: 8.20695e-02     Total:   4.2179     mean:   0.9100\n",
      " Task losses:   4.7583     mean:   0.9517     Sharing: 8.75761e-04     Sparsity: 8.20747e-02     Total:   4.8413     mean:   1.0346\n",
      " Task losses:   5.1350     mean:   1.0270     Sharing: 6.61085e-04     Sparsity: 8.20785e-02     Total:   5.2177     mean:   1.1097\n",
      " Task losses:   5.0308     mean:   1.0062     Sharing: 6.75668e-04     Sparsity: 8.20818e-02     Total:   5.1135     mean:   1.0889\n",
      " Task losses:   5.7675     mean:   1.1535     Sharing: 7.78357e-04     Sparsity: 8.20858e-02     Total:   5.8503     mean:   1.2364\n",
      " Task losses:   4.9678     mean:   0.9936     Sharing: 6.79339e-04     Sparsity: 8.20913e-02     Total:   5.0506     mean:   1.0763\n",
      " Task losses:   5.3943     mean:   1.0789     Sharing: 8.63776e-04     Sparsity: 8.20966e-02     Total:   5.4773     mean:   1.1618\n",
      " Task losses:   5.9236     mean:   1.1847     Sharing: 6.23013e-04     Sparsity: 8.21012e-02     Total:   6.0063     mean:   1.2674\n",
      " Task losses:   4.3256     mean:   0.8651     Sharing: 7.37379e-04     Sparsity: 8.21074e-02     Total:   4.4085     mean:   0.9480\n",
      " Task losses:   6.6609     mean:   1.3322     Sharing: 7.38611e-04     Sparsity: 8.21130e-02     Total:   6.7438     mean:   1.4150\n",
      " Task losses:   7.6741     mean:   1.5348     Sharing: 8.11378e-04     Sparsity: 8.21161e-02     Total:   7.7570     mean:   1.6177\n",
      " Task losses:   6.0372     mean:   1.2074     Sharing: 9.31571e-04     Sparsity: 8.21176e-02     Total:   6.1203     mean:   1.2905\n",
      " Task losses:   5.4537     mean:   1.0907     Sharing: 9.14509e-04     Sparsity: 8.21198e-02     Total:   5.5367     mean:   1.1738\n",
      " Task losses:   3.7808     mean:   0.7562     Sharing: 7.06717e-04     Sparsity: 8.21215e-02     Total:   3.8636     mean:   0.8390\n",
      " Task losses:   3.4505     mean:   0.6901     Sharing: 9.94404e-04     Sparsity: 8.21225e-02     Total:   3.5336     mean:   0.7732\n",
      " Task losses:   3.0795     mean:   0.6159     Sharing: 1.14137e-03     Sparsity: 8.21228e-02     Total:   3.1628     mean:   0.6992\n",
      " Task losses:   4.5935     mean:   0.9187     Sharing: 9.40785e-04     Sparsity: 8.21232e-02     Total:   4.6766     mean:   1.0018\n",
      " Task losses:   3.8924     mean:   0.7785     Sharing: 8.83073e-04     Sparsity: 8.21234e-02     Total:   3.9754     mean:   0.8615\n",
      " Task losses:   3.2806     mean:   0.6561     Sharing: 1.00475e-03     Sparsity: 8.21226e-02     Total:   3.3637     mean:   0.7392\n",
      " Task losses:   2.6290     mean:   0.5258     Sharing: 8.12804e-04     Sparsity: 8.21209e-02     Total:   2.7119     mean:   0.6087\n",
      " Task losses:   2.7830     mean:   0.5566     Sharing: 7.10035e-04     Sparsity: 8.21182e-02     Total:   2.8658     mean:   0.6394\n",
      " Task losses:   2.8559     mean:   0.5712     Sharing: 9.85409e-04     Sparsity: 8.21153e-02     Total:   2.9390     mean:   0.6543\n",
      " Task losses:   3.8266     mean:   0.7653     Sharing: 1.10324e-03     Sparsity: 8.21124e-02     Total:   3.9098     mean:   0.8485\n",
      "[e]Policy training epoch:99 iteration:  21484 -  Total Loss: 3.9098     Task Loss: 3.8266  Policy Losses:  Sparsity: 0.0821      Sharing: 1.10324e-03 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 100 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:100 iteration:  21592 -  Total Loss: 4.8387     Task Loss: 4.8387  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   | 0.00020 0.93685 0.78437 0.78287 0.74405|  4.4805  4.8983  4.6455 14.0243|   45.7|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 100 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.4450     mean:   0.8890     Sharing: 8.61471e-04     Sparsity: 8.21090e-02     Total:   4.5280     mean:   0.9720\n",
      " Task losses:   9.1878     mean:   1.8376     Sharing: 9.71004e-04     Sparsity: 8.21058e-02     Total:   9.2709     mean:   1.9206\n",
      " Task losses:  11.6705     mean:   2.3341     Sharing: 1.20269e-03     Sparsity: 8.21034e-02     Total:  11.7538     mean:   2.4174\n",
      " Task losses:  12.6444     mean:   2.5289     Sharing: 1.16858e-03     Sparsity: 8.21079e-02     Total:  12.7277     mean:   2.6122\n",
      " Task losses:  14.6914     mean:   2.9383     Sharing: 8.27049e-04     Sparsity: 8.21132e-02     Total:  14.7743     mean:   3.0212\n",
      " Task losses:  10.3994     mean:   2.0799     Sharing: 9.17474e-04     Sparsity: 8.21198e-02     Total:  10.4825     mean:   2.1629\n",
      " Task losses:   6.2193     mean:   1.2439     Sharing: 1.22700e-03     Sparsity: 8.21259e-02     Total:   6.3026     mean:   1.3272\n",
      " Task losses:   4.8700     mean:   0.9740     Sharing: 1.24199e-03     Sparsity: 8.21319e-02     Total:   4.9534     mean:   1.0574\n",
      " Task losses:   6.1572     mean:   1.2314     Sharing: 8.41096e-04     Sparsity: 8.21338e-02     Total:   6.2402     mean:   1.3144\n",
      " Task losses:  12.1492     mean:   2.4298     Sharing: 7.12802e-04     Sparsity: 8.21343e-02     Total:  12.2321     mean:   2.5127\n",
      " Task losses:   6.3154     mean:   1.2631     Sharing: 9.21806e-04     Sparsity: 8.21282e-02     Total:   6.3985     mean:   1.3461\n",
      " Task losses:   5.9183     mean:   1.1837     Sharing: 9.87396e-04     Sparsity: 8.21212e-02     Total:   6.0014     mean:   1.2668\n",
      " Task losses:   5.1225     mean:   1.0245     Sharing: 7.76758e-04     Sparsity: 8.21141e-02     Total:   5.2054     mean:   1.1074\n",
      " Task losses:   4.6920     mean:   0.9384     Sharing: 7.20496e-04     Sparsity: 8.21074e-02     Total:   4.7748     mean:   1.0212\n",
      " Task losses:   4.7624     mean:   0.9525     Sharing: 6.84197e-04     Sparsity: 8.21004e-02     Total:   4.8452     mean:   1.0353\n",
      " Task losses:   4.6355     mean:   0.9271     Sharing: 7.53512e-04     Sparsity: 8.20930e-02     Total:   4.7183     mean:   1.0099\n",
      " Task losses:   6.1586     mean:   1.2317     Sharing: 7.63570e-04     Sparsity: 8.20863e-02     Total:   6.2415     mean:   1.3146\n",
      " Task losses:  18.3926     mean:   3.6785     Sharing: 6.41733e-04     Sparsity: 8.20796e-02     Total:  18.4753     mean:   3.7612\n",
      " Task losses:  18.5537     mean:   3.7107     Sharing: 7.18633e-04     Sparsity: 8.20819e-02     Total:  18.6365     mean:   3.7935\n",
      " Task losses:   4.4447     mean:   0.8889     Sharing: 7.46687e-04     Sparsity: 8.20831e-02     Total:   4.5276     mean:   0.9718\n",
      " Task losses:   4.2857     mean:   0.8571     Sharing: 9.58413e-04     Sparsity: 8.20830e-02     Total:   4.3687     mean:   0.9402\n",
      " Task losses:   4.3525     mean:   0.8705     Sharing: 6.26316e-04     Sparsity: 8.20822e-02     Total:   4.4352     mean:   0.9532\n",
      " Task losses:   4.6389     mean:   0.9278     Sharing: 7.70569e-04     Sparsity: 8.20839e-02     Total:   4.7217     mean:   1.0106\n",
      " Task losses:   5.4192     mean:   1.0838     Sharing: 1.18996e-03     Sparsity: 8.20841e-02     Total:   5.5025     mean:   1.1671\n",
      " Task losses:   6.0047     mean:   1.2009     Sharing: 1.03524e-03     Sparsity: 8.20835e-02     Total:   6.0878     mean:   1.2841\n",
      " Task losses:   4.3884     mean:   0.8777     Sharing: 5.63115e-04     Sparsity: 8.20811e-02     Total:   4.4710     mean:   0.9603\n",
      " Task losses:   6.5870     mean:   1.3174     Sharing: 8.46545e-04     Sparsity: 8.20780e-02     Total:   6.6699     mean:   1.4003\n",
      " Task losses:   8.6788     mean:   1.7358     Sharing: 1.03526e-03     Sparsity: 8.20729e-02     Total:   8.7620     mean:   1.8189\n",
      " Task losses:  10.0004     mean:   2.0001     Sharing: 8.51363e-04     Sparsity: 8.20661e-02     Total:  10.0833     mean:   2.0830\n",
      " Task losses:   7.5347     mean:   1.5069     Sharing: 5.47717e-04     Sparsity: 8.20614e-02     Total:   7.6173     mean:   1.5896\n",
      " Task losses:   4.6528     mean:   0.9306     Sharing: 6.74953e-04     Sparsity: 8.20566e-02     Total:   4.7356     mean:   1.0133\n",
      " Task losses:   4.2174     mean:   0.8435     Sharing: 8.30769e-04     Sparsity: 8.20497e-02     Total:   4.3003     mean:   0.9264\n",
      " Task losses:   4.6418     mean:   0.9284     Sharing: 7.73047e-04     Sparsity: 8.20413e-02     Total:   4.7246     mean:   1.0112\n",
      " Task losses:   5.1395     mean:   1.0279     Sharing: 7.24897e-04     Sparsity: 8.20328e-02     Total:   5.2222     mean:   1.1106\n",
      " Task losses:   5.3118     mean:   1.0624     Sharing: 8.57294e-04     Sparsity: 8.20241e-02     Total:   5.3947     mean:   1.1452\n",
      " Task losses:   4.6279     mean:   0.9256     Sharing: 9.76235e-04     Sparsity: 8.20239e-02     Total:   4.7109     mean:   1.0086\n",
      " Task losses:   4.9922     mean:   0.9984     Sharing: 9.80844e-04     Sparsity: 8.20233e-02     Total:   5.0752     mean:   1.0814\n",
      " Task losses:   7.4013     mean:   1.4803     Sharing: 7.68339e-04     Sparsity: 8.20221e-02     Total:   7.4841     mean:   1.5630\n",
      " Task losses:   6.7085     mean:   1.3417     Sharing: 8.59261e-04     Sparsity: 8.20187e-02     Total:   6.7914     mean:   1.4246\n",
      " Task losses:   3.9537     mean:   0.7907     Sharing: 8.77559e-04     Sparsity: 8.20149e-02     Total:   4.0366     mean:   0.8736\n",
      " Task losses:   5.3742     mean:   1.0748     Sharing: 9.91722e-04     Sparsity: 8.20108e-02     Total:   5.4572     mean:   1.1578\n",
      " Task losses:   8.1394     mean:   1.6279     Sharing: 8.92023e-04     Sparsity: 8.20083e-02     Total:   8.2223     mean:   1.7108\n",
      " Task losses:   8.3065     mean:   1.6613     Sharing: 1.17016e-03     Sparsity: 8.20074e-02     Total:   8.3897     mean:   1.7445\n",
      " Task losses:   6.5648     mean:   1.3130     Sharing: 1.06265e-03     Sparsity: 8.20068e-02     Total:   6.6478     mean:   1.3960\n",
      " Task losses:   5.9178     mean:   1.1836     Sharing: 9.17738e-04     Sparsity: 8.20051e-02     Total:   6.0007     mean:   1.2665\n",
      " Task losses:   6.9452     mean:   1.3890     Sharing: 1.09956e-03     Sparsity: 8.20027e-02     Total:   7.0283     mean:   1.4721\n",
      " Task losses:   6.5271     mean:   1.3054     Sharing: 1.03457e-03     Sparsity: 8.19991e-02     Total:   6.6101     mean:   1.3885\n",
      " Task losses:   6.4933     mean:   1.2987     Sharing: 1.05387e-03     Sparsity: 8.19954e-02     Total:   6.5763     mean:   1.3817\n",
      " Task losses:   5.6498     mean:   1.1300     Sharing: 1.18497e-03     Sparsity: 8.19909e-02     Total:   5.7329     mean:   1.2131\n",
      " Task losses:   5.3848     mean:   1.0770     Sharing: 1.14731e-03     Sparsity: 8.19863e-02     Total:   5.4679     mean:   1.1601\n",
      " Task losses:   7.9231     mean:   1.5846     Sharing: 9.56958e-04     Sparsity: 8.19810e-02     Total:   8.0060     mean:   1.6676\n",
      " Task losses:   6.5390     mean:   1.3078     Sharing: 9.44540e-04     Sparsity: 8.19758e-02     Total:   6.6219     mean:   1.3907\n",
      " Task losses:   6.9305     mean:   1.3861     Sharing: 9.77144e-04     Sparsity: 8.19705e-02     Total:   7.0135     mean:   1.4691\n",
      " Task losses:   5.4108     mean:   1.0822     Sharing: 9.99858e-04     Sparsity: 8.19631e-02     Total:   5.4937     mean:   1.1651\n",
      " Task losses:   7.5597     mean:   1.5119     Sharing: 1.02362e-03     Sparsity: 8.19551e-02     Total:   7.6427     mean:   1.5949\n",
      " Task losses:   6.7959     mean:   1.3592     Sharing: 1.01608e-03     Sparsity: 8.19459e-02     Total:   6.8789     mean:   1.4421\n",
      " Task losses:   4.8611     mean:   0.9722     Sharing: 9.09130e-04     Sparsity: 8.19354e-02     Total:   4.9439     mean:   1.0551\n",
      " Task losses:   5.6723     mean:   1.1345     Sharing: 1.13246e-03     Sparsity: 8.19248e-02     Total:   5.7553     mean:   1.2175\n",
      " Task losses:   5.8103     mean:   1.1621     Sharing: 1.08481e-03     Sparsity: 8.19141e-02     Total:   5.8933     mean:   1.2451\n",
      " Task losses:   4.8851     mean:   0.9770     Sharing: 7.84338e-04     Sparsity: 8.19027e-02     Total:   4.9678     mean:   1.0597\n",
      " Task losses:   5.1031     mean:   1.0206     Sharing: 9.23291e-04     Sparsity: 8.18916e-02     Total:   5.1859     mean:   1.1034\n",
      " Task losses:   4.7254     mean:   0.9451     Sharing: 8.32443e-04     Sparsity: 8.18799e-02     Total:   4.8081     mean:   1.0278\n",
      " Task losses:   4.6272     mean:   0.9254     Sharing: 8.04067e-04     Sparsity: 8.18690e-02     Total:   4.7098     mean:   1.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   5.1636     mean:   1.0327     Sharing: 9.72634e-04     Sparsity: 8.18596e-02     Total:   5.2464     mean:   1.1156\n",
      " Task losses:   6.8712     mean:   1.3742     Sharing: 1.12185e-03     Sparsity: 8.18506e-02     Total:   6.9542     mean:   1.4572\n",
      " Task losses:   4.4700     mean:   0.8940     Sharing: 1.24921e-03     Sparsity: 8.18418e-02     Total:   4.5531     mean:   0.9771\n",
      " Task losses:   4.8284     mean:   0.9657     Sharing: 1.08606e-03     Sparsity: 8.18330e-02     Total:   4.9113     mean:   1.0486\n",
      " Task losses:   5.1914     mean:   1.0383     Sharing: 7.19900e-04     Sparsity: 8.18245e-02     Total:   5.2739     mean:   1.1208\n",
      " Task losses:   5.1497     mean:   1.0299     Sharing: 1.01633e-03     Sparsity: 8.18156e-02     Total:   5.2325     mean:   1.1128\n",
      " Task losses:   5.1120     mean:   1.0224     Sharing: 1.36180e-03     Sparsity: 8.18069e-02     Total:   5.1952     mean:   1.1056\n",
      " Task losses:   5.9411     mean:   1.1882     Sharing: 1.20807e-03     Sparsity: 8.17986e-02     Total:   6.0241     mean:   1.2712\n",
      " Task losses:   6.0184     mean:   1.2037     Sharing: 1.02905e-03     Sparsity: 8.17902e-02     Total:   6.1012     mean:   1.2865\n",
      " Task losses:   4.8692     mean:   0.9738     Sharing: 1.02925e-03     Sparsity: 8.17824e-02     Total:   4.9521     mean:   1.0567\n",
      " Task losses:   5.5049     mean:   1.1010     Sharing: 1.13699e-03     Sparsity: 8.17743e-02     Total:   5.5878     mean:   1.1839\n",
      " Task losses:   4.1481     mean:   0.8296     Sharing: 1.24226e-03     Sparsity: 8.17656e-02     Total:   4.2311     mean:   0.9126\n",
      " Task losses:   3.9747     mean:   0.7949     Sharing: 1.22439e-03     Sparsity: 8.17582e-02     Total:   4.0577     mean:   0.8779\n",
      " Task losses:   3.3612     mean:   0.6722     Sharing: 1.08650e-03     Sparsity: 8.17506e-02     Total:   3.4440     mean:   0.7551\n",
      " Task losses:   4.0991     mean:   0.8198     Sharing: 1.33185e-03     Sparsity: 8.17433e-02     Total:   4.1822     mean:   0.9029\n",
      " Task losses:   4.4386     mean:   0.8877     Sharing: 1.19743e-03     Sparsity: 8.17358e-02     Total:   4.5215     mean:   0.9707\n",
      " Task losses:   4.6244     mean:   0.9249     Sharing: 9.28630e-04     Sparsity: 8.17273e-02     Total:   4.7070     mean:   1.0075\n",
      " Task losses:   4.8380     mean:   0.9676     Sharing: 1.20159e-03     Sparsity: 8.17199e-02     Total:   4.9210     mean:   1.0505\n",
      " Task losses:   4.5893     mean:   0.9179     Sharing: 1.20917e-03     Sparsity: 8.17123e-02     Total:   4.6722     mean:   1.0008\n",
      " Task losses:   4.6508     mean:   0.9302     Sharing: 1.26368e-03     Sparsity: 8.17048e-02     Total:   4.7337     mean:   1.0131\n",
      " Task losses:   5.2953     mean:   1.0591     Sharing: 1.14519e-03     Sparsity: 8.16976e-02     Total:   5.3782     mean:   1.1419\n",
      " Task losses:   4.9463     mean:   0.9893     Sharing: 1.08360e-03     Sparsity: 8.16906e-02     Total:   5.0290     mean:   1.0720\n",
      " Task losses:   4.2827     mean:   0.8565     Sharing: 9.20504e-04     Sparsity: 8.16833e-02     Total:   4.3653     mean:   0.9391\n",
      " Task losses:   4.8527     mean:   0.9705     Sharing: 8.31048e-04     Sparsity: 8.16759e-02     Total:   4.9352     mean:   1.0530\n",
      " Task losses:   5.2369     mean:   1.0474     Sharing: 1.14282e-03     Sparsity: 8.16685e-02     Total:   5.3197     mean:   1.1302\n",
      " Task losses:   4.7959     mean:   0.9592     Sharing: 1.03187e-03     Sparsity: 8.16611e-02     Total:   4.8786     mean:   1.0419\n",
      " Task losses:   5.4644     mean:   1.0929     Sharing: 7.42818e-04     Sparsity: 8.16537e-02     Total:   5.5468     mean:   1.1753\n",
      " Task losses:   4.9909     mean:   0.9982     Sharing: 7.83737e-04     Sparsity: 8.16464e-02     Total:   5.0734     mean:   1.0806\n",
      " Task losses:   4.5682     mean:   0.9136     Sharing: 9.88061e-04     Sparsity: 8.16386e-02     Total:   4.6508     mean:   0.9963\n",
      " Task losses:   5.8702     mean:   1.1740     Sharing: 8.79491e-04     Sparsity: 8.16310e-02     Total:   5.9527     mean:   1.2565\n",
      " Task losses:   4.2315     mean:   0.8463     Sharing: 6.77283e-04     Sparsity: 8.16279e-02     Total:   4.3138     mean:   0.9286\n",
      " Task losses:   6.8078     mean:   1.3616     Sharing: 9.04456e-04     Sparsity: 8.16246e-02     Total:   6.8903     mean:   1.4441\n",
      " Task losses:   5.6182     mean:   1.1236     Sharing: 9.62809e-04     Sparsity: 8.16225e-02     Total:   5.7008     mean:   1.2062\n",
      " Task losses:   8.9287     mean:   1.7857     Sharing: 7.31637e-04     Sparsity: 8.16203e-02     Total:   9.0110     mean:   1.8681\n",
      " Task losses:   8.2114     mean:   1.6423     Sharing: 7.92662e-04     Sparsity: 8.16297e-02     Total:   8.2938     mean:   1.7247\n",
      " Task losses:   6.2021     mean:   1.2404     Sharing: 8.95307e-04     Sparsity: 8.16379e-02     Total:   6.2847     mean:   1.3230\n",
      " Task losses:   4.7738     mean:   0.9548     Sharing: 8.43699e-04     Sparsity: 8.16435e-02     Total:   4.8563     mean:   1.0372\n",
      " Task losses:   5.6690     mean:   1.1338     Sharing: 8.59241e-04     Sparsity: 8.16483e-02     Total:   5.7515     mean:   1.2163\n",
      " Task losses:   6.8919     mean:   1.3784     Sharing: 7.36455e-04     Sparsity: 8.16516e-02     Total:   6.9743     mean:   1.4608\n",
      " Task losses:   5.6151     mean:   1.1230     Sharing: 7.73514e-04     Sparsity: 8.16531e-02     Total:   5.6975     mean:   1.2054\n",
      " Task losses:   3.5749     mean:   0.7150     Sharing: 9.17529e-04     Sparsity: 8.16523e-02     Total:   3.6575     mean:   0.7976\n",
      " Task losses:   3.2440     mean:   0.6488     Sharing: 7.67052e-04     Sparsity: 8.16502e-02     Total:   3.3264     mean:   0.7312\n",
      " Task losses:   2.9325     mean:   0.5865     Sharing: 5.15888e-04     Sparsity: 8.16479e-02     Total:   3.0147     mean:   0.6687\n",
      " Task losses:   3.1819     mean:   0.6364     Sharing: 7.79524e-04     Sparsity: 8.16446e-02     Total:   3.2643     mean:   0.7188\n",
      " Task losses:   3.9049     mean:   0.7810     Sharing: 9.64796e-04     Sparsity: 8.16403e-02     Total:   3.9875     mean:   0.8636\n",
      "[e]Policy training epoch:100 iteration:  21700 -  Total Loss: 3.9875     Task Loss: 3.9049  Policy Losses:  Sparsity: 0.0816      Sharing: 9.64796e-04 \n",
      "[e]Policy training epoch:100 decay gumbel temp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 101 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:101 iteration:  21808 -  Total Loss: 5.5946     Task Loss: 5.5946  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101   | 0.00019 0.87156 0.78725 0.79004 0.74314|  3.9025  4.2482  4.9081 13.0588|   42.5|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 101 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.1601     mean:   0.8320     Sharing: 1.09674e-03     Sparsity: 8.16352e-02     Total:   4.2428     mean:   0.9147\n",
      " Task losses:   9.4747     mean:   1.8949     Sharing: 8.21466e-04     Sparsity: 8.16301e-02     Total:   9.5571     mean:   1.9774\n",
      " Task losses:  11.0111     mean:   2.2022     Sharing: 7.27182e-04     Sparsity: 8.16287e-02     Total:  11.0935     mean:   2.2846\n",
      " Task losses:  12.6995     mean:   2.5399     Sharing: 9.04307e-04     Sparsity: 8.16266e-02     Total:  12.7820     mean:   2.6224\n",
      " Task losses:  11.9265     mean:   2.3853     Sharing: 8.78563e-04     Sparsity: 8.16244e-02     Total:  12.0090     mean:   2.4678\n",
      " Task losses:  10.1910     mean:   2.0382     Sharing: 8.75389e-04     Sparsity: 8.16226e-02     Total:  10.2735     mean:   2.1207\n",
      " Task losses:   5.1352     mean:   1.0270     Sharing: 6.33265e-04     Sparsity: 8.16215e-02     Total:   5.2174     mean:   1.1093\n",
      " Task losses:   4.1763     mean:   0.8353     Sharing: 7.45629e-04     Sparsity: 8.16194e-02     Total:   4.2587     mean:   0.9176\n",
      " Task losses:   4.6989     mean:   0.9398     Sharing: 7.32973e-04     Sparsity: 8.16165e-02     Total:   4.7813     mean:   1.0221\n",
      " Task losses:   7.3023     mean:   1.4605     Sharing: 7.38601e-04     Sparsity: 8.16118e-02     Total:   7.3847     mean:   1.5428\n",
      " Task losses:   4.9291     mean:   0.9858     Sharing: 7.40603e-04     Sparsity: 8.16077e-02     Total:   5.0114     mean:   1.0682\n",
      " Task losses:   6.0671     mean:   1.2134     Sharing: 6.61939e-04     Sparsity: 8.16038e-02     Total:   6.1494     mean:   1.2957\n",
      " Task losses:   4.9789     mean:   0.9958     Sharing: 7.52558e-04     Sparsity: 8.15991e-02     Total:   5.0612     mean:   1.0781\n",
      " Task losses:   4.2951     mean:   0.8590     Sharing: 7.04100e-04     Sparsity: 8.15940e-02     Total:   4.3774     mean:   0.9413\n",
      " Task losses:   3.9385     mean:   0.7877     Sharing: 6.59814e-04     Sparsity: 8.15890e-02     Total:   4.0208     mean:   0.8700\n",
      " Task losses:   3.6354     mean:   0.7271     Sharing: 5.59474e-04     Sparsity: 8.15832e-02     Total:   3.7176     mean:   0.8092\n",
      " Task losses:   7.9194     mean:   1.5839     Sharing: 7.42043e-04     Sparsity: 8.15793e-02     Total:   8.0017     mean:   1.6662\n",
      " Task losses:  23.5832     mean:   4.7166     Sharing: 7.06981e-04     Sparsity: 8.15779e-02     Total:  23.6655     mean:   4.7989\n",
      " Task losses:  24.0480     mean:   4.8096     Sharing: 5.36849e-04     Sparsity: 8.15868e-02     Total:  24.1302     mean:   4.8917\n",
      " Task losses:   3.8017     mean:   0.7603     Sharing: 9.29331e-04     Sparsity: 8.16044e-02     Total:   3.8843     mean:   0.8429\n",
      " Task losses:   3.4995     mean:   0.6999     Sharing: 1.03157e-03     Sparsity: 8.16204e-02     Total:   3.5821     mean:   0.7825\n",
      " Task losses:   3.8569     mean:   0.7714     Sharing: 8.56206e-04     Sparsity: 8.16341e-02     Total:   3.9394     mean:   0.8539\n",
      " Task losses:   4.1590     mean:   0.8318     Sharing: 6.64090e-04     Sparsity: 8.16457e-02     Total:   4.2413     mean:   0.9141\n",
      " Task losses:   5.3841     mean:   1.0768     Sharing: 9.03577e-04     Sparsity: 8.16554e-02     Total:   5.4667     mean:   1.1594\n",
      " Task losses:   6.6791     mean:   1.3358     Sharing: 1.05875e-03     Sparsity: 8.16637e-02     Total:   6.7618     mean:   1.4185\n",
      " Task losses:   4.1055     mean:   0.8211     Sharing: 8.64004e-04     Sparsity: 8.16703e-02     Total:   4.1880     mean:   0.9036\n",
      " Task losses:   5.2330     mean:   1.0466     Sharing: 7.18087e-04     Sparsity: 8.16756e-02     Total:   5.3154     mean:   1.1290\n",
      " Task losses:   6.9575     mean:   1.3915     Sharing: 5.83157e-04     Sparsity: 8.16797e-02     Total:   7.0397     mean:   1.4738\n",
      " Task losses:   7.1780     mean:   1.4356     Sharing: 7.39296e-04     Sparsity: 8.16825e-02     Total:   7.2604     mean:   1.5180\n",
      " Task losses:   5.7965     mean:   1.1593     Sharing: 8.02383e-04     Sparsity: 8.16843e-02     Total:   5.8790     mean:   1.2418\n",
      " Task losses:   5.6852     mean:   1.1370     Sharing: 8.39735e-04     Sparsity: 8.16843e-02     Total:   5.7677     mean:   1.2196\n",
      " Task losses:   5.7950     mean:   1.1590     Sharing: 8.69135e-04     Sparsity: 8.16832e-02     Total:   5.8775     mean:   1.2416\n",
      " Task losses:   5.8074     mean:   1.1615     Sharing: 9.61726e-04     Sparsity: 8.16812e-02     Total:   5.8900     mean:   1.2441\n",
      " Task losses:   8.4469     mean:   1.6894     Sharing: 1.22743e-03     Sparsity: 8.16787e-02     Total:   8.5298     mean:   1.7723\n",
      " Task losses:  12.5334     mean:   2.5067     Sharing: 1.16376e-03     Sparsity: 8.16666e-02     Total:  12.6162     mean:   2.5895\n",
      " Task losses:   4.4939     mean:   0.8988     Sharing: 1.15958e-03     Sparsity: 8.16417e-02     Total:   4.5767     mean:   0.9816\n",
      " Task losses:   4.9928     mean:   0.9986     Sharing: 1.01105e-03     Sparsity: 8.16187e-02     Total:   5.0754     mean:   1.0812\n",
      " Task losses:   6.8883     mean:   1.3777     Sharing: 1.01269e-03     Sparsity: 8.15976e-02     Total:   6.9709     mean:   1.4603\n",
      " Task losses:   6.2244     mean:   1.2449     Sharing: 1.21917e-03     Sparsity: 8.15797e-02     Total:   6.3072     mean:   1.3277\n",
      " Task losses:   3.9601     mean:   0.7920     Sharing: 1.64632e-03     Sparsity: 8.15630e-02     Total:   4.0433     mean:   0.8752\n",
      " Task losses:   4.6182     mean:   0.9236     Sharing: 1.54823e-03     Sparsity: 8.15471e-02     Total:   4.7013     mean:   1.0067\n",
      " Task losses:   7.4890     mean:   1.4978     Sharing: 1.01752e-03     Sparsity: 8.15315e-02     Total:   7.5715     mean:   1.5803\n",
      " Task losses:   7.2829     mean:   1.4566     Sharing: 1.38344e-03     Sparsity: 8.15202e-02     Total:   7.3658     mean:   1.5395\n",
      " Task losses:   4.2797     mean:   0.8559     Sharing: 1.67633e-03     Sparsity: 8.15096e-02     Total:   4.3629     mean:   0.9391\n",
      " Task losses:   3.9094     mean:   0.7819     Sharing: 1.26616e-03     Sparsity: 8.14993e-02     Total:   3.9921     mean:   0.8646\n",
      " Task losses:   4.5892     mean:   0.9178     Sharing: 1.16691e-03     Sparsity: 8.14895e-02     Total:   4.6718     mean:   1.0005\n",
      " Task losses:   4.9285     mean:   0.9857     Sharing: 1.33458e-03     Sparsity: 8.14789e-02     Total:   5.0113     mean:   1.0685\n",
      " Task losses:   5.1577     mean:   1.0315     Sharing: 1.24663e-03     Sparsity: 8.14686e-02     Total:   5.2405     mean:   1.1143\n",
      " Task losses:   4.1961     mean:   0.8392     Sharing: 1.06098e-03     Sparsity: 8.14584e-02     Total:   4.2786     mean:   0.9217\n",
      " Task losses:   4.2208     mean:   0.8442     Sharing: 1.30507e-03     Sparsity: 8.14489e-02     Total:   4.3035     mean:   0.9269\n",
      " Task losses:   4.9622     mean:   0.9924     Sharing: 1.27362e-03     Sparsity: 8.14394e-02     Total:   5.0449     mean:   1.0751\n",
      " Task losses:   6.3179     mean:   1.2636     Sharing: 1.12134e-03     Sparsity: 8.14304e-02     Total:   6.4005     mean:   1.3461\n",
      " Task losses:   5.7598     mean:   1.1520     Sharing: 1.02395e-03     Sparsity: 8.14216e-02     Total:   5.8423     mean:   1.2344\n",
      " Task losses:   4.3180     mean:   0.8636     Sharing: 1.01668e-03     Sparsity: 8.14139e-02     Total:   4.4004     mean:   0.9460\n",
      " Task losses:   5.7683     mean:   1.1537     Sharing: 1.00811e-03     Sparsity: 8.14065e-02     Total:   5.8507     mean:   1.2361\n",
      " Task losses:   5.1108     mean:   1.0222     Sharing: 6.31640e-04     Sparsity: 8.13996e-02     Total:   5.1928     mean:   1.1042\n",
      " Task losses:   4.1813     mean:   0.8363     Sharing: 7.16930e-04     Sparsity: 8.13942e-02     Total:   4.2634     mean:   0.9184\n",
      " Task losses:   4.8365     mean:   0.9673     Sharing: 9.97111e-04     Sparsity: 8.13903e-02     Total:   4.9189     mean:   1.0497\n",
      " Task losses:   4.9680     mean:   0.9936     Sharing: 8.13330e-04     Sparsity: 8.13890e-02     Total:   5.0502     mean:   1.0758\n",
      " Task losses:   4.0169     mean:   0.8034     Sharing: 6.30890e-04     Sparsity: 8.13874e-02     Total:   4.0989     mean:   0.8854\n",
      " Task losses:   4.5138     mean:   0.9028     Sharing: 1.12848e-03     Sparsity: 8.13853e-02     Total:   4.5963     mean:   0.9853\n",
      " Task losses:   4.1670     mean:   0.8334     Sharing: 1.12106e-03     Sparsity: 8.13831e-02     Total:   4.2495     mean:   0.9159\n",
      " Task losses:   4.6386     mean:   0.9277     Sharing: 8.93335e-04     Sparsity: 8.13802e-02     Total:   4.7209     mean:   1.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.4492     mean:   0.8898     Sharing: 6.69410e-04     Sparsity: 8.13775e-02     Total:   4.5313     mean:   0.9719\n",
      " Task losses:   5.0914     mean:   1.0183     Sharing: 9.65501e-04     Sparsity: 8.13746e-02     Total:   5.1737     mean:   1.1006\n",
      " Task losses:   4.0752     mean:   0.8150     Sharing: 1.08667e-03     Sparsity: 8.13716e-02     Total:   4.1577     mean:   0.8975\n",
      " Task losses:   4.8531     mean:   0.9706     Sharing: 1.33260e-03     Sparsity: 8.13701e-02     Total:   4.9358     mean:   1.0533\n",
      " Task losses:   4.7712     mean:   0.9542     Sharing: 1.18672e-03     Sparsity: 8.13684e-02     Total:   4.8537     mean:   1.0368\n",
      " Task losses:   5.1639     mean:   1.0328     Sharing: 6.97161e-04     Sparsity: 8.13664e-02     Total:   5.2460     mean:   1.1148\n",
      " Task losses:   5.8070     mean:   1.1614     Sharing: 6.70224e-04     Sparsity: 8.13639e-02     Total:   5.8890     mean:   1.2434\n",
      " Task losses:   5.1561     mean:   1.0312     Sharing: 9.50396e-04     Sparsity: 8.13633e-02     Total:   5.2384     mean:   1.1135\n",
      " Task losses:   4.9411     mean:   0.9882     Sharing: 1.01421e-03     Sparsity: 8.13624e-02     Total:   5.0235     mean:   1.0706\n",
      " Task losses:   4.2557     mean:   0.8511     Sharing: 9.29465e-04     Sparsity: 8.13612e-02     Total:   4.3380     mean:   0.9334\n",
      " Task losses:   4.1190     mean:   0.8238     Sharing: 7.55673e-04     Sparsity: 8.13600e-02     Total:   4.2011     mean:   0.9059\n",
      " Task losses:   3.5224     mean:   0.7045     Sharing: 8.54418e-04     Sparsity: 8.13586e-02     Total:   3.6046     mean:   0.7867\n",
      " Task losses:   3.0256     mean:   0.6051     Sharing: 1.04758e-03     Sparsity: 8.13568e-02     Total:   3.1080     mean:   0.6875\n",
      " Task losses:   3.1367     mean:   0.6273     Sharing: 8.85343e-04     Sparsity: 8.13546e-02     Total:   3.2189     mean:   0.7096\n",
      " Task losses:   3.4183     mean:   0.6837     Sharing: 6.48111e-04     Sparsity: 8.13522e-02     Total:   3.5003     mean:   0.7657\n",
      " Task losses:   4.0845     mean:   0.8169     Sharing: 5.48522e-04     Sparsity: 8.13496e-02     Total:   4.1664     mean:   0.8988\n",
      " Task losses:   3.7716     mean:   0.7543     Sharing: 8.49063e-04     Sparsity: 8.13468e-02     Total:   3.8538     mean:   0.8365\n",
      " Task losses:   4.1765     mean:   0.8353     Sharing: 7.01914e-04     Sparsity: 8.13433e-02     Total:   4.2586     mean:   0.9174\n",
      " Task losses:   3.2919     mean:   0.6584     Sharing: 2.54725e-04     Sparsity: 8.13398e-02     Total:   3.3735     mean:   0.7400\n",
      " Task losses:   3.8645     mean:   0.7729     Sharing: 9.21870e-04     Sparsity: 8.13356e-02     Total:   3.9468     mean:   0.8552\n",
      " Task losses:   3.4380     mean:   0.6876     Sharing: 1.28274e-03     Sparsity: 8.13336e-02     Total:   3.5207     mean:   0.7702\n",
      " Task losses:   3.9810     mean:   0.7962     Sharing: 1.06322e-03     Sparsity: 8.13311e-02     Total:   4.0634     mean:   0.8786\n",
      " Task losses:   3.9490     mean:   0.7898     Sharing: 7.06290e-04     Sparsity: 8.13283e-02     Total:   4.0310     mean:   0.8718\n",
      " Task losses:   4.3361     mean:   0.8672     Sharing: 8.73744e-04     Sparsity: 8.13256e-02     Total:   4.4183     mean:   0.9494\n",
      " Task losses:   4.8359     mean:   0.9672     Sharing: 1.14944e-03     Sparsity: 8.13230e-02     Total:   4.9184     mean:   1.0497\n",
      " Task losses:   4.5830     mean:   0.9166     Sharing: 1.01465e-03     Sparsity: 8.13220e-02     Total:   4.6653     mean:   0.9989\n",
      " Task losses:   4.6615     mean:   0.9323     Sharing: 8.60487e-04     Sparsity: 8.13209e-02     Total:   4.7437     mean:   1.0145\n",
      " Task losses:   4.4825     mean:   0.8965     Sharing: 8.20826e-04     Sparsity: 8.13212e-02     Total:   4.5646     mean:   0.9786\n",
      " Task losses:   4.1862     mean:   0.8372     Sharing: 7.71046e-04     Sparsity: 8.13209e-02     Total:   4.2683     mean:   0.9193\n",
      " Task losses:   5.4663     mean:   1.0933     Sharing: 8.34788e-04     Sparsity: 8.13198e-02     Total:   5.5484     mean:   1.1754\n",
      " Task losses:   3.5113     mean:   0.7023     Sharing: 9.08375e-04     Sparsity: 8.13223e-02     Total:   3.5935     mean:   0.7845\n",
      " Task losses:   5.0327     mean:   1.0065     Sharing: 8.14661e-04     Sparsity: 8.13238e-02     Total:   5.1149     mean:   1.0887\n",
      " Task losses:   4.0554     mean:   0.8111     Sharing: 9.96148e-04     Sparsity: 8.13248e-02     Total:   4.1377     mean:   0.8934\n",
      " Task losses:   6.0785     mean:   1.2157     Sharing: 9.90997e-04     Sparsity: 8.13259e-02     Total:   6.1608     mean:   1.2980\n",
      " Task losses:   4.7156     mean:   0.9431     Sharing: 1.15146e-03     Sparsity: 8.13259e-02     Total:   4.7981     mean:   1.0256\n",
      " Task losses:   3.7678     mean:   0.7536     Sharing: 1.07990e-03     Sparsity: 8.13251e-02     Total:   3.8503     mean:   0.8360\n",
      " Task losses:   3.7154     mean:   0.7431     Sharing: 6.29341e-04     Sparsity: 8.13239e-02     Total:   3.7973     mean:   0.8250\n",
      " Task losses:   3.1513     mean:   0.6303     Sharing: 7.67916e-04     Sparsity: 8.13233e-02     Total:   3.2334     mean:   0.7124\n",
      " Task losses:   4.5175     mean:   0.9035     Sharing: 1.12387e-03     Sparsity: 8.13223e-02     Total:   4.5999     mean:   0.9859\n",
      " Task losses:   3.2373     mean:   0.6475     Sharing: 1.03594e-03     Sparsity: 8.13213e-02     Total:   3.3197     mean:   0.7298\n",
      " Task losses:   3.2780     mean:   0.6556     Sharing: 5.40644e-04     Sparsity: 8.13198e-02     Total:   3.3598     mean:   0.7375\n",
      " Task losses:   2.4419     mean:   0.4884     Sharing: 9.22814e-04     Sparsity: 8.13178e-02     Total:   2.5241     mean:   0.5706\n",
      " Task losses:   2.8498     mean:   0.5700     Sharing: 1.17080e-03     Sparsity: 8.13146e-02     Total:   2.9323     mean:   0.6524\n",
      " Task losses:   2.8660     mean:   0.5732     Sharing: 8.18729e-04     Sparsity: 8.13113e-02     Total:   2.9481     mean:   0.6553\n",
      " Task losses:   4.3114     mean:   0.8623     Sharing: 5.17165e-04     Sparsity: 8.13100e-02     Total:   4.3933     mean:   0.9441\n",
      "[e]Policy training epoch:101 iteration:  21916 -  Total Loss: 4.3933     Task Loss: 4.3114  Policy Losses:  Sparsity: 0.0813      Sharing: 5.17165e-04 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 102 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:102 iteration:  22024 -  Total Loss: 3.7359     Task Loss: 3.7359  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102   | 0.00019 0.84758 0.78752 0.78717 0.74562|  4.2292  3.9030  4.5807 12.7129|   43.5|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 102 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.3565     mean:   0.6713     Sharing: 8.78170e-04     Sparsity: 8.13084e-02     Total:   3.4387     mean:   0.7535\n",
      " Task losses:   7.3708     mean:   1.4742     Sharing: 8.75130e-04     Sparsity: 8.13058e-02     Total:   7.4530     mean:   1.5563\n",
      " Task losses:  11.1368     mean:   2.2274     Sharing: 4.19224e-04     Sparsity: 8.12994e-02     Total:  11.2185     mean:   2.3091\n",
      " Task losses:  10.3907     mean:   2.0781     Sharing: 6.54946e-04     Sparsity: 8.12876e-02     Total:  10.4726     mean:   2.1601\n",
      " Task losses:  13.3828     mean:   2.6766     Sharing: 8.76461e-04     Sparsity: 8.12716e-02     Total:  13.4650     mean:   2.7587\n",
      " Task losses:   7.8359     mean:   1.5672     Sharing: 8.75687e-04     Sparsity: 8.12508e-02     Total:   7.9181     mean:   1.6493\n",
      " Task losses:   4.4522     mean:   0.8904     Sharing: 7.40131e-04     Sparsity: 8.12290e-02     Total:   4.5341     mean:   0.9724\n",
      " Task losses:   4.1412     mean:   0.8282     Sharing: 9.71983e-04     Sparsity: 8.12078e-02     Total:   4.2233     mean:   0.9104\n",
      " Task losses:   5.5341     mean:   1.1068     Sharing: 7.50487e-04     Sparsity: 8.11856e-02     Total:   5.6160     mean:   1.1887\n",
      " Task losses:  14.8054     mean:   2.9611     Sharing: 8.29210e-04     Sparsity: 8.11644e-02     Total:  14.8874     mean:   3.0431\n",
      " Task losses:   5.7743     mean:   1.1549     Sharing: 1.02263e-03     Sparsity: 8.11506e-02     Total:   5.8565     mean:   1.2370\n",
      " Task losses:   4.9773     mean:   0.9955     Sharing: 1.06314e-03     Sparsity: 8.11362e-02     Total:   5.0595     mean:   1.0777\n",
      " Task losses:   4.2442     mean:   0.8488     Sharing: 7.92111e-04     Sparsity: 8.11216e-02     Total:   4.3261     mean:   0.9307\n",
      " Task losses:   3.7394     mean:   0.7479     Sharing: 8.79193e-04     Sparsity: 8.11071e-02     Total:   3.8214     mean:   0.8299\n",
      " Task losses:   3.9906     mean:   0.7981     Sharing: 1.12144e-03     Sparsity: 8.10931e-02     Total:   4.0728     mean:   0.8803\n",
      " Task losses:   3.1108     mean:   0.6222     Sharing: 7.89836e-04     Sparsity: 8.10783e-02     Total:   3.1927     mean:   0.7040\n",
      " Task losses:   4.5193     mean:   0.9039     Sharing: 6.25824e-04     Sparsity: 8.10654e-02     Total:   4.6010     mean:   0.9856\n",
      " Task losses:  12.7788     mean:   2.5558     Sharing: 6.92859e-04     Sparsity: 8.10525e-02     Total:  12.8606     mean:   2.6375\n",
      " Task losses:  14.3151     mean:   2.8630     Sharing: 7.15936e-04     Sparsity: 8.10417e-02     Total:  14.3969     mean:   2.9448\n",
      " Task losses:   3.5228     mean:   0.7046     Sharing: 1.06627e-03     Sparsity: 8.10400e-02     Total:   3.6049     mean:   0.7867\n",
      " Task losses:   3.1927     mean:   0.6385     Sharing: 1.09002e-03     Sparsity: 8.10379e-02     Total:   3.2748     mean:   0.7207\n",
      " Task losses:   3.1772     mean:   0.6354     Sharing: 1.07802e-03     Sparsity: 8.10346e-02     Total:   3.2593     mean:   0.7175\n",
      " Task losses:   3.1592     mean:   0.6318     Sharing: 9.72018e-04     Sparsity: 8.10309e-02     Total:   3.2412     mean:   0.7139\n",
      " Task losses:   4.7689     mean:   0.9538     Sharing: 9.47441e-04     Sparsity: 8.10265e-02     Total:   4.8509     mean:   1.0358\n",
      " Task losses:   5.2387     mean:   1.0477     Sharing: 7.91426e-04     Sparsity: 8.10219e-02     Total:   5.3205     mean:   1.1296\n",
      " Task losses:   3.7879     mean:   0.7576     Sharing: 9.33980e-04     Sparsity: 8.10170e-02     Total:   3.8698     mean:   0.8395\n",
      " Task losses:   5.2499     mean:   1.0500     Sharing: 8.02596e-04     Sparsity: 8.10124e-02     Total:   5.3318     mean:   1.1318\n",
      " Task losses:   5.9330     mean:   1.1866     Sharing: 9.84654e-04     Sparsity: 8.10076e-02     Total:   6.0150     mean:   1.2686\n",
      " Task losses:   6.3630     mean:   1.2726     Sharing: 7.70688e-04     Sparsity: 8.10029e-02     Total:   6.4448     mean:   1.3544\n",
      " Task losses:   4.7068     mean:   0.9414     Sharing: 9.18945e-04     Sparsity: 8.09977e-02     Total:   4.7887     mean:   1.0233\n",
      " Task losses:   4.1600     mean:   0.8320     Sharing: 1.21492e-03     Sparsity: 8.09922e-02     Total:   4.2422     mean:   0.9142\n",
      " Task losses:   3.8021     mean:   0.7604     Sharing: 9.07148e-04     Sparsity: 8.09868e-02     Total:   3.8840     mean:   0.8423\n",
      " Task losses:   4.1521     mean:   0.8304     Sharing: 8.50360e-04     Sparsity: 8.09809e-02     Total:   4.2339     mean:   0.9123\n",
      " Task losses:   5.5059     mean:   1.1012     Sharing: 8.26150e-04     Sparsity: 8.09746e-02     Total:   5.5877     mean:   1.1830\n",
      " Task losses:   9.2942     mean:   1.8588     Sharing: 9.31054e-04     Sparsity: 8.09691e-02     Total:   9.3761     mean:   1.9407\n",
      " Task losses:   4.1657     mean:   0.8331     Sharing: 9.52164e-04     Sparsity: 8.09618e-02     Total:   4.2476     mean:   0.9151\n",
      " Task losses:   5.4012     mean:   1.0802     Sharing: 9.54002e-04     Sparsity: 8.09545e-02     Total:   5.4831     mean:   1.1622\n",
      " Task losses:   8.7740     mean:   1.7548     Sharing: 1.02292e-03     Sparsity: 8.09493e-02     Total:   8.8559     mean:   1.8368\n",
      " Task losses:   6.0846     mean:   1.2169     Sharing: 1.14608e-03     Sparsity: 8.09430e-02     Total:   6.1667     mean:   1.2990\n",
      " Task losses:   3.1844     mean:   0.6369     Sharing: 1.02881e-03     Sparsity: 8.09367e-02     Total:   3.2664     mean:   0.7188\n",
      " Task losses:   3.8044     mean:   0.7609     Sharing: 1.00332e-03     Sparsity: 8.09301e-02     Total:   3.8864     mean:   0.8428\n",
      " Task losses:   5.6520     mean:   1.1304     Sharing: 9.91325e-04     Sparsity: 8.09242e-02     Total:   5.7339     mean:   1.2123\n",
      " Task losses:   5.3656     mean:   1.0731     Sharing: 1.53513e-03     Sparsity: 8.09238e-02     Total:   5.4481     mean:   1.1556\n",
      " Task losses:   4.7269     mean:   0.9454     Sharing: 1.50240e-03     Sparsity: 8.09237e-02     Total:   4.8093     mean:   1.0278\n",
      " Task losses:   4.2808     mean:   0.8562     Sharing: 1.23285e-03     Sparsity: 8.09226e-02     Total:   4.3630     mean:   0.9383\n",
      " Task losses:   5.1893     mean:   1.0379     Sharing: 1.09955e-03     Sparsity: 8.09195e-02     Total:   5.2713     mean:   1.1199\n",
      " Task losses:   5.0145     mean:   1.0029     Sharing: 8.30342e-04     Sparsity: 8.09143e-02     Total:   5.0962     mean:   1.0846\n",
      " Task losses:   5.3628     mean:   1.0726     Sharing: 1.21318e-03     Sparsity: 8.09080e-02     Total:   5.4449     mean:   1.1547\n",
      " Task losses:   5.1942     mean:   1.0388     Sharing: 1.20544e-03     Sparsity: 8.09016e-02     Total:   5.2763     mean:   1.1209\n",
      " Task losses:   5.2651     mean:   1.0530     Sharing: 1.28623e-03     Sparsity: 8.08950e-02     Total:   5.3472     mean:   1.1352\n",
      " Task losses:   6.2509     mean:   1.2502     Sharing: 1.19243e-03     Sparsity: 8.08885e-02     Total:   6.3330     mean:   1.3323\n",
      " Task losses:   6.4570     mean:   1.2914     Sharing: 9.13759e-04     Sparsity: 8.08816e-02     Total:   6.5388     mean:   1.3732\n",
      " Task losses:   5.2791     mean:   1.0558     Sharing: 4.91415e-04     Sparsity: 8.08745e-02     Total:   5.3604     mean:   1.1372\n",
      " Task losses:   4.1982     mean:   0.8396     Sharing: 8.79586e-04     Sparsity: 8.08669e-02     Total:   4.2799     mean:   0.9214\n",
      " Task losses:   4.9951     mean:   0.9990     Sharing: 1.12873e-03     Sparsity: 8.08594e-02     Total:   5.0770     mean:   1.0810\n",
      " Task losses:   5.2241     mean:   1.0448     Sharing: 8.91010e-04     Sparsity: 8.08507e-02     Total:   5.3058     mean:   1.1266\n",
      " Task losses:   3.6506     mean:   0.7301     Sharing: 4.23426e-04     Sparsity: 8.08421e-02     Total:   3.7319     mean:   0.8114\n",
      " Task losses:   3.6506     mean:   0.7301     Sharing: 8.18839e-04     Sparsity: 8.08338e-02     Total:   3.7322     mean:   0.8118\n",
      " Task losses:   4.2816     mean:   0.8563     Sharing: 9.02553e-04     Sparsity: 8.08256e-02     Total:   4.3634     mean:   0.9381\n",
      " Task losses:   3.6432     mean:   0.7286     Sharing: 9.36344e-04     Sparsity: 8.08180e-02     Total:   3.7250     mean:   0.8104\n",
      " Task losses:   3.7123     mean:   0.7425     Sharing: 7.93760e-04     Sparsity: 8.08099e-02     Total:   3.7939     mean:   0.8241\n",
      " Task losses:   3.9648     mean:   0.7930     Sharing: 5.67868e-04     Sparsity: 8.08022e-02     Total:   4.0462     mean:   0.8743\n",
      " Task losses:   3.7013     mean:   0.7403     Sharing: 1.15099e-03     Sparsity: 8.07947e-02     Total:   3.7833     mean:   0.8222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.9833     mean:   0.7967     Sharing: 1.34855e-03     Sparsity: 8.07872e-02     Total:   4.0654     mean:   0.8788\n",
      " Task losses:   3.7994     mean:   0.7599     Sharing: 9.99639e-04     Sparsity: 8.07799e-02     Total:   3.8811     mean:   0.8417\n",
      " Task losses:   3.4955     mean:   0.6991     Sharing: 5.63815e-04     Sparsity: 8.07726e-02     Total:   3.5768     mean:   0.7804\n",
      " Task losses:   4.3904     mean:   0.8781     Sharing: 1.16497e-03     Sparsity: 8.07672e-02     Total:   4.4724     mean:   0.9600\n",
      " Task losses:   5.3371     mean:   1.0674     Sharing: 1.34793e-03     Sparsity: 8.07605e-02     Total:   5.4192     mean:   1.1495\n",
      " Task losses:   4.9900     mean:   0.9980     Sharing: 1.12594e-03     Sparsity: 8.07541e-02     Total:   5.0719     mean:   1.0799\n",
      " Task losses:   5.2456     mean:   1.0491     Sharing: 9.43904e-04     Sparsity: 8.07478e-02     Total:   5.3273     mean:   1.1308\n",
      " Task losses:   5.0081     mean:   1.0016     Sharing: 9.94975e-04     Sparsity: 8.07409e-02     Total:   5.0898     mean:   1.0833\n",
      " Task losses:   4.7532     mean:   0.9506     Sharing: 9.75837e-04     Sparsity: 8.07341e-02     Total:   4.8349     mean:   1.0323\n",
      " Task losses:   3.6990     mean:   0.7398     Sharing: 1.03125e-03     Sparsity: 8.07271e-02     Total:   3.7808     mean:   0.8216\n",
      " Task losses:   4.2093     mean:   0.8419     Sharing: 1.07425e-03     Sparsity: 8.07202e-02     Total:   4.2911     mean:   0.9237\n",
      " Task losses:   3.6234     mean:   0.7247     Sharing: 1.24642e-03     Sparsity: 8.07136e-02     Total:   3.7054     mean:   0.8066\n",
      " Task losses:   3.4791     mean:   0.6958     Sharing: 1.16273e-03     Sparsity: 8.07064e-02     Total:   3.5610     mean:   0.7777\n",
      " Task losses:   3.7304     mean:   0.7461     Sharing: 8.91184e-04     Sparsity: 8.06989e-02     Total:   3.8120     mean:   0.8277\n",
      " Task losses:   3.5081     mean:   0.7016     Sharing: 7.88147e-04     Sparsity: 8.06907e-02     Total:   3.5896     mean:   0.7831\n",
      " Task losses:   3.8022     mean:   0.7604     Sharing: 8.87245e-04     Sparsity: 8.06817e-02     Total:   3.8838     mean:   0.8420\n",
      " Task losses:   3.6445     mean:   0.7289     Sharing: 9.55572e-04     Sparsity: 8.06724e-02     Total:   3.7261     mean:   0.8105\n",
      " Task losses:   4.1825     mean:   0.8365     Sharing: 9.68546e-04     Sparsity: 8.06626e-02     Total:   4.2642     mean:   0.9181\n",
      " Task losses:   4.5380     mean:   0.9076     Sharing: 5.51452e-04     Sparsity: 8.06523e-02     Total:   4.6192     mean:   0.9888\n",
      " Task losses:   5.3796     mean:   1.0759     Sharing: 8.84155e-04     Sparsity: 8.06418e-02     Total:   5.4611     mean:   1.1574\n",
      " Task losses:   6.9159     mean:   1.3832     Sharing: 1.27100e-03     Sparsity: 8.06315e-02     Total:   6.9978     mean:   1.4651\n",
      " Task losses:   6.2244     mean:   1.2449     Sharing: 1.06242e-03     Sparsity: 8.06247e-02     Total:   6.3060     mean:   1.3266\n",
      " Task losses:   3.7457     mean:   0.7491     Sharing: 7.38606e-04     Sparsity: 8.06158e-02     Total:   3.8270     mean:   0.8305\n",
      " Task losses:   3.5770     mean:   0.7154     Sharing: 1.10608e-03     Sparsity: 8.06079e-02     Total:   3.6587     mean:   0.7971\n",
      " Task losses:   3.7236     mean:   0.7447     Sharing: 1.49299e-03     Sparsity: 8.05999e-02     Total:   3.8057     mean:   0.8268\n",
      " Task losses:   3.4741     mean:   0.6948     Sharing: 1.17526e-03     Sparsity: 8.05914e-02     Total:   3.5558     mean:   0.7766\n",
      " Task losses:   3.1872     mean:   0.6374     Sharing: 8.20175e-04     Sparsity: 8.05832e-02     Total:   3.2686     mean:   0.7188\n",
      " Task losses:   3.7638     mean:   0.7528     Sharing: 9.02270e-04     Sparsity: 8.05753e-02     Total:   3.8452     mean:   0.8342\n",
      " Task losses:   3.5204     mean:   0.7041     Sharing: 9.08852e-04     Sparsity: 8.05676e-02     Total:   3.6019     mean:   0.7856\n",
      " Task losses:   3.8102     mean:   0.7620     Sharing: 1.07019e-03     Sparsity: 8.05604e-02     Total:   3.8919     mean:   0.8437\n",
      " Task losses:   3.7920     mean:   0.7584     Sharing: 1.04953e-03     Sparsity: 8.05533e-02     Total:   3.8736     mean:   0.8400\n",
      " Task losses:   6.3912     mean:   1.2782     Sharing: 8.51865e-04     Sparsity: 8.05464e-02     Total:   6.4726     mean:   1.3596\n",
      " Task losses:   6.8572     mean:   1.3714     Sharing: 8.43446e-04     Sparsity: 8.05391e-02     Total:   6.9386     mean:   1.4528\n",
      " Task losses:   7.2114     mean:   1.4423     Sharing: 1.17751e-03     Sparsity: 8.05313e-02     Total:   7.2931     mean:   1.5240\n",
      " Task losses:   5.1120     mean:   1.0224     Sharing: 8.70476e-04     Sparsity: 8.05235e-02     Total:   5.1934     mean:   1.1038\n",
      " Task losses:   3.8468     mean:   0.7694     Sharing: 9.76756e-04     Sparsity: 8.05154e-02     Total:   3.9283     mean:   0.8509\n",
      " Task losses:   3.3544     mean:   0.6709     Sharing: 1.17039e-03     Sparsity: 8.05074e-02     Total:   3.4361     mean:   0.7526\n",
      " Task losses:   2.9278     mean:   0.5856     Sharing: 1.45191e-03     Sparsity: 8.05000e-02     Total:   3.0098     mean:   0.6675\n",
      " Task losses:   4.3772     mean:   0.8754     Sharing: 1.17026e-03     Sparsity: 8.04925e-02     Total:   4.4589     mean:   0.9571\n",
      " Task losses:   3.6456     mean:   0.7291     Sharing: 9.96336e-04     Sparsity: 8.04859e-02     Total:   3.7271     mean:   0.8106\n",
      " Task losses:   2.9838     mean:   0.5968     Sharing: 1.40875e-03     Sparsity: 8.04795e-02     Total:   3.0657     mean:   0.6786\n",
      " Task losses:   2.6283     mean:   0.5257     Sharing: 1.22498e-03     Sparsity: 8.04729e-02     Total:   2.7100     mean:   0.6074\n",
      " Task losses:   2.5161     mean:   0.5032     Sharing: 1.19362e-03     Sparsity: 8.04667e-02     Total:   2.5978     mean:   0.5849\n",
      " Task losses:   2.4625     mean:   0.4925     Sharing: 1.47712e-03     Sparsity: 8.04602e-02     Total:   2.5444     mean:   0.5744\n",
      " Task losses:   3.3959     mean:   0.6792     Sharing: 1.58366e-03     Sparsity: 8.04536e-02     Total:   3.4780     mean:   0.7612\n",
      "[e]Policy training epoch:102 iteration:  22132 -  Total Loss: 3.4780     Task Loss: 3.3959  Policy Losses:  Sparsity: 0.0805      Sharing: 1.58366e-03 \n",
      "[e]Policy training epoch:102 decay gumbel temp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 103 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:103 iteration:  22240 -  Total Loss: 3.8568     Task Loss: 3.8568  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103   | 0.00019 0.87391 0.78622 0.79087 0.74204|  4.5421  3.8349  4.7219 13.0989|   42.4|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 103 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.2372     mean:   0.8474     Sharing: 1.52316e-03     Sparsity: 8.04470e-02     Total:   4.3192     mean:   0.9294\n",
      " Task losses:   8.4455     mean:   1.6891     Sharing: 1.43190e-03     Sparsity: 8.04405e-02     Total:   8.5274     mean:   1.7710\n",
      " Task losses:  10.0679     mean:   2.0136     Sharing: 1.38201e-03     Sparsity: 8.04357e-02     Total:  10.1497     mean:   2.0954\n",
      " Task losses:  11.1888     mean:   2.2378     Sharing: 1.30903e-03     Sparsity: 8.04335e-02     Total:  11.2706     mean:   2.3195\n",
      " Task losses:  11.5330     mean:   2.3066     Sharing: 1.34289e-03     Sparsity: 8.04312e-02     Total:  11.6148     mean:   2.3884\n",
      " Task losses:  10.3756     mean:   2.0751     Sharing: 1.41573e-03     Sparsity: 8.04285e-02     Total:  10.4574     mean:   2.1570\n",
      " Task losses:   4.7048     mean:   0.9410     Sharing: 1.42401e-03     Sparsity: 8.04298e-02     Total:   4.7866     mean:   1.0228\n",
      " Task losses:   4.1344     mean:   0.8269     Sharing: 1.22823e-03     Sparsity: 8.04291e-02     Total:   4.2161     mean:   0.9085\n",
      " Task losses:   5.1983     mean:   1.0397     Sharing: 1.01438e-03     Sparsity: 8.04273e-02     Total:   5.2798     mean:   1.1211\n",
      " Task losses:   8.7981     mean:   1.7596     Sharing: 1.07483e-03     Sparsity: 8.04255e-02     Total:   8.8796     mean:   1.8411\n",
      " Task losses:   5.0173     mean:   1.0035     Sharing: 1.25796e-03     Sparsity: 8.04233e-02     Total:   5.0990     mean:   1.0851\n",
      " Task losses:   4.6134     mean:   0.9227     Sharing: 1.36952e-03     Sparsity: 8.04204e-02     Total:   4.6952     mean:   1.0045\n",
      " Task losses:   3.8042     mean:   0.7608     Sharing: 1.09326e-03     Sparsity: 8.04157e-02     Total:   3.8857     mean:   0.8423\n",
      " Task losses:   4.4256     mean:   0.8851     Sharing: 8.18714e-04     Sparsity: 8.04101e-02     Total:   4.5069     mean:   0.9664\n",
      " Task losses:   5.1505     mean:   1.0301     Sharing: 9.82255e-04     Sparsity: 8.04036e-02     Total:   5.2319     mean:   1.1115\n",
      " Task losses:   3.9686     mean:   0.7937     Sharing: 1.10463e-03     Sparsity: 8.03970e-02     Total:   4.0501     mean:   0.8752\n",
      " Task losses:   5.9691     mean:   1.1938     Sharing: 8.73079e-04     Sparsity: 8.03899e-02     Total:   6.0504     mean:   1.2751\n",
      " Task losses:   9.1569     mean:   1.8314     Sharing: 8.75975e-04     Sparsity: 8.03819e-02     Total:   9.2382     mean:   1.9126\n",
      " Task losses:  10.0465     mean:   2.0093     Sharing: 9.63514e-04     Sparsity: 8.03675e-02     Total:  10.1279     mean:   2.0906\n",
      " Task losses:   4.6447     mean:   0.9289     Sharing: 6.71347e-04     Sparsity: 8.03559e-02     Total:   4.7258     mean:   1.0100\n",
      " Task losses:   3.7402     mean:   0.7480     Sharing: 8.88169e-04     Sparsity: 8.03444e-02     Total:   3.8215     mean:   0.8293\n",
      " Task losses:   4.5233     mean:   0.9047     Sharing: 1.18172e-03     Sparsity: 8.03319e-02     Total:   4.6048     mean:   0.9862\n",
      " Task losses:   4.9018     mean:   0.9804     Sharing: 8.13340e-04     Sparsity: 8.03198e-02     Total:   4.9829     mean:   1.0615\n",
      " Task losses:   6.5342     mean:   1.3068     Sharing: 5.29394e-04     Sparsity: 8.03083e-02     Total:   6.6150     mean:   1.3877\n",
      " Task losses:   6.9239     mean:   1.3848     Sharing: 8.37112e-04     Sparsity: 8.02974e-02     Total:   7.0050     mean:   1.4659\n",
      " Task losses:   5.1896     mean:   1.0379     Sharing: 7.00985e-04     Sparsity: 8.02870e-02     Total:   5.2706     mean:   1.1189\n",
      " Task losses:   6.2836     mean:   1.2567     Sharing: 6.03204e-04     Sparsity: 8.02769e-02     Total:   6.3644     mean:   1.3376\n",
      " Task losses:   6.3587     mean:   1.2717     Sharing: 9.03333e-04     Sparsity: 8.02706e-02     Total:   6.4399     mean:   1.3529\n",
      " Task losses:   7.5812     mean:   1.5162     Sharing: 8.59434e-04     Sparsity: 8.02600e-02     Total:   7.6623     mean:   1.5974\n",
      " Task losses:   5.5618     mean:   1.1124     Sharing: 1.02857e-03     Sparsity: 8.02499e-02     Total:   5.6431     mean:   1.1936\n",
      " Task losses:   5.5838     mean:   1.1168     Sharing: 9.80760e-04     Sparsity: 8.02382e-02     Total:   5.6650     mean:   1.1980\n",
      " Task losses:   5.8649     mean:   1.1730     Sharing: 6.85920e-04     Sparsity: 8.02272e-02     Total:   5.9458     mean:   1.2539\n",
      " Task losses:   5.8993     mean:   1.1799     Sharing: 7.53706e-04     Sparsity: 8.02149e-02     Total:   5.9803     mean:   1.2608\n",
      " Task losses:   5.4841     mean:   1.0968     Sharing: 1.17033e-03     Sparsity: 8.02037e-02     Total:   5.5655     mean:   1.1782\n",
      " Task losses:   5.5805     mean:   1.1161     Sharing: 8.75677e-04     Sparsity: 8.01950e-02     Total:   5.6615     mean:   1.1972\n",
      " Task losses:   4.0565     mean:   0.8113     Sharing: 8.76337e-04     Sparsity: 8.01900e-02     Total:   4.1376     mean:   0.8924\n",
      " Task losses:   4.5577     mean:   0.9115     Sharing: 9.70637e-04     Sparsity: 8.01852e-02     Total:   4.6389     mean:   0.9927\n",
      " Task losses:   7.7608     mean:   1.5522     Sharing: 1.11966e-03     Sparsity: 8.01805e-02     Total:   7.8421     mean:   1.6335\n",
      " Task losses:   5.2483     mean:   1.0497     Sharing: 9.58825e-04     Sparsity: 8.01725e-02     Total:   5.3294     mean:   1.1308\n",
      " Task losses:   4.0740     mean:   0.8148     Sharing: 7.43220e-04     Sparsity: 8.01645e-02     Total:   4.1549     mean:   0.8957\n",
      " Task losses:   4.9885     mean:   0.9977     Sharing: 8.00520e-04     Sparsity: 8.01566e-02     Total:   5.0695     mean:   1.0787\n",
      " Task losses:   6.7770     mean:   1.3554     Sharing: 7.31573e-04     Sparsity: 8.01476e-02     Total:   6.8579     mean:   1.4363\n",
      " Task losses:   7.2981     mean:   1.4596     Sharing: 6.75634e-04     Sparsity: 8.01384e-02     Total:   7.3789     mean:   1.5404\n",
      " Task losses:   5.9855     mean:   1.1971     Sharing: 8.46262e-04     Sparsity: 8.01268e-02     Total:   6.0665     mean:   1.2781\n",
      " Task losses:   6.2884     mean:   1.2577     Sharing: 8.08393e-04     Sparsity: 8.01142e-02     Total:   6.3693     mean:   1.3386\n",
      " Task losses:   5.3618     mean:   1.0724     Sharing: 9.68238e-04     Sparsity: 8.01036e-02     Total:   5.4429     mean:   1.1534\n",
      " Task losses:   5.4123     mean:   1.0825     Sharing: 6.77556e-04     Sparsity: 8.00930e-02     Total:   5.4930     mean:   1.1632\n",
      " Task losses:   5.6989     mean:   1.1398     Sharing: 8.43893e-04     Sparsity: 8.00826e-02     Total:   5.7798     mean:   1.2207\n",
      " Task losses:   4.5774     mean:   0.9155     Sharing: 1.06407e-03     Sparsity: 8.00720e-02     Total:   4.6586     mean:   0.9966\n",
      " Task losses:   4.4845     mean:   0.8969     Sharing: 9.64100e-04     Sparsity: 8.00611e-02     Total:   4.5655     mean:   0.9779\n",
      " Task losses:   4.9025     mean:   0.9805     Sharing: 6.05916e-04     Sparsity: 8.00506e-02     Total:   4.9832     mean:   1.0612\n",
      " Task losses:   6.7942     mean:   1.3588     Sharing: 8.59072e-04     Sparsity: 8.00405e-02     Total:   6.8751     mean:   1.4397\n",
      " Task losses:   6.9685     mean:   1.3937     Sharing: 8.63497e-04     Sparsity: 8.00287e-02     Total:   7.0494     mean:   1.4746\n",
      " Task losses:   5.3291     mean:   1.0658     Sharing: 1.14594e-03     Sparsity: 8.00171e-02     Total:   5.4102     mean:   1.1470\n",
      " Task losses:   8.1135     mean:   1.6227     Sharing: 1.04040e-03     Sparsity: 8.00057e-02     Total:   8.1945     mean:   1.7037\n",
      " Task losses:   7.6908     mean:   1.5382     Sharing: 5.72388e-04     Sparsity: 7.99916e-02     Total:   7.7714     mean:   1.6187\n",
      " Task losses:   4.3049     mean:   0.8610     Sharing: 8.29374e-04     Sparsity: 7.99816e-02     Total:   4.3857     mean:   0.9418\n",
      " Task losses:   4.4647     mean:   0.8929     Sharing: 1.50345e-03     Sparsity: 7.99729e-02     Total:   4.5462     mean:   0.9744\n",
      " Task losses:   4.7283     mean:   0.9457     Sharing: 1.66650e-03     Sparsity: 7.99640e-02     Total:   4.8099     mean:   1.0273\n",
      " Task losses:   3.8586     mean:   0.7717     Sharing: 1.19005e-03     Sparsity: 7.99562e-02     Total:   3.9398     mean:   0.8529\n",
      " Task losses:   4.1788     mean:   0.8358     Sharing: 9.33046e-04     Sparsity: 7.99489e-02     Total:   4.2597     mean:   0.9166\n",
      " Task losses:   4.3314     mean:   0.8663     Sharing: 9.53893e-04     Sparsity: 7.99417e-02     Total:   4.4123     mean:   0.9472\n",
      " Task losses:   3.9809     mean:   0.7962     Sharing: 1.09281e-03     Sparsity: 7.99363e-02     Total:   4.0619     mean:   0.8772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   4.2551     mean:   0.8510     Sharing: 1.18944e-03     Sparsity: 7.99309e-02     Total:   4.3362     mean:   0.9321\n",
      " Task losses:   4.3149     mean:   0.8630     Sharing: 1.07324e-03     Sparsity: 7.99253e-02     Total:   4.3959     mean:   0.9440\n",
      " Task losses:   3.8798     mean:   0.7760     Sharing: 6.36339e-04     Sparsity: 7.99201e-02     Total:   3.9603     mean:   0.8565\n",
      " Task losses:   4.7812     mean:   0.9562     Sharing: 8.93002e-04     Sparsity: 7.99145e-02     Total:   4.8620     mean:   1.0370\n",
      " Task losses:   4.9807     mean:   0.9961     Sharing: 9.89825e-04     Sparsity: 7.99091e-02     Total:   5.0616     mean:   1.0770\n",
      " Task losses:   5.0063     mean:   1.0013     Sharing: 9.42220e-04     Sparsity: 7.99034e-02     Total:   5.0872     mean:   1.0821\n",
      " Task losses:   5.0777     mean:   1.0155     Sharing: 7.72655e-04     Sparsity: 7.98974e-02     Total:   5.1583     mean:   1.0962\n",
      " Task losses:   5.1381     mean:   1.0276     Sharing: 5.67069e-04     Sparsity: 7.98913e-02     Total:   5.2186     mean:   1.1081\n",
      " Task losses:   4.9811     mean:   0.9962     Sharing: 9.02797e-04     Sparsity: 7.98851e-02     Total:   5.0619     mean:   1.0770\n",
      " Task losses:   4.4770     mean:   0.8954     Sharing: 1.09582e-03     Sparsity: 7.98783e-02     Total:   4.5580     mean:   0.9764\n",
      " Task losses:   4.5382     mean:   0.9076     Sharing: 8.55386e-04     Sparsity: 7.98719e-02     Total:   4.6190     mean:   0.9884\n",
      " Task losses:   3.9037     mean:   0.7807     Sharing: 6.46869e-04     Sparsity: 7.98649e-02     Total:   3.9842     mean:   0.8613\n",
      " Task losses:   3.5730     mean:   0.7146     Sharing: 8.83028e-04     Sparsity: 7.98571e-02     Total:   3.6538     mean:   0.7953\n",
      " Task losses:   3.4268     mean:   0.6854     Sharing: 8.84742e-04     Sparsity: 7.98497e-02     Total:   3.5076     mean:   0.7661\n",
      " Task losses:   3.3685     mean:   0.6737     Sharing: 6.86208e-04     Sparsity: 7.98420e-02     Total:   3.4490     mean:   0.7542\n",
      " Task losses:   3.7975     mean:   0.7595     Sharing: 8.50380e-04     Sparsity: 7.98342e-02     Total:   3.8781     mean:   0.8402\n",
      " Task losses:   4.3855     mean:   0.8771     Sharing: 1.08370e-03     Sparsity: 7.98253e-02     Total:   4.4664     mean:   0.9580\n",
      " Task losses:   4.1507     mean:   0.8301     Sharing: 9.62809e-04     Sparsity: 7.98149e-02     Total:   4.2315     mean:   0.9109\n",
      " Task losses:   4.3187     mean:   0.8637     Sharing: 8.38896e-04     Sparsity: 7.98034e-02     Total:   4.3994     mean:   0.9444\n",
      " Task losses:   4.7790     mean:   0.9558     Sharing: 1.01115e-03     Sparsity: 7.97919e-02     Total:   4.8598     mean:   1.0366\n",
      " Task losses:   5.4105     mean:   1.0821     Sharing: 1.09538e-03     Sparsity: 7.97811e-02     Total:   5.4914     mean:   1.1630\n",
      " Task losses:   4.8364     mean:   0.9673     Sharing: 9.90972e-04     Sparsity: 7.97709e-02     Total:   4.9171     mean:   1.0480\n",
      " Task losses:   3.6061     mean:   0.7212     Sharing: 8.24153e-04     Sparsity: 7.97612e-02     Total:   3.6867     mean:   0.8018\n",
      " Task losses:   3.3752     mean:   0.6750     Sharing: 7.95364e-04     Sparsity: 7.97517e-02     Total:   3.4558     mean:   0.7556\n",
      " Task losses:   4.0032     mean:   0.8006     Sharing: 6.93267e-04     Sparsity: 7.97420e-02     Total:   4.0837     mean:   0.8811\n",
      " Task losses:   3.6630     mean:   0.7326     Sharing: 5.42045e-04     Sparsity: 7.97320e-02     Total:   3.7433     mean:   0.8129\n",
      " Task losses:   4.0245     mean:   0.8049     Sharing: 6.12925e-04     Sparsity: 7.97223e-02     Total:   4.1048     mean:   0.8852\n",
      " Task losses:   3.9618     mean:   0.7924     Sharing: 6.44838e-04     Sparsity: 7.97129e-02     Total:   4.0422     mean:   0.8727\n",
      " Task losses:   3.5264     mean:   0.7053     Sharing: 8.16236e-04     Sparsity: 7.97038e-02     Total:   3.6069     mean:   0.7858\n",
      " Task losses:   4.1237     mean:   0.8247     Sharing: 7.56209e-04     Sparsity: 7.96960e-02     Total:   4.2042     mean:   0.9052\n",
      " Task losses:   3.5974     mean:   0.7195     Sharing: 3.94012e-04     Sparsity: 7.96882e-02     Total:   3.6775     mean:   0.7996\n",
      " Task losses:   5.9589     mean:   1.1918     Sharing: 9.26420e-04     Sparsity: 7.96803e-02     Total:   6.0396     mean:   1.2724\n",
      " Task losses:   5.2070     mean:   1.0414     Sharing: 1.09805e-03     Sparsity: 7.96747e-02     Total:   5.2877     mean:   1.1222\n",
      " Task losses:   7.2657     mean:   1.4531     Sharing: 8.05800e-04     Sparsity: 7.96688e-02     Total:   7.3462     mean:   1.5336\n",
      " Task losses:   6.7057     mean:   1.3411     Sharing: 7.61097e-04     Sparsity: 7.96628e-02     Total:   6.7861     mean:   1.4216\n",
      " Task losses:   4.9758     mean:   0.9952     Sharing: 1.07129e-03     Sparsity: 7.96563e-02     Total:   5.0565     mean:   1.0759\n",
      " Task losses:   4.3714     mean:   0.8743     Sharing: 9.31934e-04     Sparsity: 7.96493e-02     Total:   4.4520     mean:   0.9549\n",
      " Task losses:   4.7041     mean:   0.9408     Sharing: 1.06783e-03     Sparsity: 7.96423e-02     Total:   4.7848     mean:   1.0215\n",
      " Task losses:   5.8285     mean:   1.1657     Sharing: 1.10366e-03     Sparsity: 7.96374e-02     Total:   5.9092     mean:   1.2464\n",
      " Task losses:   4.3276     mean:   0.8655     Sharing: 7.10954e-04     Sparsity: 7.96298e-02     Total:   4.4079     mean:   0.9459\n",
      " Task losses:   3.5809     mean:   0.7162     Sharing: 6.30766e-04     Sparsity: 7.96221e-02     Total:   3.6612     mean:   0.7964\n",
      " Task losses:   2.7341     mean:   0.5468     Sharing: 8.46823e-04     Sparsity: 7.96144e-02     Total:   2.8146     mean:   0.6273\n",
      " Task losses:   2.9872     mean:   0.5974     Sharing: 7.65796e-04     Sparsity: 7.96066e-02     Total:   3.0676     mean:   0.6778\n",
      " Task losses:   2.6123     mean:   0.5225     Sharing: 8.01027e-04     Sparsity: 7.96007e-02     Total:   2.6927     mean:   0.6029\n",
      " Task losses:   3.4841     mean:   0.6968     Sharing: 5.95565e-04     Sparsity: 7.95948e-02     Total:   3.5643     mean:   0.7770\n",
      "[e]Policy training epoch:103 iteration:  22348 -  Total Loss: 3.5643     Task Loss: 3.4841  Policy Losses:  Sparsity: 0.0796      Sharing: 5.95565e-04 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 104 weight training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " current_iter_w: 108   stop_iter_w: 108   (are equal)\n",
      "[e]Weight training epoch:104 iteration:  22456 -  Total Loss: 4.5576     Task Loss: 4.5576  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104   | 0.00018 0.81779 0.78394 0.78760 0.74120|  3.6731  4.3750  4.2135 12.2616|   41.9|"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ceded1de9734dd494eab3e408761e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 104 policy training:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.4041     mean:   0.6808     Sharing: 6.50709e-04     Sparsity: 7.95896e-02     Total:   3.4844     mean:   0.7611\n",
      " Task losses:   5.6915     mean:   1.1383     Sharing: 7.12931e-04     Sparsity: 7.95842e-02     Total:   5.7718     mean:   1.2186\n",
      " Task losses:   7.5775     mean:   1.5155     Sharing: 7.40478e-04     Sparsity: 7.95749e-02     Total:   7.6578     mean:   1.5958\n",
      " Task losses:   6.6792     mean:   1.3358     Sharing: 7.20481e-04     Sparsity: 7.95771e-02     Total:   6.7595     mean:   1.4161\n",
      " Task losses:   7.1678     mean:   1.4336     Sharing: 6.73950e-04     Sparsity: 7.95788e-02     Total:   7.2480     mean:   1.5138\n",
      " Task losses:   6.8238     mean:   1.3648     Sharing: 6.92169e-04     Sparsity: 7.95800e-02     Total:   6.9041     mean:   1.4450\n",
      " Task losses:   3.9758     mean:   0.7952     Sharing: 5.95644e-04     Sparsity: 7.95836e-02     Total:   4.0560     mean:   0.8753\n",
      " Task losses:   3.5648     mean:   0.7130     Sharing: 5.19072e-04     Sparsity: 7.95861e-02     Total:   3.6449     mean:   0.7931\n",
      " Task losses:   4.7066     mean:   0.9413     Sharing: 8.99851e-04     Sparsity: 7.95877e-02     Total:   4.7871     mean:   1.0218\n",
      " Task losses:  10.1731     mean:   2.0346     Sharing: 6.75102e-04     Sparsity: 7.95877e-02     Total:  10.2533     mean:   2.1149\n",
      " Task losses:   4.5971     mean:   0.9194     Sharing: 8.48144e-04     Sparsity: 7.95874e-02     Total:   4.6775     mean:   0.9999\n",
      " Task losses:   4.0343     mean:   0.8069     Sharing: 9.20599e-04     Sparsity: 7.95866e-02     Total:   4.1148     mean:   0.8874\n",
      " Task losses:   2.9725     mean:   0.5945     Sharing: 9.93456e-04     Sparsity: 7.95850e-02     Total:   3.0531     mean:   0.6751\n",
      " Task losses:   3.1848     mean:   0.6370     Sharing: 9.61622e-04     Sparsity: 7.95825e-02     Total:   3.2654     mean:   0.7175\n",
      " Task losses:   3.2650     mean:   0.6530     Sharing: 9.68983e-04     Sparsity: 7.95796e-02     Total:   3.3455     mean:   0.7335\n",
      " Task losses:   2.7852     mean:   0.5570     Sharing: 1.03612e-03     Sparsity: 7.95755e-02     Total:   2.8658     mean:   0.6377\n",
      " Task losses:   4.9312     mean:   0.9862     Sharing: 9.42399e-04     Sparsity: 7.95707e-02     Total:   5.0117     mean:   1.0668\n",
      " Task losses:  18.8069     mean:   3.7614     Sharing: 8.55054e-04     Sparsity: 7.95668e-02     Total:  18.8873     mean:   3.8418\n",
      " Task losses:  17.4616     mean:   3.4923     Sharing: 1.23388e-03     Sparsity: 7.95752e-02     Total:  17.5424     mean:   3.5731\n",
      " Task losses:   3.2342     mean:   0.6468     Sharing: 1.48182e-03     Sparsity: 7.95792e-02     Total:   3.3152     mean:   0.7279\n",
      " Task losses:   3.0382     mean:   0.6076     Sharing: 1.71672e-03     Sparsity: 7.95823e-02     Total:   3.1195     mean:   0.6889\n",
      " Task losses:   3.1295     mean:   0.6259     Sharing: 1.75090e-03     Sparsity: 7.95891e-02     Total:   3.2109     mean:   0.7072\n",
      " Task losses:   2.9219     mean:   0.5844     Sharing: 1.63091e-03     Sparsity: 7.95943e-02     Total:   3.0031     mean:   0.6656\n",
      " Task losses:   4.2437     mean:   0.8487     Sharing: 1.15835e-03     Sparsity: 7.95983e-02     Total:   4.3245     mean:   0.9295\n",
      " Task losses:   4.5716     mean:   0.9143     Sharing: 1.19007e-03     Sparsity: 7.96011e-02     Total:   4.6524     mean:   0.9951\n",
      " Task losses:   3.6271     mean:   0.7254     Sharing: 1.23898e-03     Sparsity: 7.96031e-02     Total:   3.7079     mean:   0.8063\n",
      " Task losses:   5.3979     mean:   1.0796     Sharing: 1.41100e-03     Sparsity: 7.96041e-02     Total:   5.4789     mean:   1.1606\n",
      " Task losses:   6.6902     mean:   1.3380     Sharing: 1.25578e-03     Sparsity: 7.96027e-02     Total:   6.7711     mean:   1.4189\n",
      " Task losses:   7.4903     mean:   1.4981     Sharing: 8.45075e-04     Sparsity: 7.95984e-02     Total:   7.5708     mean:   1.5785\n",
      " Task losses:   5.4598     mean:   1.0920     Sharing: 1.24742e-03     Sparsity: 7.95924e-02     Total:   5.5406     mean:   1.1728\n",
      " Task losses:   5.0201     mean:   1.0040     Sharing: 1.43925e-03     Sparsity: 7.95888e-02     Total:   5.1012     mean:   1.0851\n",
      " Task losses:   4.1882     mean:   0.8376     Sharing: 1.44339e-03     Sparsity: 7.95848e-02     Total:   4.2692     mean:   0.9187\n",
      " Task losses:   4.1936     mean:   0.8387     Sharing: 1.22486e-03     Sparsity: 7.95804e-02     Total:   4.2744     mean:   0.9195\n",
      " Task losses:   3.8350     mean:   0.7670     Sharing: 1.01716e-03     Sparsity: 7.95757e-02     Total:   3.9156     mean:   0.8476\n",
      " Task losses:   4.1373     mean:   0.8275     Sharing: 9.40427e-04     Sparsity: 7.95711e-02     Total:   4.2178     mean:   0.9080\n",
      " Task losses:   3.8241     mean:   0.7648     Sharing: 1.53552e-03     Sparsity: 7.95714e-02     Total:   3.9052     mean:   0.8459\n",
      " Task losses:   5.0969     mean:   1.0194     Sharing: 1.69764e-03     Sparsity: 7.95707e-02     Total:   5.1781     mean:   1.1006\n",
      " Task losses:   8.3392     mean:   1.6678     Sharing: 1.41427e-03     Sparsity: 7.95700e-02     Total:   8.4202     mean:   1.7488\n",
      " Task losses:   5.0192     mean:   1.0038     Sharing: 9.52552e-04     Sparsity: 7.95672e-02     Total:   5.0998     mean:   1.0844\n",
      " Task losses:   2.7922     mean:   0.5584     Sharing: 1.01026e-03     Sparsity: 7.95641e-02     Total:   2.8728     mean:   0.6390\n",
      " Task losses:   3.8215     mean:   0.7643     Sharing: 1.40884e-03     Sparsity: 7.95601e-02     Total:   3.9024     mean:   0.8453\n",
      " Task losses:   5.2287     mean:   1.0457     Sharing: 1.28847e-03     Sparsity: 7.95548e-02     Total:   5.3095     mean:   1.1266\n",
      " Task losses:   5.6195     mean:   1.1239     Sharing: 1.19139e-03     Sparsity: 7.95497e-02     Total:   5.7002     mean:   1.2046\n",
      " Task losses:   3.8638     mean:   0.7728     Sharing: 9.63196e-04     Sparsity: 7.95467e-02     Total:   3.9443     mean:   0.8533\n",
      " Task losses:   3.1896     mean:   0.6379     Sharing: 1.17249e-03     Sparsity: 7.95435e-02     Total:   3.2703     mean:   0.7186\n",
      " Task losses:   3.9823     mean:   0.7965     Sharing: 1.25880e-03     Sparsity: 7.95392e-02     Total:   4.0631     mean:   0.8773\n",
      " Task losses:   3.8833     mean:   0.7767     Sharing: 1.07745e-03     Sparsity: 7.95350e-02     Total:   3.9639     mean:   0.8573\n",
      " Task losses:   4.3901     mean:   0.8780     Sharing: 7.22542e-04     Sparsity: 7.95304e-02     Total:   4.4704     mean:   0.9583\n",
      " Task losses:   3.4098     mean:   0.6820     Sharing: 8.48333e-04     Sparsity: 7.95256e-02     Total:   3.4902     mean:   0.7623\n",
      " Task losses:   3.7173     mean:   0.7435     Sharing: 1.21816e-03     Sparsity: 7.95202e-02     Total:   3.7981     mean:   0.8242\n",
      " Task losses:   4.3231     mean:   0.8646     Sharing: 1.06333e-03     Sparsity: 7.95145e-02     Total:   4.4037     mean:   0.9452\n",
      " Task losses:   6.1514     mean:   1.2303     Sharing: 9.48841e-04     Sparsity: 7.95097e-02     Total:   6.2319     mean:   1.3107\n",
      " Task losses:   4.5783     mean:   0.9157     Sharing: 9.95686e-04     Sparsity: 7.95048e-02     Total:   4.6588     mean:   0.9962\n",
      " Task losses:   4.2560     mean:   0.8512     Sharing: 1.04959e-03     Sparsity: 7.95001e-02     Total:   4.3365     mean:   0.9317\n",
      " Task losses:   6.6995     mean:   1.3399     Sharing: 8.09451e-04     Sparsity: 7.94962e-02     Total:   6.7798     mean:   1.4202\n",
      " Task losses:   5.9897     mean:   1.1979     Sharing: 7.58688e-04     Sparsity: 7.94924e-02     Total:   6.0699     mean:   1.2782\n",
      " Task losses:   3.1998     mean:   0.6400     Sharing: 8.69696e-04     Sparsity: 7.94925e-02     Total:   3.2801     mean:   0.7203\n",
      " Task losses:   3.6743     mean:   0.7349     Sharing: 8.02060e-04     Sparsity: 7.94922e-02     Total:   3.7546     mean:   0.8152\n",
      " Task losses:   3.6279     mean:   0.7256     Sharing: 8.26756e-04     Sparsity: 7.94919e-02     Total:   3.7083     mean:   0.8059\n",
      " Task losses:   3.0787     mean:   0.6157     Sharing: 8.34480e-04     Sparsity: 7.94910e-02     Total:   3.1590     mean:   0.6961\n",
      " Task losses:   3.4981     mean:   0.6996     Sharing: 7.27475e-04     Sparsity: 7.94895e-02     Total:   3.5783     mean:   0.7798\n",
      " Task losses:   3.6288     mean:   0.7258     Sharing: 5.69110e-04     Sparsity: 7.94875e-02     Total:   3.7089     mean:   0.8058\n",
      " Task losses:   3.7638     mean:   0.7528     Sharing: 9.11752e-04     Sparsity: 7.94849e-02     Total:   3.8442     mean:   0.8332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task losses:   3.7859     mean:   0.7572     Sharing: 9.36776e-04     Sparsity: 7.94821e-02     Total:   3.8663     mean:   0.8376\n",
      " Task losses:   3.6483     mean:   0.7297     Sharing: 8.26637e-04     Sparsity: 7.94818e-02     Total:   3.7286     mean:   0.8100\n",
      " Task losses:   2.9111     mean:   0.5822     Sharing: 9.38058e-04     Sparsity: 7.94808e-02     Total:   2.9916     mean:   0.6626\n",
      " Task losses:   3.6320     mean:   0.7264     Sharing: 9.92383e-04     Sparsity: 7.94809e-02     Total:   3.7125     mean:   0.8069\n",
      " Task losses:   4.3803     mean:   0.8761     Sharing: 1.02717e-03     Sparsity: 7.94803e-02     Total:   4.4608     mean:   0.9566\n",
      " Task losses:   4.0451     mean:   0.8090     Sharing: 9.20778e-04     Sparsity: 7.94794e-02     Total:   4.1255     mean:   0.8894\n",
      " Task losses:   4.1916     mean:   0.8383     Sharing: 9.38202e-04     Sparsity: 7.94781e-02     Total:   4.2720     mean:   0.9187\n",
      " Task losses:   3.9677     mean:   0.7935     Sharing: 1.06413e-03     Sparsity: 7.94781e-02     Total:   4.0483     mean:   0.8741\n",
      " Task losses:   3.6774     mean:   0.7355     Sharing: 9.66713e-04     Sparsity: 7.94773e-02     Total:   3.7578     mean:   0.8159\n",
      " Task losses:   3.9124     mean:   0.7825     Sharing: 8.07106e-04     Sparsity: 7.94764e-02     Total:   3.9927     mean:   0.8628\n",
      " Task losses:   4.5671     mean:   0.9134     Sharing: 9.64900e-04     Sparsity: 7.94759e-02     Total:   4.6475     mean:   0.9939\n",
      " Task losses:   3.6162     mean:   0.7232     Sharing: 8.90528e-04     Sparsity: 7.94747e-02     Total:   3.6965     mean:   0.8036\n",
      " Task losses:   2.8215     mean:   0.5643     Sharing: 9.00333e-04     Sparsity: 7.94732e-02     Total:   2.9019     mean:   0.6447\n",
      " Task losses:   2.5910     mean:   0.5182     Sharing: 1.04051e-03     Sparsity: 7.94711e-02     Total:   2.6715     mean:   0.5987\n",
      " Task losses:   2.7622     mean:   0.5524     Sharing: 8.35001e-04     Sparsity: 7.94692e-02     Total:   2.8426     mean:   0.6328\n",
      " Task losses:   2.9443     mean:   0.5889     Sharing: 6.21393e-04     Sparsity: 7.94666e-02     Total:   3.0244     mean:   0.6689\n",
      " Task losses:   3.2773     mean:   0.6555     Sharing: 8.33953e-04     Sparsity: 7.94638e-02     Total:   3.3576     mean:   0.7358\n",
      " Task losses:   3.3037     mean:   0.6607     Sharing: 9.79558e-04     Sparsity: 7.94603e-02     Total:   3.3841     mean:   0.7412\n",
      " Task losses:   3.2543     mean:   0.6509     Sharing: 8.11130e-04     Sparsity: 7.94566e-02     Total:   3.3346     mean:   0.7311\n",
      " Task losses:   3.8505     mean:   0.7701     Sharing: 8.02423e-04     Sparsity: 7.94526e-02     Total:   3.9308     mean:   0.8504\n",
      " Task losses:   3.3244     mean:   0.6649     Sharing: 7.64156e-04     Sparsity: 7.94483e-02     Total:   3.4047     mean:   0.7451\n",
      " Task losses:   3.2148     mean:   0.6430     Sharing: 5.88253e-04     Sparsity: 7.94456e-02     Total:   3.2949     mean:   0.7230\n",
      " Task losses:   3.1155     mean:   0.6231     Sharing: 8.22013e-04     Sparsity: 7.94424e-02     Total:   3.1957     mean:   0.7034\n",
      " Task losses:   3.3091     mean:   0.6618     Sharing: 8.20274e-04     Sparsity: 7.94403e-02     Total:   3.3893     mean:   0.7421\n",
      " Task losses:   3.7985     mean:   0.7597     Sharing: 9.45265e-04     Sparsity: 7.94369e-02     Total:   3.8788     mean:   0.8401\n",
      " Task losses:   3.4477     mean:   0.6895     Sharing: 9.85369e-04     Sparsity: 7.94326e-02     Total:   3.5281     mean:   0.7700\n",
      " Task losses:   3.0626     mean:   0.6125     Sharing: 8.88382e-04     Sparsity: 7.94266e-02     Total:   3.1429     mean:   0.6928\n",
      " Task losses:   3.2688     mean:   0.6538     Sharing: 8.55391e-04     Sparsity: 7.94205e-02     Total:   3.3491     mean:   0.7340\n",
      " Task losses:   3.1600     mean:   0.6320     Sharing: 9.62630e-04     Sparsity: 7.94146e-02     Total:   3.2404     mean:   0.7124\n",
      " Task losses:   3.1426     mean:   0.6285     Sharing: 8.09044e-04     Sparsity: 7.94089e-02     Total:   3.2228     mean:   0.7087\n",
      " Task losses:   3.5772     mean:   0.7154     Sharing: 9.44684e-04     Sparsity: 7.94033e-02     Total:   3.6576     mean:   0.7958\n",
      " Task losses:   5.7590     mean:   1.1518     Sharing: 1.04909e-03     Sparsity: 7.93982e-02     Total:   5.8395     mean:   1.2323\n",
      " Task losses:   5.5578     mean:   1.1116     Sharing: 9.19486e-04     Sparsity: 7.93928e-02     Total:   5.6382     mean:   1.1919\n",
      " Task losses:   6.4797     mean:   1.2959     Sharing: 6.75281e-04     Sparsity: 7.93860e-02     Total:   6.5597     mean:   1.3760\n",
      " Task losses:   5.1594     mean:   1.0319     Sharing: 7.98638e-04     Sparsity: 7.93816e-02     Total:   5.2396     mean:   1.1121\n",
      " Task losses:   3.5423     mean:   0.7085     Sharing: 9.65387e-04     Sparsity: 7.93812e-02     Total:   3.6227     mean:   0.7888\n",
      " Task losses:   3.5090     mean:   0.7018     Sharing: 7.71383e-04     Sparsity: 7.93807e-02     Total:   3.5891     mean:   0.7819\n",
      " Task losses:   4.1499     mean:   0.8300     Sharing: 6.80854e-04     Sparsity: 7.93796e-02     Total:   4.2300     mean:   0.9100\n",
      " Task losses:   4.7331     mean:   0.9466     Sharing: 8.73129e-04     Sparsity: 7.93776e-02     Total:   4.8133     mean:   1.0269\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11250/4269474150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mprint_dbg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" num_train_layers  : {num_train_layers}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 environ.optimize(opt['lambdas'], \n\u001b[0m\u001b[1;32m    164\u001b[0m                                  \u001b[0mis_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                                  \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kusanagi/AdaSparseChem/dev/sparsechem_env_dev.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, lambdas, is_policy, flag, num_train_layers, hard_sampling, verbose)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m## Backward pass - alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'update_alpha'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kusanagi/AdaSparseChem/dev/sparsechem_env_dev.py\u001b[0m in \u001b[0;36mbackward_policy\u001b[0;34m(self, num_train_layers, verbose)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_sharing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hamming_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0msharing_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sharing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kusanagi/AdaSparseChem/dev/sparsechem_env_dev.py\u001b[0m in \u001b[0;36mget_hamming_loss\u001b[0;34m(self, num_policy_layers, verbose)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mprint_underline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" between {task_i_attr} and {task_j_attr} : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 print_dbg(f\" {task_i_attr:12s}: {logits_i[:,0]}  \"\n\u001b[0m\u001b[1;32m    334\u001b[0m                           \u001b[0;34mf\"\\n {task_j_attr:12s}: {logits_j[:,0]}  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                           \u001b[0;34mf\"\\n abs diff    : {(torch.abs(logits_i[:, 0] - logits_j[:, 0]))} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ipow__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                         \u001b[0mvalue_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'{{:.{}f}}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-gpu/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n\u001b[1;32m    645\u001b[0m                           \u001b[0;34m'Passing a tensor of different shape won\\'t change the number of '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "while curr_epoch < train_total_epochs:\n",
    "    curr_epoch+=1\n",
    "    t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the network weights\n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_w':\n",
    "        current_iter_w  = 0 \n",
    "#         stop_iter_w =   opt['train']['weight_iter_alternate']\n",
    "\n",
    "        with trange(+1, stop_iter_w+1 , initial = current_iter_w, total = stop_iter_w, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} weight training\") as t_weights :\n",
    "            \n",
    "            for current_iter_w in t_weights:    \n",
    "                current_iter += 1\n",
    "\n",
    "                start_time = time.time()\n",
    "                environ.train()\n",
    "                \n",
    "                batch = next(train_loader)\n",
    "#                 batch = next(train1_loader)\n",
    "                \n",
    "                environ.set_inputs(batch, train1_loader.dataset.input_size)\n",
    "\n",
    "                ##----------------------------------------------------------------------\n",
    "                ## Set number of layers to train based on cirriculum_speed \n",
    "                ## and p_epoch (number of epochs of policy training)\n",
    "                ## When curriculum_speed == 3, a num_train_layers is incremented \n",
    "                ## after completion of every 3 policy training epochs\n",
    "                ##----------------------------------------------------------------------\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "\n",
    "                t_weights.set_postfix({'iter': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                       'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "#                 if should(current_iter, opt['train']['print_freq']):\n",
    "#                     environ.print_loss(current_iter, start_time, title = f\"[c]Weight training epoch:{curr_epoch} iteration:\", verbose = True)\n",
    "\n",
    "        #-------------------------------------------------------\n",
    "        # validation process \n",
    "        #------------------------------------------------------- \n",
    "#       if should(current_iter_w, opt['train']['weight_iter_alternate']): \n",
    "        if (current_iter_w >= stop_iter_w):\n",
    "#             print(f\" current_iter_w: {current_iter_w}   stop_iter_w: {stop_iter_w}   (are equal)\")        \n",
    "            environ.print_loss(current_iter, start_time, title = f\"[e]Weight training epoch:{curr_epoch} iteration:\", verbose = True)\n",
    "            environ.eval()\n",
    "\n",
    "            val_metrics = environ.evaluate(val_loader, \n",
    "                                           opt['tasks'], \n",
    "                                           is_policy=opt['policy'],\n",
    "                                           num_train_layers=num_train_layers, \n",
    "                                           hard_sampling=opt['train']['hard_sampling'],\n",
    "                                           eval_iter = eval_iter,\n",
    "                                           progress = True, \n",
    "                                           leave = False, \n",
    "                                           verbose = False)  \n",
    "    \n",
    "            environ.print_metrics(current_iter, start_time, val_metrics, title = f\"[v]Weight training epoch:{curr_epoch} iteration:\", verbose = False)\n",
    "            print_metrics_cr(curr_epoch, time.time() - t0, None, environ.val_metrics , num_prints)      \n",
    "\n",
    "            num_prints += 1\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # Take check point\n",
    "            environ.save_checkpoint('latest', current_iter)\n",
    "            #-----------------------------------------------------------------------------------------------------------------------\n",
    "            #\n",
    "            #            #----------------------------------------------------------------------------------------------\n",
    "            #            # if number of iterations completed after the warm up phase is greater than the number of \n",
    "            #            # (weight/policy alternations) x (cirriculum speed) x (number of layers to be policy trained)\n",
    "            #            #\n",
    "            #            # check metrics for improvement, and issue a checkpoint if necessary\n",
    "            #            #----------------------------------------------------------------------------------------------\n",
    "            # \n",
    "            #             if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "            #                     (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "            #                 new_value = 0\n",
    "            #                 print(f\"  {current_iter - opt['train']['warm_up_iters']} IS GREATER THAN \"\n",
    "            #                        f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} -- \"\n",
    "            #                        f\"  evaluate progress and make checkpoint if necessary.\" )            \n",
    "            # \n",
    "            #                 ## compare validation metrics against reference metrics.\n",
    "            #                 \n",
    "            #                 for k in refer_metrics.keys():\n",
    "            #                     if k in val_metrics.keys():\n",
    "            #                         for kk in val_metrics[k].keys():\n",
    "            #                             if not kk in refer_metrics[k].keys():\n",
    "            #                                 continue\n",
    "            #                             if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or (\n",
    "            #                                     k == 'depth' and not kk.startswith('sigma')) or (kk == 'err'):\n",
    "            #                                 value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "            #                             else:\n",
    "            #                                 value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "            #                             value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "            #                             new_value += value\n",
    "            # \n",
    "            #                 print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "            # \n",
    "            #                 ## if results have improved, save these results and issue a checkpoint\n",
    "            # \n",
    "            #                 if (new_value > best_value):\n",
    "            #                     print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)\n",
    "            #                     best_value = new_value\n",
    "            #                     best_metrics = val_metrics\n",
    "            #                     best_iter = current_iter\n",
    "            #                     environ.save_checkpoint('best', current_iter)\n",
    "            #                     print('New      best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)                         \n",
    "            #                     print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "            #\n",
    "            #-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            environ.train()\n",
    "            #-------------------------------------------------------\n",
    "            # END validation process\n",
    "            #-------------------------------------------------------       \n",
    "            flag = 'update_alpha'\n",
    "            environ.fix_weights()\n",
    "            environ.free_alpha()\n",
    "    #-------------------------------------------------------\n",
    "    # end weight training iteration\n",
    "    #-------------------------------------------------------               \n",
    "\n",
    "    # When we want to separate the two loops\n",
    "    # while curr_epoch <= train_total_epochs:\n",
    "    #     curr_epoch+=1\n",
    "    #     t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the  policy \n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_alpha':\n",
    "        current_iter_a = 0\n",
    "\n",
    "        with trange( +1, stop_iter_a+1 , initial = 0, total = stop_iter_a, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} policy training\") as t_policy :\n",
    "            for current_iter_a in t_policy:    \n",
    "                current_iter += 1\n",
    "\n",
    "                batch = next(train_loader)\n",
    "#                 batch = next(train2_loader)\n",
    "                \n",
    "                environ.set_inputs(batch, train2_loader.dataset.input_size)\n",
    "\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = (p_epoch // opt['curriculum_speed']) + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "                print_dbg(f\" num_train_layers  : {num_train_layers}\", verbose = False)\n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "                \n",
    "                t_policy.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                      'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = f\"[c]Policy training epoch:{curr_epoch} iteration:\", verbose=False)\n",
    "#                     environ.visual_policy(current_iter)\n",
    "\n",
    "        if( current_iter_a >= stop_iter_a):            \n",
    "#             print(f\" current_iter_a: {current_iter_a}   stop_iter_a: {stop_iter_a}   (are equal)\")\n",
    "            environ.print_loss(current_iter, start_time, title = f\"[e]Policy training epoch:{curr_epoch} iteration:\", verbose=True)\n",
    "            flag = 'update_w'\n",
    "            p_epoch += 1\n",
    "            environ.fix_alpha()\n",
    "            environ.free_weights(opt['fix_BN'])\n",
    "            if should(p_epoch, opt['train']['decay_temp_freq']):\n",
    "                print(f\"[e]Policy training epoch:{p_epoch} decay gumbel temp\" )\n",
    "                environ.decay_temperature()\n",
    "\n",
    "            # print the distribution\n",
    "            print_dbg(np.concatenate(environ.get_policy_prob(), axis=-1), verbose = False)\n",
    "            \n",
    "\n",
    "            print_dbg(f\"** p_epoch incremented: {p_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62de9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[e]Policy training epoch:103 iteration:  11116 -  Total Loss: 11.3055     Task Loss: 11.2034  Policy Losses:  Sparsity: 0.1021      Sharing: 4.42266e-06 \n",
    "[c]Policy training epoch:103 iteration:  11016 -  Total Loss: 11.2994     Task Loss: 11.1965  Policy Losses:  Sparsity: 0.1029      Sharing: 5.42402e-06 \n",
    "[c]Policy training epoch:104 iteration:  11124 -  Total Loss: 11.2998     Task Loss: 11.1978  Policy Losses:  Sparsity: 0.1020      Sharing: 6.46710e-06                            \n",
    "[e]Policy training epoch:104 iteration:  11224 -  Total Loss: 11.3005     Task Loss: 11.1994  Policy Losses:  Sparsity: 0.1012      Sharing: 7.33137e-06        \n",
    "[c]Policy training epoch:105 iteration:  11232 -  Total Loss: 11.3018     Task Loss: 11.2007  Policy Losses:  Sparsity: 0.1011      Sharing: 8.82745e-06         \n",
    "[e]Policy training epoch:105 iteration:  11332 -  Total Loss: 11.2967     Task Loss: 11.1964  Policy Losses:  Sparsity: 0.1003      Sharing: 3.92199e-06       \n",
    "[c]Policy training epoch:106 iteration:  11340 -  Total Loss: 11.3006     Task Loss: 11.2004  Policy Losses:  Sparsity: 0.1002      Sharing: 5.03659e-06     \n",
    "[e]Policy training epoch:106 iteration:  11440 -  Total Loss: 11.2970     Task Loss: 11.1976  Policy Losses:  Sparsity: 0.0994      Sharing: 6.71148e-06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "[c]Policy training epoch:61 iteration:  6588 -  Total Loss: 9.6820     Task Loss: 9.5780  Policy Losses:  Sparsity: 0.1039      Sharing: 1.73569e-05 \n",
    "[e]Policy training epoch:61 iteration:  6688 -  Total Loss: 9.6665     Task Loss: 9.5635  Policy Losses:  Sparsity: 0.1030      Sharing: 3.66569e-06 \n",
    "[c]Policy training epoch:62 iteration:  6696 -  Total Loss: 9.6642     Task Loss: 9.5612  Policy Losses:  Sparsity: 0.1030      Sharing: 7.31945e-06 \n",
    "[e]Policy training epoch:62 iteration:  6796 -  Total Loss: 9.6599     Task Loss: 9.5578  Policy Losses:  Sparsity: 0.1021      Sharing: 1.82986e-06 \n",
    "[e]Policy training epoch:63 iteration:  6904 -  Total Loss: 9.6682     Task Loss: 9.5670  Policy Losses:  Sparsity: 0.1012      Sharing: 9.05991e-06 \n",
    "[c]Policy training epoch:64 iteration:  6912 -  Total Loss: 9.6548     Task Loss: 9.5537  Policy Losses:  Sparsity: 0.1011      Sharing: 9.95398e-06 \n",
    "[e]Policy training epoch:64 iteration:  7012 -  Total Loss: 9.6678     Task Loss: 9.5675  Policy Losses:  Sparsity: 0.1003      Sharing: 5.03063e-06\n",
    "[c]Policy training epoch:65 iteration:  7020 -  Total Loss: 9.6578     Task Loss: 9.5576  Policy Losses:  Sparsity: 0.1002      Sharing: 2.67029e-06\n",
    "[e]Policy training epoch:65 iteration:  7120 -  Total Loss: 9.6335     Task Loss: 9.5341  Policy Losses:  Sparsity: 0.0994      Sharing: 5.96642e-06 \n",
    "[c]Policy training epoch:66 iteration:  7128 -  Total Loss: 9.6501     Task Loss: 9.5507  Policy Losses:  Sparsity: 0.0993      Sharing: 4.99487e-06  \n",
    "[e]Policy training epoch:66 iteration:  7228 -  Total Loss: 9.6556     Task Loss: 9.5571  Policy Losses:  Sparsity: 0.0985      Sharing: 3.95775e-06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b072d3cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:04:45.774666Z",
     "start_time": "2022-01-26T19:04:44.968618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46973792 0.530262   0.4775429  0.5224572  0.4868897  0.5131103 ]\n",
      " [0.45025694 0.549743   0.45825934 0.54174066 0.47107342 0.5289266 ]\n",
      " [0.4443086  0.5556915  0.45530966 0.5446904  0.45748708 0.5425128 ]\n",
      " [0.4138397  0.58616036 0.43196857 0.5680315  0.42434993 0.5756501 ]\n",
      " [0.4140113  0.5859887  0.43017322 0.5698268  0.4313186  0.56868154]\n",
      " [0.42114905 0.57885087 0.4333356  0.5666644  0.4339512  0.56604874]]\n",
      "20\n",
      "True\n",
      "60 3 20 21\n"
     ]
    }
   ],
   "source": [
    "print_dbg(np.concatenate(environ.get_policy_prob(), axis=-1), verbose = True)\n",
    "print(num_train_layers)\n",
    "print(opt['is_curriculum'])\n",
    "print(p_epoch, opt['curriculum_speed'], (p_epoch // opt['curriculum_speed']), (p_epoch // opt['curriculum_speed'])  + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2d8ab",
   "metadata": {},
   "source": [
    "### Post Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "898a1bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:49.364883Z",
     "start_time": "2022-01-27T00:58:49.342931Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['Lambda_sharing'] = 0.5\n",
    "# opt['train']['Lambda_sharing'] = 0.5\n",
    "\n",
    "environ.opt['train']['policy_lr'] = 0.001\n",
    "opt['train']['policy_lr'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4ae94246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:50.458223Z",
     "start_time": "2022-01-27T00:58:50.430889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "0.5\n",
      "0.5\n",
      "0.05\n",
      "0.05\n",
      "0.001\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(opt['diff_sparsity_weights'])\n",
    "print(opt['is_sharing'])\n",
    "print(opt['diff_sparsity_weights'] and not opt['is_sharing'])\n",
    "print(environ.opt['train']['Lambda_sharing'])\n",
    "print(opt['train']['Lambda_sharing'])\n",
    "print(environ.opt['train']['Lambda_sparsity'])\n",
    "print(opt['train']['Lambda_sparsity'])\n",
    "print(environ.opt['train']['policy_lr'])\n",
    "print(opt['train']['policy_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7463f230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:00:21.104693Z",
     "start_time": "2022-01-27T01:00:20.765213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.3342, 0.4445],\n",
      "        [0.3510, 0.5543],\n",
      "        [0.3844, 0.6266],\n",
      "        [0.4167, 0.8072],\n",
      "        [0.4499, 0.8514],\n",
      "        [0.4822, 0.8833]], device='cuda:0'), Parameter containing:\n",
      "tensor([[0.3342, 0.4169],\n",
      "        [0.3510, 0.5310],\n",
      "        [0.3845, 0.6013],\n",
      "        [0.4167, 0.7355],\n",
      "        [0.4499, 0.7866],\n",
      "        [0.4822, 0.8466]], device='cuda:0'), Parameter containing:\n",
      "tensor([[0.3342, 0.3876],\n",
      "        [0.3510, 0.4803],\n",
      "        [0.3845, 0.5804],\n",
      "        [0.4167, 0.7637],\n",
      "        [0.4499, 0.7782],\n",
      "        [0.4822, 0.8540]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "12cbdf20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T01:00:49.058303Z",
     "start_time": "2022-01-27T01:00:49.034045Z"
    }
   },
   "outputs": [],
   "source": [
    "environ.optimizers['alphas'] = optim.Adam(arch_parameters, lr=environ.opt['train']['policy_lr'], weight_decay=5*1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82760e2",
   "metadata": {},
   "source": [
    "#### Sample Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3bb4c06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T22:11:20.860678Z",
     "start_time": "2022-01-26T22:11:20.824740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33060804 0.45180422]\n",
      " [0.3532204  0.5528529 ]\n",
      " [0.38884717 0.6125409 ]\n",
      " [0.4204008  0.76851547]\n",
      " [0.45199737 0.7994047 ]\n",
      " [0.48402908 0.8020872 ]] \n",
      "\n",
      "[[0.33064184 0.42053092]\n",
      " [0.3532089  0.52056104]\n",
      " [0.3888512  0.5680909 ]\n",
      " [0.42039296 0.694217  ]\n",
      " [0.4519742  0.73311865]\n",
      " [0.48401102 0.7522658 ]] \n",
      "\n",
      "[[0.33058274 0.38303584]\n",
      " [0.3532048  0.46904057]\n",
      " [0.38887233 0.5593353 ]\n",
      " [0.42041987 0.7253615 ]\n",
      " [0.45201355 0.7284871 ]\n",
      " [0.48402616 0.7497743 ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = environ.get_policy_logits()\n",
    "for i in logs:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2b3b6b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T22:11:18.381641Z",
     "start_time": "2022-01-26T22:11:17.858935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0]], device='cuda:0') \n",
      "\n",
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0') \n",
      "\n",
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pols = environ.sample_policy(hard_sampling = False)\n",
    "for i in pols:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4548c8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:11:01.891469Z",
     "start_time": "2022-01-27T00:11:01.863178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12809551 0.8719045 ]\n",
      " [0.4568862  0.5431138 ]\n",
      " [0.4389936  0.5610064 ]\n",
      " [0.4739472  0.5260528 ]\n",
      " [0.44630152 0.5536985 ]\n",
      " [0.37613583 0.6238642 ]] \n",
      "\n",
      "[[0.2449313  0.7550687 ]\n",
      " [0.41031924 0.58968073]\n",
      " [0.3977335  0.60226655]\n",
      " [0.8489379  0.15106209]\n",
      " [0.3237851  0.6762149 ]\n",
      " [0.64922446 0.35077554]] \n",
      "\n",
      "[[0.41088596 0.589114  ]\n",
      " [0.32043332 0.6795667 ]\n",
      " [0.11188911 0.8881109 ]\n",
      " [0.27037877 0.72962123]\n",
      " [0.8158148  0.18418525]\n",
      " [0.5808661  0.41913387]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pols  = (environ.get_current_policy())\n",
    "for i in pols:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f34e3fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:10:53.643251Z",
     "start_time": "2022-01-27T00:10:52.817789Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MTL3_Dev test_sample_policy() START -  hard_sampling: False\n",
      "\n",
      " task1 logits\n",
      "--------------\n",
      " [[0.33060804 0.45180422]\n",
      " [0.3532204  0.5528529 ]\n",
      " [0.38884717 0.6125409 ]\n",
      " [0.4204008  0.76851547]\n",
      " [0.45199737 0.7994047 ]\n",
      " [0.48402908 0.8020872 ]]\n",
      "\n",
      "  task1 softmax:\n",
      "-----------------\n",
      " [[0.46973792 0.530262  ]\n",
      " [0.45025694 0.549743  ]\n",
      " [0.4443086  0.5556915 ]\n",
      " [0.4138397  0.58616036]\n",
      " [0.4140113  0.5859887 ]\n",
      " [0.42114905 0.57885087]]\n",
      "\n",
      " task 1 sampled policy :\n",
      "-------------------------\n",
      " tensor([[0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0]], device='cuda:0')\n",
      "\n",
      "\n",
      " task2 logits\n",
      "--------------\n",
      " [[0.33064184 0.42053092]\n",
      " [0.3532089  0.52056104]\n",
      " [0.3888512  0.5680909 ]\n",
      " [0.42039296 0.694217  ]\n",
      " [0.4519742  0.73311865]\n",
      " [0.48401102 0.7522658 ]]\n",
      "\n",
      "  task2 softmax:\n",
      "-----------------\n",
      " [[0.4775429  0.5224572 ]\n",
      " [0.45825934 0.54174066]\n",
      " [0.45530966 0.5446904 ]\n",
      " [0.43196857 0.5680315 ]\n",
      " [0.43017322 0.5698268 ]\n",
      " [0.4333356  0.5666644 ]]\n",
      "\n",
      " task 2 sampled policy :\n",
      "-------------------------\n",
      " tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [1, 0],\n",
      "        [1, 0]], device='cuda:0')\n",
      "\n",
      "\n",
      " task3 logits\n",
      "--------------\n",
      " [[0.33058274 0.38303584]\n",
      " [0.3532048  0.46904057]\n",
      " [0.38887233 0.5593353 ]\n",
      " [0.42041987 0.7253615 ]\n",
      " [0.45201355 0.7284871 ]\n",
      " [0.48402616 0.7497743 ]]\n",
      "\n",
      "  task3 softmax:\n",
      "-----------------\n",
      " [[0.4868897  0.5131103 ]\n",
      " [0.47107342 0.5289266 ]\n",
      " [0.45748708 0.5425128 ]\n",
      " [0.42434993 0.5756501 ]\n",
      " [0.4313186  0.56868154]\n",
      " [0.4339512  0.56604874]]\n",
      "\n",
      " task 3 sampled policy :\n",
      "-------------------------\n",
      " tensor([[1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [1, 0]], device='cuda:0')\n",
      "\n",
      " MTL3_Dev test_sample_policy() END -  hard_sampling: False\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "pols = environ.networks['mtl-net'].test_sample_policy(hard_sampling = False, verbose = True)\n",
    "print(type(pols))\n",
    "# for i in pols:\n",
    "#     print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c758f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0fd9373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T21:58:35.372566Z",
     "start_time": "2022-01-26T21:58:35.019875Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MTL3_Dev test_sample_policy() START -  hard_sampling: True\n",
      "\n",
      " task1 logits\n",
      "--------------\n",
      " [[0.33060804 0.45180422]\n",
      " [0.3532204  0.5528529 ]\n",
      " [0.38884717 0.6125409 ]\n",
      " [0.4204008  0.76851547]\n",
      " [0.45199737 0.7994047 ]\n",
      " [0.48402908 0.8020872 ]]\n",
      "\n",
      " task1 argmax /hard_sampled policy\n",
      "-----------------------------------\n",
      " tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0')\n",
      "\n",
      "\n",
      " task2 logits\n",
      "--------------\n",
      " [[0.33064184 0.42053092]\n",
      " [0.3532089  0.52056104]\n",
      " [0.3888512  0.5680909 ]\n",
      " [0.42039296 0.694217  ]\n",
      " [0.4519742  0.73311865]\n",
      " [0.48401102 0.7522658 ]]\n",
      "\n",
      " task2 argmax /hard_sampled policy\n",
      "-----------------------------------\n",
      " tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0')\n",
      "\n",
      "\n",
      " task3 logits\n",
      "--------------\n",
      " [[0.33058274 0.38303584]\n",
      " [0.3532048  0.46904057]\n",
      " [0.38887233 0.5593353 ]\n",
      " [0.42041987 0.7253615 ]\n",
      " [0.45201355 0.7284871 ]\n",
      " [0.48402616 0.7497743 ]]\n",
      "\n",
      " task3 argmax /hard_sampled policy\n",
      "-----------------------------------\n",
      " tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0')\n",
      "\n",
      " MTL3_Dev test_sample_policy() END -  hard_sampling: True\n",
      "<class 'list'>\n",
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0') \n",
      "\n",
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0') \n",
      "\n",
      "tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pols = environ.networks['mtl-net'].test_sample_policy(hard_sampling = True, verbose = True)\n",
    "print(type(pols))\n",
    "for i in pols:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa072f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7d667cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T21:37:48.151808Z",
     "start_time": "2022-01-26T21:37:48.100335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2)\n",
      "[[0.47754285 0.52245715]\n",
      " [0.45825934 0.54174066]\n",
      " [0.45530966 0.54469034]\n",
      " [0.43196854 0.56803146]\n",
      " [0.43017322 0.56982678]\n",
      " [0.43333559 0.56666441]]\n",
      "[0.47754285 0.52245715]\n",
      "0.9999999999999998\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'p' must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11250/3305741449.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'p' must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "logs = np.array(\n",
    "[[0.33064184, 0.42053092],\n",
    " [0.3532089 , 0.52056104],\n",
    " [0.3888512 , 0.5680909 ],\n",
    " [0.42039296, 0.694217  ],\n",
    " [0.4519742 , 0.73311865],\n",
    " [0.48401102, 0.7522658 ]],\n",
    ")\n",
    "\n",
    "smax = scipy.special.softmax(logs, axis =1)\n",
    "# smax = np.array( \n",
    "# [[0.46973792, 0.530262  ],\n",
    "#  [0.45025694, 0.549743  ],\n",
    "#  [0.4443086 , 0.5556915 ],\n",
    "#  [0.4138397 , 0.58616036],\n",
    "#  [0.4140113 , 0.5859887 ],\n",
    "#  [0.42114905, 0.57885087]])\n",
    "\n",
    "print(smax.shape)\n",
    "print(smax)\n",
    "print(smax[0])\n",
    "print(smax[0].sum())\n",
    "print(np.random.choice((1,0), p =smax[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02afce52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T21:28:32.809103Z",
     "start_time": "2022-01-26T21:28:32.472814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2]) tensor([[0.3306, 0.4518],\n",
      "        [0.3532, 0.5529],\n",
      "        [0.3888, 0.6125],\n",
      "        [0.4204, 0.7685],\n",
      "        [0.4520, 0.7994],\n",
      "        [0.4840, 0.8021]])\n"
     ]
    }
   ],
   "source": [
    "task_key = 'task1_logits'\n",
    "logits = getattr(environ.networks['mtl-net'], task_key).cpu()\n",
    "print(logits.shape,logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca31ca",
   "metadata": {},
   "source": [
    "#### Sparsity Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0d3d4230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:22:20.634818Z",
     "start_time": "2022-01-27T00:22:20.444566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.networks['mtl-net'].backbone.layer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba9365de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:30:31.940280Z",
     "start_time": "2022-01-26T19:30:31.910058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0])\n",
      "tensor([0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "num_blocks = 6\n",
    "num_policy_layers = 6\n",
    "gt =  torch.ones((num_blocks)).long()\n",
    "gt0 =  torch.zeros((num_blocks)).long()\n",
    "print(gt)\n",
    "print(gt0)\n",
    "\n",
    "loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a369754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:42:31.300891Z",
     "start_time": "2022-01-26T19:42:31.257774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond 2\n",
      "Compute CrossEntropyLoss between \n",
      " Logits   : \n",
      "tensor([[0.3306, 0.4518],\n",
      "        [0.3532, 0.5529],\n",
      "        [0.3888, 0.6125],\n",
      "        [0.4204, 0.7685],\n",
      "        [0.4520, 0.7994],\n",
      "        [0.4840, 0.8021]]) \n",
      " and gt: \n",
      "tensor([1, 1, 1, 1, 1, 1]) \n",
      "\n",
      "task1_logits sparsity error:  0.5725929141044617\n",
      "\n",
      " cond 2\n",
      "Compute CrossEntropyLoss between Logits      : tensor([[0.4840, 0.8021]])  and gt: 1 \n",
      "task1_logits sparsity error:  0.5467103123664856 \n",
      "\n",
      "Compute CrossEntropyLoss between Logits      : tensor([[0.4840, 0.8021]])  and gt: 0 \n",
      "task1_logits sparsity error:  0.864768385887146 \n",
      "\n",
      "\n",
      " cond 3\n",
      "Compute CrossEntropyLoss between Logits   : tensor([[0.3306, 0.4518]])  and gt: tensor([1]) \n",
      "task1_logits sparsity error:  0.634384036064148 \n",
      "\n",
      "Compute CrossEntropyLoss between Logits   : tensor([[0.3306, 0.4518]])  and gt: tensor([0]) \n",
      "task1_logits sparsity error:  0.7555801868438721 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if environ.opt['diff_sparsity_weights'] and not environ.opt['is_sharing']:\n",
    "    print(' cond 1')\n",
    "    ## Assign higher weights to higher layers \n",
    "    loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "    print(f\"{task_key} sparsity error:  {2 * (loss_weights[-num_blocks:] * environ.cross_entropy2(logits[-num_blocks:], gt)).mean()})\")\n",
    "    print_dbg(f\" loss_weights :  {loss_weights}\", verbose = True)\n",
    "    print_dbg(f\" cross_entropy:  {environ.cross_entropy2(logits[-num_blocks:], gt)}  \", verbose = True)\n",
    "    print_dbg(f\" loss[sparsity][{task_key}]: {self.losses['sparsity'][task_key] } \", verbose = True)\n",
    "\n",
    "else:\n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between \\n Logits   : \\n{logits[-num_blocks:]} \\n and gt: \\n{gt} \\n\", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-num_blocks:], gt)}\")\n",
    "    \n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt[-1:])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt0[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt0[-1:])} \\n\")\n",
    "    \n",
    "    print('\\n cond 3')    \n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt[0:1])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt0[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt0[0:1])} \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db171b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56b64b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:14:30.155045Z",
     "start_time": "2022-01-26T00:14:30.107095Z"
    }
   },
   "outputs": [],
   "source": [
    "# flag = 'update_w'\n",
    "# environ.fix_alpha\n",
    "# environ.free_w(opt['fix_BN'])\n",
    "\n",
    "flag = 'update_alpha'\n",
    "environ.fix_weights()\n",
    "environ.free_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b03ad2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:31.184285Z",
     "start_time": "2022-01-25T23:43:31.159229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.networks['mtl-net'].num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa207f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:14:34.993711Z",
     "start_time": "2022-01-26T00:14:34.968623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_iters         : 6580\n",
      "curr_epochs           : 60\n",
      "train_total_epochs    : 60\n"
     ]
    }
   ],
   "source": [
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"curr_epochs           : {curr_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") \n",
    "\n",
    "train_total_epochs += 5\n",
    "\n",
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"curr_epochs           : {curr_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "014d94df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T17:15:02.216665Z",
     "start_time": "2022-01-25T17:15:01.848081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | logloss bceloss  aucroc   aucpr  f1_max| t1 loss t2 loss t3 lossttl loss|tr_time|\n",
      "1     | 0.00020 0.89942 0.81016 0.81627 0.75738|  4.6591  4.3649  4.4500 13.4740| 333.4|"
     ]
    }
   ],
   "source": [
    "# print_metrics_cr(curr_epoch, time.time() - t0, None, environ.val_metrics , num_prints)      \n",
    "\n",
    "# num_prints += 1\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Take check point\n",
    "# environ.save_checkpoint('latest', current_iter)\n",
    "# environ.train()\n",
    "# #-------------------------------------------------------\n",
    "# # END validation process\n",
    "# #-------------------------------------------------------       \n",
    "# flag = 'update_alpha'\n",
    "# environ.fix_w()\n",
    "# environ.free_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "70ae98e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:24:14.846382Z",
     "start_time": "2022-01-27T00:24:14.521851Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.46973792, 0.530262  ],\n",
       "        [0.45025694, 0.549743  ],\n",
       "        [0.4443086 , 0.5556915 ],\n",
       "        [0.4138397 , 0.58616036],\n",
       "        [0.4140113 , 0.5859887 ],\n",
       "        [0.42114905, 0.57885087]], dtype=float32),\n",
       " array([[0.4775429 , 0.5224572 ],\n",
       "        [0.45825934, 0.54174066],\n",
       "        [0.45530966, 0.5446904 ],\n",
       "        [0.43196857, 0.5680315 ],\n",
       "        [0.43017322, 0.5698268 ],\n",
       "        [0.4333356 , 0.5666644 ]], dtype=float32),\n",
       " array([[0.4868897 , 0.5131103 ],\n",
       "        [0.47107342, 0.5289266 ],\n",
       "        [0.45748708, 0.5425128 ],\n",
       "        [0.42434993, 0.5756501 ],\n",
       "        [0.4313186 , 0.56868154],\n",
       "        [0.4339512 , 0.56604874]], dtype=float32)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.get_policy_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff4ae7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1418081981208155"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa133f4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:08:57.366231Z",
     "start_time": "2022-01-08T01:08:57.295445Z"
    }
   },
   "outputs": [],
   "source": [
    "# dilation = 2\n",
    "# kernel_size = np.asarray((3, 3))\n",
    "# upsampled_kernel_size = (kernel_size - 1) * (dilation - 1) + kernel_size\n",
    "# print(upsampled_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecceb943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:39.763599Z",
     "start_time": "2022-01-25T23:43:39.728402Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.optimizers['weights'].param_groups[0]\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4338b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:41.628847Z",
     "start_time": "2022-01-25T23:43:41.602238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-05, 1e-05]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environ.schedulers['weights'].get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "424781ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T01:00:41.996410Z",
     "start_time": "2022-01-26T01:00:41.559006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'losses': {   'task1': tensor(2.3789, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>),\n",
      "                  'task2': tensor(2.1948, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>),\n",
      "                  'task3': tensor(2.3476, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>),\n",
      "                  'total': tensor(6.9212, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)},\n",
      "    'losses_mean': {   'task1': tensor(0.4758, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>),\n",
      "                       'task2': tensor(0.4390, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>),\n",
      "                       'task3': tensor(0.4695, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>),\n",
      "                       'total': tensor(1.3842, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)},\n",
      "    'parms': {'gumbel_temp': 2.6330653896376153, 'lr_0': 0.005, 'lr_1': 0.005},\n",
      "    'sharing': {   'total': tensor(2.1338e-06, device='cuda:0', grad_fn=<MulBackward0>)},\n",
      "    'sparsity': {   'task1_logits': tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
      "                    'task2_logits': tensor(0.5846, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
      "                    'task3_logits': tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
      "                    'total': tensor(0.0877, device='cuda:0', grad_fn=<MulBackward0>)},\n",
      "    'task1': {   'cls_loss': tensor(2.3789, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>),\n",
      "                 'cls_loss_mean': tensor(0.4758, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)},\n",
      "    'task2': {   'cls_loss': tensor(2.1948, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>),\n",
      "                 'cls_loss_mean': tensor(0.4390, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)},\n",
      "    'task3': {   'cls_loss': tensor(2.3476, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>),\n",
      "                 'cls_loss_mean': tensor(0.4695, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)},\n",
      "    'total': {   'total': tensor(7.0090, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>),\n",
      "                 'total_mean': tensor(1.4720, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)},\n",
      "    'total_mean': {}}\n"
     ]
    }
   ],
   "source": [
    "environ.losses.keys()\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24354e05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:19:43.683550Z",
     "start_time": "2022-01-08T01:19:43.571450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task1', 'task2', 'task3', 'tasks', 'total', 'sharing', 'sparsity'])\n",
      "{   'sharing': {'total': tensor(0.0002, device='cuda:0')},\n",
      "    'sparsity': {   'task1_logits': tensor(0.5147, device='cuda:0'),\n",
      "                    'task2_logits': tensor(0.5195, device='cuda:0'),\n",
      "                    'task3_logits': tensor(0.4978, device='cuda:0'),\n",
      "                    'total': tensor(0.0766, device='cuda:0')},\n",
      "    'task1': {'total': tensor(3.3657, device='cuda:0', dtype=torch.float64)},\n",
      "    'task2': {'total': tensor(3.5906, device='cuda:0', dtype=torch.float64)},\n",
      "    'task3': {'total': tensor(3.2182, device='cuda:0', dtype=torch.float64)},\n",
      "    'tasks': {   'task1': tensor(3.3657, device='cuda:0', dtype=torch.float64),\n",
      "                 'task2': tensor(3.5906, device='cuda:0', dtype=torch.float64),\n",
      "                 'task3': tensor(3.2182, device='cuda:0', dtype=torch.float64),\n",
      "                 'total': tensor(10.1745, device='cuda:0', dtype=torch.float64)},\n",
      "    'total': {'total': tensor(10.2513, device='cuda:0', dtype=torch.float64)}}\n"
     ]
    }
   ],
   "source": [
    "tmp = environ.get_loss_dict()\n",
    "print(tmp.keys())\n",
    "pp.pprint(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "980e5ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:22:22.714409Z",
     "start_time": "2022-01-08T01:22:22.395164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2513, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['task1']['total']+tmp['task2']['total']+tmp['task3']['total']+tmp['sharing']['total']+tmp['sparsity']['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f17d45c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:34:26.770937Z",
     "start_time": "2022-01-08T01:34:26.747415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17713"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e39d2493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:36:17.129535Z",
     "start_time": "2022-01-08T01:36:16.006144Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dict for weights = SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "state dict for alphas = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "{   'alphas': {   'param_groups': [   {   'amsgrad': False,\n",
      "                                          'betas': (0.9, 0.999),\n",
      "                                          'eps': 1e-08,\n",
      "                                          'lr': 0.0001,\n",
      "                                          'params': [0, 1, 2],\n",
      "                                          'weight_decay': 0.0005}],\n",
      "                  'state': {   0: {   'exp_avg': tensor([[ 0.0607, -0.0007],\n",
      "        [-0.0428, -0.0069],\n",
      "        [-0.1218,  0.0138],\n",
      "        [ 0.0086,  0.0238]], device='cuda:0'),\n",
      "                                      'exp_avg_sq': tensor([[0.0523, 0.0112],\n",
      "        [0.1734, 0.0073],\n",
      "        [0.3830, 0.0086],\n",
      "        [0.6783, 0.0108]], device='cuda:0'),\n",
      "                                      'step': 8613},\n",
      "                               1: {   'exp_avg': tensor([[ 0.0535, -0.0901],\n",
      "        [-0.0352, -0.0387],\n",
      "        [ 0.1020, -0.0073],\n",
      "        [-0.0796, -0.0576]], device='cuda:0'),\n",
      "                                      'exp_avg_sq': tensor([[0.0549, 0.0120],\n",
      "        [0.1729, 0.0070],\n",
      "        [0.3925, 0.0042],\n",
      "        [0.6830, 0.0098]], device='cuda:0'),\n",
      "                                      'step': 8613},\n",
      "                               2: {   'exp_avg': tensor([[-0.0127, -0.0090],\n",
      "        [ 0.1781, -0.0526],\n",
      "        [ 0.0248, -0.0095],\n",
      "        [ 0.1182, -0.0113]], device='cuda:0'),\n",
      "                                      'exp_avg_sq': tensor([[0.0501, 0.0096],\n",
      "        [0.1758, 0.0075],\n",
      "        [0.3727, 0.0086],\n",
      "        [0.6747, 0.0059]], device='cuda:0'),\n",
      "                                      'step': 8613}}},\n",
      "    'weights': {   'param_groups': [   {   'dampening': 0,\n",
      "                                           'lr': 0.0001,\n",
      "                                           'momentum': 0.9,\n",
      "                                           'nesterov': False,\n",
      "                                           'params': [0, 1, 2, 3, 4, 5],\n",
      "                                           'weight_decay': 0.0001},\n",
      "                                       {   'dampening': 0,\n",
      "                                           'lr': 0.0001,\n",
      "                                           'momentum': 0.9,\n",
      "                                           'nesterov': False,\n",
      "                                           'params': [   6,\n",
      "                                                         7,\n",
      "                                                         8,\n",
      "                                                         9,\n",
      "                                                         10,\n",
      "                                                         11,\n",
      "                                                         12,\n",
      "                                                         13,\n",
      "                                                         14,\n",
      "                                                         15],\n",
      "                                           'weight_decay': 0.0001}],\n",
      "                   'state': {   0: {   'momentum_buffer': tensor([[-5.4482e-01, -2.7512e-01,  4.6711e-02, -4.5902e-01, -2.5247e-01, -2.1980e-01,  9.4069e-02, -1.7776e-01, -2.0681e-01,\n",
      "         -1.0892e-01, -2.7464e-01,  9.3985e-03, -3.7082e-01, -2.7459e-01,  3.1315e-01, -2.4067e-01, -2.3865e-01, -2.0458e-01,\n",
      "         -3.9198e-01, -1.3971e-01, -2.2085e-01,  2.9561e-01, -3.2675e-01, -4.4961e-01, -2.5565e-01, -1.9292e-01,  2.3961e-02,\n",
      "         -1.4747e-01, -2.0461e-01, -2.9271e-01, -1.2636e-01, -3.2118e-01, -1.3085e-02, -3.2467e-01,  7.6504e-02, -3.8073e-01,\n",
      "         -3.4652e-01, -4.8136e-01, -1.0855e-01, -4.6362e-01],\n",
      "        [ 7.5279e-02,  5.0053e-01, -6.5901e-02,  4.2624e-01, -1.1979e-01,  1.3010e-01, -5.5052e-03,  6.0068e-02,  1.6456e-01,\n",
      "          6.9727e-02,  5.5028e-01,  7.4976e-02,  2.0544e-01, -1.8356e-01,  5.8646e-03,  3.8858e-01,  5.8229e-01, -6.4714e-02,\n",
      "         -9.7311e-02,  1.5540e-02,  2.6569e-01,  1.2685e-01,  3.5929e-01,  6.1809e-02,  6.5516e-02,  1.7281e-01,  2.4054e-01,\n",
      "          2.5444e-01,  8.1368e-02,  1.7834e-01,  6.4536e-01,  2.9524e-01,  5.1240e-02,  1.0327e-01,  5.0252e-01, -1.2009e-01,\n",
      "          5.2004e-01,  3.6589e-01,  2.5036e-02,  1.1756e-01],\n",
      "        [-2.6041e-02,  3.7978e-01, -2.9577e-02,  1.2583e-01, -1.9533e-01,  1.3204e-01, -2.4134e-01, -2.2500e-01,  1.1932e-01,\n",
      "         -2.8338e-02, -1.2620e-01,  4.1562e-02, -1.4164e-01,  1.0466e-02, -7.2124e-03, -1.7311e-01, -1.0594e-02, -1.1018e-01,\n",
      "         -4.0215e-01,  2.2231e-01, -3.5036e-01, -1.2274e-01, -2.5909e-01, -4.9505e-01,  2.7185e-01, -6.1372e-02, -3.2376e-01,\n",
      "         -4.9975e-01,  4.8624e-01, -2.4463e-03, -5.9059e-02, -2.9173e-02, -8.6035e-02,  2.1797e-02, -3.9157e-02, -1.6863e-01,\n",
      "          5.4898e-01,  1.9727e-01, -1.3099e-01,  2.3543e-01],\n",
      "        [ 2.3553e-01,  3.4450e-01, -7.3627e-02,  4.6605e-01,  4.3280e-02, -6.7811e-02,  7.9972e-02,  5.1132e-01, -2.0549e-02,\n",
      "          3.1941e-01,  7.4393e-01,  8.0956e-01,  5.7910e-02, -5.3678e-02,  2.2120e-01,  8.8379e-01,  8.9871e-01, -1.4795e-01,\n",
      "         -2.7687e-02,  3.6311e-01,  4.9540e-01,  3.6264e-01,  3.9721e-01,  3.3454e-01,  5.2051e-01,  1.9513e-01,  3.0391e-01,\n",
      "          1.0195e-01, -1.7816e-01,  5.9080e-01,  1.0638e+00, -2.2466e-01,  1.3106e-01,  2.6890e-01,  4.8539e-01, -1.6549e-01,\n",
      "          1.6727e-01,  1.9687e-01, -5.7773e-02,  6.5643e-04],\n",
      "        [-1.4043e+00, -4.7333e-01, -2.1537e-01, -6.8192e-01, -4.0019e-01, -3.0406e-01, -3.6066e-01, -8.0768e-01, -3.2529e-02,\n",
      "         -6.8346e-01, -1.1644e+00, -8.0557e-01, -7.2629e-01, -6.1576e-02, -2.6680e-01, -1.1771e+00, -9.3424e-01, -3.5853e-01,\n",
      "         -5.4122e-01, -5.1141e-01, -6.5359e-01, -5.1459e-01, -8.0336e-01, -8.5221e-01, -5.7714e-01, -4.6222e-01, -6.1317e-01,\n",
      "         -8.5647e-01,  1.8005e-02, -1.0541e+00, -1.6143e+00, -3.8753e-01, -6.1965e-01, -6.0904e-01, -6.1653e-01, -3.2613e-01,\n",
      "         -4.6098e-01, -5.1365e-01, -5.4302e-01, -6.4716e-01]], device='cuda:0')},\n",
      "                                1: {   'momentum_buffer': tensor([-0.0579,  0.2361, -0.0467,  0.4477, -0.6013], device='cuda:0')},\n",
      "                                2: {   'momentum_buffer': tensor([[-0.2612, -0.1668, -0.0060, -0.4843,  0.0824,  0.2120, -0.0110, -0.2463,  0.0614, -0.1931, -0.0739, -0.3940, -0.1723,\n",
      "         -0.0844, -0.0420, -0.3581, -0.3555,  0.1027,  0.3321, -0.2004,  0.0719, -0.1718, -0.1382, -0.2087, -0.4006,  0.1659,\n",
      "         -0.0612,  0.2602, -0.3789, -0.5442, -0.6798, -0.0991,  0.1144,  0.0608, -0.3799,  0.1046, -0.5230, -0.1794,  0.2387,\n",
      "         -0.3948],\n",
      "        [-0.7644, -0.4455,  0.1079, -0.6770, -0.0285, -0.0317, -0.0798, -0.5369, -0.1481, -0.5042, -0.6391, -0.6097, -0.4634,\n",
      "         -0.3808, -0.4200, -0.5469, -0.9528, -0.0851,  0.2172, -0.3774, -0.0737, -0.6040, -0.4646, -0.6076, -0.3767,  0.0727,\n",
      "         -0.1834,  0.3338, -0.3549, -0.7231, -1.1620,  0.0138, -0.1620, -0.3471, -0.4770,  0.0757, -0.6643, -0.6895,  0.0787,\n",
      "         -0.4058],\n",
      "        [ 0.2805, -0.0582,  0.1003,  0.0825,  0.2279, -0.3497,  0.1211,  0.5327,  0.0040,  0.5895,  0.3208,  0.3248,  0.1391,\n",
      "          0.3223,  0.3612,  0.3953, -0.0528,  0.2579,  0.6889,  0.3003,  0.2949,  0.3943,  0.3399,  0.3902,  0.2274,  0.1454,\n",
      "          0.2639,  0.4317,  0.0282,  0.2718,  0.7873,  0.1240, -0.0545, -0.0371, -0.2902,  0.1248,  0.0531,  0.4477,  0.1740,\n",
      "          0.3522],\n",
      "        [-0.1528, -0.1209,  0.0156, -0.2532, -0.0495, -0.1200,  0.2843, -0.2697,  0.0212, -0.3180, -0.1699,  0.0696, -0.0441,\n",
      "         -0.2312,  0.0227,  0.0530,  0.4047, -0.4066,  0.1157, -0.0749,  0.0571, -0.2684,  0.0121,  0.2005, -0.0978,  0.0283,\n",
      "          0.1963, -0.2490, -0.3016, -0.0356,  0.0103,  0.0743,  0.2626,  0.0507, -0.0169, -0.3918, -0.1736,  0.0629, -0.0922,\n",
      "         -0.0151],\n",
      "        [-0.1752,  0.0017, -0.0910, -0.4192, -0.1514,  0.1885, -0.0909, -0.2453,  0.0872, -0.2964, -0.3132, -0.5285, -0.0679,\n",
      "         -0.0362, -0.2413, -0.5825, -0.5508,  0.1931, -0.0183, -0.3240, -0.0980, -0.0574, -0.3542, -0.1842, -0.1074,  0.2090,\n",
      "         -0.1148,  0.0744, -0.1232, -0.2965, -0.8092,  0.1216,  0.0491,  0.0700,  0.0853,  0.0258,  0.0286, -0.4716,  0.1206,\n",
      "         -0.0628]], device='cuda:0')},\n",
      "                                3: {   'momentum_buffer': tensor([-0.1741, -0.3002,  0.2925,  0.2971, -0.3722], device='cuda:0')},\n",
      "                                4: {   'momentum_buffer': tensor([[-0.0849, -0.1597,  0.2053, -0.1966, -0.0118, -0.2330, -0.0241, -0.6122, -0.2113, -0.3294, -0.2063, -0.3181,  0.0481,\n",
      "         -0.4093, -0.4637, -0.7161, -0.4272, -0.1147,  0.2656, -0.4111,  0.0617, -0.3495, -0.3421, -0.1997,  0.0105,  0.0609,\n",
      "         -0.0572,  0.1777, -0.0505, -0.6948, -0.4959,  0.2197,  0.1054, -0.3540, -0.4853,  0.2524, -0.3534, -0.1041,  0.0436,\n",
      "         -0.0729],\n",
      "        [-0.2884, -0.0777, -0.1561, -0.0193, -0.2145,  0.1506,  0.0461, -0.1056,  0.1115,  0.0735,  0.0143, -0.1489, -0.1132,\n",
      "         -0.1654, -0.0719, -0.0780,  0.0905, -0.1635,  0.0411,  0.0230, -0.1094,  0.2141,  0.2209,  0.2538, -0.2747, -0.0911,\n",
      "          0.2328, -0.0246, -0.2137,  0.0164,  0.4605,  0.0623,  0.1916,  0.0445,  0.2819,  0.0821, -0.2006,  0.0480, -0.2465,\n",
      "         -0.0939],\n",
      "        [-0.3999, -0.1251, -0.2381, -0.1985, -0.1385, -0.0977, -0.1711, -0.1430, -0.2113, -0.3605, -0.0474, -0.3035,  0.0633,\n",
      "         -0.0959, -0.1873, -0.3301, -0.1024, -0.2385, -0.4791, -0.1046, -0.2390, -0.2179, -0.3163, -0.2241, -0.5092,  0.1025,\n",
      "         -0.1386, -0.4020, -0.2547, -0.1484, -0.6128, -0.2314, -0.0788,  0.2300, -0.0069, -0.2817, -0.1210, -0.0113, -0.4520,\n",
      "         -0.2268],\n",
      "        [ 0.2257,  0.3765,  0.1650, -0.1478,  0.0906,  0.1179,  0.1602, -0.1910,  0.4880, -0.0549, -0.0863,  0.1077,  0.3084,\n",
      "          0.1151, -0.0719, -0.2795,  0.0179,  0.6149,  0.4591, -0.0565, -0.0812,  0.2760, -0.0342,  0.0169,  0.0639, -0.0015,\n",
      "          0.0713,  0.1656,  0.2635,  0.0067, -0.1705,  0.2555,  0.2492, -0.0994,  0.1673,  0.3523,  0.3903,  0.0610,  0.2200,\n",
      "          0.0149],\n",
      "        [ 0.4827,  0.3845,  0.2594,  0.7605,  0.4389,  0.4478,  0.4341,  0.6531,  0.1451,  0.5494,  0.8218,  0.7176,  0.2111,\n",
      "          0.1574,  0.5871,  1.1305,  0.7570,  0.1728,  0.4147,  0.5044,  0.3479,  0.4095,  0.6468,  0.4890,  0.3662,  0.0461,\n",
      "          0.3462,  0.2534,  0.3233,  0.7041,  1.0300,  0.2806,  0.3191,  0.6002,  0.5828,  0.2223,  0.5332,  0.5004,  0.1183,\n",
      "          0.3750]], device='cuda:0')},\n",
      "                                5: {   'momentum_buffer': tensor([-0.3636,  0.1302, -0.1584, -0.0355,  0.4779], device='cuda:0')},\n",
      "                                6: {   'momentum_buffer': tensor([[ 1.5304e-01, -5.9271e-03,  4.8917e-01,  ..., -2.2712e-01, -1.2944e-01, -1.5610e-01],\n",
      "        [ 2.3775e-02, -9.0718e-03,  2.1647e-02,  ...,  3.9088e-02,  2.5611e-02, -2.9572e-02],\n",
      "        [ 1.6747e-02, -1.2741e-02,  1.7129e-02,  ...,  1.8492e-02, -8.0235e-03, -2.1392e-02],\n",
      "        ...,\n",
      "        [-3.1285e-06, -2.0295e-06,  7.8620e-06,  ...,  1.1699e-06,  3.7180e-07,  5.9421e-07],\n",
      "        [ 7.3640e-06, -1.7594e-05,  7.4498e-05,  ..., -2.5983e-05,  1.2307e-05,  3.2505e-05],\n",
      "        [-9.6068e-06, -8.5949e-06,  1.0171e-05,  ..., -6.1072e-06, -8.3978e-07, -6.7925e-06]], device='cuda:0')},\n",
      "                                7: {   'momentum_buffer': tensor([ 0.3887, -0.0579,  0.3107,  0.1551,  0.8331, -0.1376, -0.3844, -0.0353,  0.4420, -0.5766,  0.0788, -0.0751, -0.1867,\n",
      "         0.1001, -0.3312, -0.2118, -0.3287, -0.0677,  0.3000, -0.2234,  0.1245, -0.1919, -0.2328,  0.3670,  0.0290, -0.1925,\n",
      "         0.4300, -0.1144, -0.4987, -0.1107, -0.3611,  0.4313, -0.4842,  0.6604,  0.0137,  0.0144,  0.2782, -0.2992, -0.2614,\n",
      "        -0.2438], device='cuda:0')},\n",
      "                                8: {   'momentum_buffer': tensor([[ 1.5098e-01,  6.8066e-02, -1.1790e-01,  ...,  9.2484e-02, -4.3672e-02,  6.0582e-02],\n",
      "        [-2.7928e-02, -3.8709e-02,  1.1351e-02,  ...,  1.4022e-02,  1.4181e-02, -2.3581e-02],\n",
      "        [ 1.2368e-05,  1.6269e-04, -2.8662e-04,  ..., -4.4968e-05, -1.8164e-04, -3.3822e-04],\n",
      "        ...,\n",
      "        [-4.0677e-02,  2.7337e-02,  1.5571e-02,  ..., -4.8724e-02, -3.2856e-02, -1.0650e-02],\n",
      "        [ 1.6598e-01,  9.5019e-02, -4.6000e-02,  ...,  6.4921e-02,  3.7100e-02,  8.7817e-02],\n",
      "        [-8.4905e-03,  2.2479e-02, -2.5620e-03,  ..., -7.5068e-03, -5.1343e-03, -1.6064e-02]], device='cuda:0')},\n",
      "                                9: {   'momentum_buffer': tensor([ 3.4776e-01, -3.5319e-02,  1.0072e-04,  3.8190e-03,  4.6922e-01, -1.4719e-02, -1.9333e-01, -1.0430e-01, -4.9097e-02,\n",
      "        -3.4282e-01, -1.2632e-01,  1.1640e-01,  1.4245e-01,  3.9221e-01, -1.5241e-01,  1.0361e-01,  2.8693e-01, -3.8438e-02,\n",
      "         5.4297e-01,  2.2322e-02,  2.2954e-01, -2.4616e-01,  8.4137e-02,  2.5481e-01, -9.6045e-02,  4.1123e-02,  1.7366e-01,\n",
      "        -3.3607e-02, -7.8799e-02, -8.1003e-02, -1.5968e-01,  1.4050e-01,  7.0586e-02,  2.4685e-01,  8.4280e-02,  1.2348e-01,\n",
      "        -1.0331e-01, -9.8404e-02,  2.8721e-01, -3.6698e-02], device='cuda:0')},\n",
      "                                10: {   'momentum_buffer': tensor([[ 0.0122, -0.0239, -0.0222,  ...,  0.0248,  0.0130, -0.0082],\n",
      "        [ 0.0409, -0.0863, -0.0585,  ...,  0.0649, -0.0058,  0.0468],\n",
      "        [ 0.0525,  0.0079, -0.0112,  ...,  0.0785,  0.0505,  0.0502],\n",
      "        ...,\n",
      "        [ 0.2707,  0.1044, -0.0254,  ...,  0.1337,  0.0744,  0.0592],\n",
      "        [ 0.0154,  0.0331,  0.0019,  ...,  0.0142, -0.0596, -0.0396],\n",
      "        [-0.0857,  0.0114, -0.0162,  ..., -0.0332, -0.1018, -0.0657]], device='cuda:0')},\n",
      "                                11: {   'momentum_buffer': tensor([ 0.0760, -0.0063,  0.1234, -0.0805,  0.1407, -0.0494,  0.0783,  0.1751, -0.0619, -0.1029,  0.0626, -0.1621,  0.0215,\n",
      "         0.0589,  0.0777, -0.0114,  0.0356,  0.0869,  0.1138,  0.0837,  0.0116, -0.0266,  0.1767,  0.2595,  0.0828, -0.0264,\n",
      "         0.1041,  0.0221,  0.0534, -0.0933,  0.0266, -0.0506, -0.0109, -0.0572, -0.0290,  0.0525, -0.0016,  0.2201, -0.0559,\n",
      "        -0.0764], device='cuda:0')},\n",
      "                                12: {   'momentum_buffer': tensor([[ 0.0542,  0.0008, -0.0059,  ...,  0.0547,  0.0933,  0.0654],\n",
      "        [-0.0124, -0.0217, -0.0174,  ...,  0.0780, -0.0404,  0.0119],\n",
      "        [-0.0024, -0.0011, -0.0005,  ...,  0.0003, -0.0004, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0027,  0.0664,  0.0873,  ...,  0.0592,  0.0687,  0.0510],\n",
      "        [ 0.0913,  0.0534, -0.0025,  ...,  0.0306, -0.0440,  0.0299],\n",
      "        [-0.0215,  0.0202, -0.0636,  ..., -0.0170, -0.0338, -0.0677]], device='cuda:0')},\n",
      "                                13: {   'momentum_buffer': tensor([ 0.1389, -0.0492, -0.0026, -0.1231, -0.0199,  0.0288,  0.0002,  0.0369,  0.0689,  0.0349, -0.0123, -0.0443, -0.0297,\n",
      "         0.0026,  0.0345,  0.0489,  0.0253,  0.0764,  0.0739,  0.0602, -0.0033,  0.0222, -0.0743,  0.0255,  0.0509,  0.1373,\n",
      "         0.0104,  0.0234, -0.0108,  0.0034, -0.0159,  0.0054,  0.0429,  0.0476, -0.0150, -0.0057, -0.0413,  0.0307,  0.0383,\n",
      "        -0.0091], device='cuda:0')},\n",
      "                                14: {   'momentum_buffer': tensor([[ 0.1029,  0.0980,  0.0202,  ...,  0.1078,  0.0647,  0.0686],\n",
      "        [ 0.1083,  0.0676,  0.0507,  ...,  0.1634,  0.0056,  0.0213],\n",
      "        [ 0.0021, -0.0008, -0.0076,  ...,  0.0037, -0.0008, -0.0021],\n",
      "        ...,\n",
      "        [ 0.0117,  0.0903,  0.0301,  ..., -0.0045,  0.0020,  0.0292],\n",
      "        [ 0.0616,  0.0727, -0.0032,  ...,  0.0555,  0.0145,  0.0146],\n",
      "        [-0.0441, -0.0322, -0.0147,  ...,  0.0057, -0.0240, -0.0302]], device='cuda:0')},\n",
      "                                15: {   'momentum_buffer': tensor([ 0.0819,  0.0197, -0.0006, -0.0349,  0.0058,  0.0291, -0.0123, -0.0388, -0.0164, -0.0855,  0.0779,  0.0347, -0.0715,\n",
      "        -0.0225,  0.0360,  0.0105, -0.0919,  0.2164,  0.0061,  0.0852,  0.1488, -0.1035,  0.0128, -0.0246,  0.1463,  0.0421,\n",
      "         0.0069, -0.0251,  0.0041, -0.0824, -0.1332, -0.0114, -0.0141,  0.0515,  0.0716,  0.0047, -0.0701, -0.0178,  0.0446,\n",
      "        -0.0167], device='cuda:0')}}}}\n"
     ]
    }
   ],
   "source": [
    "current_state = {}\n",
    "for k, v in environ.optimizers.items():\n",
    "    print(f'state dict for {k} = {v}')\n",
    "    current_state[k] = v.state_dict()\n",
    "pp.pprint(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8b87b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:40:22.032247Z",
     "start_time": "2022-01-08T01:40:22.006953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dict for weights = <torch.optim.lr_scheduler.StepLR object at 0x7f90c01c0ca0>\n",
      "{'step_size': 4000, 'gamma': 0.5, 'base_lrs': [0.0001, 0.0001], 'last_epoch': 9100, '_step_count': 9101, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [2.5e-05, 2.5e-05]}\n"
     ]
    }
   ],
   "source": [
    "current_state = {}\n",
    "for k, v in environ.schedulers.items():\n",
    "    print(f'state dict for {k} = {v}')\n",
    "    print(v.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240104e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Warm-up:  validation - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3eaa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:41:10.772791Z",
     "start_time": "2021-12-15T20:41:05.690320Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# validation\n",
    "if should(current_iter, opt['train']['val_freq']):\n",
    "    print(f\"**  {timestring()}  START VALIDATION iteration: {current_iter} \")    \n",
    "\n",
    "    environ.eval()     # set to evaluation mode (train = False)\n",
    "    num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "    val_metrics = eval_dev(environ, \n",
    "                          val_loader, \n",
    "                          opt['tasks'], \n",
    "                          policy=False, \n",
    "                          num_train_layers=None, \n",
    "                          eval_iter = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac77c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:41:11.489480Z",
     "start_time": "2021-12-15T20:41:11.461278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_metrics.keys()\n",
    "val_metrics['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad14d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:41:55.074366Z",
     "start_time": "2021-12-15T20:41:55.029329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in val_metrics:\n",
    "    print(f'\\n {i} \\n -----------------')\n",
    "    print(val_metrics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d951d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:37:44.423495Z",
     "start_time": "2021-12-15T20:37:44.400109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    for t_id, task in enumerate(environ.tasks):\n",
    "        task_key = f\"task{t_id+1}\"    \n",
    "        environ.print_loss(current_iter, start_time, val_metrics[task_key][\"classification_agg\"], title='validation')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5783f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:36:43.503203Z",
     "start_time": "2021-12-15T20:36:43.371900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    environ.save_checkpoint('latest', current_iter)\n",
    "\n",
    "    print(f\"** {timestring()} - END VALIDATION iteration:  {current_iter} \")                \n",
    "    environ.train()    # set to training mode (train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3074d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T22:26:36.573475Z",
     "start_time": "2021-12-12T22:26:36.455074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for i in val_metrics.keys():\n",
    "#     print(i, type(val_metrics[i]))\n",
    "#     for k in val_metrics[i].keys():\n",
    "#         print(i,k, type(val_metrics[i][k]))\n",
    "#         if isinstance(val_metrics[i][k], pd.core.series.Series):\n",
    "#             print(f\"val_metrics[{i}][{k}] is a series\")\n",
    "#         elif isinstance(val_metrics[i][k], pd.core.frame.DataFrame):\n",
    "#             print(f\"val_metrics[{i}][{k}] is a dataframe\")        \n",
    "\n",
    "# s = val_metrics['task1']['classification_agg']\n",
    "# print(s)\n",
    "# print(s.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea9c4a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Weight Training Dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561e1a9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Weight training - prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454204e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:40:00.014577Z",
     "start_time": "2021-12-20T23:39:59.990525Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# arch_parms = environ.networks['mtl-net'].named_parameters()\n",
    "# print(arch_parms)\n",
    "# for name, parm in arch_parms:\n",
    "#     print(name, '    ',parm.requires_grad)\n",
    "# print_underline('MTL3_Dev Policys', verbose = True)\n",
    "# for i in   environ.networks['mtl-net'].policys:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ad431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:57:34.590953Z",
     "start_time": "2021-12-21T22:57:34.234886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_heading(f\"** {timestring()} - Training current iteration {current_iter}  flag: {flag} \", verbose = True)    \n",
    "\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0 \n",
    "batch_idx_a = 0 \n",
    "batch_idx_w = 0 \n",
    "\n",
    "if flag_warmup:\n",
    "    print_heading(f\"** Set optimizer and scheduler to policy_learning = True\", verbose = True)\n",
    "    environ.define_optimizer(policy_learning=True)\n",
    "    environ.define_scheduler(policy_learning=True)\n",
    "    flag_warmup = False\n",
    "\n",
    "if current_iter == opt['train']['warm_up_iters']:\n",
    "    print_heading(f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "                  f\"   Take checkpoint and block gradient flow through Policy net\", verbose=True)\n",
    "    environ.save_checkpoint('warmup', current_iter)\n",
    "    environ.fix_alpha()\n",
    "    \n",
    "# batch_enumerator1 = enumerate(train1_loader,1)  \n",
    "# batch_enumerator2 = enumerate(train2_loader,1)  \n",
    "\n",
    "train_total_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c0a04",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Weight training - main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc47806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:57:57.069633Z",
     "start_time": "2021-12-21T22:57:57.026843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"opt['train']['print_freq']         {opt['train']['print_freq']}\")\n",
    "print(f\"opt['train']['hard_sampling']      {opt['train']['hard_sampling']}\")\n",
    "print(f\"opt['policy']                      {opt['policy']}\")\n",
    "print(f\"opt['tasks']                       {opt['tasks']}\")\n",
    "print(f\"weight_iter_alternate:             {opt['train']['weight_iter_alternate']}\")\n",
    "print(f\"alpha_iter_alternate :             {opt['train']['alpha_iter_alternate']}\")\n",
    "print(f\"current_iter                       {current_iter  }\")\n",
    "print(f\"current_iter_w                     {current_iter_w}\")\n",
    "print(f\"current_iter_a                     {current_iter_a}\")\n",
    "print(f\"batch_idx_w                        {batch_idx_w}\")\n",
    "print(f\"flag                               {flag          }\")\n",
    "print(f\"train_total_epochs                 {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8a69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:41:10.189643Z",
     "start_time": "2021-12-21T22:41:10.164497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c31fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:58:10.493069Z",
     "start_time": "2021-12-21T22:58:10.469631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------     \n",
    "## Weight / Policy Training\n",
    "##--------------------------------------------------------------- \n",
    "# stop_iter = current_iter_w +  opt['train']['weight_iter_alternate']\n",
    "# print(f\" Current Weight iteration {current_iter_w} - Run  from {current_iter_w+1} to {stop_iter+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee9029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:58:12.219746Z",
     "start_time": "2021-12-21T22:58:12.193318Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f076d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T22:58:13.757096Z",
     "start_time": "2021-12-21T22:58:13.731942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with tnrange(start_iter_t , stop_iter_t  , initial = start_iter_t , total = stop_iter_t, position=0, leave= True, desc=\"master\") as t :\n",
    "# with tqdm_notebook(total=train_total_epochs) as t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d3730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T18:50:23.014384Z",
     "start_time": "2021-12-21T18:50:22.955398Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534cdee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T23:08:28.021068Z",
     "start_time": "2021-12-21T22:59:25.316087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "curr_epoch = 0\n",
    "main_iter_ctr = 0 \n",
    "verbose = False\n",
    "t = tqdm(total=train_total_epochs, desc=f\" Alternate Weight/Policy training\")\n",
    "\n",
    "while curr_epoch < train_total_epochs:\n",
    "    curr_epoch+=1\n",
    "    t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the network weights\n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_w':\n",
    "        current_iter_w  = 0 \n",
    "        stop_iter_w =   opt['train']['weight_iter_alternate']\n",
    "\n",
    "        with trange(+1, stop_iter_w+1 , initial = current_iter_w, total = stop_iter_w, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} weight training\") as t_weights :\n",
    "            for current_iter_w in t_weights:    \n",
    "                current_iter += 1\n",
    "\n",
    "                start_time = time.time()\n",
    "                environ.train()\n",
    "                \n",
    "#                 if batch_idx_w == len(train1_loader):\n",
    "#                     print_dbg(f\"  Reenumerate train1_loader -  index_w: {batch_idx_w}   len(train1_loader) = {len(train1_loader)} \", verbose)\n",
    "#                     batch_enumerator1 = enumerate(train1_loader,1)    \n",
    "                    \n",
    "                batch = next(train1_loader)\n",
    "                environ.set_inputs(batch, train1_loader.dataset.input_size)\n",
    "\n",
    "                ##----------------------------------------------------------------------\n",
    "                ## Set number of layers to train based on cirriculum_speed \n",
    "                ## and p_epoch (number of epochs of policy training)\n",
    "                ## When curriculum_speed == 3, a num_train_layers is incremented \n",
    "                ## after completion of every 3 policy training epochs\n",
    "                ##----------------------------------------------------------------------\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "\n",
    "#                 print_heading(f\"{timestring()} CALL ENVIRON.OPTIMIZE()    current_iter: {current_iter}     flag: {flag}\\n\"\n",
    "#                       f\"{' ':10s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                       f\"{' ':10s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \\n\"                          \n",
    "#                       f\"{' ':10s} is_policy: {opt['policy']}     p_epoch: {p_epoch}       num_train_layers: {num_train_layers}\", verbose = False) \n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "\n",
    "                t_weights.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                       'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = \"Weight training iteration\", verbose = True)\n",
    "                    environ.resize_results()\n",
    "\n",
    "#                 print_heading(f\"{timestring()} - CONTINUE WEIGHT TRAINING   current_iter: {current_iter}\\n\"\n",
    "#                   f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                   f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']}\",\n",
    "#                   verbose = False)        \n",
    "\n",
    "        #-------------------------------------------------------\n",
    "        # validation process\n",
    "        #------------------------------------------------------- \n",
    "\n",
    "#         if should(current_iter_w, opt['train']['weight_iter_alternate']): \n",
    "\n",
    "        if (current_iter_w >= stop_iter_w):\n",
    "            environ.eval()\n",
    "            print_dbg(\"++ Weight Training Validation  and then Switch to update_alpha\", verbose = False)\n",
    "\n",
    "            val_metrics = eval_dev(environ, \n",
    "                                  val_loader, \n",
    "                                  opt['tasks'], \n",
    "                                  policy=opt['policy'],\n",
    "                                  num_train_layers=num_train_layers, \n",
    "                                  hard_sampling=opt['train']['hard_sampling'],\n",
    "                                  eval_iter = -1)        \n",
    "\n",
    "            if (verbose):\n",
    "                for t_id, task in enumerate(environ.tasks):\n",
    "                    task_key = f\"task{t_id+1}\"    \n",
    "                    environ.print_metrics(current_iter, start_time, val_metrics[task_key][\"classification_agg\"], title='validation', verbose = verbose)        \n",
    "\n",
    "            environ.save_checkpoint('latest', current_iter)\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # if number of iterations completed after the warm up phase is greater than the number of \n",
    "            # (weight/policy alternations) x (cirriculum speed) x (number of layers to be policy trained)\n",
    "            #\n",
    "            # check metrics for improvement, and issue a checkpoint if necessary\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "            if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "                    (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "                new_value = 0\n",
    "#                 print_heading(f\"  evaluate progress and make checkpoint if necessary.\" , verbose = True)\n",
    "#                 print(f\" current iter                                 : {current_iter} \\n\"\n",
    "#                       f\" opt['train']['warm_up_iters']                : {opt['train']['warm_up_iters']} \\n\"\n",
    "#                       f\" num_blocks                                   : {num_blocks} \\n\"\n",
    "#                       f\" opt['curriculum_speed']                      : {opt['curriculum_speed']}\\n\"\n",
    "#                       f\" opt['train']['weight_iter_alternate']        : {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                       f\" opt['train']['alpha_iter_alternate']         : {opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#                       f\" alpha_iter_alternate + weight_iter_alternate : {opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#                       f\" num_blks * curriculum_speed * (alpha_alternate + weight_alternate): \"\n",
    "#                       f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} \\n\"\n",
    "\n",
    "                print(f\"  {current_iter - opt['train']['warm_up_iters']} IS GREATER THAN \"\n",
    "                       f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} -- \"\n",
    "                       f\"  evaluate progress and make checkpoint if necessary.\" )            \n",
    "#               ## compare validation metrics against reference metrics.\n",
    "\n",
    "#                 for k in refer_metrics.keys():\n",
    "#                     if k in val_metrics.keys():\n",
    "#                         for kk in val_metrics[k].keys():\n",
    "#                             if not kk in refer_metrics[k].keys():\n",
    "#                                 continue\n",
    "#                             if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or (\n",
    "#                                     k == 'depth' and not kk.startswith('sigma')) or (kk == 'err'):\n",
    "#                                 value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "#                             else:\n",
    "#                                 value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "#                             value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "#                             new_value += value\n",
    "\n",
    "#                 print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                # if results have improved, save these results and issue a checkpoint\n",
    "\n",
    "#                 if (new_value > best_value):\n",
    "#                     print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)\n",
    "#                     best_value = new_value\n",
    "#                     best_metrics = val_metrics\n",
    "#                     best_iter = current_iter\n",
    "#                     environ.save_checkpoint('best', current_iter)\n",
    "#                     print('New      best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)                         \n",
    "#                     print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                # if results have improved, save these results and issue a checkpoint   \n",
    "\n",
    "            environ.train()\n",
    "            #-------------------------------------------------------\n",
    "            # END validation process\n",
    "            #-------------------------------------------------------       \n",
    "            print_heading(f\"{timestring()} - SWITCH TO ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "              f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "              f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']}\",\n",
    "              verbose = False)       \n",
    "            flag = 'update_alpha'\n",
    "            environ.fix_w()\n",
    "            environ.free_alpha()\n",
    "        #-------------------------------------------------------\n",
    "        # end validation process\n",
    "        #-------------------------------------------------------               \n",
    "\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the  policy \n",
    "    #-----------------------------------------\n",
    "    if flag == 'update_alpha':\n",
    "        current_iter_a = 0\n",
    "        stop_iter_a = opt['train']['alpha_iter_alternate']\n",
    "\n",
    "        with trange( +1, stop_iter_a+1 , initial = 0, total = stop_iter_a, \n",
    "                     position=0, leave= False, desc=f\"Epoch {curr_epoch} policy training\") as t_policy :\n",
    "            for current_iter_a in t_policy:    \n",
    "                current_iter += 1\n",
    "\n",
    "#                 batch_idx_a, batch = next(batch_enumerator2)\n",
    "                batch = next(train2_loader)\n",
    "                environ.set_inputs(batch, train2_loader.dataset.input_size)\n",
    "\n",
    "#                 if batch_idx_a == len(train2_loader):\n",
    "#                     print_dbg(f\" Re-enumerate train2_loader  batch_idx_a: {batch_idx_a}   len(train2_loader) = {len(train2_loader)}\", verbose=False)                \n",
    "#                     batch_enumerator2 = enumerate(train2_loader,1)        \n",
    "\n",
    "#                 print_heading(f\"{timestring()} - ENVIRON.OPTIMIZE()    flag: {flag}    current_iter: {current_iter}   \\n\"\n",
    "#                               f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                               f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \\n\"\n",
    "#                               f\" is_policy: {opt['policy']}   num_train_layers: {num_train_layers}  hard_sampling: {opt['train']['hard_sampling']}\\n\"\n",
    "#                               f\" is_curriculum: {opt['is_curriculum']}     curriculum_speed: {opt['curriculum_speed']}   p_epoch: {p_epoch}\"\n",
    "#                               , verbose = False) \n",
    "\n",
    "                if opt['is_curriculum']:\n",
    "                    num_train_layers = (p_epoch // opt['curriculum_speed']) + 1\n",
    "                else:\n",
    "                    num_train_layers = None\n",
    "\n",
    "                print_dbg(f\" num_train_layers  : {num_train_layers}\", verbose = False)\n",
    "\n",
    "\n",
    "                environ.optimize(opt['lambdas'], \n",
    "                                 is_policy=opt['policy'], \n",
    "                                 flag=flag, \n",
    "                                 num_train_layers=num_train_layers,\n",
    "                                 hard_sampling=opt['train']['hard_sampling'],\n",
    "                                 verbose = False)\n",
    "                \n",
    "                t_policy.set_postfix({'iteration': current_iter, 'Loss': f\"{environ.losses['total']['total'].item():.4f}\" , \n",
    "                                      'row_ids':f\"{batch['row_id'][0]}-{batch['row_id'][-1]}\"})\n",
    "                \n",
    "                if should(current_iter, opt['train']['print_freq']):\n",
    "                    environ.print_loss(current_iter, start_time, title = \"Policy training iteration\", verbose=True)\n",
    "                    environ.resize_results()\n",
    "                    # environ.visual_policy(current_iter)\n",
    "\n",
    "#                 print_heading(f\"{timestring()} - CONTINUE ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "#                               f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                               f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \", \n",
    "#                               verbose = False )      \n",
    "\n",
    "        ## if (current_iter_a % alpha_iter_alternate) == 0 \n",
    "#         if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "#         print(f\" policy loop ended - current_iter_a: {current_iter_a}   stop_iter_a: {stop_iter_a}\")\n",
    "        if( current_iter_a >= stop_iter_a):            \n",
    "#             print_heading(f\"{timestring()} - SWITCH TO WEIGHT TRAINING  urrent_iter: {current_iter}\\n\"\n",
    "#                           f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#                           f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \",\n",
    "#                           verbose = False )       \n",
    "\n",
    "            flag = 'update_w'\n",
    "            environ.fix_alpha()\n",
    "            environ.free_w(opt['fix_BN'])\n",
    "            environ.decay_temperature()\n",
    "\n",
    "            # print the distribution\n",
    "            print_dbg(np.concatenate(environ.get_policy_prob(), axis=-1), verbose = False)\n",
    "            \n",
    "            p_epoch += 1\n",
    "            print_dbg(f\"** p_epoch incremented: {p_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ae87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T23:20:09.766802Z",
     "start_time": "2021-12-21T23:20:09.716093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"{opt['train']['Lambda_sharing']:.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9f0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:26:17.162189Z",
     "start_time": "2021-12-21T21:26:16.777249Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value))\n",
    "print(best_metrics)\n",
    "best_value = new_value\n",
    "best_metrics = val_metrics\n",
    "best_iter = current_iter\n",
    "environ.save_checkpoint('best', current_iter)\n",
    "print('New best iter : %d, best_value: %.4f \\n' % (best_iter, best_value))                         \n",
    "print(best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f37b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:27:15.488947Z",
     "start_time": "2021-12-21T21:27:15.382938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.losses['tasks'] = {'total' : torch.tensor(0.0, device  = environ.device, dtype=torch.float64)}\n",
    "# environ.device\n",
    "\n",
    "# print(val_metrics)\n",
    "pp.pprint(environ.losses)\n",
    "# environ.print_loss_2(current_iter, start_time, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b334b1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Policy Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97645f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-18T06:36:24.639972Z",
     "start_time": "2021-12-18T06:36:24.620024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(f\" current iter                                 : {current_iter} \\n\"\n",
    "#       f\" opt['train']['warm_up_iters']                : {opt['train']['warm_up_iters']} \\n\"\n",
    "#       f\" num_blocks                                   : {num_blocks} \\n\"\n",
    "#       f\" opt['curriculum_speed']                      : {opt['curriculum_speed']}\\n\"\n",
    "#       f\" opt['train']['weight_iter_alternate']        : {opt['train']['weight_iter_alternate']}\\n\"\n",
    "#       f\" opt['train']['alpha_iter_alternate']         : {opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#       f\" alpha_iter_alternate + weight_iter_alternate : {opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']}\\n\"\n",
    "#       f\" num_blocks * curriculum_speed * (alpha_iter_alternate + weight_iter_alternate): \\\n",
    "#           {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])} \\n\"\n",
    "#       f\" IF {current_iter - opt['train']['warm_up_iters']} IS GREATER THAN  ??\"\n",
    "#       f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ff6a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-18T06:36:36.364353Z",
     "start_time": "2021-12-18T06:36:36.341535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(f\" task1_logits: {environ.networks['mtl-net'].task1_logits} \\n\")\n",
    "# print(f\" task2_logits: {environ.networks['mtl-net'].task2_logits} \\n\")\n",
    "# print(f\" task3_logits: {environ.networks['mtl-net'].task3_logits} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dcee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T02:07:18.061296Z",
     "start_time": "2021-12-20T02:07:18.038742Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter_a , opt['train']['alpha_iter_alternate'],flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9dc0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T02:07:19.091639Z",
     "start_time": "2021-12-20T02:07:19.060936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##---------------------------------------------------------------     \n",
    "## part one: warm up\n",
    "##--------------------------------------------------------------- \n",
    "# print(current_iter_a , opt['train']['alpha_iter_alternate'],flag)\n",
    "# stop_iter = current_iter_a +  opt['train']['alpha_iter_alternate']\n",
    "# print(f\" Run iteration {current_iter_a+1} to {stop_iter+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01179d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T02:16:10.956462Z",
     "start_time": "2021-12-20T02:16:10.915819Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter_a, stop_iter, flag)\n",
    "# print(current_iter_a , opt['train']['alpha_iter_alternate'],flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8deabd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if flag == 'update_alpha':\n",
    "\n",
    "    stop_iter = current_iter_a +  opt['train']['alpha_iter_alternate']\n",
    "    print(f\" Current Alpha iteration {current_iter_a} - Run  from {current_iter_a+1} to {stop_iter+1}\")\n",
    "    \n",
    "    with tnrange(current_iter_a+1, stop_iter+1 , initial = current_iter_a+1, total = stop_iter+1, position=0, leave= True, desc=\"weight training\") as t :\n",
    "        for current_iter_a in t:    \n",
    "            current_iter += 1\n",
    " \n",
    "            batch_idx_a, batch = next(batch_enumerator2)\n",
    "            environ.set_inputs(batch, train2_loader.dataset.input_size)\n",
    "\n",
    "            if batch_idx_a == len(train2_loader):\n",
    "                print_dbg(f\" Re-enumerate train2_loader  batch_idx_a: {batch_idx_a}   len(train2_loader) = {len(train2_loader)}\", verbose=False)                \n",
    "                batch_enumerator2 = enumerate(train2_loader,1)        \n",
    "                  \n",
    "            print_heading(f\"{timestring()} - ENVIRON.OPTIMIZE()    flag: {flag}    current_iter: {current_iter}   \\n\"\n",
    "                          f\" current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                          f\" current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \\n\"\n",
    "                          f\" is_policy: {opt['policy']}   num_train_layers: {num_train_layers}  hard_sampling: {opt['train']['hard_sampling']}\\n\"\n",
    "                          f\" is_curriculum: {opt['is_curriculum']}     curriculum_speed: {opt['curriculum_speed']}   p_epoch: {p_epoch}\"\n",
    "                          , verbose = False) \n",
    "    \n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = (p_epoch // opt['curriculum_speed']) + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "\n",
    "            print_dbg(f\" num_train_layers  : {num_train_layers}\", verbose = False)\n",
    "\n",
    "\n",
    "            environ.optimize(opt['lambdas'], \n",
    "                             is_policy=opt['policy'], \n",
    "                             flag=flag, \n",
    "                             num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'],\n",
    "                             verbose = False)\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss_2(current_iter, start_time, verbose=True)\n",
    "                environ.resize_results()\n",
    "                # environ.visual_policy(current_iter)\n",
    "\n",
    "            print_heading(f\"{timestring()} - CONTINUE ALPHA TRAINING    current_iter: {current_iter}\\n\"\n",
    "                          f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                          f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \", \n",
    "                          verbose = False )      \n",
    "    \n",
    "    ## if (current_iter_a % alpha_iter_alternate) == 0 \n",
    "    if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "        print_dbg(f\"** Switch training to update_weight\")                \n",
    "        print_heading(f\"{timestring()} - SWITCH TO WEIGHT TRAINING  urrent_iter: {current_iter}\\n\"\n",
    "                      f\"{' ':15s} current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w}   weight_iter_alternate: {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                      f\"{' ':15s} current_iter_a: {current_iter_a}  batch_idx_a:{batch_idx_a}   alpha_iter_alternate : {opt['train']['alpha_iter_alternate']} \",\n",
    "                      verbose = True )       \n",
    "        \n",
    "        flag = 'update_w'\n",
    "        environ.fix_alpha()\n",
    "        environ.free_w(opt['fix_BN'])\n",
    "        environ.decay_temperature()\n",
    "\n",
    "        # print the distribution\n",
    "        dists = environ.get_policy_prob()\n",
    "\n",
    "        print(np.concatenate(dists, axis=-1))\n",
    "        p_epoch += 1\n",
    "        print(f\"** p_epoch incremented: {p_epoch}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efcd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T04:06:20.995174Z",
     "start_time": "2021-12-21T04:06:20.972230Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320533e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-21T21:26:28.603565Z",
     "start_time": "2021-12-21T21:26:28.286326Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" task1_logits: \\n {environ.networks['mtl-net'].task1_logits.detach().cpu().numpy()} \\n\")\n",
    "print(f\" task2_logits: \\n {environ.networks['mtl-net'].task2_logits.detach().cpu().numpy()} \\n\")\n",
    "print(f\" task3_logits: \\n {environ.networks['mtl-net'].task3_logits.detach().cpu().numpy()} \\n\")\n",
    "print(f\" task1 softmax: \\n {softmax(environ.networks['mtl-net'].task1_logits.detach().cpu().numpy(), axis = -1)} \\n\")\n",
    "print(f\" task2 softmax: \\n {softmax(environ.networks['mtl-net'].task2_logits.detach().cpu().numpy(), axis = -1)} \\n\")\n",
    "print(f\" task3 softmax: \\n {softmax(environ.networks['mtl-net'].task3_logits.detach().cpu().numpy(), axis = -1)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a32b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:06:36.002509Z",
     "start_time": "2021-12-17T00:06:35.981793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for i in [1,2,3]:\n",
    "#     task_pred = f\"task{i}_pred\"\n",
    "#     task_logits = f\"task{i}_logits\"\n",
    "#     policy_attr = f\"policy{i}\"\n",
    "#     logits_attr = f\"logit{i}\"\n",
    "#     print_heading(f\"{task_pred}\")\n",
    "#     print(getattr(environ, task_pred))\n",
    "#     print(policy_attr)\n",
    "#     print(getattr(environ, policy_attr)) \n",
    "#     print(logits_attr)\n",
    "#     print(getattr(environ, logits_attr)) \n",
    "#     print(task_logits)\n",
    "#     print(getattr(environ.networks['mtl-net'], task_logits)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c1877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T01:06:53.675694Z",
     "start_time": "2021-12-20T01:06:53.648732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tqdm.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fce8c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T01:12:07.284532Z",
     "start_time": "2021-12-20T01:12:07.262680Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current_iter    = 2174\n",
    "current_iter_a  = 348\n",
    "current_iter_w  = 348"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e101600",
   "metadata": {},
   "source": [
    "## Load previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1db09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T20:30:30.175802Z",
     "start_time": "2022-01-19T20:30:30.149533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " command line parms :  {'config': 'yamls/adashare/chembl_2task.yml', 'exp_instance': None, 'exp_ids': [0], 'batch_size': 9999, 'backbone_lr': None, 'task_lr': None, 'decay_lr_rate': None, 'decay_lr_freq': None, 'gpus': [0], 'cpu': True}\n",
      "Namespace(config='yamls/adashare/chembl_2task.yml', exp_instance=None, exp_ids=[0], batch_size=9999, backbone_lr=None, task_lr=None, decay_lr_rate=None, decay_lr_freq=None, gpus=[0], cpu=True)\n",
      "\n",
      "0119_1230 yamls/adashare/chembl_2task.yml\n"
     ]
    }
   ],
   "source": [
    "input_args = \" --config yamls/adashare/chembl_2task.yml --cpu --batch_size 09999\".split()\n",
    "# get command line arguments\n",
    "args = get_command_line_args(input_args)\n",
    "print(args)\n",
    "\n",
    "print()\n",
    "\n",
    "if args.exp_instance is None:\n",
    "    args.exp_instance = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    \n",
    "print(args.exp_instance, args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b88b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T20:31:01.630624Z",
     "start_time": "2022-01-19T20:31:01.595882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "####################READ YAML#####################\n",
      "##################################################\n",
      "[0] [0] best\n"
     ]
    }
   ],
   "source": [
    "print_separator('READ YAML')\n",
    "opt, gpu_ids, exp_ids =read_yaml_from_input(args)\n",
    "print(gpu_ids, exp_ids,  opt['train']['policy_iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3afa36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:46:09.100795Z",
     "start_time": "2021-12-20T23:46:08.702295Z"
    }
   },
   "outputs": [],
   "source": [
    "current_iter = environ.load_checkpoint('latest')\n",
    "\n",
    "print('Evaluating the snapshot saved at %d iter' % current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc853a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:46:09.100795Z",
     "start_time": "2021-12-20T23:46:08.702295Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate'] = opt['train'].get('alpha_iter_alternate'  , len(train2_loader))\n",
    "\n",
    "print(opt['train']['weight_iter_alternate'], opt['train']['alpha_iter_alternate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9316b4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Softmax & Gumbel Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62dddc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Softmax, LogSoftMax, NegLogLikelihood and Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f6f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:41:36.552551Z",
     "start_time": "2021-12-17T00:41:36.484747Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# print(nn.CrossEntropyLoss.__doc__)\n",
    "loss = nn.CrossEntropyLoss(reduction ='none')\n",
    "# i1 = torch.randn(3, 5, requires_grad=True)\n",
    "# t1 = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "# print(i1)\n",
    "# print(i1)\n",
    "# print(t1)\n",
    "# output = loss(i1, t1)\n",
    "# print(output, output.sum(), output.mean())\n",
    "\n",
    "# i2 = torch.randn(1, 2, requires_grad=True)\n",
    "i0 = torch.tensor([[0.0, 1.0]], dtype=torch.float)\n",
    "i1 = torch.tensor([[1.0, 0.0]], dtype=torch.float)\n",
    "i2 = torch.tensor([[0.5, 0.5]], dtype=torch.float)\n",
    "i3 = torch.tensor([[0.4656, 0.5388]], dtype=torch.float)\n",
    "sm = nn.Softmax(dim =-1)\n",
    "lsm = nn.LogSoftmax(dim = -1)\n",
    "nll = nn.NLLLoss(reduction='none')\n",
    "\n",
    "t1 = torch.tensor([1], dtype=torch.int64)\n",
    "t0 = torch.tensor([0], dtype=torch.int64)\n",
    "t2 = torch.tensor([2], dtype=torch.int64)\n",
    "print('i0     : ', i0)\n",
    "print('sm(i0) : ', sm(i0))\n",
    "print('lsm(i0): ', lsm(i0))\n",
    "print()\n",
    "print('i1     : ', i1)\n",
    "print('sm(i1) : ', sm(i1))\n",
    "print('lsm(i1): ', lsm(i1))\n",
    "print()\n",
    "print('i2     : ', i2)\n",
    "print('sm(i2) : ', sm(i2))\n",
    "print('lsm(i2): ', lsm(i2))\n",
    "print()\n",
    "\n",
    "print('t0: ',t0)\n",
    "print('t1: ',t1)\n",
    "print()\n",
    "output1 = loss(i0, t0)\n",
    "output2 = nll(lsm(i0), t0)\n",
    "print('loss [0,1] and [0] : ', output1)\n",
    "print('nll between lsm(i0): ', output2)\n",
    "print()\n",
    "output1 = loss(i0, t1)\n",
    "output2 = nll(lsm(i0), t1)\n",
    "print('loss [0,1] and [1] : ', output1)\n",
    "print('nll between lsm(i0): ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i1, t0)\n",
    "output2 = nll(lsm(i1), t0)\n",
    "print('loss [1,0] and [0] : ', output1)\n",
    "print('nll between lsm(i1): ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i1, t1)\n",
    "output2 = nll(lsm(i1), t1)\n",
    "print('loss [1,0] and [1] : ', output1)\n",
    "print('nll between lsm(i1): ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i2, t0)\n",
    "output2 = nll(lsm(i2), t0)\n",
    "print('loss [0.5, 0.5] and [0] : ', output1)\n",
    "print('nll between lsm(i1)   and [0] : ', output2)\n",
    "print()\n",
    "\n",
    "output1 = loss(i2, t1)\n",
    "output2 = nll(lsm(i2), t1)\n",
    "print('loss [0.5, 0.5] and [1] : ', output1)\n",
    "print('nll between lsm(i1)   and [1] : ', output2)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b2691",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46139b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T22:21:49.726731Z",
     "start_time": "2021-12-03T22:21:49.391724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.5000, 0.5000],\n",
    "        [0.5000, 0.5000],\n",
    "        [0.5000, 0.5000],\n",
    "        [0.5000, 0.5000]], device='cuda:0', requires_grad=True) \n",
    "\n",
    "b = torch.tensor([0.5000, 0.5000,  0.5000, 0.5000], device='cuda:0', requires_grad=True) \n",
    "print(b.shape)\n",
    "c = torch.tensor([[0.000, 0.000,  0.000, 0.000]], device='cuda:0', requires_grad=True) \n",
    "print(c.shape)\n",
    "d = torch.tensor([[0.5000], [0.5000],  [0.5000], [0.5000]], device='cuda:0', requires_grad=True) \n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70dfbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:41:45.109937Z",
     "start_time": "2021-12-17T00:41:45.086013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(i0)\n",
    "print(i1)\n",
    "print(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd0c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T00:52:33.490328Z",
     "start_time": "2021-12-17T00:52:33.457768Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp  = 2.5\n",
    "print(F.gumbel_softmax( i1, temp, hard=False))\n",
    "print(F.gumbel_softmax( i1, temp, hard=False))\n",
    "print(F.gumbel_softmax( i1, temp, hard=False))\n",
    "print()\n",
    "\n",
    "print(F.gumbel_softmax( i2, temp, hard=False))\n",
    "print(F.gumbel_softmax( i2, temp, hard=False))\n",
    "print(F.gumbel_softmax( i2, temp, hard=False))\n",
    "print()\n",
    "\n",
    "print(F.gumbel_softmax( i3, temp, hard=False))\n",
    "print(F.gumbel_softmax( i3, temp, hard=False))\n",
    "print(F.gumbel_softmax( i3, temp, hard=False))\n",
    "# print(F.gumbel_softmax( d, 5, hard=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875518ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:54.606453Z",
     "start_time": "2021-12-01T01:34:54.586164Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits = torch.randn(20, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0890b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:57.902781Z",
     "start_time": "2021-12-01T01:34:57.880455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(logits[:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85ce6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:44:54.225922Z",
     "start_time": "2021-12-01T01:44:54.204482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format)\n",
    "print(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac484d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:45:28.342309Z",
     "start_time": "2021-12-01T01:45:28.320145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(tmp[0])\n",
    "tmp1 = tmp.exponential_()\n",
    "print(tmp1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b9eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:45:55.855272Z",
     "start_time": "2021-12-01T01:45:55.825274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp2 = tmp1.log()\n",
    "print(tmp2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5db59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:43:07.405165Z",
     "start_time": "2021-12-01T01:43:07.383324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " gumbels = (\n",
    "        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35984a4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:43:29.214582Z",
     "start_time": "2021-12-01T01:43:29.193794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(gumbels.shape)\n",
    "print(gumbels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65270223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:35:04.984426Z",
     "start_time": "2021-12-01T01:35:04.949756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample soft categorical using reparametrization trick:\n",
    "gumbel_soft = F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "\n",
    "# Sample hard categorical using \"Straight-through\" trick:\n",
    "gumbel_hard  = F.gumbel_softmax(logits, tau=1, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89520b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:37:20.518987Z",
     "start_time": "2021-12-01T01:37:20.490071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(logits.shape)\n",
    "print(logits[0])\n",
    "print(np.argmax(logits[0]))\n",
    "print('\\n')\n",
    "\n",
    "print(gumbel_soft.shape)\n",
    "print(gumbel_soft[0])\n",
    "print(np.argmax(gumbel_soft[0]))\n",
    "print('\\n')\n",
    "\n",
    "print(gumbel_hard.shape)\n",
    "print(gumbel_hard[0])\n",
    "print(np.argmax(gumbel_hard[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b49944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:11.889231Z",
     "start_time": "2021-12-01T01:47:11.865177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gumbel_soft.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8b677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:20.629635Z",
     "start_time": "2021-12-01T01:47:20.607058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tau = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376a731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:21.278402Z",
     "start_time": "2021-12-01T01:47:21.254276Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.empty_like(logits, memory_format=torch.legacy_contiguous_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb14b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:21.920473Z",
     "start_time": "2021-12-01T01:47:21.899432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e5688",
   "metadata": {
    "hidden": true
   },
   "source": [
    "fill tensor `a` with elements drawn from exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636a394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:32.866091Z",
     "start_time": "2021-12-01T01:47:32.838226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e = a.exponential_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e5bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:34.003871Z",
     "start_time": "2021-12-01T01:47:33.978450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682752d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "draw natural log `ln()` on elements of a_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8f568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:47.286259Z",
     "start_time": "2021-12-01T01:47:47.265716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e_l = a_e.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46e180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:47.554155Z",
     "start_time": "2021-12-01T01:47:47.532995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_e_l[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86dab1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Neg log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841bf82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:47:51.788290Z",
     "start_time": "2021-12-01T01:47:51.763038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_el_neg = -a_e_l\n",
    "\n",
    "a_el_neg[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991926d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:08.613360Z",
     "start_time": "2021-12-01T01:48:08.591331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2849ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:09.724747Z",
     "start_time": "2021-12-01T01:48:09.701886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gumbels = (logits + a_el_neg) / tau \n",
    "\n",
    "gumbels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b2b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:14.682363Z",
     "start_time": "2021-12-01T01:48:14.660407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dim = -1\n",
    "gumbels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28f692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:15.779652Z",
     "start_time": "2021-12-01T01:48:15.758743Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " y_soft = gumbels.softmax(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09316403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:17.797567Z",
     "start_time": "2021-12-01T01:48:17.776922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09910a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:48:19.084522Z",
     "start_time": "2021-12-01T01:48:19.062543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_soft[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a373cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T21:41:56.235154Z",
     "start_time": "2021-09-22T21:41:56.228186Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = y_soft.max(dim, keepdim=True)\n",
    "print(index[0].T)\n",
    "print(index[1].T)\n",
    "y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index[1], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6dc2ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T21:42:54.751798Z",
     "start_time": "2021-09-22T21:42:54.744929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.argmax(y_hard,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3e1f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:43:17.720809Z",
     "start_time": "2021-09-24T03:43:17.662341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp_d= [0,1,0]\n",
    "for i in range(10):\n",
    "    sampled = np.random.choice((2, 1, 0), p=tmp_d)\n",
    "    print(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0baccc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1716e81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T08:43:47.166466Z",
     "start_time": "2022-01-12T08:43:47.133961Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:12.104370Z",
     "iopub.status.busy": "2022-01-07T22:44:12.103595Z",
     "iopub.status.idle": "2022-01-07T22:44:12.133070Z",
     "shell.execute_reply": "2022-01-07T22:44:12.131731Z",
     "shell.execute_reply.started": "2022-01-07T22:44:12.104302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "\n",
    "# cuda_device = 0 \n",
    "\n",
    "# def free_gpu_cache(cuda_device):\n",
    "#     print(\"Initial GPU Usage\")    \n",
    "#     gpu_usage()                             \n",
    "#     print(\"GPU Usage after emptying the cache\")\n",
    "#     gpu_usage()\n",
    "#     print(\"CUDA empty cache\")\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(\"Close and reopen device\")\n",
    "#     cuda.select_device(cuda_device)\n",
    "#     print(\"Close device\")    \n",
    "#     cuda.close()\n",
    "#     print(\"Reopen device\")    \n",
    "#     cuda.select_device(cuda_device)\n",
    "#     print(\"GPU Usage after closing and reopening\")\n",
    "#     gpu_usage()\n",
    "\n",
    "# free_gpu_cache(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81188503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T21:45:43.118975Z",
     "start_time": "2022-01-07T21:45:43.089201Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def print_separator(text, total_len=50):\n",
    "#     print('#' * total_len)\n",
    "#     left_width = (total_len - len(text))//2\n",
    "#     right_width = total_len - len(text) - left_width\n",
    "#     print(\"#\" * left_width + text + \"#\" * right_width)\n",
    "#     print('#' * total_len)\n",
    "\n",
    "# def print_dbg(text, verbose = False):\n",
    "#     if verbose:\n",
    "#         print(text)\n",
    "\n",
    "# @debug_off\n",
    "# def print_heading(text,  verbose = False):\n",
    "#     len_ttl = max(len(text)+4, 50)\n",
    "#     if verbose:\n",
    "#         print('-' * len_ttl)\n",
    "#         print(f\" {text}\")\n",
    "#         # left_width = (total_len - len(text))//2\n",
    "#         # right_width = total_len - len(text) - left_width\n",
    "#         # print(\"#\" * left_width + text + \"#\" * right_width)\n",
    "#         print('-' * len_ttl,'\\n')\n",
    "\n",
    "# print_heading(\"hello_kevin\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f559c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Chembl Data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a1002b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-12T17:06:53.545985Z",
     "start_time": "2022-01-12T17:06:53.520879Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.909003Z",
     "iopub.status.busy": "2022-01-07T22:44:13.907310Z",
     "iopub.status.idle": "2022-01-07T22:44:13.953692Z",
     "shell.execute_reply": "2022-01-07T22:44:13.952354Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.908963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dataroot = opt['dataload']['dataroot']\n",
    "# ecfp     = load_sparse(dataroot, opt['dataload']['x'])\n",
    "\n",
    "# total_input = ecfp.shape[0]\n",
    "# ranges      = (np.cumsum([0]+opt['dataload']['x_split_ratios'])* total_input).astype(np.int32)\n",
    "\n",
    "\n",
    "# idx_train  = np.arange(ranges[0], ranges[1])\n",
    "# idx_train1 = np.arange(ranges[1], ranges[2])\n",
    "# idx_train2 = np.arange(ranges[2], ranges[3])\n",
    "# idx_val    = np.arange(ranges[3], ranges[4])\n",
    "\n",
    "# print(f\" Total input    :  {total_input}   Cummulative dataset sizes: {ranges}\")\n",
    "# print(f\" Ranges         :  {ranges}\")\n",
    "# print()\n",
    "# print(f\" X Dataset      :  {os.path.join(opt['dataload']['dataroot'], opt['dataload']['x'])}\")\n",
    "# print(f\" y Dataset      :  {os.path.join(opt['dataload']['dataroot'], opt['dataload']['y_tasks'][0])}\")\n",
    "# print(f\" Folding Dataset:  {os.path.join(opt['dataload']['dataroot'], opt['dataload']['folding'])}\")\n",
    "# print(f\" Weights_class  :  {opt['dataload']['weights_class']}\")\n",
    "# print()\n",
    "# print(f' idx_train    dataset size: {len(idx_train)  :6d}  - rows: {(idx_train)} ')\n",
    "# print(f' idx_train1   dataset size: {len(idx_train1) :6d}  - rows: {(idx_train1)} ')\n",
    "# print(f' idx_train2   dataset size: {len(idx_train2) :6d}  - rows: {(idx_train2)} ')\n",
    "# print(f' val_train    dataset size: {len(idx_val)    :6d}  - rows: {(idx_val)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6083b24",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test dataloader output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eddc639f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T02:25:55.121332Z",
     "start_time": "2022-01-07T02:25:55.026894Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# val_batch_idx, val_batch = next(val_enumerator)\n",
    "# print(type(val_batch['row_id']),val_batch['row_id'][0], val_batch['row_id'][-1] )\n",
    "\n",
    "# ctr = 0\n",
    "# for i in range(100):\n",
    "#     val_batch_1 = next(val_loader)\n",
    "#     print(' iteration: ', ctr,' len: ', len(val_batch_1['row_id']),'start: [', val_batch_1['row_id'][0],   val_batch_1['row_id'][-1],']' )\n",
    "#     ctr += 1\n",
    "\n",
    "# ctr = 0    \n",
    "# for val_batch_1 in iter(val_loader):\n",
    "# #     val_batch_1 = next(val_loader)\n",
    "#     print(' iteration: ', ctr,' len: ', len(val_batch_1['row_id']),'start: [', val_batch_1['row_id'][0],   val_batch_1['row_id'][-1],']' )\n",
    "#     ctr += 1    \n",
    "#     if ctr == 105:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e60b2eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T02:25:55.212013Z",
     "start_time": "2022-01-07T02:25:55.124386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# val_batch_1 = next(val_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf2511b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T02:25:55.240326Z",
     "start_time": "2022-01-07T02:25:55.216394Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  batch_idx, batch = next(batch_enumerator)\n",
    "\n",
    "# print(batch.keys())\n",
    "# print(batch['x_ind'].shape)\n",
    "# print(type(batch['batch_size']))\n",
    "# for i in batch.keys():\n",
    "#     if not isinstance(batch[i], int):\n",
    "#         print(i, batch[i].shape)\n",
    "\n",
    "# task0_Y =  torch.sparse_coo_tensor(\n",
    "#         batch[\"task0_ind\"],\n",
    "#         batch[\"task0_data\"],\n",
    "#         size = [batch[\"batch_size\"], 5]).to(\"cpu\", non_blocking=True).to_dense().numpy()\n",
    "\n",
    "# print(task0_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7991589c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T02:25:55.275968Z",
     "start_time": "2022-01-07T02:25:55.244246Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(f\" train_loader: dataset input size       :  {train_loader.dataset.input_size}\")\n",
    "# print(f\" train_loader: class output size        :  {train_loader.dataset.class_output_size}\")\n",
    "# print()\n",
    "# print(f\" size of training set 0 (warm up)       :  {len(trainset)}\")\n",
    "# print(f\" size of training set 1 (network parms) :  {len(trainset1)}\")\n",
    "# print(f\" size of training set 2 (policy weights):  {len(trainset2)}\")\n",
    "# print(f\" size of validation set                 :  {len(valset)}\")\n",
    "# print(f\"                                Total   :  {len(trainset)+len(trainset1)+len(trainset2)+len(valset)}\")\n",
    "\n",
    "# print(f\" batch size       : {opt['train']['batch_size']}\")\n",
    "# print(f' len train_loader : {len(train_loader)}')\n",
    "# print(f' len train1_loader: {len(train1_loader)}')\n",
    "# print(f' len train2_loader: {len(train2_loader)}')\n",
    "# print(f' len val_loader   : {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4bc41",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d5a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:42:25.945090Z",
     "start_time": "2021-12-20T22:42:25.917655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49b9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:41:50.353599Z",
     "start_time": "2021-12-20T22:41:50.331414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "curr_iter_t  = 0\n",
    "curr_iter_a  = 0\n",
    "curr_iter_w  = 0\n",
    "stop_iter_t  = 0\n",
    "stop_iter_w  = 0 \n",
    "stop_iter_a  = 0\n",
    "total_weight_epochs = 0\n",
    "total_policy_epochs = 0 \n",
    "train_total_iters = 8\n",
    "weight_iter_alternate = 17\n",
    "alpha_iter_alternate = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7018b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:41:50.588581Z",
     "start_time": "2021-12-20T22:41:50.555942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(curr_iter_t, stop_iter_t, flag, train_total_iters,opt['train']['print_freq'] )\n",
    "start_iter_t = curr_iter_t\n",
    "stop_iter_t = curr_iter_t +  train_total_iters \n",
    "print(f\" Current iteration {curr_iter_t} - Run  from {start_iter_t} to {stop_iter_t}\")\n",
    "\n",
    "print(curr_iter_w, weight_iter_alternate , flag)\n",
    "stop_iter_w = curr_iter_w +  weight_iter_alternate \n",
    "print(f\" Current Weight iteration {curr_iter_w} - Run  from {curr_iter_w+1} to {stop_iter_w}\")\n",
    "\n",
    "\n",
    "print(curr_iter_a ,  alpha_iter_alternate ,flag)\n",
    "stop_iter_a = curr_iter_a +  alpha_iter_alternate \n",
    "print(f\" Current alpha iteration {curr_iter_a} - Run  from {curr_iter_a+1} to {stop_iter_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839ecee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:37:56.218327Z",
     "start_time": "2021-12-20T22:37:10.168700Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del t, t_w, t_a\n",
    "main_iter_ctr = 0 \n",
    "with tnrange(start_iter_t , stop_iter_t  , initial = start_iter_t , total = stop_iter_t, position=0, leave= True, desc=\"master\") as t :\n",
    "    for curr_t in t:\n",
    "        \n",
    "        with  tnrange(0, weight_iter_alternate , initial = 0, total = weight_iter_alternate, \n",
    "                      position=1, leave= False, desc=f\"epoch {curr_t} weight training\") as t_w :\n",
    "            for curr_w in t_w:    \n",
    "                sleep(0.35)\n",
    "                main_iter_ctr += 1\n",
    "                curr_iter_w  = curr_w\n",
    "                t.set_postfix({'epoch': f\"{curr_t}/{train_total_iters}\", 'main_iter_ctr': main_iter_ctr})\n",
    "                t_w.set_postfix({'weight training epoch': curr_t, 'batch #': curr_iter_w})\n",
    "\n",
    "            print(f\"** Epoch {curr_t}/{train_total_iters} weight training complete - Loss: \"\n",
    "                  f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_t:{curr_t}  main_iter_ctr:{main_iter_ctr}\" )\n",
    "                 \n",
    "        \n",
    "        with  tnrange(0, alpha_iter_alternate  , initial = 0, total = alpha_iter_alternate , \n",
    "                      position=2, leave= False, desc=f\"epoch {curr_t} policy training\") as t_a :\n",
    "            for curr_a in t_a:    \n",
    "                sleep(0.35)\n",
    "                main_iter_ctr += 1                \n",
    "                curr_iter_a = curr_a\n",
    "                t.set_postfix({'epoch': f\"{curr_t}/{train_total_iters}\", 'main_iter_ctr':main_iter_ctr})\n",
    "                t_a.set_postfix({'policy training epoch': curr_t, 'batch #': curr_iter_a})            \n",
    "                \n",
    "            print(f\"** Epoch {curr_t}/{train_total_iters} policy training complete - Loss: \"\n",
    "                  f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_t:{curr_t}  main_iter_ctr:{main_iter_ctr}\" )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2c8ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:04:03.776382Z",
     "start_time": "2021-12-20T23:04:03.746093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "curr_iter_t  = 0\n",
    "curr_iter_a  = 0\n",
    "curr_iter_w  = 0\n",
    "stop_iter_t  = 0\n",
    "stop_iter_w  = 0 \n",
    "stop_iter_a  = 0\n",
    "total_weight_epochs = 0\n",
    "total_policy_epochs = 0 \n",
    "train_total_iters = 100\n",
    "train_total_epochs = 10\n",
    "weight_iter_alternate = 17\n",
    "alpha_iter_alternate = 17\n",
    "\n",
    "print(curr_iter_t, stop_iter_t, flag, train_total_iters,opt['train']['print_freq'] )\n",
    "start_iter_t = curr_iter_t\n",
    "stop_iter_t = curr_iter_t +  train_total_iters \n",
    "print(f\" Current iteration {curr_iter_t} - Run  from {start_iter_t} to {stop_iter_t}\")\n",
    "\n",
    "print(curr_iter_w, weight_iter_alternate , flag)\n",
    "stop_iter_w = curr_iter_w +  weight_iter_alternate \n",
    "print(f\" Current Weight iteration {curr_iter_w} - Run  from {curr_iter_w+1} to {stop_iter_w}\")\n",
    "\n",
    "\n",
    "print(curr_iter_a ,  alpha_iter_alternate ,flag)\n",
    "stop_iter_a = curr_iter_a +  alpha_iter_alternate \n",
    "print(f\" Current alpha iteration {curr_iter_a} - Run  from {curr_iter_a+1} to {stop_iter_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba7fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T23:08:57.280036Z",
     "start_time": "2021-12-20T23:08:41.214078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del t, t_w, t_a\n",
    "curr_epoch = 0\n",
    "main_iter_ctr = 0 \n",
    "# with tnrange(start_iter_t , stop_iter_t  , initial = start_iter_t , total = stop_iter_t, position=0, leave= True, desc=\"master\") as t :\n",
    "# with tqdm_notebook(total=train_total_epochs) as t:\n",
    "t = tqdm_notebook(total=train_total_epochs)\n",
    "\n",
    "while curr_epoch < train_total_epochs:\n",
    "    curr_epoch+=1\n",
    "    t.update(1)\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the network weights\n",
    "    #-----------------------------------------        \n",
    "    with  tnrange(0, weight_iter_alternate , initial = 0, total = weight_iter_alternate, \n",
    "                  position=1, leave= False, desc=f\"epoch {curr_epoch} weight training\") as t_w :\n",
    "        for curr_w in t_w:    \n",
    "            sleep(0.35)\n",
    "            main_iter_ctr += 1\n",
    "            curr_iter_w  = curr_w\n",
    "\n",
    "            t.set_postfix({'epoch': f\"{curr_epoch}/{train_total_epochs}\", 'main_iter_ctr': main_iter_ctr})\n",
    "            t_w.set_postfix({'weight training epoch': curr_epoch, 'batch #': curr_iter_w})\n",
    "\n",
    "        tqdm.write(f\"** Epoch {curr_epoch}/{train_total_epochs} weight training complete - Loss: \"\n",
    "              f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_epoch:{curr_epoch}  main_iter_ctr:{main_iter_ctr}\" )\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # Train & Update the  policy \n",
    "    #-----------------------------------------        \n",
    "    with  tnrange(0, alpha_iter_alternate  , initial = 0, total = alpha_iter_alternate , \n",
    "                  position=2, leave= False, desc=f\"epoch {curr_epoch} policy training\") as t_a :\n",
    "        for curr_a in t_a:    \n",
    "            sleep(0.35)\n",
    "            main_iter_ctr += 1                \n",
    "            curr_iter_a = curr_a\n",
    "\n",
    "            t.set_postfix({'epoch': f\"{curr_epoch}/{train_total_epochs}\", 'main_iter_ctr':main_iter_ctr})\n",
    "            t_a.set_postfix({'policy training epoch': curr_epoch, 'batch #': curr_iter_a})            \n",
    "\n",
    "        tqdm.write(f\"** Epoch {curr_epoch}/{train_total_epochs} policy training complete - Loss: \"\n",
    "              f\"curr_w:{curr_w}    curr_iter_w:{curr_iter_w}  curr_epoch:{curr_epoch}  main_iter_ctr:{main_iter_ctr}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141aa027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-20T22:11:57.104094Z",
     "start_time": "2021-12-20T22:11:57.081272Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with tnrange(start_iter, stop_iter , initial = current_iter_w, total = stop_iter,  position=0, leave= True, desc=\"training\") as t:\n",
    "#     for current_iter_w in t:\n",
    "#         print(current_iter_w)\n",
    "#         current_iter_w += 1\n",
    "#         print(current_iter_w)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3dbaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T21:23:27.031154Z",
     "start_time": "2021-12-15T21:23:27.007124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start = current_iter\n",
    "# end = current_iter + opt['train']['warm_up_iters']\n",
    "# curr_range = range(start,end)\n",
    "# print(start, end)\n",
    "\n",
    "# for i in tqdm.notebook.tnrange(start, end, initial = start, total = end):\n",
    "#     sleep(0.25)\n",
    "#     current_iter += 1\n",
    "# #     print(i)\n",
    "#     pass\n",
    "\n",
    "# print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d53f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T21:23:27.061710Z",
     "start_time": "2021-12-15T21:23:27.035963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start = current_iter\n",
    "# end = current_iter + opt['train']['warm_up_iters']\n",
    "# curr_range = range(start,end)\n",
    "# print(start, end)\n",
    "\n",
    "# for i in tqdm.notebook.tqdm_notebook(cur_range, initial = start, total = end, disable=False, position=0, desc = \"validation\"):\n",
    "#     current_iter += 1\n",
    "#     pass\n",
    "\n",
    "# print(current_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0cd55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T20:40:03.819876Z",
     "start_time": "2021-12-14T20:40:03.768686Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import trange\n",
    "# from time import sleep\n",
    "\n",
    "# for i in trange(40, desc='1st loop', position=0, leave = False):\n",
    "#     sleep(1.1)\n",
    "#     for j in trange(5, desc='2nd loop', position =1, leave = False):\n",
    "#         sleep(0.01)\n",
    "#         for k in trange(50, desc='3rd loop', position =0,leave=False):\n",
    "#             sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92f08b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T18:51:59.433086Z",
     "start_time": "2021-12-15T18:51:59.396500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with tqdm(batch_enumerator, leave=False, disable=False) as t:\n",
    "# with tqdm(total=10, bar_format=\"{postfix[0]} {postfix[1][value]:>8.2g}\", postfix=[\"Batch\", dict(value=0)]) as t:\n",
    "# with trange(opt['train']['warm_up_iters'], bar_format=\"{postfix[0]} {postfix[1][value]:>8.2g}\", postfix=[\"Batch\", dict(value=0)]) as t:\n",
    "# with trange(opt['train']['warm_up_iters']) as t:\n",
    "\n",
    "#     for current_iter in t:\n",
    "#         batch_idx, batch = next(batch_enumerator)\n",
    "#         ran = random.randint(1, 100)\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         environ.train()\n",
    "\n",
    "#         print_heading(f\" {timestring()} - WARMUP Training iter {current_iter}/{opt['train']['warm_up_iters']}    batch_idx: {batch_idx}\"    \n",
    "#                       f\"    Warm-up iters: {opt['train']['warm_up_iters']}\"\n",
    "#                       f\"    Validation freq:  {opt['train']['val_freq']}\", verbose = False)\n",
    "\n",
    "#         if batch_idx == len(train_loader) :\n",
    "#     #         print_heading(f\" ******* {timestring()}  re-enumerate train_loader() *******\")\n",
    "#             batch_enumerator = enumerate(train_loader,1)   \n",
    "\n",
    "#         t.set_postfix({'batch_idx': batch_idx, 'num_vowels': ran})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fee04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T18:51:59.469748Z",
     "start_time": "2021-12-15T18:51:59.436522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import tqdm.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868106a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:45.617713Z",
     "start_time": "2021-12-01T01:34:45.588274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19,20]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15829f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:46.630947Z",
     "start_time": "2021-12-01T01:34:46.608486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(a[:,::-1])\n",
    "print()\n",
    "print(a[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9215987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T01:34:53.104308Z",
     "start_time": "2021-12-01T01:34:53.081558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = 4\n",
    "b = 4\n",
    "c = 1\n",
    "\n",
    "0 // b + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3f558",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Scipy Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1d7fdde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T18:43:27.323846Z",
     "start_time": "2022-01-11T18:43:27.180307Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created y_class # dims: 2    shape: (15, 1)\n",
      "Created y_2 # dims: 2    shape: (15, 0)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "row = np.array([5])\n",
    "col = np.array([0])\n",
    "data = np.array([6])\n",
    "y_class = scipy.sparse.csr_matrix((data, (row, col)), shape=(15,1))\n",
    "y_2 = scipy.sparse.csr_matrix((15,0))\n",
    "print(f\"Created y_class # dims: {y_class.ndim}    shape: {y_class.shape}\")\n",
    "print(f\"Created y_2 # dims: {y_2.ndim}    shape: {y_2.shape}\")\n",
    "\n",
    "# y_class[5]= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcabda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T20:34:45.811804Z",
     "start_time": "2021-09-24T20:34:45.795397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(y_class.toarray().T)\n",
    "print(y_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c27bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T20:33:18.869928Z",
     "start_time": "2021-09-24T20:33:18.853474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_class[8,0]= 999\n",
    "y_class.toarray().squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e9591",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### folding step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de820ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:43.426753Z",
     "start_time": "2021-11-03T17:03:43.366179Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_dev = copy.copy(x_file)\n",
    "print(x_dev.shape, type(x_dev))\n",
    "\n",
    "idx = x_dev.nonzero()\n",
    "print(idx[0][:82])\n",
    "print(idx[1][:82])\n",
    "print(x_dev[0].sum(), x_dev[1].sum())\n",
    "print(x_dev[0,0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3fd2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:43.639800Z",
     "start_time": "2021-11-03T17:03:43.592684Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folding_size = 30\n",
    "print(f\" fold - folding_size:{folding_size}\" )\n",
    "\n",
    "## collapse x into folding_size columns\n",
    "idx = x_dev.nonzero()\n",
    "folded = idx[1] % folding_size\n",
    "\n",
    "print(folded[:82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc03a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:43.985453Z",
     "start_time": "2021-11-03T17:03:43.884440Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_fold = scipy.sparse.csr_matrix((x_dev.data, (idx[0], folded)), shape=(x_dev.shape[0], folding_size))\n",
    "print(x_fold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44a0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:44.191775Z",
     "start_time": "2021-11-03T17:03:44.177092Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_fold.sum_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d88c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:44.485347Z",
     "start_time": "2021-11-03T17:03:44.465953Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d2f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T17:03:44.739813Z",
     "start_time": "2021-11-03T17:03:44.721549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(type(y_files[0]), type(y_class[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d7163",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Torch tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cc7205de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:49:34.676799Z",
     "start_time": "2022-01-11T21:49:34.654094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a = np.random.rand(4,3)\n",
    "# print(a.shape)\n",
    "# print(a)\n",
    "# rows = [1,3,2,0,2]\n",
    "# cols = [0,0,1,2,2]\n",
    "# a1 = a[rows,cols]\n",
    "# print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "44c7858d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T21:49:34.676799Z",
     "start_time": "2022-01-11T21:49:34.654094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a = torch.randn(64, 3, 16, 16)\n",
    "# print(a.shape, a.view(-1).shape, a.permute(0,2,3,1).shape)\n",
    "# b = torch.randint(0,2, [64,3,16,16])\n",
    "# print(b.shape, b.view(-1).shape, b.permute(0,2,3,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d3c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:44:12.409875Z",
     "start_time": "2021-09-24T03:44:12.395721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873b312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:44:14.996701Z",
     "start_time": "2021-09-24T03:44:14.978676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459af2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T03:48:01.639457Z",
     "start_time": "2021-09-24T03:48:01.623099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input.contiguous().view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392718ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478262b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b29ff118",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### using eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd00a86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:14:03.644641Z",
     "start_time": "2021-11-30T21:14:03.622475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(opt['dataload']['y_tasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aa7bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T08:07:39.604778Z",
     "start_time": "2021-10-27T08:07:39.579603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task1 = [1,2,3]\n",
    "task2 = None\n",
    "task3 = {'4': 'Kevin', '5':'Bardool'}\n",
    "\n",
    "for i in [1,2,3]:\n",
    "    print('task{:d}'.format(i))\n",
    "    if eval('task{:d}'.format(i)) is None:\n",
    "        print('task{:d} :  has not been defined '.format(i))\n",
    "#         exec_str = 'task{:d} =  np.random.rand({:d},{:d})'.format(i,3,2)\n",
    "        exec_str = 'y_task{:d} = scipy.sparse.csr_matrix(({:d}, {:d})) '.format(i,3,2)\n",
    "        print(exec_str)\n",
    "        exec(exec_str)\n",
    "        print('task{:d} : '.format(i), eval('task{:d}'.format(i)))\n",
    "        print(eval('type(task{:d})'.format(i)))\n",
    "    else:\n",
    "        print('task{:d} : '.format(i), eval('task{:d}'.format(i)))\n",
    "        \n",
    "print(f\"Created task{i} shape        : {eval('len(task{:d})'.format(i))}\")\n",
    "print(len(task3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce01f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T08:07:44.515337Z",
     "start_time": "2021-10-27T08:07:44.497413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(3,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef347b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T08:07:46.338515Z",
     "start_time": "2021-10-27T08:07:46.319087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(eval('len(task{:d})'.format(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdd594",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load datasets, perform folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:25:56.648521Z",
     "start_time": "2021-12-01T20:25:56.626710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Verify presence of Y label data\n",
    "if (opt['dataload']['y_tasks'] is None) and (opt['dataload']['y_regr'] is None):\n",
    "   raise ValueError(\"No label data specified, please add --y_class and/or --y_regr.\")\n",
    "\n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['x']))  \n",
    "print(os.path.join(opt['dataload']['dataroot'], opt['dataload']['folding']))\n",
    "for fl in opt['dataload']['y_tasks']:\n",
    "    print(os.path.join(opt['dataload']['dataroot'], fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca3c95",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load X data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31c4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:26:58.627148Z",
     "start_time": "2021-12-01T20:26:58.580407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Load data files \n",
    "##\n",
    "# ecfp     = sc.load_sparse(args.x)\n",
    "# y_class  = sc.load_sparse(args.y_class)\n",
    "# y_regr   = sc.load_sparse(args.y_regr)\n",
    "# y_censor = sc.load_sparse(args.y_censor)\n",
    "\n",
    "dataroot = opt['dataload']['dataroot']\n",
    "\n",
    "ecfp     = load_sparse(dataroot, opt['dataload']['x'])\n",
    "# x_file   = copy.copy(ecfp)\n",
    "\n",
    "print(f\" Input    {opt['dataload']['x']} - type : {type(ecfp)} shape : {ecfp.shape}\")\n",
    "# print(f\" Input    {opt['dataload']['x']} - type : {type(x_file)} shape : {x_file.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe1168",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load Y label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948aed46",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730a3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T20:43:36.174351Z",
     "start_time": "2021-11-30T20:43:36.139816Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,( y_task, y_type )in enumerate(zip(opt['dataload']['y_tasks'],opt['sc_tasks']),1):\n",
    "    print(full_path := os.path.join(dataroot, y_task ))\n",
    "    tmp = load_sparse(full_path)\n",
    "#     np.load(full_path, allow_pickle=True) \n",
    "#     print(type(tmp), tmp.shape)\n",
    "#     tmp_sparse = scipy.sparse.csr_matrix(tmp)\n",
    "    print(type(tmp_sparse), tmp_sparse.shape)\n",
    "    print('indicies: ', len(tmp_sparse.__dict__['indices']), tmp_sparse.__dict__['indices'])\n",
    "    print('indptr  : ', len(tmp_sparse.__dict__['indptr']) , tmp_sparse.__dict__['indptr'])\n",
    "    print('data    : ', len(tmp_sparse.__dict__['data'])   , tmp_sparse.__dict__['data'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e32cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:33.745342Z",
     "start_time": "2021-11-30T21:06:33.703310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_class  = load_sparse(dataroot, opt['dataload']['y_tasks'][0])\n",
    "# print(f\" Input     - type : {type(y_class)} shape : {y_class.shape}\")\n",
    "\n",
    "# y_regr  = load_sparse(dataroot, opt['dataload']['y_tasks'][1])\n",
    "# print(f\" Input     - type : {type(y_regr)} shape : {y_regr.shape}\")\n",
    "\n",
    "##\n",
    "## Load Y label files \n",
    "##\n",
    "y_files=[]\n",
    "\n",
    "for i,( y_task, y_type )in enumerate(zip(opt['dataload']['y_tasks'],opt['sc_tasks']),1):\n",
    "    y_tmp = load_sparse(dataroot,  y_task)\n",
    "    print(f\" y_task:{i}  task type: {y_type:5s}  dataset: {y_task} - type : {type(y_tmp)} shape : {y_tmp.shape}\")\n",
    "    ## Get number of positive / neg and total for each classes\n",
    "    num_pos    = np.array((y_tmp == +1).sum(0)).flatten()\n",
    "    num_neg    = np.array((y_tmp == -1).sum(0)).flatten()\n",
    "    num_class  = np.array((y_tmp !=  0).sum(0)).flatten()\n",
    "    if (num_class != num_pos + num_neg).any():\n",
    "        raise ValueError(\"For classification all y values (--y_class/--y) must be 1 or -1.\")\n",
    "    else:\n",
    "        y_files.append(y_tmp)\n",
    "\n",
    "y_class = copy.copy(y_files[0])\n",
    "# y_regr = copy.copy(y_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31defb05",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load folding file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d2773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:35.307158Z",
     "start_time": "2021-11-30T21:06:35.285431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## load folding file\n",
    "##\n",
    "folding_file = os.path.join(dataroot,opt['dataload']['folding'])\n",
    "folding  = np.load(folding_file)\n",
    "print(f\" Folding  {folding_file} - type : {type(folding)} shape : {folding.shape}\")\n",
    "print(f\"          {folding[:20]}\")\n",
    "\n",
    "assert ecfp.shape[0] == folding.shape[0], \"x and folding must have same number of rows\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773adb77",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load Y censor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00530d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.238543Z",
     "start_time": "2021-11-25T18:42:45.220911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_censor = load_sparse(dataroot, opt['dataload']['y_censor'])\n",
    "# if y_censor is not None:\n",
    "#     print(f\" Input     - type : {type(y_censor)} shape : {y_censor.shape}\") \n",
    "\n",
    "##\n",
    "## Load Y censor file\n",
    "##\n",
    "\n",
    "# y_censor = load_sparse(dataroot, opt['dataload']['y_censor'])\n",
    "# if y_censor is None:\n",
    "#     y_censor = scipy.sparse.csr_matrix(y_regr.shape)\n",
    "#     vprint(f\" y_sensor is {opt['dataload']['y_censor']}   Created y_censor shape       : {y_censor.shape}\")\n",
    "    \n",
    "# y_censor_shape = y_censor.shape if y_censor is not None else \"n/a\"\n",
    "# print(f\" y_censor  - type : {type(y_censor)}  shape: {y_censor_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be4c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:03:53.990463Z",
     "start_time": "2021-11-30T21:03:53.968756Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if (y_regr is None) and (y_censor is not None):\n",
    "#     raise ValueError(\"y_censor provided please also provide --y_regr.\")\n",
    "\n",
    "# # if y_class is None:\n",
    "# #     y_class = scipy.sparse.csr_matrix((ecfp.shape[0], 0))\n",
    "# #     vprint(f\"Created y_class shape        : {y_class.shape}\")\n",
    "\n",
    "# if y_regr is None:\n",
    "#     y_regr  = scipy.sparse.csr_matrix((ecfp.shape[0], 0))\n",
    "#     vprint(f\"Created y_regr shape         : {y_regr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb45a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T12:54:02.548598Z",
     "start_time": "2021-10-22T12:54:02.529641Z"
    },
    "hidden": true
   },
   "source": [
    "#### Input folding & transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c0a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:39.945186Z",
     "start_time": "2021-11-30T21:06:39.922823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"args.fold_inputs : {opt['dataload']['fold_inputs']} \\t\\t  transform: {opt['dataload']['input_transform']}\\n\")\n",
    "print(repr(ecfp))\n",
    "ecfp = fold_and_transform_inputs(ecfp, folding_size=opt['dataload']['fold_inputs'], transform=opt['dataload']['input_transform'])\n",
    "print(repr(ecfp))\n",
    "\n",
    "print(type(ecfp), ecfp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8f8e5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  Loading weights files for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c6e9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:06:58.845909Z",
     "start_time": "2021-11-30T21:06:58.817727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# num_regr   = np.bincount(y_regr.indices, minlength=y_regr.shape[1])\n",
    "print(' Classification weights: ',opt['dataload']['weights_class'])\n",
    "tasks_class = load_task_weights(opt['dataload']['weights_class'], y=y_class[0], label=\"y_class\")\n",
    "# tasks_regr  = load_task_weights(opt['dataload']['weights_regr'] , y=y_regr , label=\"y_regr\")\n",
    "\n",
    "print(tasks_class)\n",
    "print(tasks_class.training_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0c5b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:07:06.169693Z",
     "start_time": "2021-11-30T21:07:06.149534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869b822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:07:08.157368Z",
     "start_time": "2021-11-30T21:07:08.129096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(y_files[0].shape, y_files[1].shape, y_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1d75d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:09:55.522938Z",
     "start_time": "2021-11-30T21:09:55.500886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if tasks_class.aggregation_weight is None:\n",
    "    '''\n",
    "    fold classes \n",
    "    '''\n",
    "    ## using min_samples rule\n",
    "    fold_pos, fold_neg = class_fold_counts(y_class, folding)\n",
    "    n = opt['dataload']['min_samples_class']\n",
    "    tasks_class.aggregation_weight = ((fold_pos >= n).all(0) & (fold_neg >= n)).all(0).astype(np.float64)\n",
    "    print(f\" tasks_class.aggregation_weight WAS NOT passed \")\n",
    "    print(f\" min_samples_class: opt['dataload']['min_samples_class']\")\n",
    "    print(f\" Class fold counts: \\n  fold_pos:\\n{fold_pos}  \\n\\n  fold_neg:\\n{fold_neg}\") \n",
    "else:\n",
    "    print(f\"  tasks_class.aggregation_weight passed \")\n",
    "    \n",
    "print(f\" tasks_class.aggregation_weight.shape: {tasks_class.aggregation_weight.shape} \\n {tasks_class.aggregation_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04975aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.554463Z",
     "start_time": "2021-11-25T18:42:45.536741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if tasks_regr.aggregation_weight is None:\n",
    "#     if y_censor.nnz == 0:\n",
    "#         y_regr2 = y_regr.copy()\n",
    "#         y_regr2.data[:] = 1\n",
    "#     else:\n",
    "#         ## only counting uncensored data\n",
    "#         y_regr2      = y_censor.copy()\n",
    "#         y_regr2.data = (y_regr2.data == 0).astype(np.int32)\n",
    "  \n",
    "#     fold_regr, _ = sc.class_fold_counts(y_regr2, folding)\n",
    "#     del y_regr2\n",
    "#     tasks_regr.aggregation_weight = (fold_regr >= args.min_samples_regr).all(0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ece76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:10:10.546093Z",
     "start_time": "2021-11-30T21:10:07.896313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Display dataset dimensions \n",
    "##\n",
    "print(f\"Input dimension      : {ecfp.shape[1]}\")\n",
    "print(f\"#samples             : {ecfp.shape[0]}\")\n",
    "print(f\"#classification tasks: {y_class[0].shape[1]}\")\n",
    "print(f\"Using {(tasks_class.aggregation_weight > 0).sum()} classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\")\n",
    "\n",
    "# vprint(f\"#regression tasks    : {y_regr.shape[1]}\")\n",
    "# vprint(f\"Using {(tasks_regr.aggregation_weight > 0).sum()} regression tasks for calculating metrics (RMSE, Rsquared, correlation).\")\n",
    "# print(ecfp[18387,:10].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb078d30",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  Compute batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a1fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:10:25.779206Z",
     "start_time": "2021-11-30T21:10:25.742332Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" batch_ratio        : {opt['batch_ratio']}\")\n",
    "print(f\" internal_batch_max : {opt['internal_batch_max']}\")\n",
    "\n",
    "# batch_size  = int(np.ceil(opt['batch_ratio'] * idx_tr.shape[0]))\n",
    "# num_int_batches = 1\n",
    "# print(f\" batch_ratio * # idx_tr:   {opt['batch_ratio']} * {idx_tr.shape[0]} = {opt['batch_ratio'] * idx_tr.shape[0]}\")\n",
    "\n",
    "\n",
    "# if opt['internal_batch_max'] is not None:\n",
    "#     if opt['internal_batch_max'] < batch_size:\n",
    "#         num_int_batches = int(np.ceil(batch_size / opt['internal_batch_max']))\n",
    "#         print(f\"\\n\\n internal_batch_max: {opt['internal_batch_max']}   batch_size: {batch_size}\")\n",
    "#         print(f\" batch_size / internal_batch_max: {batch_size / opt['internal_batch_max']}   num_int_batches: {num_int_batches}\")\n",
    "#         batch_size      = int(np.ceil(batch_size / num_int_batches))\n",
    "#         print(f\" batch_size / num_int_batches: {batch_size / num_int_batches}   modified batch_size: {batch_size}\")\n",
    "        \n",
    "\n",
    "batch_size = 320 \n",
    "print(f\" batch size:   {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2645c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Separate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0704f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:10:38.737304Z",
     "start_time": "2021-11-30T21:10:38.713730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"opt['dataload']['fold_te'] : {opt['dataload']['fold_te'] }\")\n",
    "print(f\"opt['dataload']['fold_va'] : {opt['dataload']['fold_va'] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0718e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:11:17.886147Z",
     "start_time": "2021-11-30T21:11:17.861334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if opt['dataload']['fold_te'] is not None and opt['dataload']['fold_te'] >= 0:\n",
    "    ## removing test data\n",
    "    print(f\" Remove test data\")\n",
    "    assert opt['dataload']['fold_te'] != opt['dataload']['fold_va'], \"fold_va and fold_te must not be equal.\"\n",
    "    keep    = (folding != args.fold_te)\n",
    "    ecfp    = ecfp[keep]\n",
    "    y_class = y_class[keep]\n",
    "    y_regr  = y_regr[keep]\n",
    "    y_censor= y_censor[keep]\n",
    "    folding = folding[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056298c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:11:23.295720Z",
     "start_time": "2021-11-30T21:11:23.267229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prop = (np.cumsum([0.3, 0.3, 0.3, 0.1])* ecfp.shape[0]+1).astype(np.int32)\n",
    "print(prop, prop.astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21f1d3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Separate train, train1, train2, and validation  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ed184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T21:12:32.402571Z",
     "start_time": "2021-11-30T21:12:32.372192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fold_va = opt['dataload']['fold_va']\n",
    "fold_va = 0\n",
    "fold_train1 = 1\n",
    "fold_train2 = 2\n",
    "\n",
    "idx_val    = np.where(folding == fold_va)[0]\n",
    "idx_train  = np.where(folding == fold_train1)[0]\n",
    "idx_train1 = np.where(folding == fold_train2)[0]\n",
    "idx_train2 = np.where(folding >  fold_train2)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5f106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:27:23.242326Z",
     "start_time": "2021-12-01T20:27:23.206716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataroot = opt['dataload']['dataroot']\n",
    "ecfp     = load_sparse(dataroot, opt['dataload']['x'])\n",
    "\n",
    "total_input = ecfp.shape[0]\n",
    "ranges      = (np.cumsum([0.3, 0.3, 0.1, 0.3])* total_input).astype(np.int32)\n",
    "print(total_input, '     ', ranges)\n",
    "\n",
    "idx_train  = np.arange(ranges[0])\n",
    "idx_train1 = np.arange(ranges[0], ranges[1])\n",
    "idx_train2 = np.arange(ranges[1], ranges[2])\n",
    "idx_val    = np.arange(ranges[2], ranges[-1])\n",
    "\n",
    "print( f' idx_train   len: {len(idx_train) :6d}  - {(idx_train)} ')\n",
    "print( f' idx_train1  len: {len(idx_train1):6d}  - {(idx_train1)}')\n",
    "print( f' idx_train2  len: {len(idx_train2):6d}  - {(idx_train2)}')\n",
    "print( f' idx_val     len: {len(idx_val)   :6d}  - {(idx_val)}   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118c6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T18:42:45.746087Z",
     "start_time": "2021-11-25T18:42:45.726692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# y_class_tr = y_class[idx_train]\n",
    "# y_class_va = y_class[idx_va]\n",
    "\n",
    "# y_regr_tr  = y_regr[idx_tr]\n",
    "# y_regr_va  = y_regr[idx_va]\n",
    "\n",
    "# y_censor_tr = y_censor[idx_tr]\n",
    "# y_censor_va = y_censor[idx_va]\n",
    "\n",
    "# num_pos_va  = np.array((y_class_va == +1).sum(0)).flatten()\n",
    "# num_neg_va  = np.array((y_class_va == -1).sum(0)).flatten()\n",
    "# num_regr_va = np.bincount(y_regr_va.indices, minlength=y_regr.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-gpu",
   "language": "python",
   "name": "pyt-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
