project_name         : AdaSparseChem-cb29-10Task
## Leave empty to have date_time used 
exp_id               :
exp_name_pfx         :
exp_name             : 
exp_folder           : 
exp_description      : brief description
folder_sfx           :
random_seed          : 
seed_list            : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
config               :
cpu:
gpu_ids:

backbone             : SparseChem
tasks                : ["class", "class", "class", "class", "class", "class", "class", "class", "class", "class"]
tasks_num_class      : [472, 624, 688, 192, 620, 184, 224, 148, 344, 72]
lambdas              : [1, 1, 1, 1, 1,  1, 1, 1, 1, 1]
verbose              : False
 

input_size_freq      : 
input_size           : 32000
hidden_sizes         : [100]
tail_hidden_size     : [100]

first_non_linearity  : "relu"
middle_non_linearity : "relu"
last_non_linearity   : "relu"
 
first_dropout        : 0.0
middle_dropout       : 0.0
last_dropout         : 0.0  

class_output_size    : 
regr_output_size     :

# policy               : False
policy               : True
policy_model         : task-specific

skip_residual        : False
skip_hidden          : False

is_sparse            : True
is_sharing           : True
diff_sparsity_weights: False

skip_layer           :     0

is_curriculum        : False
# curriculum_speed   :    20
curriculum_speed     :     3

fix_BN               : False
retrain_from_pl      : False

##------------------------------------------------------
## training parameters
##------------------------------------------------------
train:
  batch_size     :     128
  
  ## Use with caution - Increasing warmp_up epochs too much 
  ## introduces  instability in the validation 
  warmup_epochs  :     100
  training_epochs:     250
  total_iters    :   25000
  warm_up_iters  :
  
  # Learning Rates
  task_lr                  : 0.001
  backbone_lr              : 0.001

  weight_optimizer         : adam
  policy_optimizer         : adam
  # Decay parameters for weights training
  decay_lr_rate            : 0.75
  decay_lr_freq            :   40
  decay_lr_cooldown        :    0
  # Decay parameters for policy training
  policy_lr                :       0.001  # 1.0e-03
  policy_decay_lr_rate     : 0.75
  policy_decay_lr_freq     :   50
  policy_decay_lr_cooldown :    0
  # Policy Learning Rates
  # policy_lr      :       0.1    # 1.0e-01
  # policy_lr      :       0.01   # 1.0e-02
  # policy_lr      :       0.0001 # 1.0e-04
  
  # Lambda sparsity (cross entropy) 
  # lambda_sparsity:       1.0e+01
  # lambda_sparsity:       1.0e-01
  # lambda_sparsity:       1.0e-02
  # lambda_sparsity:       1.0e-03
  # lambda_sparsity:       1.0e-04 
  # lambda_sparsity:       1.0e-05 
  # lambda_sparsity:       1.0e-06 
  # lambda_sparsity:       1.0e-07 
  lambda_sparsity:       2.0e-02
  
  # Lambda Sharing (hamming distance)
  # lambda_sharing :       1.0e+02
  # lambda_sharing :       1.0e+01
  # lambda_sharing :       1.0e+00
  # lambda_sharing :       1.0e-01
  lambda_sharing :       1.0e-02
  # lambda_sharing :       1.0e-03
  # lambda_sharing :       5.0e-02

  # Lambda sharing (hamming) - original 0.05
  # lambda_sharing :     0.5
  
  # Regularization of classification loss during policy training - Added 1-31-2022
  lambda_tasks: 1 

  # Gumbel Softmax Temperature Parameters
  # decay_temp_freq: number of policy training epochs that have to occur 
  #                  before the temp is decayed. 
  # logits initialization method
  init_method    :    random
  # init_temp    :         5
  # decay_temp   :     0.965
  # decay_temp_freq:       2    
  init_temp      :         4
  decay_temp     :     0.965
  decay_temp_freq:        16

  init_neg_logits:
  hard_sampling  :    False

  #

  val_freq       :     500
  print_freq     :      -1
  val_iters      :      -1
  #
  resume         :   False
  retrain_resume :   False
  policy_iter    :    best
  which_iter     :  warmup

##------------------------------------------------------
## Data & Folder parameters
##------------------------------------------------------
paths:
  log_dir        :     ../../experiments/AdaSparseChem-cb29-10task
  result_dir     :     ../../experiments/AdaSparseChem-cb29-10task
  checkpoint_dir :     ../../experiments/AdaSparseChem-cb29-10task

dataload:
  dataset        : Chembl29
  dataroot       : "../../MLDatasets/chembl29_10task"
  x              : "chembl_29_X.npy"

  ## Data split ratios: [Warmup, Weight Trn, Policy Trn, Validation]
  # x_split_ratios : [0.2, 0.35, 0.35, 0.1]
  # x_split_ratios : [0.75, 0.001, 0.001, 0.248]
  # x_split_ratios : [0.3, 0.3, 0.3, 0.05, 0.05]
  x_split_ratios : [0.725, 0.225, 0.05]


  folding        : "chembl_29_folding.npy"  
  y_tasks        : ["chembl_29_Y_all_tg_0_cols_472.npy",
                    "chembl_29_Y_all_tg_1_cols_624.npy",
                    "chembl_29_Y_all_tg_6_cols_688.npy",
                    "chembl_29_Y_all_tg_10_cols_192.npy",
                    "chembl_29_Y_all_tg_11_cols_620.npy",
                    "chembl_29_Y_all_tg_643_cols_184.npy",
                    "chembl_29_Y_all_tg_836_cols_224.npy",
                    "chembl_29_Y_all_tg_1005_cols_148.npy",
                    "chembl_29_Y_all_tg_1028_cols_344.npy",
                    "chembl_29_Y_all_tg_1031_cols_72.npy"
                   ]
  y_censor       :
  fold_inputs    : 32000
  input_transform: 
  weights_class  :  
  
  min_samples_class : 1
  fold_va        : 0
  fold_te        :  

##------------------------------------------------------
## sparsechem related parameters
##------------------------------------------------------
SC:
  # batch_ratio: 0.02 
  # internal_batch_max: 200
  normalize_loss:


##------------------------------------------------------
## test parameters
##------------------------------------------------------
# test:
  # which_iter: best