exp_name         : SparseChem
## Leave empty to have date_time used 
exp_instance     :
exp_description  : brief description
seed             : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
backbone         : SparseChem
backbone_orig    : ResNet18
orig_tasks       : ["seg", "sn"]
tasks            : ["class", "class", "class"]
tasks_num_class  : [5, 5, 5]
lambdas          : [1, 1, 1]
policy_model     : task-specific
verbose          : False

input_size_freq  : 
input_size       : 32000
hidden_sizes     : [50, 50, 50, 50, 50, 50]
tail_hidden_size : 50


first_non_linearity : "relu"

middle_non_linearity: "relu"
middle_dropout      : 0.2

last_non_linearity  : "relu"
last_dropout        : 0.2  

class_output_size:
regr_output_size:

policy: True

is_sparse            :   True
is_sharing           :   True
diff_sparsity_weights:   False

skip_layer           :     0

is_curriculum        :   False
# curriculum_speed   :     20
curriculum_speed     :     1

fix_BN               :  False
retrain_from_pl      :  False

##------------------------------------------------------
## training parameters
##------------------------------------------------------
train:
  batch_size     :     128
  
  # Learning Rates
  task_lr        :    0.01
  backbone_lr    :    0.01
  # policy_lr      :  0.001
  policy_lr      :  0.001
  
  
  # Decay parameters for weights training
  decay_lr_rate  :          0.85
  decay_lr_freq  :          2000
  
  # Decay parameters for policy training
  policy_decay_lr_rate  :   0.85
  policy_decay_lr_freq  :   2200
  
  # Lambda sparsity (cross entropy) 
  lambda_sparsity:      1.0
  # lambda_sparsity:      0.0001

  # lambda_sparsity:    0.05
  # lambda_sparsity:    0.1

  # Lambda sharing (hamming) - original 0.05
  lambda_sharing :       0.1  
  
  # reg_w_hamming  :     0.1
  # lambda_sharing :     0.1
  # lambda_sharing :     0.5
  # lambda_sharing :       1.0  ## when using task_loss
  
  # Regularization of classification loss during policy training - Added 1-31-2022
  lambda_tasks: 1 

  # Gumbel Softmax Temperature Parameters
  # decay_temp_freq: number of policy training epochs that have to occur 
  #                  before the temp is decayed. 
  # logits initialization method
  init_method    :   random
  init_temp      :       4
  # decay_temp     :   0.965
  decay_temp     :     0.9
  decay_temp_freq:       2   
  init_neg_logits:
  hard_sampling  :    False
  # init_temp      :      5
  # decay_temp     :    0.5
  # decay_temp_freq:     2    

  #
  total_iters    :   25000
  warm_up_iters  :     400
  print_freq     :      -1
  val_freq       :     500
  val_iters      :      -1


  #
  resume         :   False
  retrain_resume :   False
  policy_iter    :    best
  which_iter     :  warmup

##------------------------------------------------------
## Data & Folder parameters
##------------------------------------------------------
paths:
  experiment_dir :     ../experiments/AdaSparseChem
  log_dir        :     ../experiments/AdaSparseChem
  result_dir     :     ../experiments/AdaSparseChem
  checkpoint_dir :     ../experiments/AdaSparseChem

dataload:
  dataset        : Chembl_23_mini
  dataroot       : "/home/kbardool/kusanagi/MLDatasets/chembl_23mini_synthetic"
  x              : "chembl_23mini_x.npy"
  x_split_ratios : [0.75, 0.001, 0.001, 0.248]
  # x_split_ratios : [0.2, 0.35, 0.35, 0.1]
  folding        : "chembl_23mini_folds.npy"  
  weights_class  :
  fold_inputs    : 32000
  input_transform: 
  y_tasks        : ["chembl_23mini_adashare_y1_bin_sparse.npy", "chembl_23mini_adashare_y2_bin_sparse.npy", "chembl_23mini_adashare_y3_bin_sparse.npy"]
  y_censor       :
  fold_te        :
  weights_class  :  
  crop_h         : 321
  crop_w         : 321
  min_samples_class : 5
  fold_va        : 0
  fold_te        :  

##------------------------------------------------------
## sparsechem related parameters
##------------------------------------------------------
SC:
  batch_ratio: 0.02
  # internal_batch_max: 200
  normalize_loss:


##------------------------------------------------------
## test parameters
##------------------------------------------------------
test:
  which_iter: best