project_name         : AdaSparseChem
## Leave empty to have date_time used 
exp_id               :
exp_name             : 
exp_folder           : 
exp_description      : brief description
folder_sfx           :
random_seed          : 
seed_list            : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]

cpu:
gpu_ids:

backbone             : SparseChem
tasks                : ["class", "class", "class"]
tasks_num_class      : [5, 5, 5]
lambdas              : [1, 1, 1]
policy_model         : task-specific
verbose              : False
backbone_orig        : ResNet18
tasks_orig           : ["seg", "sn"]

input_size_freq      : 
input_size           : 32000
# hidden_sizes         : [50, 50, 50, 50, 50, 50]
# tail_hidden_size     : 50
# hidden_sizes         : [75, 75, 75, 75, 75, 75]
# tail_hidden_size     : 75
# hidden_sizes         : [100, 100, 100, 100, 100, 100]
# tail_hidden_size     : 100
# hidden_sizes         : [150, 150, 150, 150, 150, 150]
# tail_hidden_size     : 150


first_non_linearity  : "relu"
middle_non_linearity : "relu"
last_non_linearity   : "relu"
 
first_dropout        : 0.0
middle_dropout       : 0.0
last_dropout         : 0.0  

class_output_size    : 
regr_output_size     :

# policy               : False
policy               : True

is_sparse            : True
is_sharing           : True
diff_sparsity_weights: False

skip_layer           :     0

is_curriculum        : False
# curriculum_speed   :     20
curriculum_speed     :     3

fix_BN               : False
retrain_from_pl      : False

##------------------------------------------------------
## training parameters
##------------------------------------------------------
train:
  batch_size     :     128
  
  ## Use with caution - Increasing warmp_up epochs too much 
  ## introduces  instability in the validation 
  warmup_epochs  :      75
  training_epochs:     250
  total_iters    :   25000
  warm_up_iters  :

  # Learning Rates
  task_lr        :    0.001
  backbone_lr    :    0.001

  weight_optimizer: sgd
  policy_optimizer: adam
  # Decay parameters for weights training
  decay_lr_rate  :     0.75
  decay_lr_freq  :       40
  
  # Decay parameters for policy training
  policy_decay_lr_rate  :   0.75
  policy_decay_lr_freq  :     50

  # Policy Learning Rates
  # policy_lr      :       0.1    # 1.0e-01
  policy_lr      :       0.01   # 1.0e-02
  # policy_lr      :       0.001  # 1.0e-03
  # policy_lr      :       0.0001 # 1.0e-04
  
  # Lambda sparsity (cross entropy) 
  # lambda_sparsity:       1.0e+01
  # lambda_sparsity:       1.0e-01
  lambda_sparsity:       1.0e-02
  # lambda_sparsity:       1.0e-03
  # lambda_sparsity:       1.0e-04 
  # lambda_sparsity:       1.0e-05 
  # lambda_sparsity:       1.0e-06 
  # lambda_sparsity:       1.0e-07 
  # lambda_sparsity:       5.0e-02
  
  # Lambda Sharing (hamming distance)
  # lambda_sharing :       1.0e+02
  # lambda_sharing :       1.0e+01
  # lambda_sharing :       1.0e+00
  # lambda_sharing :       1.0e-01
  lambda_sharing :       1.0e-02
  # lambda_sharing :       1.0e-03

  # lambda_sharing :       5.0e-02

  # Lambda sharing (hamming) - original 0.05
  # lambda_sharing :     0.5
  
  # Regularization of classification loss during policy training - Added 1-31-2022
  lambda_tasks: 1 

  # Gumbel Softmax Temperature Parameters
  # decay_temp_freq: number of policy training epochs that have to occur 
  #                  before the temp is decayed. 
  # logits initialization method
  init_method    :    random
  # init_temp    :         5
  # decay_temp   :     0.965
  # decay_temp_freq:       2    
  init_temp      :         4
  decay_temp     :     0.965
  decay_temp_freq:        16

  init_neg_logits:
  hard_sampling  :    False

  #

  val_freq       :     500
  print_freq     :      -1
  val_iters      :      -1
  #
  resume         :   False
  retrain_resume :   False
  policy_iter    :    best
  which_iter     :  warmup

##------------------------------------------------------
## Data & Folder parameters
##------------------------------------------------------
paths:
  log_dir        :     ../../experiments/AdaSparseChem
  result_dir     :     ../../experiments/AdaSparseChem
  checkpoint_dir :     ../../experiments/AdaSparseChem

dataload:
  dataset        : Chembl23_mini
  dataroot       : "/home/kbardool/WSL-projs/MLDatasets/chembl23_synthetic"
  x              : "chembl_23mini_x.npy"

  ## Data split ratios: [Warmup, Weight Trn, Policy Trn, Validation]
  # x_split_ratios : [0.2, 0.35, 0.35, 0.1]
  # x_split_ratios : [0.75, 0.001, 0.001, 0.248]
  # x_split_ratios : [0.3, 0.3, 0.3, 0.05, 0.05]
  x_split_ratios : [0.725, 0.225, 0.05]


  y_tasks        : ["chembl_23mini_adashare_y1_bin_sparse.npy", 
                    "chembl_23mini_adashare_y2_bin_sparse.npy", 
                    "chembl_23mini_adashare_y3_bin_sparse.npy"]
  y_censor       :
  folding        : "chembl_23mini_folds.npy"  
  fold_inputs    : 32000
  input_transform: 
  weights_class  :  
  
  crop_h         : 321
  crop_w         : 321
  
  min_samples_class : 5
  fold_va        : 0
  fold_te        :  

##------------------------------------------------------
## sparsechem related parameters
##------------------------------------------------------
SC:
  # batch_ratio: 0.02 
  # internal_batch_max: 200
  normalize_loss:


##------------------------------------------------------
## test parameters
##------------------------------------------------------
# test:
  # which_iter: best