{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5690a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T22:07:59.635392Z",
     "start_time": "2021-12-02T22:07:59.609549Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f4f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T22:05:17.141840Z",
     "start_time": "2021-09-23T22:05:17.085995Z"
    }
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d70846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:03:30.781952Z",
     "start_time": "2021-12-15T23:03:30.423716Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataloaders.nyu_v2_dataloader import NYU_v2\n",
    "# from dataloaders.cityscapes_dataloader import CityScapes\n",
    "# from dataloaders.taskonomy_dataloader import Taskonomy\n",
    "from dev.blockdrop_env_dev import BlockDropEnv_Dev\n",
    "\n",
    "from utils.util import makedir, print_separator, create_path, print_yaml, should, fix_random_seed, read_yaml_from_input, timestring\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from models.deeplab_resnet import MTL2, MTL_Instance\n",
    "# from models.base import Bottleneck, BasicBlock\n",
    "# from scipy.special import softmax\n",
    "# import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# from models.base import Bottleneck, BasicBlock\n",
    "\n",
    "import torch.nn.functional as F\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cc539a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:03:33.748349Z",
     "start_time": "2021-12-15T23:03:33.477028Z"
    }
   },
   "outputs": [],
   "source": [
    "from dev.train_dev import eval_dev\n",
    "from dev.MTL2_Dev import MTL2_Dev\n",
    "from dev.blockdrop_env_dev import BlockDropEnv_Dev\n",
    "from dev.deeplab_resnet_dev import Deeplab_ResNet_Backbone_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d1a780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:03:36.403402Z",
     "start_time": "2021-12-15T23:03:36.397387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--config', 'yamls/adashare/nyu_v2_2task.yml', '--cpu']\n"
     ]
    }
   ],
   "source": [
    "# input_args = \" --config  yamls/adashare/nyu_v2_3task_test.yml --cpu \".split()\n",
    "input_args = \" --config yamls/adashare/nyu_v2_2task.yml --cpu \".split()\n",
    "# pretrained_path = \"/mnt/f/models_adashare/NYU_v2_3Task_pretrain\"\n",
    "print(input_args)\n",
    "# print(pretrained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2211367",
   "metadata": {},
   "source": [
    "### Create folders and print options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c42f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:03:37.483542Z",
     "start_time": "2021-12-15T23:03:37.119214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "####################READ YAML#####################\n",
      "##################################################\n",
      "{'config': 'yamls/adashare/nyu_v2_2task.yml', 'exp_ids': [0], 'gpus': [0], 'cpu': True}\n",
      " Create folder ../experiments/logs/nyu_v2_2task\n",
      " Create folder ../experiments/results/nyu_v2_2task\n",
      " Create folder ../experiments/checkpoints/nyu_v2_2task\n",
      "\n",
      "exp_name. : nyu_v2_2task\n",
      "seed. : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "backbone. : ResNet34\n",
      "original_backbone. : ResNet18\n",
      "tasks. : ['seg', 'sn']\n",
      "lambdas. : [1, 20]\n",
      "tasks_num_class. : [40, 3]\n",
      "policy_model. : task-specific\n",
      "paths.\n",
      "paths.log_dir. : ../experiments/logs/\n",
      "paths.result_dir. : ../experiments/results\n",
      "paths.checkpoint_dir. : ../experiments/checkpoints\n",
      "dataload.\n",
      "dataload.dataset. : NYU_v2\n",
      "dataload.dataroot. : /home/kbardool/MLDatasets/nyu_v2\n",
      "dataload.orig_dataroot. : /data/datasets/nyu_v2\n",
      "dataload.crop_h. : 321\n",
      "dataload.crop_w. : 321\n",
      "policy. : True\n",
      "init_neg_logits. : None\n",
      "is_sparse. : True\n",
      "is_sharing. : True\n",
      "skip_layer. : 0\n",
      "is_curriculum. : True\n",
      "curriculum_speed. : 3\n",
      "fix_BN. : False\n",
      "diff_sparsity_weights. : True\n",
      "retrain_from_pl. : False\n",
      "train.\n",
      "train.batch_size. : 32\n",
      "train.total_iters. : 20000\n",
      "train.warm_up_iters. : 4000\n",
      "train.lr. : 0.001\n",
      "train.policy_lr. : 0.01\n",
      "train.backbone_lr. : 0.001\n",
      "train.reg_w. : 0.05\n",
      "train.reg_w_hamming. : 0.05\n",
      "train.print_freq. : 100\n",
      "train.val_freq. : 400\n",
      "train.decay_lr_freq. : 4000\n",
      "train.decay_lr_rate. : 0.5\n",
      "train.decay_temp_freq. : 100\n",
      "train.init_temp. : 5\n",
      "train.decay_temp. : 0.965\n",
      "train.resume. : False\n",
      "train.retrain_resume. : False\n",
      "train.policy_iter. : best\n",
      "train.which_iter. : warmup\n",
      "train.init_method. : equal\n",
      "train.hard_sampling. : False\n",
      "test.\n",
      "test.which_iter. : best\n",
      "cpu. : True\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************\n",
    "# ****************** create folders and print options ****************\n",
    "# ********************************************************************\n",
    "# read the yaml\n",
    "print_separator('READ YAML')\n",
    "opt, gpu_ids, _ = read_yaml_from_input(input_args)\n",
    "fix_random_seed(opt[\"seed\"][0])\n",
    "create_path(opt)\n",
    "# print yaml on the screen\n",
    "lines = print_yaml(opt)\n",
    "for line in lines: print(line)\n",
    "    \n",
    "# print to file\n",
    "with open(os.path.join(opt['paths']['log_dir'], opt['exp_name'], 'opt.txt'), 'w+') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3ebc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.512260Z",
     "start_time": "2021-12-02T20:00:47.482619Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['dataload']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b2930",
   "metadata": {},
   "source": [
    "## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30085b26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.563675Z",
     "start_time": "2021-12-02T20:00:47.515792Z"
    }
   },
   "outputs": [],
   "source": [
    "    # ********************************************************************\n",
    "    # ******************** Prepare the dataloaders ***********************\n",
    "    # ********************************************************************\n",
    "    # load the dataloader\n",
    "    print_separator('CREATE DATALOADERS')\n",
    "    \n",
    "    if opt['dataload']['dataset'] == 'NYU_v2':\n",
    "        # To warm up\n",
    "        trainset  = NYU_v2(opt['dataload']['dataroot'], 'train', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To update the network parameters\n",
    "        trainset1 = NYU_v2(opt['dataload']['dataroot'], 'train1', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To update the policy weights\n",
    "        trainset2 = NYU_v2(opt['dataload']['dataroot'], 'train2', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To validate\n",
    "        valset    = NYU_v2(opt['dataload']['dataroot'], 'test')\n",
    " \n",
    "    else:\n",
    "        raise NotImplementedError('Dataset %s is not implemented' % opt['dataload']['dataset'])\n",
    "## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d4f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.593172Z",
     "start_time": "2021-12-02T20:00:47.565996Z"
    }
   },
   "outputs": [],
   "source": [
    "len(valset.__dict__['triples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494aeadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.622066Z",
     "start_time": "2021-12-02T20:00:47.597232Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
    "# print(IMG_MEAN.shape, IMG_MEAN)\n",
    "# print(IMG_MEAN[np.newaxis, np.newaxis, :].shape, IMG_MEAN[np.newaxis, np.newaxis, :])\n",
    "# IMG_MEAN = np.tile(IMG_MEAN[np.newaxis, np.newaxis, :], (480, 640,1))\n",
    "# print(IMG_MEAN.shape) \n",
    "# print(IMG_MEAN[:5,:5,0])\n",
    "# print(IMG_MEAN[:5,:5,1])\n",
    "# print(IMG_MEAN[:5,:5,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d547124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.653973Z",
     "start_time": "2021-12-02T20:00:47.627267Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(trainset , batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "train1_loader = DataLoader(trainset1, batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "train2_loader = DataLoader(trainset2, batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "val_loader    = DataLoader(valset   , batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a891e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:49:33.553156Z",
     "start_time": "2021-09-22T22:49:33.532706Z"
    }
   },
   "source": [
    "When Batch Size == 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c92345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.727998Z",
     "start_time": "2021-12-02T20:00:47.690445Z"
    }
   },
   "outputs": [],
   "source": [
    "print('size of training set 0 (warm up)       : ', len(trainset))\n",
    "print('size of training set 1 (network parms) : ', len(trainset1))\n",
    "print('size of training set 2 (policy weights): ', len(trainset2))\n",
    "print('size of validation set                 : ', len(valset))\n",
    "print('                               Total   : ', len(trainset)+len(trainset1)+len(trainset2)+len(valset))\n",
    "\n",
    "print(f\" batch size       : {opt['train']['batch_size']}\")\n",
    "print(f' len train_loader : {len(train_loader)}')\n",
    "print(f' len train1_loader: {len(train1_loader)}')\n",
    "print(f' len train2_loader: {len(train2_loader)}')\n",
    "print(f' len val_loader   : {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a03100",
   "metadata": {},
   "source": [
    "##  Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b053d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T22:35:06.794067Z",
     "start_time": "2021-12-02T22:35:06.714960Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate'] = opt['train'].get('alpha_iter_alternate'  , len(train2_loader))\n",
    "\n",
    "print(opt['train']['weight_iter_alternate'], opt['train']['alpha_iter_alternate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70b53c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T22:20:19.438946Z",
     "start_time": "2021-12-02T22:20:19.253276Z"
    }
   },
   "outputs": [],
   "source": [
    "opt['train']['total_iters']   = 30\n",
    "opt['train']['warm_up_iters'] = 4\n",
    "opt['train']['print_freq']    = 1\n",
    "opt['train']['val_freq']      = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f298f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T22:20:19.438946Z",
     "start_time": "2021-12-02T22:20:19.253276Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( \n",
    "    f\"\\n backbone                : {opt['backbone']}\",\n",
    "    f\"\\n paths.log_dir           : {opt['paths']['log_dir']}\", \n",
    "    f\"\\n paths.checkpoint_dir    : {opt['paths']['checkpoint_dir']}\", \n",
    "    f\"\\n experiment name         : {opt['exp_name']}\",\n",
    "    f\"\\n tasks_num_class         : {opt['tasks_num_class'],}\",\n",
    "    f\"\\n init_neg_logits         : {opt['init_neg_logits'],}\",\n",
    "    f\"\\n device id               : {gpu_ids[0]}\",\n",
    "    f\"\\n init temp               : {opt['train']['init_temp'],}\",\n",
    "    f\"\\n decay temp              : {opt['train']['decay_temp']}\",\n",
    "    f\"\\n fix BN parms            : {opt['fix_BN']}\",\n",
    "    f\"\\n skip_layer              : {opt['skip_layer']}\",\n",
    "    f\"\\n train.init_method       : {opt['train']['init_method']}\")\n",
    "print(f\" Total iterations        : {opt['train']['total_iters']}\")\n",
    "print(f\" Warm-up iterations      : {opt['train']['warm_up_iters']}\")\n",
    "print(f\" Print Frequency         : {opt['train']['print_freq']}\")\n",
    "print(f\" Validation Frequency    : {opt['train']['val_freq']}\")\n",
    "print(f\" Weight iter alternate   : {opt['train']['weight_iter_alternate'] }\")\n",
    "print(f\" Alpha  iter alternate   : {opt['train']['alpha_iter_alternate'] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c567bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.804614Z",
     "start_time": "2021-12-02T20:00:47.773208Z"
    }
   },
   "outputs": [],
   "source": [
    "from dev.MTL2_Dev import MTL2_Dev\n",
    "from dev.blockdrop_env_dev import BlockDropEnv_Dev\n",
    "from dev.sparsechem_env_dev import SparseChemEnv_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439a3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.832512Z",
     "start_time": "2021-12-02T20:00:47.808232Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# del environ\n",
    "# del BlockDropEnv_Dev\n",
    "# del MTL2_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635215f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:47.865544Z",
     "start_time": "2021-12-02T20:00:47.836855Z"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "# environ = SparseChemEnv_Dev(opt['paths']['log_dir'], \n",
    "#                        opt['paths']['checkpoint_dir'], \n",
    "#                        opt['exp_name'],\n",
    "#                        opt['tasks_num_class'], \n",
    "#                        opt['init_neg_logits'], \n",
    "#                        gpu_ids[0],\n",
    "#                        opt['train']['init_temp'], \n",
    "#                        opt['train']['decay_temp'], \n",
    "#                        is_train=True, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede26499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-03T20:20:34.930362Z",
     "start_time": "2021-12-03T20:20:34.901381Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12], [13,14,15]])\n",
    "print(a.shape)\n",
    "print(a.contiguous().view(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a210d2",
   "metadata": {},
   "source": [
    "### Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66cc00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T20:00:48.721705Z",
     "start_time": "2021-12-02T20:00:47.870330Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ********************************************************************\n",
    "# ********************Create the environment *************************\n",
    "# ********************************************************************\n",
    "# create the model and the pretrain model\n",
    "print_separator('CREATE THE ENVIRONMENT')\n",
    "environ = BlockDropEnv_Dev(opt['paths']['log_dir'], \n",
    "                       opt['paths']['checkpoint_dir'], \n",
    "                       opt['exp_name'],\n",
    "                       opt['tasks_num_class'], \n",
    "                       opt['init_neg_logits'], \n",
    "                       gpu_ids[0],\n",
    "                       opt['train']['init_temp'], \n",
    "                       opt['train']['decay_temp'], \n",
    "                       is_train=True, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dc450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T19:11:06.286066Z",
     "start_time": "2021-12-02T19:11:06.254070Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"environment successfully created\")\n",
    "\n",
    "print(environ.num_tasks)\n",
    "# hasattr(environ, 'policy1')\n",
    "print(hasattr(environ, 'policy1'))\n",
    "# print(environ.networks['mtl-net'])\n",
    "print(environ.networks['mtl-net'].policys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2d681",
   "metadata": {},
   "source": [
    "##  Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dde1bb",
   "metadata": {},
   "source": [
    "### Training Loop Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4d8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:48.558522Z",
     "start_time": "2021-11-04T15:52:48.381961Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"which_iter   : {opt['train']['which_iter']}\\n\"\n",
    "      f\"train_resume :  {opt['train']['resume']}\")\n",
    "\n",
    "print(environ.networks['mtl-net'].layers)\n",
    "\n",
    "current_iter   = 0\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0\n",
    "\n",
    "if opt['train']['resume']:\n",
    "    print(' Resume training')\n",
    "    current_iter = environ.load(opt['train']['which_iter'])\n",
    "    environ.networks['mtl-net'].reset_logits()\n",
    "else:\n",
    "    print(' Initiate Training ')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    environ.cuda(gpu_ids)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    environ.cpu()\n",
    "print('\\n')\n",
    "\n",
    "environ.define_optimizer(policy_learning=False)\n",
    "environ.define_scheduler(policy_learning=False)\n",
    "# Fix Alpha     - disable gradient flow in the policy network\n",
    "# Free weights  - allow gradient flow through the weights\n",
    "environ.fix_alpha()\n",
    "environ.free_w(opt['fix_BN'])\n",
    "\n",
    "####  continue training preparation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if opt['dataload']['dataset'] == 'NYU_v2':\n",
    "    \n",
    "    if len(opt['tasks_num_class']) == 2:\n",
    "        \n",
    "        refer_metrics = {'seg': {'mIoU'         : 0.413, \n",
    "                                 'Pixel Acc'    : 0.691},\n",
    "                          \n",
    "                         'sn' : {'Angle Mean'   : 15, \n",
    "                                 'Angle Median' : 11.5, \n",
    "                                 'Angle 11.25'  : 49.2, \n",
    "                                 'Angle 22.5'   : 76.7,\n",
    "                                 'Angle 30'     : 86.8}}\n",
    "        \n",
    "    elif len(opt['tasks_num_class']) == 3:\n",
    "        \n",
    "        refer_metrics = {'seg': {'mIoU'         : 0.275, \n",
    "                                 'Pixel Acc'    : 0.589},\n",
    "                          \n",
    "                         'sn' : {'Angle Mean'   : 17.5, \n",
    "                                 'Angle Median' : 14.2, \n",
    "                                 'Angle 11.25'  : 34.9, \n",
    "                                 'Angle 22.5'   : 73.3,\n",
    "                                 'Angle 30'     : 85.7},\n",
    "                     \n",
    "                         'depth':{'abs_err'     : 0.62, \n",
    "                                  'rel_err'     : 0.25, \n",
    "                                  'sigma_1.25'  : 57.9,\n",
    "                                  'sigma_1.25^2': 85.8, \n",
    "                                  'sigma_1.25^3': 95.7}}\n",
    "    else:\n",
    "        raise ValueError('num_class = %d is invalid' % len(opt['tasks_num_class']))\n",
    "else:\n",
    "    raise NotImplementedError('Dataset %s is not implemented' % opt['dataload']['dataset'])\n",
    "\n",
    "flag         = 'update_w'\n",
    "\n",
    "\n",
    "best_value   = 0 \n",
    "best_iter    = 0\n",
    "p_epoch      = 0\n",
    "best_metrics = None\n",
    "flag_warmup  = True\n",
    "\n",
    "if opt['backbone'] == 'ResNet18':\n",
    "    num_blocks = 8\n",
    "elif opt['backbone'] in ['ResNet34', 'ResNet50']:\n",
    "    num_blocks = 18\n",
    "elif opt['backbone'] == 'ResNet101':\n",
    "    num_blocks = 33\n",
    "elif opt['backbone'] == 'WRN':\n",
    "    num_blocks = 15\n",
    "else:\n",
    "    raise ValueError('Backbone %s is invalid' % opt['backbone'])\n",
    "\n",
    "print(opt['backbone'], 'num_blocks: ', num_blocks)\n",
    "\n",
    "\n",
    "pp.pprint(refer_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e369e9",
   "metadata": {},
   "source": [
    "###  Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_enumerator  = enumerate(train_loader)\n",
    "# batch_enumerator1 = enumerate(train1_loader)\n",
    "# batch_enumerator2 = enumerate(train2_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c44d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T16:11:55.007232Z",
     "start_time": "2021-11-04T15:52:50.852181Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while current_iter < opt['train']['total_iters']:\n",
    "    start_time = time.time()\n",
    "\n",
    "    environ.train()\n",
    "    current_iter += 1\n",
    "\n",
    "##---------------------------------------------------------------     \n",
    "## part one: warm up\n",
    "##--------------------------------------------------------------- \n",
    "\n",
    "    if current_iter < opt['train']['warm_up_iters']:\n",
    "        print(f\"\\n{'-'*100}\")        \n",
    "        print(f\"** {timestring()} - Training iteration {current_iter} WARMUP iteration: {current_iter}\")    \n",
    "        print(f\"{'-'*100}\\n\")\n",
    "        \n",
    "        batch_idx, batch = next(batch_enumerator)\n",
    "        environ.set_inputs(batch)\n",
    "        \n",
    "        environ.optimize(opt['lambdas'], is_policy=False, flag='update_w')\n",
    "        \n",
    "        ## If we've exhausted the dataload, reset to beginning\n",
    "        if batch_idx == len(train_loader) - 1:\n",
    "            batch_enumerator = enumerate(train_loader)\n",
    "\n",
    "        if should(current_iter, opt['train']['print_freq']):\n",
    "            environ.print_loss(current_iter, start_time)\n",
    "            environ.resize_results()\n",
    "\n",
    "        # validation\n",
    "        if should(current_iter, opt['train']['val_freq']):\n",
    "            print(f\"**  {timestring()}  START VALIDATION iteration: {current_iter} \")    \n",
    "            \n",
    "            environ.eval()     # set to evaluation mode (train = False)\n",
    "            num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "            val_metrics = training_validation(environ, \n",
    "                                              val_loader, \n",
    "                                              opt['tasks'], \n",
    "                                              policy=False, \n",
    "                                              num_train_layers=None, \n",
    "                                              num_seg_cls=num_seg_class,\n",
    "                                              eval_iter = 3)\n",
    "            \n",
    "            environ.print_loss(current_iter, start_time, val_metrics, title='validation')\n",
    "            environ.save_checkpoint('latest', current_iter)\n",
    "            \n",
    "            print(f\"** {timestring()} - END VALIDATION iteration:  {current_iter} \")                \n",
    "            environ.train()    # set to training mode (train = True)\n",
    "\n",
    "##------------------------------------------------------------------------------------------\n",
    "## part two:   current_iter >= warm_up_iters\n",
    "##            - set flag_warmup switch to false\n",
    "##            - first iteration here - save warmup model to checkpoint \n",
    "##                                   - turn off gradient flow through policy network    \n",
    "##------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    else:\n",
    "        print_heading(f\"** {timestring()} - Training iteration {current_iter}  flag: {flag} \")    \n",
    "\n",
    "        if flag_warmup:\n",
    "            environ.define_optimizer(policy_learning=True)\n",
    "            environ.define_scheduler(policy_learning=True)\n",
    "            flag_warmup = False\n",
    "\n",
    "        if current_iter == opt['train']['warm_up_iters']:\n",
    "            environ.save_checkpoint('warmup', current_iter)\n",
    "            environ.fix_alpha()\n",
    "\n",
    "        #-----------------------------------------\n",
    "        # Train & Update the network weights\n",
    "        #-----------------------------------------\n",
    "        if flag == 'update_w':\n",
    "            \n",
    "            current_iter_w += 1\n",
    "            batch_idx_w, batch = next(batch_enumerator1,1)\n",
    "            environ.set_inputs(batch)\n",
    "\n",
    "            ##----------------------------------------------------------------------\n",
    "            ## Set number of layers to train based on cirriculum_speed \n",
    "            ## and p_epoch (number of epochs of policy training)\n",
    "            ## When curriculum_speed == 3, a num_train_layers is incremented \n",
    "            ## after completion of every 3 policy training epochs\n",
    "            ##----------------------------------------------------------------------\n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "                \n",
    "            print(f\"++ CALL ENVIRON.OPTIMIZE() flag: {flag} is_policy: {opt['policy']} p_epoch: {p_epoch}  num_train_layers: {num_train_layers}\\n\") \n",
    "            \n",
    "            environ.optimize(opt['lambdas'], \n",
    "                             is_policy=opt['policy'], \n",
    "                             flag=flag, \n",
    "                             num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'])\n",
    " \n",
    "            ## if (current_iter % print_freq) == 0\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss(current_iter, start_time)\n",
    "                environ.resize_results()\n",
    "\n",
    "            ## when current_iter_w == len(train1_loader) switch to policy training\n",
    "\n",
    "            print(f\"++ current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w} current_iter_a: {current_iter_a} ['weight_iter_alternate']:\"\\\n",
    "                  f\" {opt['train']['weight_iter_alternate']}\")            \n",
    "            \n",
    "            \n",
    "            ## if (current_iter_w % weight_iter_alternate) == 0 \n",
    "            \n",
    "            if should(current_iter_w, opt['train']['weight_iter_alternate']):\n",
    "                print(f\"++ Switch to update_alpha\")\n",
    "                flag = 'update_alpha'\n",
    "                environ.fix_w()\n",
    "                environ.free_alpha()\n",
    "                \n",
    "                #-------------------------------------------------------\n",
    "                # START validation process\n",
    "                #-------------------------------------------------------                \n",
    "                environ.eval()\n",
    "                print('++ Weight Training Validation ...')\n",
    "                \n",
    "                num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "                \n",
    "                val_metrics = training_validation(environ, \n",
    "                                                  val_loader, \n",
    "                                                  opt['tasks'], \n",
    "                                                  policy=opt['policy'],\n",
    "                                                  num_train_layers=num_train_layers, \n",
    "                                                  hard_sampling=opt['train']['hard_sampling'],\n",
    "                                                  num_seg_cls=num_seg_class)\n",
    "                \n",
    "                environ.print_loss(current_iter, start_time, val_metrics)\n",
    "                environ.save_checkpoint('latest', current_iter)\n",
    "                \n",
    "                print(f\" current iter: {current_iter}       warm_up_iters: {opt['train']['warm_up_iters']} \\n\"\n",
    "                      f\" num_blocks  : {num_blocks}         cirriculum_speed: {opt['curriculum_speed']}\\n\"\n",
    "                      f\" opt['train']['weight_iter_alternate'] : {opt['train']['weight_iter_alternate']}\\n\"\n",
    "                      f\" opt['train']['alpha_iter_alternate']  : {opt['train']['alpha_iter_alternate']}\\n\"\n",
    "                      f\" {current_iter - opt['train']['warm_up_iters']}  ??\"\n",
    "                      f\" {num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])}\")\n",
    "                                             \n",
    "                ## if the training iteration is larger than a certain amount \n",
    "                ## (num_blocks (8 for ResNet))* (curriculum_speed(3)) * (weight training iterations + policy training iterations (19+4))\n",
    "                ## check metrics for improvement, and issue a checkpoint if necessary\n",
    "                \n",
    "                if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "                        (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "                    new_value = 0\n",
    "\n",
    "                    for k in refer_metrics.keys():\n",
    "                        if k in val_metrics.keys():\n",
    "                            for kk in val_metrics[k].keys():\n",
    "                                if not kk in refer_metrics[k].keys():\n",
    "                                    continue\n",
    "                                if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or \n",
    "                                   (k == 'depth' and not kk.startswith('sigma')) or \n",
    "                                   (kk == 'err'):\n",
    "                                    value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "                                else:\n",
    "                                    value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "                                ## matching keys count\n",
    "                                ## matching_keys_cnt = len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "                                value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "                                new_value += value\n",
    "                    \n",
    "                    print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                    if new_value > best_value:\n",
    "                        print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)\n",
    "                        best_value = new_value\n",
    "                        best_metrics = val_metrics\n",
    "                        best_iter = current_iter\n",
    "                        environ.save_checkpoint('best', current_iter)\n",
    "                        print('New      best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)                    \n",
    "\n",
    "                environ.train()\n",
    "                #-------------------------------------------------------\n",
    "                # END validation process\n",
    "                #-------------------------------------------------------       \n",
    "                \n",
    "            print(f\"++ Current loader (train1) index: {batch_idx_w}   len(train1_loader) = {len(train1_loader)} \")\n",
    "            \n",
    "            if batch_idx_w == len(train1_loader):\n",
    "                batch_enumerator1 = enumerate(train1_loader,1)\n",
    "                \n",
    "        #---------------------------------------------------------------------------\n",
    "        # Train & update the policy network\n",
    "        #---------------------------------------------------------------------------\n",
    "        elif flag == 'update_alpha':\n",
    "            \n",
    "            current_iter_a += 1\n",
    "            batch_idx_a, batch = next(batch_enumerator2, 1)\n",
    "            environ.set_inputs(batch)\n",
    "            \n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "                \n",
    "            print(f\"++ CALL ENVIRON.OPTIMIZE() flag: {flag} is_policy: {opt['policy']} p_epoch: {p_epoch}  num_train_layers: {num_train_layers}\\n\") \n",
    "            \n",
    "            environ.optimize(opt['lambdas'], \n",
    "                             is_policy=opt['policy'], \n",
    "                             flag=flag, \n",
    "                             num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'])\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss(current_iter, start_time)\n",
    "                environ.resize_results()\n",
    "                # environ.visual_policy(current_iter)\n",
    "            \n",
    "            ## when current_iter_a == len(train2_loader) switch to policy training\n",
    "            print(f\"** current_iter_a: {current_iter_a} ['alpha_iter_alternate']: {opt['train']['alpha_iter_alternate']}\",\n",
    "                  f\"   batch_idx_a {batch_ix_a}  Current iter w: {current_iter_w} \")            \n",
    "           \n",
    "            ## if (current_iter_a % alpha_iter_alternate) == 0 \n",
    "            \n",
    "            if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "                print(f\"** Switch training to update_weight\")                \n",
    " \n",
    "                flag = 'update_w'\n",
    "  \n",
    "                environ.fix_alpha()\n",
    "                environ.free_w(opt['fix_BN'])\n",
    "                environ.decay_temperature()\n",
    "                \n",
    "                # print the distribution\n",
    "                dists = environ.get_policy_prob()\n",
    "                print(np.concatenate(dists, axis=-1))\n",
    "                p_epoch += 1\n",
    "                print(f\"** p_epoch incremented: {p_epoch}\")\n",
    "\n",
    "            \n",
    "            print(f\"++ Current loader (train2 index:: {batch_idx_a} len(train2_loader) = {len(train2_loader)}\")                \n",
    "            if batch_idx_a == len(train2_loader):\n",
    "                batch_enumerator2 = enumerate(train2_loader,1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('flag %s is not recognized' % flag)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5319dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c1492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T23:05:19.684592Z",
     "start_time": "2021-12-02T23:05:19.660996Z"
    }
   },
   "outputs": [],
   "source": [
    "( 43 // 3 )+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ef5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0133a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3496d09",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Display parameters of the network. \n",
    "\n",
    "`taskn_logits` are architecture parms\n",
    "others are network parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc55455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T19:31:59.143549Z",
     "start_time": "2021-09-23T19:31:59.040784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(environ.networks['mtl-net'].layers)\n",
    "print(environ.networks['mtl-net'].backbone.layer_config)\n",
    "for m in environ.networks['mtl-net'].backbone.modules():\n",
    "#     print(type(m))\n",
    "    print('==> ',type(m),'\\n',m)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.kernel_size, m.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1122604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T19:39:05.357994Z",
     "start_time": "2021-09-22T19:39:05.318080Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planes = 128\n",
    "keep_channels = (planes * np.cumsum([0, 0.25, 0.25, 0.5])).astype(np.int32)\n",
    "print(keep_channels)\n",
    "keep_masks = []\n",
    "for kc in keep_channels:\n",
    "    mask = np.zeros([1, planes, 1, 1])\n",
    "    print(f' keep channels: {kc:2d}  mask: {mask.shape} - set mask[:, :{kc:2d}] =1')\n",
    "    mask[:, :kc] = 1\n",
    "    keep_masks.append(mask)\n",
    "    \n",
    "for i in keep_masks:\n",
    "    print(type(i), i.shape, i.sum())\n",
    "    \n",
    "keep_masks_py = torch.from_numpy(np.concatenate(keep_masks)).float()\n",
    "\n",
    "print(keep_masks_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139ec6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T20:54:26.471166Z",
     "start_time": "2021-09-21T20:54:26.156423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dilation = 1 \n",
    "kernel_size = np.asarray((3, 3))\n",
    "upsampled_kernel_size = (kernel_size - 1) * (dilation - 1) + kernel_size\n",
    "print(upsampled_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ea815",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a6fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T19:29:35.681008Z",
     "start_time": "2021-09-21T19:29:35.598776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(environ.get_arch_parameters())\n",
    "for i,j in environ.networks['mtl-net'].named_parameters():\n",
    "    print(f\"name: {i}    shape: {j.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7a11e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22057a15",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Call MTL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a01a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T18:59:25.718262Z",
     "start_time": "2021-09-21T18:59:25.701695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "block = BasicBlock\n",
    "layers = [2, 2, 2, 2]\n",
    "num_classes_tasks =  opt['tasks_num_class']\n",
    "init_method =  opt['train']['init_method']\n",
    "init_neg_logits = opt['init_neg_logits']\n",
    "skip_layer = opt['skip_layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e15f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T18:59:00.365777Z",
     "start_time": "2021-09-21T18:59:00.328214Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtl2 = MTL2_Dev(block, layers, num_classes_tasks, init_method, init_neg_logits, skip_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3679d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a152e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3b2f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4982a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-gpu",
   "language": "python",
   "name": "pyt-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
