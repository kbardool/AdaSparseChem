{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d70846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:23:46.339100Z",
     "start_time": "2021-09-15T19:23:46.312055Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "# sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloaders.nyu_v2_dataloader import NYU_v2\n",
    "# from dataloaders.cityscapes_dataloader import CityScapes\n",
    "# from dataloaders.taskonomy_dataloader import Taskonomy\n",
    "from envs.blockdrop_env import BlockDropEnv\n",
    "import torch\n",
    "from utils.util import makedir, print_separator, create_path, print_yaml, should, fix_random_seed\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07dae599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:23:48.093748Z",
     "start_time": "2021-09-15T19:23:48.058313Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(environ, dataloader, tasks, policy=False, num_train_layers=None, hard_sampling=False, num_seg_cls=-1,\n",
    "         eval_iter=10):\n",
    "    batch_size = []\n",
    "    records = {}\n",
    "    val_metrics = {}\n",
    "    if 'seg' in tasks:\n",
    "        assert (num_seg_cls != -1)\n",
    "        records['seg'] = {'mIoUs': [], 'pixelAccs': [],  'errs': [], 'conf_mat': np.zeros((num_seg_cls, num_seg_cls)),\n",
    "                          'labels': np.arange(num_seg_cls)}\n",
    "    if 'sn' in tasks:\n",
    "        records['sn'] = {'cos_similaritys': []}\n",
    "    if 'depth' in tasks:\n",
    "        records['depth'] = {'abs_errs': [], 'rel_errs': [], 'sq_rel_errs': [], 'ratios': [], 'rms': [], 'rms_log': []}\n",
    "    if 'keypoint' in tasks:\n",
    "        records['keypoint'] = {'errs': []}\n",
    "    if 'edge' in tasks:\n",
    "        records['edge'] = {'errs': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "            if eval_iter != -1:\n",
    "                if batch_idx > eval_iter:\n",
    "                    break\n",
    "\n",
    "            environ.set_inputs(batch)\n",
    "            # seg_pred, seg_gt, pixelAcc, cos_similarity = environ.val(policy, num_train_layers, hard_sampling)\n",
    "            metrics = environ.val2(policy, num_train_layers, hard_sampling)\n",
    "\n",
    "            # environ.networks['mtl-net'].task1_logits\n",
    "            # mIoUs.append(mIoU)\n",
    "            if 'seg' in tasks:\n",
    "                new_mat = confusion_matrix(metrics['seg']['gt'], metrics['seg']['pred'], records['seg']['labels'])\n",
    "                assert (records['seg']['conf_mat'].shape == new_mat.shape)\n",
    "                records['seg']['conf_mat'] += new_mat\n",
    "                records['seg']['pixelAccs'].append(metrics['seg']['pixelAcc'])\n",
    "                records['seg']['errs'].append(metrics['seg']['err'])\n",
    "            if 'sn' in tasks:\n",
    "                records['sn']['cos_similaritys'].append(metrics['sn']['cos_similarity'])\n",
    "            if 'depth' in tasks:\n",
    "                records['depth']['abs_errs'].append(metrics['depth']['abs_err'])\n",
    "                records['depth']['rel_errs'].append(metrics['depth']['rel_err'])\n",
    "                records['depth']['sq_rel_errs'].append(metrics['depth']['sq_rel_err'])\n",
    "                records['depth']['ratios'].append(metrics['depth']['ratio'])\n",
    "                records['depth']['rms'].append(metrics['depth']['rms'])\n",
    "                records['depth']['rms_log'].append(metrics['depth']['rms_log'])\n",
    "            if 'keypoint' in tasks:\n",
    "                records['keypoint']['errs'].append(metrics['keypoint']['err'])\n",
    "            if 'edge' in tasks:\n",
    "                records['edge']['errs'].append(metrics['edge']['err'])\n",
    "            batch_size.append(len(batch['img']))\n",
    "\n",
    "    # overall_mIoU = (np.array(mIoUs) * np.array(batch_size)).sum() / sum(batch_size)\n",
    "    if 'seg' in tasks:\n",
    "        val_metrics['seg'] = {}\n",
    "        jaccard_perclass = []\n",
    "        for i in range(num_seg_cls):\n",
    "            if not records['seg']['conf_mat'][i, i] == 0:\n",
    "                jaccard_perclass.append(records['seg']['conf_mat'][i, i] / (np.sum(records['seg']['conf_mat'][i, :]) +\n",
    "                                                                            np.sum(records['seg']['conf_mat'][:, i]) -\n",
    "                                                                            records['seg']['conf_mat'][i, i]))\n",
    "\n",
    "        val_metrics['seg']['mIoU'] = np.sum(jaccard_perclass) / len(jaccard_perclass)\n",
    "\n",
    "        val_metrics['seg']['Pixel Acc'] = (np.array(records['seg']['pixelAccs']) * np.array(batch_size)).sum() / sum(\n",
    "            batch_size)\n",
    "\n",
    "        val_metrics['seg']['err'] = (np.array(records['seg']['errs']) * np.array(batch_size)).sum() / sum(batch_size)\n",
    "\n",
    "    if 'sn' in tasks:\n",
    "        val_metrics['sn'] = {}\n",
    "        overall_cos = np.clip(np.concatenate(records['sn']['cos_similaritys']), -1, 1)\n",
    "\n",
    "        angles = np.arccos(overall_cos) / np.pi * 180.0\n",
    "        val_metrics['sn']['cosine_similarity'] = overall_cos.mean()\n",
    "        val_metrics['sn']['Angle Mean'] = np.mean(angles)\n",
    "        val_metrics['sn']['Angle Median'] = np.median(angles)\n",
    "        val_metrics['sn']['Angle RMSE'] = np.sqrt(np.mean(angles ** 2))\n",
    "        val_metrics['sn']['Angle 11.25'] = np.mean(np.less_equal(angles, 11.25)) * 100\n",
    "        val_metrics['sn']['Angle 22.5'] = np.mean(np.less_equal(angles, 22.5)) * 100\n",
    "        val_metrics['sn']['Angle 30'] = np.mean(np.less_equal(angles, 30.0)) * 100\n",
    "        val_metrics['sn']['Angle 45'] = np.mean(np.less_equal(angles, 45.0)) * 100\n",
    "\n",
    "    if 'depth' in tasks:\n",
    "        val_metrics['depth'] = {}\n",
    "        records['depth']['abs_errs'] = np.stack(records['depth']['abs_errs'], axis=0)\n",
    "        records['depth']['rel_errs'] = np.stack(records['depth']['rel_errs'], axis=0)\n",
    "        records['depth']['sq_rel_errs'] = np.stack(records['depth']['sq_rel_errs'], axis=0)\n",
    "        records['depth']['ratios'] = np.concatenate(records['depth']['ratios'], axis=0)\n",
    "        records['depth']['rms'] = np.concatenate(records['depth']['rms'], axis=0)\n",
    "        records['depth']['rms_log'] = np.concatenate(records['depth']['rms_log'], axis=0)\n",
    "        records['depth']['rms_log'] = records['depth']['rms_log'][~np.isnan(records['depth']['rms_log'])]\n",
    "        val_metrics['depth']['abs_err'] = (records['depth']['abs_errs'] * np.array(batch_size)).sum() / sum(batch_size)\n",
    "        val_metrics['depth']['rel_err'] = (records['depth']['rel_errs'] * np.array(batch_size)).sum() / sum(batch_size)\n",
    "        val_metrics['depth']['sq_rel_err'] = (records['depth']['sq_rel_errs'] * np.array(batch_size)).sum() / sum(\n",
    "            batch_size)\n",
    "        val_metrics['depth']['sigma_1.25'] = np.mean(np.less_equal(records['depth']['ratios'], 1.25)) * 100\n",
    "        val_metrics['depth']['sigma_1.25^2'] = np.mean(np.less_equal(records['depth']['ratios'], 1.25 ** 2)) * 100\n",
    "        val_metrics['depth']['sigma_1.25^3'] = np.mean(np.less_equal(records['depth']['ratios'], 1.25 ** 3)) * 100\n",
    "        val_metrics['depth']['rms'] = (np.sum(records['depth']['rms']) / len(records['depth']['rms'])) ** 0.5\n",
    "        # val_metrics['depth']['rms_log'] = (np.sum(records['depth']['rms_log']) / len(records['depth']['rms_log'])) ** 0.5\n",
    "\n",
    "    if 'keypoint' in tasks:\n",
    "        val_metrics['keypoint'] = {}\n",
    "        val_metrics['keypoint']['err'] = (np.array(records['keypoint']['errs']) * np.array(batch_size)).sum() / sum(\n",
    "            batch_size)\n",
    "\n",
    "    if 'edge' in tasks:\n",
    "        val_metrics['edge'] = {}\n",
    "        val_metrics['edge']['err'] = (np.array(records['edge']['errs']) * np.array(batch_size)).sum() / sum(\n",
    "            batch_size)\n",
    "\n",
    "    return val_metrics## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfed1249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:23:49.031467Z",
     "start_time": "2021-09-15T19:23:49.013835Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_yaml(input_args):\n",
    "    # read in yaml\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", required=True, help=\"Path for the config file\")\n",
    "    parser.add_argument(\"--exp_ids\", type=int, nargs='+', default=[0], help=\"Path for the config file\")\n",
    "    parser.add_argument(\"--gpus\", type=int, nargs='+', default=[0], help=\"Path for the config file\")\n",
    "    parser.add_argument(\"--cpu\", default=False, action=\"store_true\",  help=\"CPU instead of GPU\")\n",
    "    args = parser.parse_args(input_args)\n",
    "\n",
    "    print(vars(args))\n",
    "    # torch.cuda.set_device(args.gpu)\n",
    "    with open(args.config) as f:\n",
    "        opt = yaml.load(f)\n",
    "    opt['cpu'] = args.cpu\n",
    "    return opt, args.gpus, args.exp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d1a780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:23:52.255334Z",
     "start_time": "2021-09-15T19:23:52.235750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--config', 'yamls/adashare/nyu_v2_2task.yml', '--cpu']\n"
     ]
    }
   ],
   "source": [
    "# input_args = \" --config  yamls/adashare/nyu_v2_3task_test.yml --cpu \".split()\n",
    "input_args = \" --config yamls/adashare/nyu_v2_2task.yml --cpu \".split()\n",
    "# pretrained_path = \"/mnt/f/models_adashare/NYU_v2_3Task_pretrain\"\n",
    "print(input_args)\n",
    "# print(pretrained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf604f8",
   "metadata": {},
   "source": [
    "#  Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2211367",
   "metadata": {},
   "source": [
    "## Create folders and print options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c42f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:23:54.426217Z",
     "start_time": "2021-09-15T19:23:54.385168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "####################READ YAML#####################\n",
      "##################################################\n",
      "{'config': 'yamls/adashare/nyu_v2_2task.yml', 'exp_ids': [0], 'gpus': [0], 'cpu': True}\n",
      "\n",
      "exp_name. : nyu_v2_2task\n",
      "seed. : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "backbone. : ResNet18\n",
      "tasks. : ['seg', 'sn']\n",
      "lambdas. : [1, 20]\n",
      "tasks_num_class. : [40, 3]\n",
      "policy_model. : task-specific\n",
      "paths.\n",
      "paths.log_dir. : ../experiments/logs/\n",
      "paths.result_dir. : ../experiments/results\n",
      "paths.checkpoint_dir. : ../experiments/checkpoints\n",
      "dataload.\n",
      "dataload.dataset. : NYU_v2\n",
      "dataload.dataroot. : /mnt/f/MLDatasets/nyu_v2\n",
      "dataload.orig_dataroot. : /data/datasets/nyu_v2\n",
      "dataload.crop_h. : 321\n",
      "dataload.crop_w. : 321\n",
      "policy. : True\n",
      "init_neg_logits. : None\n",
      "is_sparse. : True\n",
      "is_sharing. : True\n",
      "skip_layer. : 0\n",
      "is_curriculum. : True\n",
      "curriculum_speed. : 3\n",
      "fix_BN. : False\n",
      "diff_sparsity_weights. : True\n",
      "retrain_from_pl. : False\n",
      "train.\n",
      "train.batch_size. : 16\n",
      "train.total_iters. : 20000\n",
      "train.warm_up_iters. : 4000\n",
      "train.lr. : 0.001\n",
      "train.policy_lr. : 0.01\n",
      "train.backbone_lr. : 0.001\n",
      "train.reg_w. : 0.05\n",
      "train.reg_w_hamming. : 0.05\n",
      "train.print_freq. : 100\n",
      "train.val_freq. : 400\n",
      "train.decay_lr_freq. : 4000\n",
      "train.decay_lr_rate. : 0.5\n",
      "train.decay_temp_freq. : 100\n",
      "train.init_temp. : 5\n",
      "train.decay_temp. : 0.965\n",
      "train.resume. : False\n",
      "train.retrain_resume. : False\n",
      "train.policy_iter. : best\n",
      "train.which_iter. : warmup\n",
      "train.init_method. : equal\n",
      "train.hard_sampling. : False\n",
      "test.\n",
      "test.which_iter. : best\n",
      "cpu. : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-6ef41f77d514>:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  opt = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************\n",
    "# ****************** create folders and print options ****************\n",
    "# ********************************************************************\n",
    "# read the yaml\n",
    "print_separator('READ YAML')\n",
    "opt, gpu_ids, _ = read_yaml(input_args)\n",
    "fix_random_seed(opt[\"seed\"][0])\n",
    "create_path(opt)\n",
    "# print yaml on the screen\n",
    "lines = print_yaml(opt)\n",
    "for line in lines: print(line)\n",
    "    \n",
    "# print to file\n",
    "with open(os.path.join(opt['paths']['log_dir'], opt['exp_name'], 'opt.txt'), 'w+') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd3ebc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T19:24:49.129465Z",
     "start_time": "2021-09-15T19:24:49.108703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'NYU_v2',\n",
       " 'dataroot': '/mnt/f/MLDatasets/nyu_v2',\n",
       " 'orig_dataroot': '/data/datasets/nyu_v2',\n",
       " 'crop_h': 321,\n",
       " 'crop_w': 321}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt['dataload']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b2930",
   "metadata": {},
   "source": [
    "## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30085b26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:33.427035Z",
     "start_time": "2021-09-13T23:22:33.375648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "################CREATE DATALOADERS################\n",
      "##################################################\n",
      "NYU_v2\n",
      "NYU_v2\n",
      "NYU_v2\n",
      "NYU_v2\n"
     ]
    }
   ],
   "source": [
    "    # ********************************************************************\n",
    "    # ******************** Prepare the dataloaders ***********************\n",
    "    # ********************************************************************\n",
    "    # load the dataloader\n",
    "    print_separator('CREATE DATALOADERS')\n",
    "    \n",
    "    if opt['dataload']['dataset'] == 'NYU_v2':\n",
    "        # To warm up\n",
    "        trainset  = NYU_v2(opt['dataload']['dataroot'], 'train', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To update the network parameters\n",
    "        trainset1 = NYU_v2(opt['dataload']['dataroot'], 'train1', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To update the policy weights\n",
    "        trainset2 = NYU_v2(opt['dataload']['dataroot'], 'train2', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To validate\n",
    "        valset    = NYU_v2(opt['dataload']['dataroot'], 'test')\n",
    " \n",
    "    else:\n",
    "        raise NotImplementedError('Dataset %s is not implemented' % opt['dataload']['dataset'])\n",
    "## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "494aeadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:33.449792Z",
     "start_time": "2021-09-13T23:22:33.433020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) [104.00699 116.66877 122.67892]\n",
      "(1, 1, 3) [[[104.00699 116.66877 122.67892]]]\n",
      "(480, 640, 3)\n",
      "[[104.00699 104.00699 104.00699 104.00699 104.00699]\n",
      " [104.00699 104.00699 104.00699 104.00699 104.00699]\n",
      " [104.00699 104.00699 104.00699 104.00699 104.00699]\n",
      " [104.00699 104.00699 104.00699 104.00699 104.00699]\n",
      " [104.00699 104.00699 104.00699 104.00699 104.00699]]\n",
      "[[116.66877 116.66877 116.66877 116.66877 116.66877]\n",
      " [116.66877 116.66877 116.66877 116.66877 116.66877]\n",
      " [116.66877 116.66877 116.66877 116.66877 116.66877]\n",
      " [116.66877 116.66877 116.66877 116.66877 116.66877]\n",
      " [116.66877 116.66877 116.66877 116.66877 116.66877]]\n",
      "[[122.67892 122.67892 122.67892 122.67892 122.67892]\n",
      " [122.67892 122.67892 122.67892 122.67892 122.67892]\n",
      " [122.67892 122.67892 122.67892 122.67892 122.67892]\n",
      " [122.67892 122.67892 122.67892 122.67892 122.67892]\n",
      " [122.67892 122.67892 122.67892 122.67892 122.67892]]\n"
     ]
    }
   ],
   "source": [
    "IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
    "print(IMG_MEAN.shape, IMG_MEAN)\n",
    "print(IMG_MEAN[np.newaxis, np.newaxis, :].shape, IMG_MEAN[np.newaxis, np.newaxis, :])\n",
    "IMG_MEAN = np.tile(IMG_MEAN[np.newaxis, np.newaxis, :], (480, 640,1))\n",
    "print(IMG_MEAN.shape) \n",
    "print(IMG_MEAN[:5,:5,0])\n",
    "print(IMG_MEAN[:5,:5,1])\n",
    "print(IMG_MEAN[:5,:5,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1a05fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:33.463855Z",
     "start_time": "2021-09-13T23:22:33.453565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set 0 (warm up)       :  795\n",
      "size of training set 1 (network parms) :  636\n",
      "size of training set 2 (policy weights):  159\n",
      "size of validation set                 :  654\n"
     ]
    }
   ],
   "source": [
    "print('size of training set 0 (warm up)       : ', len(trainset))\n",
    "print('size of training set 1 (network parms) : ', len(trainset1))\n",
    "print('size of training set 2 (policy weights): ', len(trainset2))\n",
    "print('size of validation set                 : ', len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d547124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:33.472763Z",
     "start_time": "2021-09-13T23:22:33.467440Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(trainset , batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "train1_loader = DataLoader(trainset1, batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "train2_loader = DataLoader(trainset2, batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "val_loader    = DataLoader(valset   , batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04365a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:33.490804Z",
     "start_time": "2021-09-13T23:22:33.476179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch size       : 16\n",
      " len train_loader : 49\n",
      " len train1_loader: 39\n",
      " len train2_loader: 9\n",
      " len val_loader   : 40\n"
     ]
    }
   ],
   "source": [
    "print(f\" batch size       : {opt['train']['batch_size']}\")\n",
    "print(f' len train_loader : {len(train_loader)}')\n",
    "print(f' len train1_loader: {len(train1_loader)}')\n",
    "print(f' len train2_loader: {len(train2_loader)}')\n",
    "print(f' len val_loader   : {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5874bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:33.502775Z",
     "start_time": "2021-09-13T23:22:33.494361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 9\n"
     ]
    }
   ],
   "source": [
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate'] = opt['train'].get('alpha_iter_alternate', len(train2_loader))\n",
    "\n",
    "print(opt['train']['weight_iter_alternate'], opt['train']['alpha_iter_alternate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a03100",
   "metadata": {},
   "source": [
    "##  Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e3a0a76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.122324Z",
     "start_time": "2021-09-13T23:22:33.506314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "##############CREATE THE ENVIRONMENT##############\n",
      "##################################################\n",
      "BlockDropEnv\n",
      "Create MTL2 with \n",
      " block: <class 'models.base.BasicBlock'> \n",
      " layers: [2, 2, 2, 2] \n",
      " tasks_num_class: [40, 3] \n",
      " init_method: equal\n",
      "define the scheduler (not policy learning)\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************\n",
    "# ********************Create the environment *************************\n",
    "# ********************************************************************\n",
    "# create the model and the pretrain model\n",
    "print_separator('CREATE THE ENVIRONMENT')\n",
    "environ = BlockDropEnv(opt['paths']['log_dir'], \n",
    "                       opt['paths']['checkpoint_dir'], \n",
    "                       opt['exp_name'],\n",
    "                       opt['tasks_num_class'], \n",
    "                       opt['init_neg_logits'], \n",
    "                       gpu_ids[0],\n",
    "                       opt['train']['init_temp'], \n",
    "                       opt['train']['decay_temp'], is_train=True, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "625b237f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.130418Z",
     "start_time": "2021-09-13T23:22:34.125582Z"
    }
   },
   "outputs": [],
   "source": [
    "current_iter = 0\n",
    "current_iter_w, current_iter_a = 0, 0\n",
    "if opt['train']['resume']:\n",
    "    print(' Resume training')\n",
    "    current_iter = environ.load(opt['train']['which_iter'])\n",
    "    environ.networks['mtl-net'].reset_logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae5b06a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.145693Z",
     "start_time": "2021-09-13T23:22:34.133627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define the scheduler (not policy learning)\n"
     ]
    }
   ],
   "source": [
    "environ.define_optimizer(policy_learning=False)\n",
    "environ.define_scheduler(policy_learning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a2786de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.164790Z",
     "start_time": "2021-09-13T23:22:34.148960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda not available\n",
      "base_env.cpu()\n",
      " Network item mtl-net moved to cpu\n",
      "blockdrop_env.cpu()\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    environ.cuda(gpu_ids)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    environ.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161e5514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.375658Z",
     "start_time": "2021-09-13T23:22:34.167961Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_enumerator = enumerate(train_loader)\n",
    "batch_enumerator1 = enumerate(train1_loader)\n",
    "batch_enumerator2 = enumerate(train2_loader)\n",
    "flag = 'update_w'\n",
    "environ.fix_alpha()\n",
    "environ.free_w(opt['fix_BN'])\n",
    "best_value, best_iter = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca53708f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.398134Z",
     "start_time": "2021-09-13T23:22:34.382139Z"
    }
   },
   "outputs": [],
   "source": [
    "if opt['dataload']['dataset'] == 'NYU_v2':\n",
    "    if len(opt['tasks_num_class']) == 2:\n",
    "        refer_metrics = {'seg': {'mIoU': 0.413, 'Pixel Acc': 0.691},\n",
    "                         'sn': {'Angle Mean': 15, 'Angle Median': 11.5, 'Angle 11.25': 49.2, 'Angle 22.5': 76.7,\n",
    "                                'Angle 30': 86.8}}\n",
    "    elif len(opt['tasks_num_class']) == 3:\n",
    "        refer_metrics = {'seg': {'mIoU': 0.275, 'Pixel Acc': 0.589},\n",
    "                         'sn': {'Angle Mean': 17.5, 'Angle Median': 14.2, 'Angle 11.25': 34.9, 'Angle 22.5': 73.3,\n",
    "                                'Angle 30': 85.7},\n",
    "                         'depth': {'abs_err': 0.62, 'rel_err': 0.25, 'sigma_1.25': 57.9,\n",
    "                                   'sigma_1.25^2': 85.8, 'sigma_1.25^3': 95.7}}\n",
    "    else:\n",
    "        raise ValueError('num_class = %d is invalid' % len(opt['tasks_num_class']))\n",
    "else:\n",
    "    raise NotImplementedError('Dataset %s is not implemented' % opt['dataload']['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa816aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.417628Z",
     "start_time": "2021-09-13T23:22:34.407846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seg': {'mIoU': 0.413, 'Pixel Acc': 0.691}, 'sn': {'Angle Mean': 15, 'Angle Median': 11.5, 'Angle 11.25': 49.2, 'Angle 22.5': 76.7, 'Angle 30': 86.8}}\n",
      "ResNet18\n",
      " Total iterations: 20000\n",
      " Warm-up iterations: 4000\n"
     ]
    }
   ],
   "source": [
    "print(refer_metrics)\n",
    "print(opt['backbone'])\n",
    "print(f\" Total iterations: {opt['train']['total_iters']}\")\n",
    "print(f\" Warm-up iterations: {opt['train']['warm_up_iters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc08f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.437110Z",
     "start_time": "2021-09-13T23:22:34.422913Z"
    }
   },
   "outputs": [],
   "source": [
    "best_metrics = None\n",
    "p_epoch = 0\n",
    "flag_warmup = True\n",
    "\n",
    "if opt['backbone'] == 'ResNet18':\n",
    "    num_blocks = 8\n",
    "elif opt['backbone'] in ['ResNet34', 'ResNet50']:\n",
    "    num_blocks = 18\n",
    "elif opt['backbone'] == 'ResNet101':\n",
    "    num_blocks = 33\n",
    "elif opt['backbone'] == 'WRN':\n",
    "    num_blocks = 15\n",
    "else:\n",
    "    raise ValueError('Backbone %s is invalid' % opt['backbone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3ed32fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T23:22:34.456481Z",
     "start_time": "2021-09-13T23:22:34.442258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-09-13 16:22:34:449787'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%F %H:%M:%S:%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d62608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T03:01:22.126898Z",
     "start_time": "2021-09-13T23:22:40.016158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2021-09-13 16:22:40:044094 - iteration: 0\n",
      " 2021-09-13 16:23:36:040962 - iteration: 1\n",
      " 2021-09-13 16:24:40:076664 - iteration: 2\n",
      " 2021-09-13 16:25:36:355366 - iteration: 3\n",
      " 2021-09-13 16:26:31:331287 - iteration: 4\n",
      " 2021-09-13 16:27:29:040309 - iteration: 5\n",
      " 2021-09-13 16:28:27:601892 - iteration: 6\n",
      " 2021-09-13 16:29:23:527433 - iteration: 7\n",
      " 2021-09-13 16:30:21:045091 - iteration: 8\n",
      " 2021-09-13 16:31:17:560061 - iteration: 9\n",
      " 2021-09-13 16:32:13:252647 - iteration: 10\n",
      " 2021-09-13 16:33:08:472513 - iteration: 11\n",
      " 2021-09-13 16:34:04:040421 - iteration: 12\n",
      " 2021-09-13 16:34:57:914646 - iteration: 13\n",
      " 2021-09-13 16:35:53:499784 - iteration: 14\n",
      " 2021-09-13 16:36:47:755138 - iteration: 15\n",
      " 2021-09-13 16:37:44:207223 - iteration: 16\n",
      " 2021-09-13 16:38:39:126548 - iteration: 17\n",
      " 2021-09-13 16:39:34:257094 - iteration: 18\n",
      " 2021-09-13 16:40:28:806056 - iteration: 19\n",
      " 2021-09-13 16:41:24:330273 - iteration: 20\n",
      " 2021-09-13 16:42:19:162600 - iteration: 21\n",
      " 2021-09-13 16:43:13:806921 - iteration: 22\n",
      " 2021-09-13 16:44:08:760823 - iteration: 23\n",
      " 2021-09-13 16:45:03:346736 - iteration: 24\n",
      " 2021-09-13 16:45:59:102537 - iteration: 25\n",
      " 2021-09-13 16:46:54:272169 - iteration: 26\n",
      " 2021-09-13 16:47:49:477704 - iteration: 27\n",
      " 2021-09-13 16:48:43:442198 - iteration: 28\n",
      " 2021-09-13 16:49:38:396145 - iteration: 29\n",
      " 2021-09-13 16:50:32:292807 - iteration: 30\n",
      " 2021-09-13 16:51:26:687204 - iteration: 31\n",
      " 2021-09-13 16:52:20:807440 - iteration: 32\n",
      " 2021-09-13 16:53:16:446020 - iteration: 33\n",
      " 2021-09-13 16:54:12:139310 - iteration: 34\n",
      " 2021-09-13 16:55:05:936820 - iteration: 35\n",
      " 2021-09-13 16:56:00:552783 - iteration: 36\n",
      " 2021-09-13 16:56:54:005137 - iteration: 37\n",
      " 2021-09-13 16:57:48:608535 - iteration: 38\n",
      " 2021-09-13 16:58:42:576875 - iteration: 39\n",
      " 2021-09-13 16:59:37:101182 - iteration: 40\n",
      " 2021-09-13 17:00:35:809498 - iteration: 41\n",
      " 2021-09-13 17:01:28:846047 - iteration: 42\n",
      " 2021-09-13 17:02:23:311083 - iteration: 43\n",
      " 2021-09-13 17:03:16:873930 - iteration: 44\n",
      " 2021-09-13 17:04:11:339955 - iteration: 45\n",
      " 2021-09-13 17:05:04:582413 - iteration: 46\n",
      " 2021-09-13 17:05:58:546694 - iteration: 47\n",
      " 2021-09-13 17:06:52:125494 - iteration: 48\n",
      " 2021-09-13 17:07:49:102439 - iteration: 49\n",
      " 2021-09-13 17:08:47:332102 - iteration: 50\n",
      " 2021-09-13 17:09:42:380137 - iteration: 51\n",
      " 2021-09-13 17:10:36:492345 - iteration: 52\n",
      " 2021-09-13 17:11:30:567129 - iteration: 53\n",
      " 2021-09-13 17:12:24:988773 - iteration: 54\n",
      " 2021-09-13 17:13:18:929946 - iteration: 55\n",
      " 2021-09-13 17:14:12:924806 - iteration: 56\n",
      " 2021-09-13 17:15:06:843294 - iteration: 57\n",
      " 2021-09-13 17:16:01:843099 - iteration: 58\n",
      " 2021-09-13 17:16:57:112921 - iteration: 59\n",
      " 2021-09-13 17:17:52:394415 - iteration: 60\n",
      " 2021-09-13 17:18:45:702208 - iteration: 61\n",
      " 2021-09-13 17:19:40:284256 - iteration: 62\n",
      " 2021-09-13 17:20:33:969535 - iteration: 63\n",
      " 2021-09-13 17:21:27:881732 - iteration: 64\n",
      " 2021-09-13 17:22:22:142922 - iteration: 65\n",
      " 2021-09-13 17:23:15:897629 - iteration: 66\n",
      " 2021-09-13 17:24:10:450099 - iteration: 67\n",
      " 2021-09-13 17:25:05:194968 - iteration: 68\n",
      " 2021-09-13 17:25:59:752366 - iteration: 69\n",
      " 2021-09-13 17:26:53:644976 - iteration: 70\n",
      " 2021-09-13 17:27:48:537986 - iteration: 71\n",
      " 2021-09-13 17:28:42:245500 - iteration: 72\n",
      " 2021-09-13 17:29:36:314967 - iteration: 73\n",
      " 2021-09-13 17:30:30:628645 - iteration: 74\n",
      " 2021-09-13 17:31:24:135314 - iteration: 75\n",
      " 2021-09-13 17:32:18:692449 - iteration: 76\n",
      " 2021-09-13 17:33:12:972304 - iteration: 77\n",
      " 2021-09-13 17:34:07:197743 - iteration: 78\n",
      " 2021-09-13 17:35:00:864077 - iteration: 79\n",
      " 2021-09-13 17:35:55:753657 - iteration: 80\n",
      " 2021-09-13 17:36:49:896933 - iteration: 81\n",
      " 2021-09-13 17:37:44:361595 - iteration: 82\n",
      " 2021-09-13 17:38:37:529765 - iteration: 83\n",
      " 2021-09-13 17:39:31:758626 - iteration: 84\n",
      " 2021-09-13 17:40:27:253935 - iteration: 85\n",
      " 2021-09-13 17:41:21:170698 - iteration: 86\n",
      " 2021-09-13 17:42:14:795223 - iteration: 87\n",
      " 2021-09-13 17:43:08:792584 - iteration: 88\n",
      " 2021-09-13 17:44:03:565591 - iteration: 89\n",
      " 2021-09-13 17:44:56:961016 - iteration: 90\n",
      " 2021-09-13 17:45:51:010580 - iteration: 91\n",
      " 2021-09-13 17:46:45:008720 - iteration: 92\n",
      " 2021-09-13 17:47:39:052047 - iteration: 93\n",
      " 2021-09-13 17:48:33:115128 - iteration: 94\n",
      " 2021-09-13 17:49:26:111602 - iteration: 95\n",
      " 2021-09-13 17:50:20:028742 - iteration: 96\n",
      " 2021-09-13 17:51:14:129118 - iteration: 97\n",
      " 2021-09-13 17:52:08:640809 - iteration: 98\n",
      " 2021-09-13 17:53:08:578397 - iteration: 99\n",
      "-------------------------------------------------------------\n",
      "seg:\n",
      "update: 100, time: 56.752 total: 2.662 \n",
      "sn:\n",
      "update: 100, time: 56.754 total: 0.065 \n",
      "total:\n",
      "update: 100, time: 56.755 total: 3.959 \n",
      " 2021-09-13 17:54:05:609823 - iteration: 100\n",
      " 2021-09-13 17:54:59:141713 - iteration: 101\n",
      " 2021-09-13 17:55:54:081028 - iteration: 102\n",
      " 2021-09-13 17:56:48:389378 - iteration: 103\n",
      " 2021-09-13 17:57:43:268445 - iteration: 104\n",
      " 2021-09-13 17:58:37:432298 - iteration: 105\n",
      " 2021-09-13 17:59:31:092512 - iteration: 106\n",
      " 2021-09-13 18:00:26:245882 - iteration: 107\n",
      " 2021-09-13 18:01:19:958344 - iteration: 108\n",
      " 2021-09-13 18:02:15:160940 - iteration: 109\n",
      " 2021-09-13 18:03:08:960023 - iteration: 110\n",
      " 2021-09-13 18:04:03:551959 - iteration: 111\n",
      " 2021-09-13 18:04:57:478904 - iteration: 112\n",
      " 2021-09-13 18:05:52:810583 - iteration: 113\n",
      " 2021-09-13 18:06:46:651396 - iteration: 114\n",
      " 2021-09-13 18:07:42:959130 - iteration: 115\n",
      " 2021-09-13 18:08:38:874856 - iteration: 116\n",
      " 2021-09-13 18:09:33:178477 - iteration: 117\n",
      " 2021-09-13 18:10:27:917170 - iteration: 118\n",
      " 2021-09-13 18:11:21:588080 - iteration: 119\n",
      " 2021-09-13 18:12:16:022861 - iteration: 120\n",
      " 2021-09-13 18:13:10:657250 - iteration: 121\n",
      " 2021-09-13 18:14:07:253322 - iteration: 122\n",
      " 2021-09-13 18:15:02:616022 - iteration: 123\n",
      " 2021-09-13 18:15:58:535708 - iteration: 124\n",
      " 2021-09-13 18:16:53:127733 - iteration: 125\n",
      " 2021-09-13 18:17:49:198188 - iteration: 126\n",
      " 2021-09-13 18:18:44:444174 - iteration: 127\n",
      " 2021-09-13 18:19:40:005260 - iteration: 128\n",
      " 2021-09-13 18:20:34:918174 - iteration: 129\n",
      " 2021-09-13 18:21:30:234997 - iteration: 130\n",
      " 2021-09-13 18:22:25:153447 - iteration: 131\n",
      " 2021-09-13 18:23:21:166288 - iteration: 132\n",
      " 2021-09-13 18:24:16:335158 - iteration: 133\n",
      " 2021-09-13 18:25:14:603078 - iteration: 134\n",
      " 2021-09-13 18:26:18:088779 - iteration: 135\n",
      " 2021-09-13 18:27:14:860652 - iteration: 136\n",
      " 2021-09-13 18:28:10:023283 - iteration: 137\n",
      " 2021-09-13 18:29:04:855665 - iteration: 138\n",
      " 2021-09-13 18:30:01:743279 - iteration: 139\n",
      " 2021-09-13 18:30:57:840629 - iteration: 140\n",
      " 2021-09-13 18:31:54:590101 - iteration: 141\n",
      " 2021-09-13 18:32:50:383079 - iteration: 142\n",
      " 2021-09-13 18:33:46:711601 - iteration: 143\n",
      " 2021-09-13 18:34:42:138573 - iteration: 144\n",
      " 2021-09-13 18:35:38:088370 - iteration: 145\n",
      " 2021-09-13 18:36:38:259296 - iteration: 146\n",
      " 2021-09-13 18:37:46:335467 - iteration: 147\n",
      " 2021-09-13 18:38:48:370808 - iteration: 148\n",
      " 2021-09-13 18:39:45:783126 - iteration: 149\n",
      " 2021-09-13 18:40:44:557144 - iteration: 150\n",
      " 2021-09-13 18:41:42:832186 - iteration: 151\n",
      " 2021-09-13 18:42:39:911540 - iteration: 152\n",
      " 2021-09-13 18:43:38:234883 - iteration: 153\n",
      " 2021-09-13 18:44:34:710757 - iteration: 154\n",
      " 2021-09-13 18:45:31:160495 - iteration: 155\n",
      " 2021-09-13 18:46:28:358230 - iteration: 156\n",
      " 2021-09-13 18:47:27:487302 - iteration: 157\n",
      " 2021-09-13 18:48:26:710064 - iteration: 158\n",
      " 2021-09-13 18:49:24:070860 - iteration: 159\n",
      " 2021-09-13 18:50:21:196740 - iteration: 160\n",
      " 2021-09-13 18:51:17:691288 - iteration: 161\n",
      " 2021-09-13 18:52:13:529913 - iteration: 162\n",
      " 2021-09-13 18:53:09:615471 - iteration: 163\n",
      " 2021-09-13 18:54:07:037977 - iteration: 164\n",
      " 2021-09-13 18:55:07:267318 - iteration: 165\n",
      " 2021-09-13 18:56:03:583876 - iteration: 166\n",
      " 2021-09-13 18:56:59:207447 - iteration: 167\n",
      " 2021-09-13 18:57:55:791222 - iteration: 168\n",
      " 2021-09-13 18:58:52:573819 - iteration: 169\n",
      " 2021-09-13 18:59:48:341048 - iteration: 170\n",
      " 2021-09-13 19:00:46:347497 - iteration: 171\n",
      " 2021-09-13 19:01:44:504174 - iteration: 172\n",
      " 2021-09-13 19:02:40:404332 - iteration: 173\n",
      " 2021-09-13 19:03:37:321301 - iteration: 174\n",
      " 2021-09-13 19:04:31:933528 - iteration: 175\n",
      " 2021-09-13 19:05:27:793043 - iteration: 176\n",
      " 2021-09-13 19:06:25:885356 - iteration: 177\n",
      " 2021-09-13 19:07:24:654771 - iteration: 178\n",
      " 2021-09-13 19:08:22:236358 - iteration: 179\n",
      " 2021-09-13 19:09:19:015742 - iteration: 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2021-09-13 19:10:16:348803 - iteration: 181\n",
      " 2021-09-13 19:11:13:799563 - iteration: 182\n",
      " 2021-09-13 19:12:10:260383 - iteration: 183\n",
      " 2021-09-13 19:13:07:598266 - iteration: 184\n",
      " 2021-09-13 19:14:05:673744 - iteration: 185\n",
      " 2021-09-13 19:15:05:235403 - iteration: 186\n",
      " 2021-09-13 19:16:05:296143 - iteration: 187\n",
      " 2021-09-13 19:17:04:322919 - iteration: 188\n",
      " 2021-09-13 19:18:00:427744 - iteration: 189\n",
      " 2021-09-13 19:18:57:200859 - iteration: 190\n",
      " 2021-09-13 19:19:53:790155 - iteration: 191\n",
      " 2021-09-13 19:20:57:721688 - iteration: 192\n",
      " 2021-09-13 19:22:06:288556 - iteration: 193\n",
      " 2021-09-13 19:23:03:306991 - iteration: 194\n",
      " 2021-09-13 19:24:02:308190 - iteration: 195\n",
      " 2021-09-13 19:24:59:339017 - iteration: 196\n",
      " 2021-09-13 19:26:01:834515 - iteration: 197\n",
      " 2021-09-13 19:26:58:957791 - iteration: 198\n",
      " 2021-09-13 19:27:56:131716 - iteration: 199\n",
      "-------------------------------------------------------------\n",
      "seg:\n",
      "update: 200, time: 59.275 total: 2.455 \n",
      "sn:\n",
      "update: 200, time: 59.276 total: 0.081 \n",
      "total:\n",
      "update: 200, time: 59.276 total: 4.067 \n",
      " 2021-09-13 19:28:55:917263 - iteration: 200\n",
      " 2021-09-13 19:29:56:236518 - iteration: 201\n",
      " 2021-09-13 19:30:55:574939 - iteration: 202\n",
      " 2021-09-13 19:31:54:058038 - iteration: 203\n",
      " 2021-09-13 19:32:52:864618 - iteration: 204\n",
      " 2021-09-13 19:33:50:350449 - iteration: 205\n",
      " 2021-09-13 19:34:44:746017 - iteration: 206\n",
      " 2021-09-13 19:35:39:485338 - iteration: 207\n",
      " 2021-09-13 19:36:34:632918 - iteration: 208\n",
      " 2021-09-13 19:37:29:381808 - iteration: 209\n",
      " 2021-09-13 19:38:25:330600 - iteration: 210\n",
      " 2021-09-13 19:39:20:174197 - iteration: 211\n",
      " 2021-09-13 19:40:15:402209 - iteration: 212\n",
      " 2021-09-13 19:41:09:449355 - iteration: 213\n",
      " 2021-09-13 19:42:04:417579 - iteration: 214\n",
      " 2021-09-13 19:42:58:781717 - iteration: 215\n",
      " 2021-09-13 19:43:53:944363 - iteration: 216\n",
      " 2021-09-13 19:44:48:002706 - iteration: 217\n",
      " 2021-09-13 19:45:42:696453 - iteration: 218\n",
      " 2021-09-13 19:46:37:438223 - iteration: 219\n",
      " 2021-09-13 19:47:31:784397 - iteration: 220\n",
      " 2021-09-13 19:48:29:015028 - iteration: 221\n",
      " 2021-09-13 19:49:27:500353 - iteration: 222\n",
      " 2021-09-13 19:50:23:872030 - iteration: 223\n",
      " 2021-09-13 19:51:22:208365 - iteration: 224\n",
      " 2021-09-13 19:52:21:128018 - iteration: 225\n",
      " 2021-09-13 19:53:20:019000 - iteration: 226\n",
      " 2021-09-13 19:54:18:927398 - iteration: 227\n",
      " 2021-09-13 19:55:21:938986 - iteration: 228\n",
      " 2021-09-13 19:56:20:341470 - iteration: 229\n",
      " 2021-09-13 19:57:23:855889 - iteration: 230\n",
      " 2021-09-13 19:58:22:254365 - iteration: 231\n",
      " 2021-09-13 19:59:23:161971 - iteration: 232\n",
      " 2021-09-13 20:00:20:777109 - iteration: 233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-32697cb45e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_enumerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambdas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'update_w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mbatch_enumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-local/AdaShare/envs/blockdrop_env.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, lambdas, is_policy, flag, num_train_layers, hard_sampling)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_depth_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'keypoint'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keypoint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'edge'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_edge_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-local/AdaShare/envs/blockdrop_env.py\u001b[0m in \u001b[0;36mbackward_network\u001b[0;34m(self, lambdas)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alphas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while current_iter < opt['train']['total_iters']:\n",
    "    start_time = time.time()\n",
    "    print(f\" {datetime.now().strftime('%F %H:%M:%S:%f')} - iteration: {current_iter}\")\n",
    "    environ.train()\n",
    "    current_iter += 1\n",
    "    \n",
    "    # warm up\n",
    "\n",
    "    if current_iter < opt['train']['warm_up_iters']:\n",
    "        \n",
    "        batch_idx, batch = next(batch_enumerator)\n",
    "        environ.set_inputs(batch)\n",
    "        \n",
    "        environ.optimize(opt['lambdas'], is_policy=False, flag='update_w')\n",
    "        \n",
    "        if batch_idx == len(train_loader) - 1:\n",
    "            batch_enumerator = enumerate(train_loader)\n",
    "\n",
    "        if should(current_iter, opt['train']['print_freq']):\n",
    "            environ.print_loss(current_iter, start_time)\n",
    "            environ.resize_results()\n",
    "\n",
    "        # validation\n",
    "        if should(current_iter, opt['train']['val_freq']):\n",
    "            environ.eval()\n",
    "            num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "            val_metrics = eval(environ, val_loader, opt['tasks'], policy=False, num_train_layers=None, num_seg_cls=num_seg_class)\n",
    "            environ.print_loss(current_iter, start_time, val_metrics)\n",
    "            environ.save('latest', current_iter)\n",
    "            environ.train()\n",
    "\n",
    "    else:\n",
    "        if flag_warmup:\n",
    "            environ.define_optimizer(policy_learning=True)\n",
    "            environ.define_scheduler(policy_learning=True)\n",
    "\n",
    "            flag_warmup = False\n",
    "\n",
    "        if current_iter == opt['train']['warm_up_iters']:\n",
    "            environ.save('warmup', current_iter)\n",
    "            environ.fix_alpha()\n",
    "\n",
    "        #-----------------------------------------\n",
    "        # Update the network weights\n",
    "        #-----------------------------------------\n",
    "        if flag == 'update_w':\n",
    "            \n",
    "            current_iter_w += 1\n",
    "            batch_idx_w, batch = next(batch_enumerator1)\n",
    "            environ.set_inputs(batch)\n",
    "\n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "\n",
    "            environ.optimize(opt['lambdas'], is_policy=opt['policy'], flag=flag, num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'])\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss(current_iter, start_time)\n",
    "                environ.resize_results()\n",
    "\n",
    "            if should(current_iter_w, opt['train']['weight_iter_alternate']):\n",
    "                flag = 'update_alpha'\n",
    "                environ.fix_w()\n",
    "                environ.free_alpha()\n",
    "                # do the validation on the test set\n",
    "                environ.eval()\n",
    "                print('Evaluating...')\n",
    "                num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "                val_metrics = eval(environ, val_loader, opt['tasks'], policy=opt['policy'],\n",
    "                                   num_train_layers=num_train_layers, hard_sampling=opt['train']['hard_sampling'],\n",
    "                                   num_seg_cls=num_seg_class)\n",
    "                environ.print_loss(current_iter, start_time, val_metrics)\n",
    "                environ.save('latest', current_iter)\n",
    "\n",
    "                if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "                        (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "                    new_value = 0\n",
    "\n",
    "                    for k in refer_metrics.keys():\n",
    "                        if k in val_metrics.keys():\n",
    "                            for kk in val_metrics[k].keys():\n",
    "                                if not kk in refer_metrics[k].keys():\n",
    "                                    continue\n",
    "                                if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or (\n",
    "                                        k == 'depth' and not kk.startswith('sigma')) or (kk == 'err'):\n",
    "                                    value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "                                else:\n",
    "                                    value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "\n",
    "                                value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "                                new_value += value\n",
    "\n",
    "                    if new_value > best_value:\n",
    "                        best_value = new_value\n",
    "                        best_metrics = val_metrics\n",
    "                        best_iter = current_iter\n",
    "                        environ.save('best', current_iter)\n",
    "                    print('new value: %.3f' % new_value)\n",
    "                    print('best iter: %d, best_value: %.3f' % (best_iter, best_value), best_metrics)\n",
    "                environ.train()\n",
    "\n",
    "            if batch_idx_w == len(train1_loader) - 1:\n",
    "                batch_enumerator1 = enumerate(train1_loader)\n",
    "                \n",
    "        #-----------------------------------------\n",
    "        # update the policy network\n",
    "        #-----------------------------------------\n",
    "        elif flag == 'update_alpha':\n",
    "            current_iter_a += 1\n",
    "            batch_idx_a, batch = next(batch_enumerator2)\n",
    "            environ.set_inputs(batch)\n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "\n",
    "            environ.optimize(opt['lambdas'], is_policy=opt['policy'], flag=flag, num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'])\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss(current_iter, start_time)\n",
    "                environ.resize_results()\n",
    "                # environ.visual_policy(current_iter)\n",
    "\n",
    "            if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "                flag = 'update_w'\n",
    "                environ.fix_alpha()\n",
    "                environ.free_w(opt['fix_BN'])\n",
    "                environ.decay_temperature()\n",
    "                # print the distribution\n",
    "                dists = environ.get_policy_prob()\n",
    "                print(np.concatenate(dists, axis=-1))\n",
    "                p_epoch += 1\n",
    "\n",
    "            if batch_idx_a == len(train2_loader) - 1:\n",
    "                batch_enumerator2 = enumerate(train2_loader)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('flag %s is not recognized' % flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730be87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b57a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc512cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-cpu",
   "language": "python",
   "name": "pyt-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
