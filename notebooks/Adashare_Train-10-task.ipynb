{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408f1261",
   "metadata": {},
   "source": [
    "## Initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55604c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:32.317132Z",
     "start_time": "2022-08-29T18:50:32.285894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0c686b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:34.183370Z",
     "start_time": "2022-08-29T18:50:32.319641Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:08.233990Z",
     "iopub.status.busy": "2022-01-07T22:44:08.233053Z",
     "iopub.status.idle": "2022-01-07T22:44:08.273284Z",
     "shell.execute_reply": "2022-01-07T22:44:08.271908Z",
     "shell.execute_reply.started": "2022-01-07T22:44:08.233943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "# sys.path.insert(0, '/home/kbardool/kusanagi/AdaSparseChem/src') ; print(sys.path)\n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "import types, copy, pprint\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from utils import (initialize, init_dataloaders, init_environment, init_wandb, training_initializations, model_initializations, \n",
    "                   check_for_resume_training, disp_dataloader_info, disp_info_1, warmup_phase, weight_policy_training, \n",
    "                   display_gpu_info, init_dataloaders_by_fold_id, print_separator, print_heading, print_underline,\n",
    "                   timestring, print_loss, print_metrics_cr, get_command_line_args, load_from_pickle) \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "pd.options.display.width = 132\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Adashare_Train.ipynb\"\n",
    "\n",
    "## Set visible GPU device \n",
    "##----------------------------------------------\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33ac6e",
   "metadata": {},
   "source": [
    "# Initialization and  Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee652c57",
   "metadata": {},
   "source": [
    "### Parse Input Args  - Read YAML config file - wandb initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca1c17d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:34.209034Z",
     "start_time": "2022-08-29T18:50:34.185697Z"
    }
   },
   "outputs": [],
   "source": [
    "# synthetic_config_file  = \"../yamls/chembl_synt_train.yaml\"\n",
    "# config_file      = \"../yamls/chembl_mini_train.yaml\"\n",
    "config_file      = \"../yamls/chembl_cb29_train_1task.yaml\"\n",
    "config_file      = \"../yamls/chembl_cb29_train_10task.yaml\"\n",
    "batch_size=4098\n",
    "# batch_size=2048\n",
    "# RESUME_MODEL_CKPT = 'model_train_ep_25_seed_0088'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edb398",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####   For Resume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c33f25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:34.292405Z",
     "start_time": "2022-08-29T18:50:34.211766Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "restart_input_args = f\" --config  {config_file} \" \\\n",
    "             f\" --batch_size       {batch_size} \"  \\\n",
    "             \" --exp_desc            10-task warmup with policy training \" \\\n",
    "             \" --hidden_size             4000 4000 4000 4000 4000 4000 \"  \\\n",
    "             \" --warmup_epochs             50 \"  \\\n",
    "             \" --tail_hidden_size        4000 \"  \\\n",
    "             \" --first_dropout           0.80 \"  \\\n",
    "             \" --middle_dropout          0.80 \"  \\\n",
    "             \" --last_dropout            0.80 \"  \\\n",
    "             \" --seed_idx                   0 \"  \\\n",
    "             \" --task_lr                0.001 \"  \\\n",
    "             \" --backbone_lr            0.001 \"  \\\n",
    "             \" --decay_lr_rate            0.5 \"  \\\n",
    "             \" --decay_lr_freq             40 \"  \\\n",
    "             \" --decay_lr_cooldown         10 \"  \\\n",
    "             \" --policy_lr               0.01 \"  \\\n",
    "             \" --policy_decay_lr_rate     0.5 \"  \\\n",
    "             \" --policy_decay_lr_freq      40 \"  \\\n",
    "             \" --policy_decay_lr_cooldown  10 \"  \\\n",
    "             \" --lambda_tasks             1.0 \"  \\\n",
    "             \" --lambda_sparsity        0.001 \"  \\\n",
    "             \" --lambda_sharing          0.05 \"  \\\n",
    "             \" --pytorch_threads            7 \"  \\\n",
    "             \" --cuda_devices               2\"   \\\n",
    "             \" --gpu_ids                    0 \"  \\\n",
    "             \" --resume\"                       \\\n",
    "             \" --resume_path        ../../experiments/AdaSparseChem-cb29-10task/4000x6_0822_1755_lr0.001_do0.8\" \\\n",
    "             \" --resume_ckpt        model_warmup_last_ep_10\" \\\n",
    "             \" --resume_metrics     metrics_warmup_last_ep_10.pickle\" \\\n",
    "             \" --exp_id             1x50t0va\" \\\n",
    "             \" --exp_name           0822_1755 \" \\\n",
    "             \" --folder_sfx         RESUME_2 \"\n",
    "\n",
    "#              \" --resume_ckpt        model_best_model\" \\\n",
    "#              \" --resume_metrics     metrics_best.pickle\" \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68145e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T17:38:07.664270Z",
     "start_time": "2022-06-24T17:38:07.630274Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "source": [
    "####  For Initiating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3a7f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:34.319297Z",
     "start_time": "2022-08-29T18:50:34.294010Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [],
   "source": [
    "input_args = f\" --config          {config_file} \" \\\n",
    "             f\" --batch_size       {batch_size} \"  \\\n",
    "             \" --exp_desc            10-task warmup with policy training \" \\\n",
    "             \" --exp_desc            10-task with policy training \" \\\n",
    "             \" --hidden_size             4000 4000 4000 4000 4000 4000 \"  \\\n",
    "             \" --tail_hidden_size        4000 \"  \\\n",
    "             \" --warmup_epochs             20 \"  \\\n",
    "             \" --first_dropout           0.80 \"  \\\n",
    "             \" --middle_dropout          0.80 \"  \\\n",
    "             \" --last_dropout            0.80 \"  \\\n",
    "             \" --seed_idx                   0 \"  \\\n",
    "             \" --task_lr                0.001 \"  \\\n",
    "             \" --backbone_lr            0.001 \"  \\\n",
    "             \" --decay_lr_rate            0.5 \"  \\\n",
    "             \" --decay_lr_freq             20 \"  \\\n",
    "             \" --decay_lr_cooldown          5 \"  \\\n",
    "             \" --policy_lr               0.01 \"  \\\n",
    "             \" --policy_decay_lr_rate     0.5 \"  \\\n",
    "             \" --policy_decay_lr_freq      20 \"  \\\n",
    "             \" --policy_decay_lr_cooldown   5 \"  \\\n",
    "             \" --lambda_tasks             1.0 \"  \\\n",
    "             \" --lambda_sparsity        0.005 \"  \\\n",
    "             \" --lambda_sharing          0.05 \"  \\\n",
    "             \" --pytorch_threads            7 \"  \\\n",
    "             \" --cuda_devices               2\"   \\\n",
    "             \" --gpu_ids                    0 \"  \\\n",
    "\n",
    "#              \" --decay_lr_rate       0.3 \"  \\\n",
    "#              \" --decay_lr_freq        10 \"  \\\n",
    "#              \" --policy_lr         0.001 \"  \\\n",
    "#              \" --lambda_sparsity    0.02 \"  \\\n",
    "#              \" --lambda_sharing     0.01 \"  \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ffb30",
   "metadata": {},
   "source": [
    "### Read yaml Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fcbe87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:34.354559Z",
     "start_time": "2022-08-29T18:50:34.321185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " command line parms : \n",
      "------------------------\n",
      " config...................  ../yamls/chembl_cb29_train_10task.yaml\n",
      " project_name.............  None\n",
      " exp_id...................  154b30qo\n",
      " exp_name.................  None\n",
      " folder_sfx...............  None\n",
      " exp_desc.................  10-task with policy training\n",
      " hidden_sizes.............  [4000, 4000, 4000, 4000, 4000, 4000]\n",
      " tail_hidden_size.........  [4000]\n",
      " warmup_epochs............  20\n",
      " training_epochs..........  None\n",
      " seed_idx.................  0\n",
      " batch_size...............  4098\n",
      " first_dropout............  0.8\n",
      " middle_dropout...........  0.8\n",
      " last_dropout.............  0.8\n",
      " backbone_lr..............  0.001\n",
      " task_lr..................  0.001\n",
      " policy_lr................  0.01\n",
      " decay_lr_rate............  0.5\n",
      " decay_lr_freq............  20\n",
      " decay_lr_cooldown........  5\n",
      " policy_decay_lr_rate.....  0.5\n",
      " policy_decay_lr_freq.....  20\n",
      " policy_decay_lr_cooldown.  5\n",
      " lambda_tasks.............  1.0\n",
      " lambda_sparsity..........  0.005\n",
      " lambda_sharing...........  0.05\n",
      " cuda_devices.............  2\n",
      " gpu_ids..................  [0]\n",
      " pytorch_threads..........  7\n",
      " skip_residual............  False\n",
      " skip_hidden..............  False\n",
      " resume...................  False\n",
      " resume_path..............  None\n",
      " resume_ckpt..............  None\n",
      " resume_metrics...........  None\n",
      " cpu......................  False\n",
      " min_samples_class........  None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns = types.SimpleNamespace()\n",
    "input_args = input_args.split() if input_args is not None else input_args\n",
    "# input_args = restart_input_args.split() \n",
    "ns.args = get_command_line_args(input_args, display = True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=ns.args.cuda_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0c6844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:34.402934Z",
     "start_time": "2022-08-29T18:50:34.356367Z"
    }
   },
   "outputs": [],
   "source": [
    "# display_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddaf625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:42.746760Z",
     "start_time": "2022-08-29T18:50:34.404842Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "################### READ YAML ####################\n",
      "##################################################\n",
      " Pytorch thread count: 20\n",
      " Set Pytorch thread count to : 7\n",
      " Pytorch thread count set to : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkbardool\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kevin/WSL-projs/AdaSparseChem/notebooks/wandb/run-20220829_205037-154b30qo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/154b30qo\" target=\"_blank\">0829_2050</a></strong> to <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WandB Initialization -----------------------------------------------------------\n",
      " PROJECT NAME: AdaSparseChem-cb29-10Task\n",
      " RUN ID      : 154b30qo \n",
      " RUN NAME    : 0829_2050\n",
      " --------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " log_dir              create folder:  ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      " result_dir           folder exists:  ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      " checkpoint_dir       folder exists:  ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " experiment name       : 0829_2050 \n",
      " experiment id         : 154b30qo \n",
      " folder_name           : 4000x6_0829_2050_lr0.001_do0.8 \n",
      " experiment description: 10-task with policy training\n",
      " Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      " Random  seed used     : 88 \n",
      " log folder            : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      " checkpoint folder     : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      " Gpu ids               : [0]\n",
      " Seed index            : 0\n",
      " policy_iter           : best\n",
      " Data Split ratios     : [0.725, 0.225, 0.05]\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "        project_name : AdaSparseChem-cb29-10Task\n",
      "              exp_id : 154b30qo\n",
      "        exp_name_pfx : 0829_2050\n",
      "            exp_name : 0829_2050\n",
      "          exp_folder : 4000x6_0829_2050_lr0.001_do0.8\n",
      "     exp_description : 10-task with policy training\n",
      "          folder_sfx : None\n",
      "         random_seed : 88\n",
      "           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "              config : ../yamls/chembl_cb29_train_10task.yaml\n",
      "                 cpu : None\n",
      "             gpu_ids : [0]\n",
      "            backbone : SparseChem\n",
      "               tasks : ['class', 'class', 'class', 'class', 'class', 'class', 'class', 'class', 'class', 'class']\n",
      "     tasks_num_class : [472, 624, 688, 192, 620, 184, 224, 148, 344, 72]\n",
      "             lambdas : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "             verbose : False\n",
      "     input_size_freq : None\n",
      "          input_size : 32000\n",
      "        hidden_sizes : [4000, 4000, 4000, 4000, 4000, 4000]\n",
      "    tail_hidden_size : [4000]\n",
      " first_non_linearity : relu\n",
      "middle_non_linearity : relu\n",
      "  last_non_linearity : relu\n",
      "       first_dropout : 0.8\n",
      "      middle_dropout : 0.8\n",
      "        last_dropout : 0.8\n",
      "   class_output_size : None\n",
      "    regr_output_size : None\n",
      "              policy : True\n",
      "        policy_model : task-specific\n",
      "       skip_residual : False\n",
      "         skip_hidden : False\n",
      "           is_sparse : True\n",
      "diff_sparsity_weights : False\n",
      "          is_sharing : True\n",
      "diff_sharing_weights : False\n",
      "          skip_layer : 0\n",
      "       is_curriculum : False\n",
      "    curriculum_speed : 3\n",
      "              fix_BN : False\n",
      "     retrain_from_pl : False\n",
      "\n",
      "train\n",
      "-----\n",
      "          batch_size : 128\n",
      "       warmup_epochs : 20\n",
      "     training_epochs : 250\n",
      "         total_iters : 25000\n",
      "       warm_up_iters : None\n",
      "             task_lr : 0.001\n",
      "         backbone_lr : 0.001\n",
      "    weight_optimizer : adam\n",
      "    policy_optimizer : adam\n",
      "       decay_lr_rate : 0.5\n",
      "       decay_lr_freq : 20\n",
      "   decay_lr_cooldown : 5\n",
      "           policy_lr : 0.01\n",
      "policy_decay_lr_rate : 0.5\n",
      "policy_decay_lr_freq : 20\n",
      "policy_decay_lr_cooldown : 5\n",
      "     lambda_sparsity : 0.005\n",
      "      lambda_sharing : 0.05\n",
      "        lambda_tasks : 1.0\n",
      "         init_method : random\n",
      "           init_temp : 2.5\n",
      "          decay_temp : 0.75\n",
      "     decay_temp_freq : 3\n",
      "     init_neg_logits : None\n",
      "       hard_sampling : False\n",
      "            val_freq : 500\n",
      "          print_freq : -1\n",
      "warmup_iter_alternate : -1\n",
      "weight_iter_alternate : -1\n",
      "alpha_iter_alternate : -1\n",
      "           val_iters : -1\n",
      "              resume : False\n",
      "      retrain_resume : False\n",
      "         policy_iter : best\n",
      "          which_iter : warmup\n",
      "\n",
      "paths\n",
      "-----\n",
      "             log_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      "          result_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      "      checkpoint_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0829_2050_lr0.001_do0.8\n",
      "\n",
      "dataload\n",
      "--------\n",
      "             dataset : Chembl29\n",
      "            dataroot : ../../MLDatasets/chembl29_10task\n",
      "                   x : chembl_29_X.npy\n",
      "      x_split_ratios : [0.725, 0.225, 0.05]\n",
      "             folding : chembl_29_folding.npy\n",
      "             y_tasks : ['chembl_29_Y_tg_0_cols_472.npy', 'chembl_29_Y_tg_1_cols_624.npy', 'chembl_29_Y_tg_6_cols_688.npy', 'chembl_29_Y_tg_10_cols_192.npy', 'chembl_29_Y_tg_11_cols_620.npy', 'chembl_29_Y_tg_643_cols_184.npy', 'chembl_29_Y_tg_836_cols_224.npy', 'chembl_29_Y_tg_1005_cols_148.npy', 'chembl_29_Y_tg_1028_cols_344.npy', 'chembl_29_Y_tg_1031_cols_72.npy']\n",
      "            y_censor : None\n",
      "         fold_inputs : 32000\n",
      "     input_transform : None\n",
      "       weights_class : None\n",
      "   min_samples_class : 1\n",
      "           fold_test : [0]\n",
      "             fold_va : [1]\n",
      "         fold_warmup : [2, 3, 4]\n",
      "        fold_weights : [2, 3]\n",
      "         fold_policy : [4]\n",
      "\n",
      "SC\n",
      "--\n",
      "      normalize_loss : None\n",
      "     pytorch_threads : 7\n",
      "            seed_idx : 0\n",
      "         resume_path : None\n",
      "         resume_ckpt : None\n",
      "      resume_metrics : None\n"
     ]
    }
   ],
   "source": [
    "opt = initialize(ns, build_folders = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8e71cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:42.794603Z",
     "start_time": "2022-08-29T18:50:42.750956Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.wandb_run.finish()\n",
    "# ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58caea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T07:40:24.684944Z",
     "start_time": "2022-07-04T07:40:24.654093Z"
    }
   },
   "source": [
    "### Setup Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a2edeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:49.481517Z",
     "start_time": "2022-08-29T18:50:42.798316Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Warmup folds    : [2, 3, 4]\n",
      " Weights folds   : [2, 3]\n",
      " Policy folds    : [4]\n",
      " Validation folds: [1]\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 472  Y rows with populated labels: 21581  non zero cols: 53309\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 624  Y rows with populated labels: 22820  non zero cols: 55015\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 688  Y rows with populated labels: 53858  non zero cols: 186792\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 192  Y rows with populated labels: 12262  non zero cols: 27798\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 620  Y rows with populated labels: 30988  non zero cols: 86678\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 184  Y rows with populated labels: 9818  non zero cols: 27152\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 224  Y rows with populated labels: 7090  non zero cols: 22638\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 148  Y rows with populated labels: 13262  non zero cols: 28925\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 344  Y rows with populated labels: 20684  non zero cols: 63517\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 72  Y rows with populated labels: 4475  non zero cols: 10677\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 472  Y rows with populated labels: 15323  non zero cols: 38126\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 624  Y rows with populated labels: 15572  non zero cols: 38310\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 688  Y rows with populated labels: 34821  non zero cols: 121231\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 192  Y rows with populated labels: 7752  non zero cols: 17657\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 620  Y rows with populated labels: 20914  non zero cols: 58596\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 184  Y rows with populated labels: 7179  non zero cols: 19397\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 224  Y rows with populated labels: 4476  non zero cols: 14840\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 148  Y rows with populated labels: 8900  non zero cols: 19704\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 344  Y rows with populated labels: 12593  non zero cols: 39248\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 72  Y rows with populated labels: 2897  non zero cols: 6734\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 472  Y rows with populated labels: 6258  non zero cols: 15183\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 624  Y rows with populated labels: 7248  non zero cols: 16705\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 688  Y rows with populated labels: 19037  non zero cols: 65561\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 192  Y rows with populated labels: 4510  non zero cols: 10141\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 620  Y rows with populated labels: 10074  non zero cols: 28082\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 184  Y rows with populated labels: 2639  non zero cols: 7755\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 224  Y rows with populated labels: 2614  non zero cols: 7798\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 148  Y rows with populated labels: 4362  non zero cols: 9221\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 344  Y rows with populated labels: 8091  non zero cols: 24269\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 72  Y rows with populated labels: 1578  non zero cols: 3943\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 472  Y rows with populated labels: 5643  non zero cols: 14167\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 624  Y rows with populated labels: 8661  non zero cols: 20196\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 688  Y rows with populated labels: 17973  non zero cols: 61209\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 192  Y rows with populated labels: 3776  non zero cols: 8372\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 620  Y rows with populated labels: 10004  non zero cols: 27223\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 184  Y rows with populated labels: 2519  non zero cols: 6085\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 224  Y rows with populated labels: 2379  non zero cols: 7992\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 148  Y rows with populated labels: 4764  non zero cols: 9313\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 344  Y rows with populated labels: 7460  non zero cols: 21553\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 72  Y rows with populated labels: 1916  non zero cols: 4391\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      " dataloader preparation - set number of batches per warmup training epoch to: 1989\n",
      " dataloader preparation - set number of batches per weight training epoch to: 1318\n",
      " dataloader preparation - set number of batches per policy training epoch to: 671\n",
      " dataloader preparation - set number of batches per validation to           : 675\n",
      "\n",
      " trainset.y_class                                   :  [(254529, 472), (254529, 624), (254529, 688), (254529, 192), (254529, 620), (254529, 184), (254529, 224), (254529, 148), (254529, 344), (254529, 72)] \n",
      " trainset1.y_class                                  :  [(168649, 472), (168649, 624), (168649, 688), (168649, 192), (168649, 620), (168649, 184), (168649, 224), (168649, 148), (168649, 344), (168649, 72)] \n",
      " trainset2.y_class                                  :  [(85880, 472), (85880, 624), (85880, 688), (85880, 192), (85880, 620), (85880, 184), (85880, 224), (85880, 148), (85880, 344), (85880, 72)] \n",
      " valset.y_class                                     :  [(86274, 472), (86274, 624), (86274, 688), (86274, 192), (86274, 620), (86274, 184), (86274, 224), (86274, 148), (86274, 344), (86274, 72)]  \n",
      "\n",
      "                                Total                :  595332 \n",
      "\n",
      "\n",
      "Training dataset :\n",
      "--------------------\n",
      "  Size of training set 0 (warm up)                   :  254529 \n",
      "  Number of batches in training 0 (warm up)          :  1989 \n",
      "  Size of training set 1 (network parms)             :  168649 \n",
      "  Number of batches in training 1 (network parms)    :  1318 \n",
      "  Size of training set 2 (policy weights)            :  85880 \n",
      "  Number of batches in training 2 (policy weights)   :  671 \n",
      "  training set num of positive                       :  18631 \n",
      "  training set num of negative                       :  107922 \n",
      "  task_weights_list[0].aggregation_weight sum        :  199.0\n",
      "\n",
      "\n",
      "Validation dataset :\n",
      "----------------------\n",
      "  Rows in dataset                                    : 86274\n",
      "  Number of batches in dataset                       : 675\n",
      "  validation set num of positive                     : 18631\n",
      "  validation set num of negative                     : 107922\n",
      "  task_weights_list[0].aggregation_weight sum        : 199.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dldrs = init_dataloaders(opt, verbose = False)\n",
    "dldrs = init_dataloaders_by_fold_id(opt, verbose = False)\n",
    "disp_dataloader_info(dldrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bfa8d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62717fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:53.496783Z",
     "start_time": "2022-08-29T18:50:49.485708Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:16.229028Z",
     "iopub.status.busy": "2022-01-07T22:44:16.227544Z",
     "iopub.status.idle": "2022-01-07T22:44:16.659397Z",
     "shell.execute_reply": "2022-01-07T22:44:16.658348Z",
     "shell.execute_reply.started": "2022-01-07T22:44:16.228966Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "############# CREATE THE ENVIRONMENT #############\n",
      "##################################################\n",
      " device is  cuda:0\n",
      "--------------------------------------------------\n",
      " SparseChem_Backbone  Ver: 1.0 Init() Start \n",
      "-------------------------------------------------- \n",
      "\n",
      " layer config        : [1, 1, 1, 1, 1, 1] \n",
      " skip residual layers: False   skip hidden layers  : False\n",
      " SparseChem_BackBone() Input Layer  - Input: 32000  output: 4000  non-linearity:<class 'torch.nn.modules.activation.ReLU'>\n",
      " Hidden layer 0 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 1 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 2 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 3 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 4 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Final Hidden layer 4 : Input size: 4000   output size:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Module List \n",
      "--------------------------------------------------\n",
      " Initialize weights \n",
      "-------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " SparseChem Backbone -- Final configuration(2) : \n",
      "                     self.blocks: <class 'torch.nn.modules.container.ModuleList'>  len:6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "\n",
      " Input_Layer  type:<class 'torch.nn.modules.container.Sequential'>  \n",
      "----------------------------------------------------------------------\n",
      "self.Input_layer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layers/Blocks    : <class 'torch.nn.modules.container.ModuleList'>   len:6 \n",
      "Resdiual layers  : <class 'torch.nn.modules.container.ModuleList'>   len:6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 1  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 1  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 2  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 2  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 3  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 3  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 4  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 4  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 5  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 5  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 6  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 6  type:<class 'NoneType'> \n",
      " None\n",
      "\n",
      "\n",
      "\n",
      " SparseChem_Backbone Init() End \n",
      "----------------------------------------------------\n",
      "* SparseChemEnv environment successfully created\n",
      "---------------------------------------------------- \n",
      "\n",
      " \n",
      "\n",
      "SparseChemEnv  Configuration       \n",
      "---------------------------------------- \n",
      "\n",
      "----------------\n",
      "networks       :\n",
      "----------------\n",
      " {'mtl-net': MTL3(\n",
      "  (backbone): SparseChem_Backbone(\n",
      "    (Input_Layer): Sequential(\n",
      "      (linear): SparseLinear(in_features=32000, out_features=4000, bias=True)\n",
      "      (non_linear): ReLU()\n",
      "      (dropout): Dropout(p=0.8, inplace=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (1): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (2): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (3): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (4): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (5): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (residuals): ModuleList(\n",
      "      (0): None\n",
      "      (1): None\n",
      "      (2): None\n",
      "      (3): None\n",
      "      (4): None\n",
      "      (5): None\n",
      "    )\n",
      "  )\n",
      "  (task1_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=472, bias=True)\n",
      "  )\n",
      "  (task2_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=624, bias=True)\n",
      "  )\n",
      "  (task3_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=688, bias=True)\n",
      "  )\n",
      "  (task4_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=192, bias=True)\n",
      "  )\n",
      "  (task5_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=620, bias=True)\n",
      "  )\n",
      "  (task6_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=184, bias=True)\n",
      "  )\n",
      "  (task7_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=224, bias=True)\n",
      "  )\n",
      "  (task8_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=148, bias=True)\n",
      "  )\n",
      "  (task9_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=344, bias=True)\n",
      "  )\n",
      "  (task10_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=72, bias=True)\n",
      "  )\n",
      ")}\n",
      "\n",
      "----------------\n",
      "optimizers     :\n",
      "----------------\n",
      " {}\n",
      "\n",
      "----------------\n",
      "schedulers     :\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "environ = init_environment(ns, opt, is_train = True, display_cfg = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738062",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Initiate / Resume Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba16a95c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:53.531378Z",
     "start_time": "2022-08-29T18:50:53.498964Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt['train']['which_iter'] :  warmup\n",
      "##################################################\n",
      "######## Initiate Training from scratch  #########\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "check_for_resume_training(ns, opt, environ, epoch = 0 , iter = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278d6d1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Warmup Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa99797",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Warmup Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f8a21db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:50:53.782810Z",
     "start_time": "2022-08-29T18:50:53.533584Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model optimizers defined . . . policy_learning: False\n",
      " Model schedulers defined . . . policy_learning: False\n",
      " Metrics CSV file header written . . . \n",
      " Model initializations complete . . . \n",
      " training preparation: - check for CUDA - cuda available as device id: [0]\n",
      "sparsechem_env.cuda()\n",
      " training preparation: - set print_freq to                                 : 1989 \n",
      " training preparation: - set number of batches per warmup training epoch to: 1989\n",
      " training preparation: - set number of batches per weight training epoch to: 1318\n",
      " training preparation: - set number of batches per policy training epoch to: 671\n",
      " training preparation: - set number of batches per validation to           : 675\n",
      " training preparation complete . . .\n"
     ]
    }
   ],
   "source": [
    "model_initializations(ns, opt, environ, phase = 'update_weights', policy_learning = False)\n",
    "\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 1000, weight_iterations = 750, policy_iterations = 250, eval_iterations = 250, warmup = True)\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 2, eval_iterations = 2, warmup = True)\n",
    "training_initializations(ns, opt, environ, dldrs, warmup = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ce6a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T19:44:29.498531Z",
     "start_time": "2022-08-26T19:44:29.465536Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print('-'*80)\n",
    "disp_info_1(ns, opt, environ)\n",
    "print('-'*80)\n",
    "print(environ.disp_for_excel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33100539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T19:44:29.523723Z",
     "start_time": "2022-08-26T19:44:29.500199Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# environ.display_trained_logits(ns.current_epoch,out=sys.stdout) \n",
    "# environ.display_trained_policy(ns.current_epoch,out=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73de409d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T18:51:39.084241Z",
     "start_time": "2022-08-29T18:51:39.047038Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n",
      "1989\n",
      "1318\n",
      "671\n",
      "------------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  60 - Run epochs 1 to 60\n",
      "------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ns.eval_iters = 250\n",
    "# ns.trn_iters_warmup = 750\n",
    "# ns.eval_iters = 2\n",
    "# ns.trn_iters_warmup = 2\n",
    "ns.warmup_epochs = 60\n",
    "print(ns.eval_iters )\n",
    "print(ns.trn_iters_warmup)\n",
    "print(ns.trn_iters_weights)\n",
    "print(ns.trn_iters_policy)\n",
    "\n",
    "# ns.check_for_improvment_wait = 0\n",
    "# ns.current_epoch =0 \n",
    "# ns.write_checkpoint = False\n",
    "print_heading(f\" Last Epoch: {ns.current_epoch}   # of warm-up epochs to do:  {ns.warmup_epochs} - Run epochs {ns.current_epoch+1} to {ns.current_epoch + ns.warmup_epochs}\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f7eb2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Warmup Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bd29e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:30:15.468523Z",
     "start_time": "2022-08-30T16:09:19.923824Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      " Last Epoch: 120   # of warm-up epochs to do:  30 - Run epochs 121 to 150\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 121 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3435   5.332e-04   2.128e-04    1.3442 |  4.068e-06   0.45152   0.66510   0.76085   0.65214   0.69632 |   2.1100   2.960e-04   1.181e-04    2.1105 | 385.6 |\n",
      " 122 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.2361   5.332e-04   2.128e-04    1.2369 |  4.065e-06   0.45282   0.66397   0.76082   0.65044   0.69507 |   2.1085   2.960e-04   1.181e-04    2.1090 | 380.8 |\n",
      " 123 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.2975   5.332e-04   2.128e-04    1.2983 |  4.038e-06   0.44953   0.66490   0.76153   0.65120   0.69535 |   2.0945   2.960e-04   1.181e-04    2.0949 | 379.8 |\n",
      "Previous best_epoch:   117   best iter: 232713   best_accuracy: 0.66448    best ROC auc: 0.76147\n",
      "Previous best_epoch:   123   best iter: 244647   best_accuracy: 0.66490    best ROC auc: 0.76153\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " 124 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3577   5.332e-04   2.128e-04    1.3585 |  4.056e-06   0.45183   0.66428   0.76105   0.65060   0.69553 |   2.1039   2.960e-04   1.181e-04    2.1043 | 383.0 |\n",
      " 125 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3640   5.332e-04   2.128e-04    1.3647 |  4.056e-06   0.45183   0.66402   0.76112   0.65039   0.69520 |   2.1041   2.960e-04   1.181e-04    2.1045 | 382.7 |\n",
      " 126 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3491   5.332e-04   2.128e-04    1.3499 |  4.073e-06   0.45361   0.66279   0.75947   0.64905   0.69443 |   2.1126   2.960e-04   1.181e-04    2.1130 | 379.0 |\n",
      " 127 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   2.3788   5.332e-04   2.128e-04    2.3795 |  4.071e-06   0.45260   0.66324   0.76007   0.64947   0.69431 |   2.1115   2.960e-04   1.181e-04    2.1119 | 381.2 |\n",
      " 128 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.4624   5.332e-04   2.128e-04    1.4632 |  4.072e-06   0.45167   0.66458   0.76077   0.65141   0.69477 |   2.1120   2.960e-04   1.181e-04    2.1124 | 380.2 |\n",
      " 129 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.2232   5.332e-04   2.128e-04    1.2239 |  4.071e-06   0.45075   0.66428   0.76046   0.65103   0.69477 |   2.1118   2.960e-04   1.181e-04    2.1122 | 382.1 |\n",
      " 130 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.5876   5.332e-04   2.128e-04    1.5884 |  4.075e-06   0.45092   0.66449   0.76043   0.65133   0.69534 |   2.1138   2.960e-04   1.181e-04    2.1143 | 384.1 |\n",
      " 131 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3368   5.332e-04   2.128e-04    1.3375 |  4.064e-06   0.45073   0.66455   0.76021   0.65155   0.69582 |   2.1078   2.960e-04   1.181e-04    2.1082 | 378.6 |\n",
      " 132 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.5985   5.332e-04   2.128e-04    1.5992 |  4.060e-06   0.45002   0.66584   0.76188   0.65275   0.69594 |   2.1062   2.960e-04   1.181e-04    2.1066 | 381.1 |\n",
      "Previous best_epoch:   123   best iter: 244647   best_accuracy: 0.66490    best ROC auc: 0.76153\n",
      "Previous best_epoch:   132   best iter: 262548   best_accuracy: 0.66584    best ROC auc: 0.76188\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " 133 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3150   5.332e-04   2.128e-04    1.3157 |  4.066e-06   0.45008   0.66494   0.76105   0.65152   0.69547 |   2.1091   2.960e-04   1.181e-04    2.1096 | 381.8 |\n",
      " 134 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.0417   5.332e-04   2.128e-04    1.0424 |  4.079e-06   0.45255   0.66501   0.76031   0.65207   0.69493 |   2.1158   2.960e-04   1.181e-04    2.1162 | 378.2 |\n",
      " 135 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.0816   5.332e-04   2.128e-04    1.0823 |  4.050e-06   0.45151   0.66604   0.76161   0.65265   0.69602 |   2.1006   2.960e-04   1.181e-04    2.1010 | 380.7 |\n",
      " 136 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.3385   5.332e-04   2.128e-04    1.3392 |  4.058e-06   0.45130   0.66556   0.76123   0.65180   0.69636 |   2.1048   2.960e-04   1.181e-04    2.1052 | 383.3 |\n",
      " 137 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.2423   5.332e-04   2.128e-04    1.2430 |  4.050e-06   0.45104   0.66671   0.76186   0.65351   0.69670 |   2.1010   2.960e-04   1.181e-04    2.1014 | 380.6 |\n",
      " 138 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.0402   5.332e-04   2.128e-04    1.0409 |  4.071e-06   0.45333   0.66663   0.76184   0.65340   0.69638 |   2.1115   2.960e-04   1.181e-04    2.1119 | 382.4 |\n",
      " 139 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.0755   5.332e-04   2.128e-04    1.0762 |  4.088e-06   0.45358   0.66499   0.76022   0.65209   0.69451 |   2.1207   2.960e-04   1.181e-04    2.1211 | 381.6 |\n",
      " 140 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.1783   5.332e-04   2.128e-04    1.1790 |  4.072e-06   0.45179   0.66506   0.76028   0.65193   0.69505 |   2.1124   2.960e-04   1.181e-04    2.1128 | 384.2 |\n",
      " 141 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.1684   5.332e-04   2.128e-04    1.1691 |  4.053e-06   0.45126   0.66352   0.75985   0.64978   0.69479 |   2.1024   2.960e-04   1.181e-04    2.1028 | 580.5 |\n",
      " 142 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.0513   5.332e-04   2.128e-04    1.0521 |  4.075e-06   0.45348   0.66590   0.75997   0.65244   0.69538 |   2.1137   2.960e-04   1.181e-04    2.1141 | 400.2 |\n",
      " 143 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.2781   5.332e-04   2.128e-04    1.2789 |  4.054e-06   0.45104   0.66609   0.76094   0.65247   0.69649 |   2.1030   2.960e-04   1.181e-04    2.1034 | 396.0 |\n",
      " 144 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.2672   5.332e-04   2.128e-04    1.2679 |  4.044e-06   0.44976   0.66556   0.76077   0.65208   0.69548 |   2.0979   2.960e-04   1.181e-04    2.0983 | 395.0 |\n",
      "Epoch   144: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   144: reducing learning rate of group 1 to 1.2500e-04.\n",
      " 145 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   1.0319   5.332e-04   2.128e-04    1.0326 |  4.078e-06   0.45178   0.66629   0.76110   0.65285   0.69563 |   2.1153   2.960e-04   1.181e-04    2.1157 | 570.7 |\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 146 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   1.0217   5.332e-04   2.128e-04    1.0224 |  4.058e-06   0.45128   0.66636   0.76068   0.65297   0.69598 |   2.1051   2.960e-04   1.181e-04    2.1055 | 383.8 |\n",
      " 147 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   1.3491   5.332e-04   2.128e-04    1.3499 |  4.072e-06   0.45287   0.66618   0.76077   0.65276   0.69565 |   2.1123   2.960e-04   1.181e-04    2.1127 | 424.5 |\n",
      " 148 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   1.1516   5.332e-04   2.128e-04    1.1523 |  4.085e-06   0.45301   0.66578   0.76086   0.65249   0.69585 |   2.1187   2.960e-04   1.181e-04    2.1191 | 414.2 |\n",
      " 149 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   1.0413   5.332e-04   2.128e-04    1.0421 |  4.052e-06   0.45164   0.66628   0.76147   0.65278   0.69607 |   2.1017   2.960e-04   1.181e-04    2.1021 | 400.2 |\n",
      " 150 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   0.9958   5.332e-04   2.128e-04    0.9966 |  4.072e-06   0.45279   0.66637   0.76108   0.65298   0.69623 |   2.1121   2.960e-04   1.181e-04    2.1125 | 389.5 |\n",
      " save warmup checkpoint  to :  model_warmup_last_ep_150\n",
      " save warmup metrics to     :  metrics_warmup_last_ep_150.pickle\n",
      "[Final] ep:150  it:298350 -  Losses:   \t Task: 2.1121   \t Sparsity: 2.96030e-04    \t Sharing: 1.18136e-04    \t Total: 2.1125 \n",
      "\n",
      " ep:  150   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.0008   -0.0005  1    0.0013    0.0009  1   -0.0001    0.0017  0    0.0017   -0.0002  1    0.0005   -0.0006  1    0.0003   -0.0005  1    0.0010    0.0008  1   -0.0007    0.0006  0    0.0002   -0.0007  1   -0.0003   -0.0013  1\n",
      "  1    0.0004   -0.0003  1   -0.0003    0.0003  0   -0.0009    0.0003  0   -0.0003    0.0010  0    0.0009    0.0000  1   -0.0004   -0.0005  1    0.0012    0.0008  1   -0.0003    0.0020  0    0.0006    0.0001  1   -0.0008    0.0009  0\n",
      "  2    0.0008    0.0018  0   -0.0011   -0.0003  0    0.0016   -0.0010  1   -0.0006   -0.0006  1    0.0016    0.0012  1   -0.0001   -0.0008  1   -0.0016   -0.0004  0   -0.0009   -0.0008  0    0.0006   -0.0002  1   -0.0011    0.0007  0\n",
      "  3   -0.0002    0.0012  0   -0.0003   -0.0002  0    0.0008   -0.0003  1   -0.0006    0.0009  0    0.0011    0.0002  1   -0.0008   -0.0014  1   -0.0004   -0.0001  0    0.0009   -0.0021  1   -0.0006    0.0000  0    0.0001   -0.0002  1\n",
      "  4   -0.0013    0.0009  0    0.0003   -0.0003  1    0.0020    0.0008  1   -0.0016    0.0009  0    0.0006    0.0003  1   -0.0002    0.0004  0   -0.0010    0.0009  0   -0.0015    0.0003  0    0.0011   -0.0021  1   -0.0013   -0.0008  0\n",
      "  5   -0.0001   -0.0014  1   -0.0003    0.0013  0    0.0008   -0.0012  1    0.0001   -0.0004  1   -0.0005   -0.0010  1    0.0007    0.0004  1    0.0000   -0.0005  1   -0.0009    0.0001  0    0.0006   -0.0006  1   -0.0011    0.0001  0\n",
      "\n",
      "\n",
      " ep:  150    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.5003    0.4997  -    0.5001    0.4999  -    0.4996    0.5004  -    0.5005    0.4995  -    0.5003    0.4997  -    0.5002    0.4998  -    0.5000    0.5000  -    0.4997    0.5003  -    0.5002    0.4998  -    0.5003    0.4997  -\n",
      "  1    0.5002    0.4998  -    0.4999    0.5001  -    0.4997    0.5003  -    0.4997    0.5003  -    0.5002    0.4998  -    0.5000    0.5000  -    0.5001    0.4999  -    0.4994    0.5006  -    0.5001    0.4999  -    0.4996    0.5004  -\n",
      "  2    0.4997    0.5003  -    0.4998    0.5002  -    0.5006    0.4994  -    0.5000    0.5000  -    0.5001    0.4999  -    0.5002    0.4998  -    0.4997    0.5003  -    0.5000    0.5000  -    0.5002    0.4998  -    0.4996    0.5004  -\n",
      "  3    0.4996    0.5004  -    0.5000    0.5000  -    0.5003    0.4997  -    0.4996    0.5004  -    0.5002    0.4998  -    0.5002    0.4998  -    0.4999    0.5001  -    0.5008    0.4992  -    0.4998    0.5002  -    0.5001    0.4999  -\n",
      "  4    0.4994    0.5006  -    0.5001    0.4999  -    0.5003    0.4997  -    0.4994    0.5006  -    0.5001    0.4999  -    0.4999    0.5001  -    0.4995    0.5005  -    0.4995    0.5005  -    0.5008    0.4992  -    0.4999    0.5001  -\n",
      "  5    0.5003    0.4997  -    0.4996    0.5004  -    0.5005    0.4995  -    0.5001    0.4999  -    0.5001    0.4999  -    0.5001    0.4999  -    0.5001    0.4999  -    0.4997    0.5003  -    0.5003    0.4997  -    0.4997    0.5003  -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warmup_phase(ns,opt, environ, dldrs, epochs = 30, verbose = False, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66b6349d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T15:59:52.338104Z",
     "start_time": "2022-08-30T08:40:11.385051Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Last Epoch: 60   # of warm-up epochs to do:  60 - Run epochs 61 to 120\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      "  61 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4284   5.332e-04   2.128e-04    1.4291 |  4.206e-06   0.46881   0.64644   0.74295   0.63351   0.68343 |   2.1818   2.960e-04   1.181e-04    2.1822 | 399.8 |\n",
      "Previous best_epoch:    60   best iter: 119340   best_accuracy: 0.64416    best ROC auc: 0.73959\n",
      "Previous best_epoch:    61   best iter: 121329   best_accuracy: 0.64644    best ROC auc: 0.74295\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  62 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.8176   5.332e-04   2.128e-04    1.8183 |  4.196e-06   0.46903   0.64973   0.74450   0.63702   0.68614 |   2.1763   2.960e-04   1.181e-04    2.1767 | 577.8 |\n",
      "Previous best_epoch:    61   best iter: 121329   best_accuracy: 0.64644    best ROC auc: 0.74295\n",
      "Previous best_epoch:    62   best iter: 123318   best_accuracy: 0.64973    best ROC auc: 0.74450\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  63 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   2.6636   5.332e-04   2.128e-04    2.6643 |  4.206e-06   0.46835   0.65052   0.74482   0.63796   0.68515 |   2.1816   2.960e-04   1.181e-04    2.1820 | 612.7 |\n",
      "Previous best_epoch:    62   best iter: 123318   best_accuracy: 0.64973    best ROC auc: 0.74450\n",
      "Previous best_epoch:    63   best iter: 125307   best_accuracy: 0.65052    best ROC auc: 0.74482\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  64 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.6483   5.332e-04   2.128e-04    1.6491 |  4.197e-06   0.46545   0.65208   0.74787   0.63927   0.68793 |   2.1771   2.960e-04   1.181e-04    2.1775 | 609.7 |\n",
      "Previous best_epoch:    63   best iter: 125307   best_accuracy: 0.65052    best ROC auc: 0.74482\n",
      "Previous best_epoch:    64   best iter: 127296   best_accuracy: 0.65208    best ROC auc: 0.74787\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  65 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4629   5.332e-04   2.128e-04    1.4636 |  4.181e-06   0.46477   0.65144   0.74867   0.63842   0.68732 |   2.1688   2.960e-04   1.181e-04    2.1692 | 564.7 |\n",
      "Previous best_epoch:    64   best iter: 127296   best_accuracy: 0.65208    best ROC auc: 0.74787\n",
      "Previous best_epoch:    65   best iter: 129285   best_accuracy: 0.65144    best ROC auc: 0.74867\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  66 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4520   5.332e-04   2.128e-04    1.4528 |  4.170e-06   0.46393   0.65256   0.75026   0.63969   0.68717 |   2.1628   2.960e-04   1.181e-04    2.1632 | 612.5 |\n",
      "Previous best_epoch:    65   best iter: 129285   best_accuracy: 0.65144    best ROC auc: 0.74867\n",
      "Previous best_epoch:    66   best iter: 131274   best_accuracy: 0.65256    best ROC auc: 0.75026\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  67 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5265   5.332e-04   2.128e-04    1.5273 |  4.158e-06   0.46399   0.65571   0.75183   0.64284   0.69060 |   2.1568   2.960e-04   1.181e-04    2.1572 | 510.4 |\n",
      "Previous best_epoch:    66   best iter: 131274   best_accuracy: 0.65256    best ROC auc: 0.75026\n",
      "Previous best_epoch:    67   best iter: 133263   best_accuracy: 0.65571    best ROC auc: 0.75183\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  68 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.9229   5.332e-04   2.128e-04    1.9237 |  4.177e-06   0.46450   0.65420   0.75054   0.64138   0.68823 |   2.1665   2.960e-04   1.181e-04    2.1669 | 619.5 |\n",
      "  69 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4946   5.332e-04   2.128e-04    1.4954 |  4.131e-06   0.46223   0.65652   0.75265   0.64361   0.69015 |   2.1430   2.960e-04   1.181e-04    2.1434 | 623.3 |\n",
      "Previous best_epoch:    67   best iter: 133263   best_accuracy: 0.65571    best ROC auc: 0.75183\n",
      "Previous best_epoch:    69   best iter: 137241   best_accuracy: 0.65652    best ROC auc: 0.75265\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  70 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.7234   5.332e-04   2.128e-04    1.7242 |  4.159e-06   0.46407   0.65502   0.75108   0.64221   0.68959 |   2.1571   2.960e-04   1.181e-04    2.1575 | 587.3 |\n",
      "  71 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   4.2003   5.332e-04   2.128e-04    4.2010 |  4.162e-06   0.46104   0.65670   0.75168   0.64383   0.69030 |   2.1590   2.960e-04   1.181e-04    2.1594 | 441.8 |\n",
      "  72 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.7596   5.332e-04   2.128e-04    1.7603 |  4.128e-06   0.45951   0.65737   0.75350   0.64457   0.69095 |   2.1412   2.960e-04   1.181e-04    2.1416 | 385.4 |\n",
      "Previous best_epoch:    69   best iter: 137241   best_accuracy: 0.65652    best ROC auc: 0.75265\n",
      "Previous best_epoch:    72   best iter: 143208   best_accuracy: 0.65737    best ROC auc: 0.75350\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  73 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5465   5.332e-04   2.128e-04    1.5473 |  4.139e-06   0.46104   0.65583   0.75283   0.64244   0.68975 |   2.1467   2.960e-04   1.181e-04    2.1471 | 396.6 |\n",
      "  74 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5895   5.332e-04   2.128e-04    1.5902 |  4.127e-06   0.45945   0.65560   0.75218   0.64276   0.68874 |   2.1408   2.960e-04   1.181e-04    2.1412 | 411.4 |\n",
      "  75 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.6876   5.332e-04   2.128e-04    1.6883 |  4.144e-06   0.46148   0.65540   0.75235   0.64202   0.68875 |   2.1498   2.960e-04   1.181e-04    2.1502 | 565.8 |\n",
      "  76 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4797   5.332e-04   2.128e-04    1.4805 |  4.135e-06   0.46045   0.65670   0.75221   0.64329   0.68986 |   2.1449   2.960e-04   1.181e-04    2.1453 | 539.1 |\n",
      "  77 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.6066   5.332e-04   2.128e-04    1.6074 |  4.122e-06   0.45897   0.65699   0.75329   0.64397   0.69045 |   2.1382   2.960e-04   1.181e-04    2.1386 | 495.7 |\n",
      "  78 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.9119   5.332e-04   2.128e-04    1.9127 |  4.118e-06   0.46038   0.65684   0.75249   0.64344   0.69061 |   2.1362   2.960e-04   1.181e-04    2.1366 | 429.8 |\n",
      "  79 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3405   5.332e-04   2.128e-04    1.3413 |  4.128e-06   0.46032   0.65802   0.75262   0.64471   0.69213 |   2.1410   2.960e-04   1.181e-04    2.1414 | 390.6 |\n",
      " Warmup Epoch 80/120:  91%|██████████████████████████████████████████████████████████████████████████████████████████         | 1810/1989 [05:20<00:32,  5.55it/s, curr_iter=158941, Loss=1.4821]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  81 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4768   5.332e-04   2.128e-04    1.4776 |  4.121e-06   0.45838   0.65682   0.75269   0.64399   0.69059 |   2.1374   2.960e-04   1.181e-04    2.1378 | 399.6 |\n",
      " Warmup Epoch 82/120:  60%|███████████████████████████████████████████████████████████▎                                       | 1192/1989 [04:00<02:48,  4.74it/s, curr_iter=162302, Loss=1.7939]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  83 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5478   5.332e-04   2.128e-04    1.5486 |  4.114e-06   0.45786   0.65873   0.75447   0.64592   0.69208 |   2.1342   2.960e-04   1.181e-04    2.1346 | 413.9 |\n",
      " Warmup Epoch 84/120:  42%|█████████████████████████████████████████▊                                                          | 831/1989 [02:28<03:22,  5.72it/s, curr_iter=165919, Loss=2.1109]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3140   5.332e-04   2.128e-04    1.3148 |  4.116e-06   0.45968   0.65888   0.75305   0.64557   0.69274 |   2.1351   2.960e-04   1.181e-04    2.1355 | 398.8 |\n",
      " Warmup Epoch 86/120:  15%|███████████████▏                                                                                    | 303/1989 [00:52<04:51,  5.78it/s, curr_iter=169369, Loss=1.2121]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      "  86 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5896   5.332e-04   2.128e-04    1.5904 |  4.120e-06   0.45726   0.65762   0.75333   0.64430   0.69070 |   2.1373   2.960e-04   1.181e-04    2.1377 | 397.2 |\n",
      "  87 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.7394   5.332e-04   2.128e-04    1.7401 |  4.092e-06   0.45665   0.65925   0.75426   0.64618   0.69267 |   2.1226   2.960e-04   1.181e-04    2.1230 | 391.9 |\n",
      "  88 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.9915   5.332e-04   2.128e-04    1.9923 |  4.098e-06   0.45710   0.65930   0.75403   0.64642   0.69201 |   2.1255   2.960e-04   1.181e-04    2.1259 | 387.9 |\n",
      "  89 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.1827   5.332e-04   2.128e-04    1.1835 |  4.121e-06   0.45671   0.65968   0.75494   0.64636   0.69268 |   2.1378   2.960e-04   1.181e-04    2.1382 | 386.1 |\n",
      "  90 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.2673   5.332e-04   2.128e-04    1.2680 |  4.103e-06   0.45717   0.65912   0.75572   0.64603   0.69189 |   2.1281   2.960e-04   1.181e-04    2.1285 | 384.3 |\n",
      "Previous best_epoch:    82   best iter: 163098   best_accuracy: 0.65787    best ROC auc: 0.75520\n",
      "Previous best_epoch:    90   best iter: 179010   best_accuracy: 0.65912    best ROC auc: 0.75572\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  91 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5362   5.332e-04   2.128e-04    1.5370 |  4.128e-06   0.45709   0.65972   0.75580   0.64677   0.69252 |   2.1414   2.960e-04   1.181e-04    2.1419 | 389.2 |\n",
      "Previous best_epoch:    90   best iter: 179010   best_accuracy: 0.65912    best ROC auc: 0.75572\n",
      "Previous best_epoch:    91   best iter: 180999   best_accuracy: 0.65972    best ROC auc: 0.75580\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  92 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3646   5.332e-04   2.128e-04    1.3654 |  4.094e-06   0.45675   0.66039   0.75742   0.64730   0.69326 |   2.1237   2.960e-04   1.181e-04    2.1241 | 389.4 |\n",
      "Previous best_epoch:    91   best iter: 180999   best_accuracy: 0.65972    best ROC auc: 0.75580\n",
      "Previous best_epoch:    92   best iter: 182988   best_accuracy: 0.66039    best ROC auc: 0.75742\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      "  93 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.2574   5.332e-04   2.128e-04    1.2582 |  4.102e-06   0.45654   0.65976   0.75563   0.64708   0.69303 |   2.1279   2.960e-04   1.181e-04    2.1283 | 385.4 |\n",
      "  94 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3796   5.332e-04   2.128e-04    1.3803 |  4.102e-06   0.45747   0.66149   0.75685   0.64905   0.69324 |   2.1278   2.960e-04   1.181e-04    2.1282 | 388.7 |\n",
      "  95 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.2672   5.332e-04   2.128e-04    1.2679 |  4.114e-06   0.45719   0.65934   0.75480   0.64611   0.69216 |   2.1342   2.960e-04   1.181e-04    2.1346 | 380.3 |\n",
      "  96 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.5575   5.332e-04   2.128e-04    1.5582 |  4.098e-06   0.45623   0.65892   0.75400   0.64643   0.69201 |   2.1258   2.960e-04   1.181e-04    2.1262 | 381.1 |\n",
      "  97 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3497   5.332e-04   2.128e-04    1.3504 |  4.114e-06   0.45587   0.65939   0.75601   0.64646   0.69160 |   2.1340   2.960e-04   1.181e-04    2.1344 | 386.4 |\n",
      "  98 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   2.0712   5.332e-04   2.128e-04    2.0719 |  4.104e-06   0.45786   0.65807   0.75535   0.64512   0.69151 |   2.1288   2.960e-04   1.181e-04    2.1293 | 385.2 |\n",
      "  99 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4549   5.332e-04   2.128e-04    1.4556 |  4.104e-06   0.45689   0.65884   0.75471   0.64618   0.69233 |   2.1286   2.960e-04   1.181e-04    2.1290 | 383.5 |\n",
      " 100 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3294   5.332e-04   2.128e-04    1.3301 |  4.098e-06   0.45651   0.66001   0.75506   0.64726   0.69327 |   2.1257   2.960e-04   1.181e-04    2.1261 | 385.2 |\n",
      " 101 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.2943   5.332e-04   2.128e-04    1.2951 |  4.102e-06   0.45584   0.65918   0.75571   0.64622   0.69319 |   2.1278   2.960e-04   1.181e-04    2.1282 | 386.4 |\n",
      " 102 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4853   5.332e-04   2.128e-04    1.4860 |  4.101e-06   0.45613   0.65904   0.75675   0.64620   0.69092 |   2.1275   2.960e-04   1.181e-04    2.1279 | 383.0 |\n",
      " 103 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.4384   5.332e-04   2.128e-04    1.4391 |  4.099e-06   0.45763   0.65714   0.75408   0.64399   0.69029 |   2.1263   2.960e-04   1.181e-04    2.1267 | 380.8 |\n",
      " 104 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.6356   5.332e-04   2.128e-04    1.6363 |  4.102e-06   0.45754   0.65782   0.75428   0.64501   0.69116 |   2.1279   2.960e-04   1.181e-04    2.1283 | 377.8 |\n",
      " 105 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3914   5.332e-04   2.128e-04    1.3922 |  4.107e-06   0.45643   0.65813   0.75519   0.64495   0.69128 |   2.1303   2.960e-04   1.181e-04    2.1307 | 380.5 |\n",
      " 106 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.7494   5.332e-04   2.128e-04    1.7501 |  4.107e-06   0.45542   0.66087   0.75728   0.64799   0.69305 |   2.1304   2.960e-04   1.181e-04    2.1308 | 379.4 |\n",
      " 107 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.3503   5.332e-04   2.128e-04    1.3510 |  4.105e-06   0.45573   0.65845   0.75562   0.64462   0.69147 |   2.1292   2.960e-04   1.181e-04    2.1296 | 380.2 |\n",
      " 108 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.8167   5.332e-04   2.128e-04    1.8174 |  4.104e-06   0.45569   0.65795   0.75383   0.64475   0.69028 |   2.1290   2.960e-04   1.181e-04    2.1294 | 384.0 |\n",
      "Epoch   108: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch   108: reducing learning rate of group 1 to 2.5000e-04.\n",
      "validation:  29%|████████████████████████▍                                                          | 199/675 [00:07<00:17, 27.25it/s, it=201, Lss=1.6885, Spr=2.7076e-04, Shr=1.0805e-04, lyr=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 110 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.5526   5.332e-04   2.128e-04    1.5534 |  4.064e-06   0.45148   0.66164   0.75714   0.64792   0.69350 |   2.1078   2.960e-04   1.181e-04    2.1082 | 386.1 |\n",
      " Warmup Epoch 111/120:  72%|██████████████████████████████████████████████████████████████████████▎                           | 1428/1989 [04:07<01:35,  5.87it/s, curr_iter=220218, Loss=1.4324]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 112 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.6171   5.332e-04   2.128e-04    1.6178 |  4.062e-06   0.45158   0.66240   0.75830   0.64904   0.69426 |   2.1068   2.960e-04   1.181e-04    2.1073 | 384.4 |\n",
      " Warmup Epoch 113/120:  50%|█████████████████████████████████████████████████▍                                                | 1003/1989 [02:55<02:48,  5.86it/s, curr_iter=223772, Loss=1.4345]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 114 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.4693   5.332e-04   2.128e-04    1.4701 |  4.063e-06   0.45131   0.66460   0.76041   0.65086   0.69591 |   2.1077   2.960e-04   1.181e-04    2.1081 | 397.2 |\n",
      "Previous best_epoch:   113   best iter: 224757   best_accuracy: 0.66300    best ROC auc: 0.75932\n",
      "Previous best_epoch:   114   best iter: 226746   best_accuracy: 0.66460    best ROC auc: 0.76041\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " Warmup Epoch 115/120:  18%|██████████████████▎                                                                                | 367/1989 [01:07<04:54,  5.51it/s, curr_iter=227114, Loss=1.2817]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 116 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.1027   5.332e-04   2.128e-04    1.1034 |  4.068e-06   0.45021   0.66466   0.76044   0.65174   0.69515 |   2.1103   2.960e-04   1.181e-04    2.1107 | 391.3 |\n",
      "Previous best_epoch:   114   best iter: 226746   best_accuracy: 0.66460    best ROC auc: 0.76041\n",
      "Previous best_epoch:   116   best iter: 230724   best_accuracy: 0.66466    best ROC auc: 0.76044\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " Warmup Epoch 117/120:   3%|██▋                                                                                                 | 54/1989 [00:09<05:40,  5.69it/s, curr_iter=230778, Loss=1.2154]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Warmup Epoch 118/120:  96%|██████████████████████████████████████████████████████████████████████████████████████████████▌   | 1918/1989 [05:38<00:12,  5.64it/s, curr_iter=234631, Loss=1.3436]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Warmup Epoch 120/120:  80%|██████████████████████████████████████████████████████████████████████████████▎                   | 1590/1989 [04:40<01:07,  5.93it/s, curr_iter=238281, Loss=1.3140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   1.4426   5.332e-04   2.128e-04    1.4433 |  4.060e-06   0.45209   0.66465   0.75996   0.65120   0.69562 |   2.1062   2.960e-04   1.181e-04    2.1066 | 390.1 |\n",
      " save warmup checkpoint  to :  model_warmup_last_ep_120\n",
      " save warmup metrics to     :  metrics_warmup_last_ep_120.pickle\n",
      "[Final] ep:120  it:238680 -  Losses:   \t Task: 2.1062   \t Sparsity: 2.96030e-04    \t Sharing: 1.18136e-04    \t Total: 2.1066 \n",
      "\n",
      " ep:  120   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.0008   -0.0005  1    0.0013    0.0009  1   -0.0001    0.0017  0    0.0017   -0.0002  1    0.0005   -0.0006  1    0.0003   -0.0005  1    0.0010    0.0008  1   -0.0007    0.0006  0    0.0002   -0.0007  1   -0.0003   -0.0013  1\n",
      "  1    0.0004   -0.0003  1   -0.0003    0.0003  0   -0.0009    0.0003  0   -0.0003    0.0010  0    0.0009    0.0000  1   -0.0004   -0.0005  1    0.0012    0.0008  1   -0.0003    0.0020  0    0.0006    0.0001  1   -0.0008    0.0009  0\n",
      "  2    0.0008    0.0018  0   -0.0011   -0.0003  0    0.0016   -0.0010  1   -0.0006   -0.0006  1    0.0016    0.0012  1   -0.0001   -0.0008  1   -0.0016   -0.0004  0   -0.0009   -0.0008  0    0.0006   -0.0002  1   -0.0011    0.0007  0\n",
      "  3   -0.0002    0.0012  0   -0.0003   -0.0002  0    0.0008   -0.0003  1   -0.0006    0.0009  0    0.0011    0.0002  1   -0.0008   -0.0014  1   -0.0004   -0.0001  0    0.0009   -0.0021  1   -0.0006    0.0000  0    0.0001   -0.0002  1\n",
      "  4   -0.0013    0.0009  0    0.0003   -0.0003  1    0.0020    0.0008  1   -0.0016    0.0009  0    0.0006    0.0003  1   -0.0002    0.0004  0   -0.0010    0.0009  0   -0.0015    0.0003  0    0.0011   -0.0021  1   -0.0013   -0.0008  0\n",
      "  5   -0.0001   -0.0014  1   -0.0003    0.0013  0    0.0008   -0.0012  1    0.0001   -0.0004  1   -0.0005   -0.0010  1    0.0007    0.0004  1    0.0000   -0.0005  1   -0.0009    0.0001  0    0.0006   -0.0006  1   -0.0011    0.0001  0\n",
      "\n",
      "\n",
      " ep:  120    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.5003    0.4997  -    0.5001    0.4999  -    0.4996    0.5004  -    0.5005    0.4995  -    0.5003    0.4997  -    0.5002    0.4998  -    0.5000    0.5000  -    0.4997    0.5003  -    0.5002    0.4998  -    0.5003    0.4997  -\n",
      "  1    0.5002    0.4998  -    0.4999    0.5001  -    0.4997    0.5003  -    0.4997    0.5003  -    0.5002    0.4998  -    0.5000    0.5000  -    0.5001    0.4999  -    0.4994    0.5006  -    0.5001    0.4999  -    0.4996    0.5004  -\n",
      "  2    0.4997    0.5003  -    0.4998    0.5002  -    0.5006    0.4994  -    0.5000    0.5000  -    0.5001    0.4999  -    0.5002    0.4998  -    0.4997    0.5003  -    0.5000    0.5000  -    0.5002    0.4998  -    0.4996    0.5004  -\n",
      "  3    0.4996    0.5004  -    0.5000    0.5000  -    0.5003    0.4997  -    0.4996    0.5004  -    0.5002    0.4998  -    0.5002    0.4998  -    0.4999    0.5001  -    0.5008    0.4992  -    0.4998    0.5002  -    0.5001    0.4999  -\n",
      "  4    0.4994    0.5006  -    0.5001    0.4999  -    0.5003    0.4997  -    0.4994    0.5006  -    0.5001    0.4999  -    0.4999    0.5001  -    0.4995    0.5005  -    0.4995    0.5005  -    0.5008    0.4992  -    0.4999    0.5001  -\n",
      "  5    0.5003    0.4997  -    0.4996    0.5004  -    0.5005    0.4995  -    0.5001    0.4999  -    0.5001    0.4999  -    0.5001    0.4999  -    0.5001    0.4999  -    0.4997    0.5003  -    0.5003    0.4997  -    0.4997    0.5003  -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warmup_phase(ns,opt, environ, dldrs, epochs = ns.warmup_epochs, verbose = False, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a605c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:30:43.518016Z",
     "start_time": "2022-08-30T19:30:43.462364Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 150 | 1.25e-04  1.25e-04  1.00e-02  2.50e+00 |   0.9958   5.332e-04   2.128e-04    0.9966 |  4.072e-06   0.45279   0.66637   0.76108   0.65298   0.69623 |   2.1121   2.960e-04   1.181e-04    2.1125 |  -0.0 |\n",
      "\n",
      "[e] Last ep:150  it:298350  -  Losses:   \t Task: 2.1121   \t Sparsity: 2.96030e-04    \t Sharing: 1.18136e-04    \t Total: 2.1125 \n",
      "\n",
      "   best_epoch:   132   best iter: 262548   best_accuracy: 0.66584    best ROC auc: 0.76188\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      \n",
    "# print()\n",
    "# environ.display_trained_logits(ns.current_epoch)\n",
    "# environ.display_trained_policy(ns.current_epoch)\n",
    "# environ.display_current_policy(ns.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b025642",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7862099d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### End WandB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bb1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T18:38:29.974583Z",
     "start_time": "2022-08-26T18:38:25.641511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ns.wandb_run.finish()\n",
    "# ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c03a5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Some data peeks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a7fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T16:43:04.282818Z",
     "start_time": "2022-08-16T16:43:04.246814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics['sparsity']['total'])\n",
    "pp.pprint(environ.val_metrics['sharing']['total'])\n",
    "pp.pprint(environ.val_metrics['sharing']['total'] +environ.val_metrics['sparsity']['total'])\n",
    "pp.pprint(environ.val_metrics['task'])\n",
    "pp.pprint(environ.val_metrics['total'])\n",
    "pp.pprint(environ.val_metrics['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbb96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T07:56:30.866019Z",
     "start_time": "2022-08-16T07:56:30.793987Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils             import censored_mse_loss, censored_mae_loss, aggregate_results\n",
    "task_key = 'task2'\n",
    "print(environ.val_data[task_key]['yc_aggr_weights'].sum())\n",
    "print(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "print(environ.val_metrics[task_key]['classification'])\n",
    "# print(environ.val_metrics[task_key]['classification'].sum())\n",
    "print(environ.val_metrics[task_key]['classification_agg'])\n",
    "# print(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "# print((environ.batch_data[task_key]['yc_aggr_weights']==environ.val_data[task_key]['yc_aggr_weights']).all())\n",
    "\n",
    "\n",
    "tmp = aggregate_results(environ.val_metrics[task_key][\"classification\"], \n",
    "                      environ.val_data[task_key]['yc_aggr_weights'],\n",
    "                      verbose = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7b40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T07:17:18.629009Z",
     "start_time": "2022-08-16T07:17:18.396695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del all_tgs, all_tgs2\n",
    "del con,con2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52214fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T07:20:29.718039Z",
     "start_time": "2022-08-16T07:20:29.611094Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# del con\n",
    "ttl = 0\n",
    "\n",
    "# con = np.ndarray()\n",
    "appd_df = []\n",
    "for i in range(1,11):\n",
    "    task_key = f\"task{i}\"\n",
    "    print(i, task_key, ' shape: ', environ.val_data[task_key]['yc_aggr_weights'].shape,  'classifiaction:', environ.val_metrics[task_key]['classification'].shape)\n",
    "    tmp_df = environ.val_metrics[task_key]['classification'].where(pd.isnull,1)\n",
    "    print(tmp_df.sum(axis=0))\n",
    "    \n",
    "    if i == 1:\n",
    "        con = np.copy(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "        all_tgs = environ.val_metrics[task_key]['classification'].copy()\n",
    "        print(\"initialize\", con.shape, all_tgs.shape)\n",
    "    else:\n",
    "        con = np.hstack((con, environ.val_data[task_key]['yc_aggr_weights']))\n",
    "        all_tgs = all_tgs.append(environ.val_metrics[task_key]['classification'])\n",
    "        print(\"concatenate: \",task_key, \"    \", con.shape, all_tgs.shape)\n",
    "        \n",
    "    ttl += environ.val_data[task_key]['yc_aggr_weights'].shape[0]\n",
    "    \n",
    "print('ttl : ', ttl,  'con.shape:', con.shape, 'all_tgs.shape', all_tgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22870bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T07:17:56.700Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs2 = pd.concat(environ.val_metrics[f\"task{i}\"]['classification'] for i in range(1,11))\n",
    "\n",
    "all_tgs2.info()\n",
    "all_tgs2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb4137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:27.145197Z",
     "start_time": "2022-08-02T08:53:27.076862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "con2 = np.hstack([ environ.val_data[f\"task{i}\"]['yc_aggr_weights'] for i in range(1,11)])\n",
    "con2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbee89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:51:56.737396Z",
     "start_time": "2022-08-02T08:51:56.667161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_tgs.index = range(all_tgs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8334f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:52:05.606811Z",
     "start_time": "2022-08-02T08:52:05.540830Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(all_tgs2[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56e5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:38.557041Z",
     "start_time": "2022-08-02T08:53:38.479724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs2_mod = all_tgs2.where(pd.isnull, 1) * con2[:,None]\n",
    "all_tgs2_mod.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba25bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:40:54.050290Z",
     "start_time": "2022-08-02T08:40:53.982441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# con3 = pd.concat([environ.val_metrics['task1']['classification'],environ.val_metrics['task2']['classification'] ])\n",
    "# print(con3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cc76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:46.438201Z",
     "start_time": "2022-08-02T08:53:46.318451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp2 = aggregate_results(all_tgs2, con2, verbose = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:20:37.470235Z",
     "start_time": "2022-08-02T09:20:37.391995Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics['aggregated'])\n",
    "print(environ.val_metrics['aggregated']['sc_loss'] )\n",
    "print(environ.val_metrics['aggregated'][\"logloss\"] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f21fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:02:49.573973Z",
     "start_time": "2022-08-02T09:02:49.497930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(tmp2)\n",
    "pp.pprint(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c58a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0711df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:56.393951Z",
     "start_time": "2022-08-02T08:53:56.231370Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tasks_classification_metrics = []\n",
    "all_tasks_aggregation_weights    = [] \n",
    "\n",
    "for i in range(1,11):\n",
    "    task_key = f\"task{i}\"\n",
    "    print(i, task_key, ' shape: ', environ.val_data[task_key]['yc_aggr_weights'].shape,  'classifiaction:', environ.val_metrics[task_key]['classification'].shape)\n",
    "    tmp_df = environ.val_metrics[task_key]['classification'].where(pd.isnull,1)\n",
    "    print(tmp_df.sum(axis=0))\n",
    "    \n",
    "    all_tasks_classification_metrics.append(environ.val_metrics[task_key]['classification'])\n",
    "    all_tasks_aggregation_weights.append(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9616c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:50:55.310369Z",
     "start_time": "2022-08-02T08:50:55.205109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs3 = pd.concat(all_tasks_classification_metrics)\n",
    "con3 = np.concatenate(all_tasks_aggregation_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdf412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:54:37.322219Z",
     "start_time": "2022-08-02T08:54:37.193928Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs3.info()\n",
    "all_tgs3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29ee1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:02:16.745152Z",
     "start_time": "2022-08-02T09:02:16.608823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp3 = aggregate_results( all_tgs3, con3, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c12369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:59:51.408133Z",
     "start_time": "2022-08-02T08:59:51.294153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(all_tgs2[0:1]['roc_auc_score'])\n",
    "print(all_tgs3[0:1]['roc_auc_score'])\n",
    "print((all_tgs2[0:1]['roc_auc_score'] == all_tgs3[0:1]['roc_auc_score']).all())\n",
    "all_tgs2.compare(all_tgs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399eae39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T15:18:30.544168Z",
     "start_time": "2022-08-01T15:18:30.511229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.val_data['task9']['yc_aggr_weights'].shape, con[3152:3496].shape)\n",
    "print((environ.val_data['task9']['yc_aggr_weights'] == con[3152:3496]).all())a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8183a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:53:09.749812Z",
     "start_time": "2022-08-01T14:53:09.749793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76406f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T13:56:23.922805Z",
     "start_time": "2022-07-08T13:56:23.891800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60564cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:50:02.608053Z",
     "start_time": "2022-07-07T13:50:02.553468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a8071",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9c724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:20:10.987625Z",
     "start_time": "2022-07-28T11:20:10.957009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()\n",
    "for key in environ.val_metrics['aggregated']:\n",
    "    print(f\"{key:20s}    {environ.val_metrics['aggregated'][key]:0.4f}\")\n",
    "# pp.pprint(environ.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48528a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb49645c",
   "metadata": {},
   "source": [
    "# Weight & Policy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aade16",
   "metadata": {},
   "source": [
    "### Weight/Policy Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f68f2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:30:50.804853Z",
     "start_time": "2022-08-30T19:30:50.769079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Backbone Initial LR            :      0.001000      Current LR : 0.000125 \n",
      " Tasks    Initial LR            :      0.001000      Current LR : 0.000125    \n",
      " Policy   Initial LR            :      0.010000      Current LR : 0.01  \n",
      "\n",
      " Backbone (Group 0) Initial LR  : 0.001000 \n",
      " Tasks    (Group 1) Initial LR  : 0.001000    \n",
      " Params : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.000125\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.000125\n",
      "    weight_decay: 0.0001\n",
      ") \n",
      "\n",
      " Policy   Initial LR            : 0.010000  \n",
      " Params : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")  \n",
      "\n",
      "\n",
      " Backbone Initial LR            : 0.001000      Current LR : 0.000125 \n",
      " Tasks    Initial LR            : 0.001000      Current LR : 0.000125    \n",
      " Policy   Initial LR            : 0.010000      Current LR : 0.01  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\" Backbone Initial LR            :      {environ.opt['train']['backbone_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            :      {environ.opt['train']['task_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[1]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            :      {environ.opt['train']['policy_lr']:4f}      Current LR : {environ.optimizers['alphas'].param_groups[0]['lr']}  \\n\")\n",
    "\n",
    "print( f\" Backbone (Group 0) Initial LR  : {environ.opt['train']['backbone_lr']:4f} \\n\"\n",
    "       f\" Tasks    (Group 1) Initial LR  : {environ.opt['train']['task_lr']:4f}    \\n Params : {environ.optimizers['weights']} \\n\\n\"\n",
    "       f\" Policy   Initial LR            : {environ.opt['train']['policy_lr']:4f}  \\n Params : {environ.optimizers['alphas']}  \\n\\n\")\n",
    "\n",
    "print( f\" Backbone Initial LR            : {environ.opt['train']['backbone_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            : {environ.opt['train']['task_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[1]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            : {environ.opt['train']['policy_lr']:4f}      Current LR : {environ.optimizers['alphas'].param_groups[0]['lr']}  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ccc02c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:31:00.823065Z",
     "start_time": "2022-08-30T19:31:00.782059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights Scheduler Parameters\n",
      "------------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0, 0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 0\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.0944523516289357\n",
      "    num_bad_epochs           value: 1\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 150\n",
      "    _last_lr                 value: [0.000125, 0.000125]\n",
      "\n",
      "Policy Scheduler Parameters\n",
      "-----------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 0\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: inf\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 0\n"
     ]
    }
   ],
   "source": [
    "print_underline('Weights Scheduler Parameters', verbose = True) \n",
    "for k,i in environ.schedulers['weights'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")\n",
    "\n",
    "print_underline('Policy Scheduler Parameters', verbose = True)\n",
    "for k,i in environ.schedulers['alphas'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b284fe5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:33:47.686028Z",
     "start_time": "2022-08-30T19:33:47.613954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model optimizers defined . . . policy_learning: True\n",
      " Model schedulers defined . . . policy_learning: True\n",
      " Metrics CSV file header written . . . \n",
      " Model initializations complete . . . \n",
      " training preparation: - check for CUDA - cuda available as device id: [0]\n",
      "sparsechem_env.cuda()\n",
      " policy policy1 is None\n",
      " policy policy2 is None\n",
      " policy policy3 is None\n",
      " policy policy4 is None\n",
      " policy policy5 is None\n",
      " policy policy6 is None\n",
      " policy policy7 is None\n",
      " policy policy8 is None\n",
      " policy policy9 is None\n",
      " policy policy10 is None\n",
      " training preparation: - set print_freq to                                 : 1989 \n",
      " training preparation: - set number of batches per warmup training epoch to: 1989\n",
      " training preparation: - set number of batches per weight training epoch to: 1318\n",
      " training preparation: - set number of batches per policy training epoch to: 671\n",
      " training preparation: - set number of batches per validation to           : 675\n",
      " training preparation complete . . .\n"
     ]
    }
   ],
   "source": [
    "ns.flag = 'update_weights'\n",
    "model_initializations(ns, opt, environ, phase = ns.flag, policy_learning = True)\n",
    "training_initializations(ns, opt, environ, dldrs, warmup = False)\n",
    "\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 200,  weight_iterations = 2, policy_iterations = 2, eval_iterations = 1, warmup = False)\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 1000, weight_iterations = 750, policy_iterations = 250, eval_iterations = 500, warmup = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed1774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-28T07:45:13.453282Z",
     "start_time": "2022-08-28T07:45:13.421740Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90b1ace9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:33:51.358513Z",
     "start_time": "2022-08-30T19:33:51.321523Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "** 2022-08-30 21:33:51:354371 \n",
      "** Training epoch: 150 iter: 298350   flag: update_weights \n",
      "** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\n",
      "** Switch from Warm Up training to Alternate training Weights & Policy \n",
      "** Take checkpoint and block gradient flow through Policy net\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_heading( f\"** {timestring()} \\n\"\n",
    "               f\"** Training epoch: {ns.current_epoch} iter: {ns.current_iter}   flag: {ns.flag} \\n\"\n",
    "               f\"** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\\n\"\n",
    "               f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "               f\"** Take checkpoint and block gradient flow through Policy net\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66affd0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:37.495267Z",
     "start_time": "2022-09-02T10:04:37.462324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "0.05\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(environ.opt['train']['lambda_sparsity'])\n",
    "print(environ.opt['train']['lambda_sharing'])\n",
    "print(environ.opt['train']['decay_temp_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "653fc4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:54.192514Z",
     "start_time": "2022-09-02T10:04:54.158926Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.opt['is_curriculum'] = True\n",
    "# environ.opt['curriculum_speed'] = 4\n",
    "# ns.num_train_layers = None\n",
    "ns.training_epochs = 10\n",
    "\n",
    "environ.opt['train']['lambda_sparsity'] = 0.01\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['decay_temp_freq'] = 6\n",
    "\n",
    "# print(environ.opt['train']['lambda_sparsity'])\n",
    "# print(environ.opt['train']['lambda_sharing'])\n",
    "# print(environ.opt['train']['decay_temp_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef3a11fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:55.839318Z",
     "start_time": "2022-09-02T10:04:55.800877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ns.flag                        :      update_weights\n",
      " num_train_layers               :      6\n",
      " environ.opt['is_curriculum']   :      False\n",
      " environ.opt['curriculum_speed']:      3\n",
      "\n",
      " Backbone Initial LR            :      0.001000      Current LR : 0.00025 \n",
      " Tasks    Initial LR            :      0.001000      Current LR : 0.00025    \n",
      " Policy   Initial LR            :      0.010000      Current LR : 0.0025  \n",
      "\n",
      " Hard Sampling                  :      False\n",
      "\n",
      " Sparsity regularization        :      0.01\n",
      " Sharing  regularization        :      0.05 \n",
      " Tasks    regularization        :      1.0   \n",
      "\n",
      "\n",
      " Gumbel Temp                    :      0.0004         \n",
      " Gumbel Temp decay              :      3 \n",
      "\n",
      " ns.current_epoch               :      240\n",
      " ns.training_epochs             :      10 \n",
      "\n",
      " ns.current_iters               :      477360\n",
      " Batches in warmup epoch        :      1989\n",
      " Batches in weight epoch        :      1318\n",
      " Batches in policy epoch        :      671\n",
      " Batches in validation          :      675\n",
      " num_train_layers               :      6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\" ns.flag                        :      {ns.flag}\")\n",
    "print( f\" num_train_layers               :      {ns.num_train_layers}\")\n",
    "print( f\" environ.opt['is_curriculum']   :      {environ.opt['is_curriculum']}\")\n",
    "print( f\" environ.opt['curriculum_speed']:      {environ.opt['curriculum_speed']}\\n\")\n",
    "print( f\" Backbone Initial LR            :      {environ.opt['train']['backbone_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            :      {environ.opt['train']['task_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[1]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            :      {environ.opt['train']['policy_lr']:4f}      Current LR : {environ.optimizers['alphas'].param_groups[0]['lr']}  \\n\")\n",
    "\n",
    "print( f\" Hard Sampling                  :      {environ.opt['train']['hard_sampling']}\\n\")\n",
    "\n",
    "print( f\" Sparsity regularization        :      {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization        :      {environ.opt['train']['lambda_sharing']} \\n\"\n",
    "       f\" Tasks    regularization        :      {environ.opt['train']['lambda_tasks']}   \\n\\n\")\n",
    "\n",
    "print( f\" Gumbel Temp                    :      {environ.gumbel_temperature:.4f}         \\n\" #\n",
    "       f\" Gumbel Temp decay frequency    :      {environ.opt['train']['decay_temp_freq']} \\n\") #\n",
    "\n",
    "print( f\" ns.current_epoch               :      {ns.current_epoch}\")\n",
    "print( f\" ns.training_epochs             :      {ns.training_epochs} \\n\") \n",
    "print( f\" ns.current_iters               :      {ns.current_iter}\")  \n",
    "print( f\" Batches in warmup epoch        :      {ns.trn_iters_warmup}\")\n",
    "print( f\" Batches in weight epoch        :      {ns.trn_iters_weights}\")\n",
    "print( f\" Batches in policy epoch        :      {ns.trn_iters_policy}\")\n",
    "print( f\" Batches in validation          :      {ns.eval_iters}\")\n",
    "print( f\" num_train_layers               :      {ns.num_train_layers} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a037fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:13.422760Z",
     "start_time": "2022-09-02T10:04:13.355097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 240 | 2.50e-04  2.50e-04  2.50e-03  5.95e-04 |   1.0158   5.050e-04   1.769e-03    1.0181 |  4.141e-06   0.45690   0.66556   0.76161   0.65192   0.69522 |   2.1478   5.176e-04   1.816e-03    2.1501 |  -0.0 |\n",
      "\n",
      "[e] Last ep:240  it:477360  -  Losses:   \t Task: 2.1478   \t Sparsity: 5.17625e-04    \t Sharing: 1.81609e-03    \t Total: 2.1501 \n",
      "\n",
      "   best_epoch:   230   best iter: 456799   best_accuracy: 0.66728    best ROC auc: 0.76332\n",
      "\n",
      "\n",
      " ep:  240    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7231    0.2769  1    0.6753    0.3247  1    0.7544    0.2456  1    0.6956    0.3044  1    0.6803    0.3197  1    0.6350    0.3650  1    0.6214    0.3786  1    0.6198    0.3802  1    0.7125    0.2875  1    0.6027    0.3973  1\n",
      "  1    0.7830    0.2170  1    0.8014    0.1986  1    0.7597    0.2403  1    0.7369    0.2631  1    0.7347    0.2653  1    0.7205    0.2795  1    0.6446    0.3554  1    0.6483    0.3517  1    0.8007    0.1993  1    0.6664    0.3336  1\n",
      "  2    0.7687    0.2313  1    0.7744    0.2256  1    0.8217    0.1783  1    0.7234    0.2766  1    0.7369    0.2631  1    0.7301    0.2699  1    0.7209    0.2791  1    0.7198    0.2802  1    0.7593    0.2407  1    0.7238    0.2762  1\n",
      "  3    0.6982    0.3018  1    0.7395    0.2605  1    0.8132    0.1868  1    0.6555    0.3445  1    0.7283    0.2717  1    0.6799    0.3201  1    0.6447    0.3553  1    0.6731    0.3269  1    0.7013    0.2987  1    0.7052    0.2948  1\n",
      "  4    0.6712    0.3288  1    0.6956    0.3044  1    0.6491    0.3509  1    0.7061    0.2939  1    0.6745    0.3255  1    0.6224    0.3776  1    0.6295    0.3705  1    0.5656    0.4344  1    0.6441    0.3559  1    0.5896    0.4104  1\n",
      "  5    0.7430    0.2570  1    0.6916    0.3084  1    0.7363    0.2637  1    0.6437    0.3563  1    0.7140    0.2860  1    0.6199    0.3801  1    0.7134    0.2866  1    0.6610    0.3390  1    0.6756    0.3244  1    0.6279    0.3721  1\n",
      "\n",
      "\n",
      " ep:  240   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.4175   -0.5425  1    0.4175   -0.3146  1    0.4176   -0.7046  1    0.4178   -0.4087  1    0.4175   -0.3378  1    0.4175   -0.1361  1    0.4175   -0.0781  1    0.4175   -0.0713  1    0.4175   -0.4900  1    0.4175    0.0009  1\n",
      "  1    0.6187   -0.6648  1    0.6328   -0.7625  1    0.6186   -0.5324  1    0.6186   -0.4112  1    0.6186   -0.4002  1    0.6187   -0.3285  1    0.6186    0.0233  1    0.6186    0.0071  1    0.6493   -0.7414  1    0.6190   -0.0731  1\n",
      "  2    0.7100   -0.4913  1    0.7100   -0.5231  1    0.7738   -0.7544  1    0.7147   -0.2466  1    0.7099   -0.3199  1    0.7100   -0.2852  1    0.7691   -0.1798  1    0.7101   -0.2332  1    0.7621   -0.3870  1    0.7457   -0.2175  1\n",
      "  3    0.5500   -0.2887  1    0.5500   -0.4931  1    0.5500   -0.9211  1    0.5502   -0.0932  1    0.5501   -0.4359  1    0.5501   -0.2030  1    0.5500   -0.0458  1    0.5500   -0.1723  1    0.5932   -0.2604  1    0.5500   -0.3224  1\n",
      "  4    0.3695   -0.3442  1    0.3695   -0.4568  1    0.3695   -0.2455  1    0.4011   -0.4752  1    0.3695   -0.3589  1    0.3694   -0.1304  1    0.3695   -0.1607  1    0.3453    0.0814  1    0.4378   -0.1553  1    0.3684    0.0060  1\n",
      "  5    0.5264   -0.5352  1    0.4819   -0.3255  1    0.4820   -0.5446  1    0.4100   -0.1815  1    0.4819   -0.4332  1    0.4818   -0.0075  1    0.4831   -0.4289  1    0.4819   -0.1858  1    0.4819   -0.2518  1    0.4819   -0.0413  1\n",
      "\n",
      "\n",
      " ep:  240    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7231    0.2769  1    0.6753    0.3247  0    0.7544    0.2456  1    0.6956    0.3044  1    0.6803    0.3197  1    0.6350    0.3650  0    0.6214    0.3786  1    0.6198    0.3802  1    0.7125    0.2875  1    0.6027    0.3973  1\n",
      "  1    0.7830    0.2170  1    0.8014    0.1986  1    0.7597    0.2403  1    0.7369    0.2631  0    0.7347    0.2653  1    0.7205    0.2795  1    0.6446    0.3554  1    0.6483    0.3517  1    0.8007    0.1993  1    0.6664    0.3336  0\n",
      "  2    0.7687    0.2313  1    0.7744    0.2256  1    0.8217    0.1783  1    0.7234    0.2766  1    0.7369    0.2631  1    0.7301    0.2699  0    0.7209    0.2791  1    0.7198    0.2802  0    0.7593    0.2407  1    0.7238    0.2762  0\n",
      "  3    0.6982    0.3018  1    0.7395    0.2605  1    0.8132    0.1868  1    0.6555    0.3445  0    0.7283    0.2717  1    0.6799    0.3201  1    0.6447    0.3553  1    0.6731    0.3269  0    0.7013    0.2987  0    0.7052    0.2948  1\n",
      "  4    0.6712    0.3288  1    0.6956    0.3044  1    0.6491    0.3509  0    0.7061    0.2939  1    0.6745    0.3255  0    0.6224    0.3776  1    0.6295    0.3705  1    0.5656    0.4344  1    0.6441    0.3559  1    0.5896    0.4104  0\n",
      "  5    0.7430    0.2570  1    0.6916    0.3084  1    0.7363    0.2637  1    0.6437    0.3563  1    0.7140    0.2860  1    0.6199    0.3801  1    0.7134    0.2866  1    0.6610    0.3390  0    0.6756    0.3244  0    0.6279    0.3721  0\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 240       # of epochs to run:  20 -->  epochs 241 to 260\n",
      " policy_learning rate : 0.01 \n",
      " lambda_sparsity      : 0.005\n",
      " lambda_sharing       : 0.05\n",
      " curriculum training  : False     cirriculum speed: 3     num_training_layers : 6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')     \n",
    "print()\n",
    "environ.display_trained_policy(ns.current_epoch)\n",
    "environ.display_trained_logits(ns.current_epoch)\n",
    "environ.display_current_policy(ns.current_epoch)\n",
    "\n",
    "print_heading(f\" Last Epoch Completed : {ns.current_epoch}       # of epochs to run:  {ns.training_epochs} -->  epochs {ns.current_epoch+1} to {ns.training_epochs + ns.current_epoch}\"\n",
    "              f\"\\n policy_learning rate : {environ.opt['train']['policy_lr']} \"\n",
    "              f\"\\n lambda_sparsity      : {environ.opt['train']['lambda_sparsity']}\"\n",
    "              f\"\\n lambda_sharing       : {environ.opt['train']['lambda_sharing']}\"\n",
    "              f\"\\n curriculum training  : {opt['is_curriculum']}     cirriculum speed: {opt['curriculum_speed']}     num_training_layers : {ns.num_train_layers}\", \n",
    "              verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c71af",
   "metadata": {},
   "source": [
    "### Weight/Policy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7f5f2f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T17:55:05.148222Z",
     "start_time": "2022-08-31T17:55:04.758878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 | 5.00e-04  5.00e-04  5.00e-03  2.51e-02 |   1.2543   5.526e-04   5.472e-04    1.2554 |  4.117e-06   0.45601   0.66590   0.76196   0.65246   0.69572 |   2.1354   5.663e-04   5.754e-04    2.1366 |  -0.0 |\n",
      "\n",
      "[e] Last ep:200  it:397800  -  Losses:   \t Task: 2.1354   \t Sparsity: 5.66264e-04    \t Sharing: 5.75399e-04    \t Total: 2.1366 \n",
      "\n",
      "   best_epoch:   191   best iter: 379899   best_accuracy: 0.66763    best ROC auc: 0.76265\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ad7d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-01T09:27:14.595Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 220       # of epochs to run:  20 -->  epochs 221 to 240\n",
      " Backbone Initial LR  : 0.001      Current LR : 0.0005 \n",
      " Heads    Initial LR  : 0.001      Current LR : 0.0005\n",
      " Policy   Initial LR  : 0.01      Current LR : 0.005\n",
      " Regularization tasks : 1.0          Sparsity: 0.005           sharing: 0.05\n",
      " curriculum training  : False      Cirriculum speed: 3     num_training_layers : 6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 221 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   0.9841   8.732e-04   4.989e-03    0.9900 |  4.124e-06   0.45618   0.66685   0.76218   0.65323   0.69657 |   2.1390   5.445e-04   3.111e-03    2.1426 |1307.3 |\n",
      " 221 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   1.0807   5.311e-04   1.895e-03    1.0831 |  4.127e-06   0.45628   0.66579   0.76193   0.65236   0.69549 |   2.1406   5.442e-04   1.950e-03    2.1431 | 247.2 |\n",
      "\n",
      " ep:  221    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7009    0.2991  1    0.7168    0.2832  0    0.8123    0.1877  1    0.7449    0.2551  1    0.8228    0.1772  0    0.6511    0.3489  1    0.7145    0.2855  1    0.7090    0.2910  1    0.6898    0.3102  0    0.6480    0.3520  1\n",
      "  1    0.7047    0.2953  0    0.8110    0.1890  1    0.7521    0.2479  1    0.7679    0.2321  1    0.7527    0.2473  1    0.7574    0.2426  1    0.7024    0.2976  0    0.6696    0.3304  1    0.7728    0.2272  1    0.6133    0.3867  1\n",
      "  2    0.7440    0.2560  0    0.7894    0.2106  0    0.8229    0.1771  1    0.7415    0.2585  1    0.7668    0.2332  1    0.6381    0.3619  1    0.7989    0.2011  0    0.7350    0.2650  1    0.7626    0.2374  1    0.7353    0.2647  1\n",
      "  3    0.7673    0.2327  1    0.7797    0.2203  1    0.8216    0.1784  0    0.6999    0.3001  0    0.7285    0.2715  0    0.6860    0.3140  0    0.6666    0.3334  0    0.7375    0.2625  1    0.6255    0.3745  1    0.7190    0.2810  1\n",
      "  4    0.6417    0.3583  1    0.7763    0.2237  1    0.6803    0.3197  1    0.6761    0.3239  1    0.6282    0.3718  0    0.6713    0.3287  1    0.6016    0.3984  0    0.6119    0.3881  0    0.6333    0.3667  1    0.5807    0.4193  0\n",
      "  5    0.6707    0.3293  1    0.7454    0.2546  1    0.7741    0.2259  1    0.6752    0.3248  1    0.7439    0.2561  1    0.6804    0.3196  1    0.7020    0.2980  1    0.6721    0.3279  1    0.7249    0.2751  0    0.6651    0.3349  1\n",
      "\n",
      " 222 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   1.2788   8.728e-04   3.128e-03    1.2828 |  4.131e-06   0.45669   0.66636   0.76196   0.65314   0.69561 |   2.1426   5.442e-04   1.950e-03    2.1451 |1297.0 |\n",
      " 222 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   0.9801   5.333e-04   1.733e-03    0.9824 |  4.129e-06   0.45632   0.66593   0.76293   0.65220   0.69618 |   2.1420   5.466e-04   1.743e-03    2.1443 | 244.2 |\n",
      "Previous best_epoch:   203   best iter: 403767   best_accuracy: 0.66709    best ROC auc: 0.76280\n",
      "Previous best_epoch:   222   best iter: 441558   best_accuracy: 0.66593    best ROC auc: 0.76293\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " decay gumbel temperature to 0.0025084781938833345\n",
      "\n",
      " ep:  222    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7407    0.2593  1    0.7627    0.2373  1    0.8051    0.1949  1    0.7417    0.2583  1    0.8147    0.1853  1    0.6926    0.3074  1    0.7524    0.2476  1    0.6660    0.3340  0    0.7496    0.2504  1    0.6554    0.3446  1\n",
      "  1    0.7044    0.2956  0    0.7919    0.2081  0    0.7725    0.2275  1    0.7749    0.2251  1    0.7310    0.2690  1    0.7472    0.2528  1    0.7145    0.2855  1    0.6686    0.3314  0    0.7967    0.2033  1    0.6567    0.3433  0\n",
      "  2    0.6962    0.3038  1    0.7652    0.2348  0    0.8079    0.1921  1    0.7247    0.2753  0    0.7656    0.2344  1    0.6593    0.3407  1    0.8041    0.1959  0    0.7093    0.2907  0    0.7330    0.2670  1    0.7273    0.2727  1\n",
      "  3    0.7619    0.2381  1    0.8085    0.1915  1    0.8212    0.1788  1    0.6802    0.3198  1    0.7167    0.2833  0    0.7339    0.2661  1    0.6524    0.3476  1    0.7094    0.2906  1    0.6199    0.3801  1    0.7160    0.2840  1\n",
      "  4    0.6413    0.3587  1    0.7559    0.2441  1    0.7173    0.2827  1    0.6811    0.3189  1    0.6544    0.3456  0    0.6436    0.3564  0    0.6188    0.3812  1    0.6472    0.3528  1    0.6194    0.3806  1    0.5473    0.4527  0\n",
      "  5    0.7065    0.2935  1    0.7517    0.2483  1    0.7583    0.2417  1    0.6901    0.3099  0    0.7340    0.2660  1    0.6570    0.3430  1    0.7351    0.2649  1    0.6691    0.3309  0    0.7356    0.2644  1    0.6394    0.3606  0\n",
      "\n",
      " 223 | 5.00e-04  5.00e-04  5.00e-03  2.51e-03 |   1.0176   8.766e-04   2.795e-03    1.0213 |  4.129e-06   0.45639   0.66601   0.76246   0.65256   0.69563 |   2.1415   5.466e-04   1.743e-03    2.1438 |1298.3 |\n",
      "Epoch    73: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    73: reducing learning rate of group 1 to 2.5000e-04.\n",
      " 223 | 2.50e-04  2.50e-04  5.00e-03  2.51e-03 |   0.9768   5.323e-04   1.680e-03    0.9790 |  4.128e-06   0.45671   0.66624   0.76162   0.65288   0.69569 |   2.1411   5.457e-04   1.728e-03    2.1434 | 241.7 |\n",
      "Epoch    73: reducing learning rate of group 0 to 2.5000e-03.\n",
      "\n",
      " ep:  223    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7027    0.2973  0    0.7450    0.2550  1    0.7878    0.2122  1    0.7048    0.2952  1    0.7506    0.2494  1    0.7047    0.2953  1    0.7038    0.2962  1    0.6290    0.3710  0    0.7561    0.2439  1    0.6658    0.3342  1\n",
      "  1    0.7115    0.2885  0    0.7697    0.2303  1    0.7858    0.2142  1    0.8030    0.1970  1    0.7407    0.2593  1    0.7078    0.2922  1    0.7056    0.2944  0    0.6734    0.3266  1    0.7871    0.2129  1    0.6986    0.3014  1\n",
      "  2    0.7420    0.2580  1    0.7860    0.2140  1    0.8293    0.1707  1    0.7564    0.2436  0    0.7739    0.2261  1    0.7005    0.2995  1    0.7917    0.2083  1    0.7435    0.2565  1    0.7801    0.2199  1    0.7106    0.2894  0\n",
      "  3    0.7459    0.2541  1    0.7953    0.2047  1    0.8274    0.1726  1    0.7093    0.2907  1    0.7072    0.2928  1    0.7226    0.2774  0    0.6633    0.3367  1    0.7284    0.2716  0    0.6491    0.3509  1    0.7125    0.2875  0\n",
      "  4    0.6644    0.3356  1    0.7840    0.2160  1    0.7543    0.2457  1    0.6693    0.3307  1    0.6569    0.3431  1    0.6397    0.3603  1    0.6239    0.3761  1    0.6458    0.3542  1    0.6114    0.3886  0    0.5951    0.4049  0\n",
      "  5    0.7108    0.2892  1    0.6978    0.3022  1    0.7250    0.2750  0    0.6826    0.3174  1    0.7200    0.2800  1    0.6426    0.3574  1    0.7195    0.2805  0    0.6827    0.3173  0    0.6949    0.3051  1    0.6214    0.3786  1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 224 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   1.1125   8.751e-04   2.771e-03    1.1162 |  4.123e-06   0.45630   0.66710   0.76259   0.65348   0.69648 |   2.1385   5.457e-04   1.728e-03    2.1407 |1306.5 |\n",
      " 224 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   1.3703   5.354e-04   1.485e-03    1.3723 |  4.122e-06   0.45620   0.66625   0.76183   0.65295   0.69601 |   2.1381   5.488e-04   1.543e-03    2.1402 | 246.4 |\n",
      "\n",
      " ep:  224    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7000    0.3000  1    0.7615    0.2385  1    0.7625    0.2375  0    0.7171    0.2829  1    0.7697    0.2303  1    0.6971    0.3029  1    0.7118    0.2882  0    0.6454    0.3546  1    0.7840    0.2160  1    0.6880    0.3120  1\n",
      "  1    0.7442    0.2558  1    0.7913    0.2087  1    0.7884    0.2116  0    0.8199    0.1801  0    0.7609    0.2391  0    0.7175    0.2825  0    0.7002    0.2998  0    0.6756    0.3244  1    0.8155    0.1845  1    0.7028    0.2972  1\n",
      "  2    0.7445    0.2555  1    0.7703    0.2297  0    0.8113    0.1887  1    0.7602    0.2398  1    0.7750    0.2250  1    0.6996    0.3004  1    0.7856    0.2144  0    0.7546    0.2454  1    0.7739    0.2261  1    0.7164    0.2836  0\n",
      "  3    0.7303    0.2697  1    0.7798    0.2202  0    0.8249    0.1751  1    0.6799    0.3201  0    0.6968    0.3032  1    0.7123    0.2877  1    0.6536    0.3464  1    0.7113    0.2887  1    0.6678    0.3322  0    0.6912    0.3088  1\n",
      "  4    0.6878    0.3122  1    0.7617    0.2383  0    0.7614    0.2386  1    0.6727    0.3273  1    0.6455    0.3545  1    0.6057    0.3943  1    0.6329    0.3671  1    0.6347    0.3653  1    0.6321    0.3679  0    0.5986    0.4014  0\n",
      "  5    0.7100    0.2900  1    0.7230    0.2770  1    0.7416    0.2584  0    0.6777    0.3223  0    0.7221    0.2779  1    0.6396    0.3604  1    0.7330    0.2670  0    0.6717    0.3283  1    0.6918    0.3082  0    0.6308    0.3692  0\n",
      "\n",
      " 225 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   0.9853   8.801e-04   2.475e-03    0.9886 |  4.162e-06   0.45699   0.66588   0.76198   0.65272   0.69538 |   2.1589   5.488e-04   1.543e-03    2.1610 |1307.5 |\n",
      " 225 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   0.9724   5.312e-04   8.965e-04    0.9738 |  4.127e-06   0.45672   0.66584   0.76119   0.65245   0.69542 |   2.1405   5.445e-04   9.087e-04    2.1420 | 248.6 |\n",
      " decay gumbel temperature to 0.0018813586454125009\n",
      "\n",
      "[e] Policy training epoch:225  it:447525 -  Losses:   \t Task: 2.1405   \t Sparsity: 5.44456e-04    \t Sharing: 9.08661e-04    \t Total: 2.1420 \n",
      "\n",
      " ep:  225    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7156    0.2844  0    0.7430    0.2570  1    0.7930    0.2070  1    0.7024    0.2976  1    0.7639    0.2361  1    0.6867    0.3133  1    0.6802    0.3198  1    0.6348    0.3652  1    0.7722    0.2278  1    0.6624    0.3376  1\n",
      "  1    0.7348    0.2652  0    0.7974    0.2026  1    0.7715    0.2285  1    0.8112    0.1888  1    0.7675    0.2325  1    0.7140    0.2860  0    0.6886    0.3114  0    0.6404    0.3596  1    0.8072    0.1928  1    0.7151    0.2849  1\n",
      "  2    0.7406    0.2594  1    0.7762    0.2238  1    0.8138    0.1862  0    0.7519    0.2481  0    0.7804    0.2196  1    0.7066    0.2934  1    0.7818    0.2182  1    0.7802    0.2198  1    0.8030    0.1970  1    0.7145    0.2855  0\n",
      "  3    0.7323    0.2677  1    0.7731    0.2269  1    0.8230    0.1770  0    0.6767    0.3233  1    0.6819    0.3181  1    0.7164    0.2836  1    0.6537    0.3463  1    0.7196    0.2804  0    0.6740    0.3260  0    0.6791    0.3209  0\n",
      "  4    0.6771    0.3229  0    0.7528    0.2472  1    0.7573    0.2427  0    0.6717    0.3283  1    0.6460    0.3540  1    0.5898    0.4102  0    0.6270    0.3730  1    0.6049    0.3951  1    0.6202    0.3798  0    0.5899    0.4101  1\n",
      "  5    0.6892    0.3108  1    0.7391    0.2609  1    0.7463    0.2537  0    0.6883    0.3117  1    0.7111    0.2889  1    0.6364    0.3636  0    0.7288    0.2712  1    0.6731    0.3269  1    0.6868    0.3132  0    0.6382    0.3618  1\n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 226 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.1930   8.732e-04   1.457e-03    1.1953 |  4.121e-06   0.45663   0.66625   0.76220   0.65275   0.69600 |   2.1375   5.445e-04   9.087e-04    2.1390 |1304.7 |\n",
      " 226 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.2375   5.245e-04   1.666e-03    1.2397 |  4.123e-06   0.45594   0.66609   0.76278   0.65263   0.69556 |   2.1384   5.376e-04   1.706e-03    2.1407 | 246.8 |\n",
      "\n",
      " ep:  226    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7114    0.2886  0    0.7319    0.2681  1    0.8046    0.1954  0    0.7026    0.2974  1    0.7554    0.2446  1    0.6941    0.3059  1    0.6752    0.3248  1    0.6147    0.3853  1    0.7549    0.2451  1    0.6457    0.3543  1\n",
      "  1    0.7314    0.2686  1    0.7918    0.2082  1    0.7344    0.2656  1    0.7891    0.2109  1    0.7475    0.2525  0    0.7115    0.2885  0    0.6691    0.3309  0    0.6150    0.3850  1    0.7940    0.2060  0    0.6797    0.3203  1\n",
      "  2    0.7269    0.2731  1    0.7697    0.2303  1    0.8012    0.1988  0    0.7470    0.2530  1    0.7813    0.2187  0    0.6974    0.3026  1    0.7746    0.2254  1    0.7849    0.2151  1    0.7907    0.2093  1    0.6990    0.3010  1\n",
      "  3    0.7518    0.2482  1    0.7994    0.2006  1    0.8272    0.1728  1    0.6811    0.3189  0    0.7083    0.2917  1    0.7147    0.2853  0    0.6750    0.3250  0    0.7213    0.2787  1    0.6913    0.3087  1    0.7055    0.2945  1\n",
      "  4    0.6766    0.3234  0    0.7219    0.2781  1    0.7415    0.2585  1    0.6547    0.3453  1    0.6117    0.3883  1    0.5809    0.4191  1    0.6132    0.3868  0    0.6097    0.3903  0    0.5912    0.4088  1    0.5698    0.4302  1\n",
      "  5    0.6917    0.3083  1    0.7377    0.2623  1    0.7274    0.2726  0    0.6956    0.3044  1    0.7481    0.2519  1    0.6370    0.3630  1    0.7411    0.2589  0    0.6801    0.3199  1    0.7055    0.2945  1    0.6407    0.3593  0\n",
      "\n",
      " 227 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.1329   8.622e-04   2.735e-03    1.1365 |  4.147e-06   0.45704   0.66637   0.76194   0.65285   0.69607 |   2.1509   5.376e-04   1.706e-03    2.1531 |1303.4 |\n",
      " 227 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.1263   5.222e-04   1.186e-03    1.1280 |  4.128e-06   0.45681   0.66641   0.76203   0.65296   0.69631 |   2.1413   5.352e-04   1.218e-03    2.1431 | 250.2 |\n",
      "\n",
      " ep:  227    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7121    0.2879  1    0.7348    0.2652  1    0.7923    0.2077  1    0.7045    0.2955  1    0.7472    0.2528  1    0.6886    0.3114  1    0.6716    0.3284  1    0.6161    0.3839  1    0.7536    0.2464  0    0.6421    0.3579  1\n",
      "  1    0.7410    0.2590  0    0.7853    0.2147  1    0.7548    0.2452  1    0.8004    0.1996  1    0.7472    0.2528  1    0.7126    0.2874  0    0.6686    0.3314  1    0.6160    0.3840  0    0.7964    0.2036  1    0.6912    0.3088  1\n",
      "  2    0.7144    0.2856  1    0.8026    0.1974  1    0.7781    0.2219  0    0.7560    0.2440  1    0.7665    0.2335  1    0.6634    0.3366  1    0.7623    0.2377  0    0.7614    0.2386  1    0.7845    0.2155  1    0.6759    0.3241  1\n",
      "  3    0.7441    0.2559  1    0.7930    0.2070  1    0.8327    0.1673  1    0.6615    0.3385  1    0.7135    0.2865  1    0.7287    0.2713  0    0.6588    0.3412  1    0.7229    0.2771  1    0.7161    0.2839  1    0.7049    0.2951  0\n",
      "  4    0.6727    0.3273  1    0.7365    0.2635  1    0.7381    0.2619  1    0.6758    0.3242  1    0.5935    0.4065  1    0.5805    0.4195  1    0.6227    0.3773  1    0.6173    0.3827  0    0.5925    0.4075  0    0.5583    0.4417  0\n",
      "  5    0.6826    0.3174  1    0.7322    0.2678  1    0.7366    0.2634  1    0.6936    0.3064  1    0.7311    0.2689  0    0.6301    0.3699  0    0.7399    0.2601  1    0.6816    0.3184  1    0.7083    0.2917  1    0.6379    0.3621  1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 228 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.0429   8.584e-04   1.954e-03    1.0457 |  4.133e-06   0.45681   0.66580   0.76230   0.65232   0.69584 |   2.1439   5.352e-04   1.218e-03    2.1456 |1305.6 |\n",
      " 228 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.0279   5.245e-04   8.995e-04    1.0294 |  4.154e-06   0.45668   0.66648   0.76262   0.65320   0.69598 |   2.1549   5.377e-04   9.312e-04    2.1564 | 250.8 |\n",
      " decay gumbel temperature to 0.0014110189840593756\n",
      "\n",
      " ep:  228    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7377    0.2623  0    0.7200    0.2800  1    0.8013    0.1987  1    0.7010    0.2990  1    0.7309    0.2691  1    0.6870    0.3130  0    0.6598    0.3402  1    0.6068    0.3932  1    0.7549    0.2451  1    0.6348    0.3652  1\n",
      "  1    0.7649    0.2351  0    0.7841    0.2159  1    0.7600    0.2400  1    0.8171    0.1829  1    0.7382    0.2618  1    0.7379    0.2621  1    0.6746    0.3254  0    0.6475    0.3525  1    0.7945    0.2055  1    0.6957    0.3043  0\n",
      "  2    0.7143    0.2857  1    0.7976    0.2024  1    0.7897    0.2103  1    0.7498    0.2502  1    0.7738    0.2262  1    0.6813    0.3187  1    0.7517    0.2483  0    0.7615    0.2385  0    0.8079    0.1921  0    0.6911    0.3089  1\n",
      "  3    0.7281    0.2719  0    0.7915    0.2085  1    0.8252    0.1748  1    0.6691    0.3309  1    0.7066    0.2934  1    0.7236    0.2764  1    0.6620    0.3380  0    0.7089    0.2911  0    0.7090    0.2910  0    0.7010    0.2990  1\n",
      "  4    0.6784    0.3216  0    0.7260    0.2740  1    0.7340    0.2660  0    0.6781    0.3219  0    0.6019    0.3981  1    0.5810    0.4190  1    0.6008    0.3992  1    0.6289    0.3711  0    0.6005    0.3995  1    0.5639    0.4361  1\n",
      "  5    0.7151    0.2849  0    0.7263    0.2737  1    0.7330    0.2670  1    0.7044    0.2956  0    0.7309    0.2691  0    0.6285    0.3715  1    0.7316    0.2684  1    0.6806    0.3194  0    0.7076    0.2924  0    0.6324    0.3676  0\n",
      "\n",
      "Ep: 229 [weights]:  98%|█████████▊| 1290/1318 [19:35<00:25,  1.11it/s, it=454782, Lss=1.0990, Spr=4.9181e-04, Shr=8.5176e-04, lyr=6]"
     ]
    }
   ],
   "source": [
    "weight_policy_training(ns, opt, environ, dldrs, epochs =20, display_policy = True, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f401632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f553102f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T17:55:05.148222Z",
     "start_time": "2022-08-31T17:55:04.758878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 | 5.00e-04  5.00e-04  5.00e-03  2.51e-02 |   1.2543   5.526e-04   5.472e-04    1.2554 |  4.117e-06   0.45601   0.66590   0.76196   0.65246   0.69572 |   2.1354   5.663e-04   5.754e-04    2.1366 |  -0.0 |\n",
      "\n",
      "[e] Last ep:200  it:397800  -  Losses:   \t Task: 2.1354   \t Sparsity: 5.66264e-04    \t Sharing: 5.75399e-04    \t Total: 2.1366 \n",
      "\n",
      "   best_epoch:   191   best iter: 379899   best_accuracy: 0.66763    best ROC auc: 0.76265\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7b3679c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:03:26.643974Z",
     "start_time": "2022-09-02T10:03:26.576850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 240 | 2.50e-04  2.50e-04  2.50e-03  5.95e-04 |   1.0158   5.050e-04   1.769e-03    1.0181 |  4.141e-06   0.45690   0.66556   0.76161   0.65192   0.69522 |   2.1478   5.176e-04   1.816e-03    2.1501 |  -0.0 |\n",
      "\n",
      "[e] Last ep:240  it:477360  -  Losses:   \t Task: 2.1478   \t Sparsity: 5.17625e-04    \t Sharing: 1.81609e-03    \t Total: 2.1501 \n",
      "\n",
      "   best_epoch:   230   best iter: 456799   best_accuracy: 0.66728    best ROC auc: 0.76332\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f3aed31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T17:55:29.703004Z",
     "start_time": "2022-08-31T17:55:29.657264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights Scheduler Parameters\n",
      "------------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0, 0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 2\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1219009263455093\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 50\n",
      "    _last_lr                 value: [0.0005, 0.0005]\n",
      "\n",
      "Policy Scheduler Parameters\n",
      "-----------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 2\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1244821764992645\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 50\n",
      "    _last_lr                 value: [0.005]\n"
     ]
    }
   ],
   "source": [
    "print_underline('Weights Scheduler Parameters', verbose = True) \n",
    "for k,i in environ.schedulers['weights'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")\n",
    "\n",
    "print_underline('Policy Scheduler Parameters', verbose = True)\n",
    "for k,i in environ.schedulers['alphas'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a9d98",
   "metadata": {},
   "source": [
    "### Weight/Policy Training - repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ff502bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:28:01.285918Z",
     "start_time": "2022-09-02T10:05:18.238547Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 240       # of epochs to run:  10 -->  epochs 241 to 250\n",
      " Backbone Initial LR  : 0.001      Current LR : 0.00025 \n",
      " Heads    Initial LR  : 0.001      Current LR : 0.00025\n",
      " Policy   Initial LR  : 0.01      Current LR : 0.0025\n",
      " Regularization tasks : 1.0          Sparsity: 0.01           sharing: 0.05\n",
      " curriculum training  : False      Cirriculum speed: 3     num_training_layers : 6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 241 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   0.7674   1.660e-03   2.913e-03    0.7719 |  4.130e-06   0.45688   0.66528   0.76121   0.65196   0.69514 |   2.1424   1.035e-03   1.816e-03    2.1453 |1306.5 |\n",
      " 241 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   1.1537   1.011e-03   1.440e-03    1.1562 |  4.127e-06   0.45676   0.66635   0.76127   0.65313   0.69615 |   2.1408   1.037e-03   1.470e-03    2.1433 | 245.9 |\n",
      "\n",
      " ep:  241    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7281    0.2719  1    0.6952    0.3048  1    0.7400    0.2600  1    0.6992    0.3008  1    0.6829    0.3171  1    0.6385    0.3615  1    0.6383    0.3617  0    0.6242    0.3758  1    0.7126    0.2874  1    0.6008    0.3992  1\n",
      "  1    0.7760    0.2240  1    0.7961    0.2039  1    0.7336    0.2664  0    0.7010    0.2990  1    0.7298    0.2702  0    0.7173    0.2827  1    0.6420    0.3580  1    0.6608    0.3392  0    0.7925    0.2075  1    0.6611    0.3389  1\n",
      "  2    0.7988    0.2012  1    0.7765    0.2235  0    0.8204    0.1796  1    0.7287    0.2713  1    0.7424    0.2576  0    0.7490    0.2510  0    0.7158    0.2842  1    0.7449    0.2551  1    0.7533    0.2467  1    0.7201    0.2799  0\n",
      "  3    0.6844    0.3156  1    0.7189    0.2811  1    0.8064    0.1936  1    0.6658    0.3342  1    0.7246    0.2754  1    0.6729    0.3271  1    0.6392    0.3608  0    0.6693    0.3307  1    0.6907    0.3093  1    0.7302    0.2698  1\n",
      "  4    0.6729    0.3271  1    0.6988    0.3012  0    0.6522    0.3478  1    0.7011    0.2989  0    0.6846    0.3154  1    0.6405    0.3595  1    0.6576    0.3424  1    0.5698    0.4302  0    0.6420    0.3580  0    0.5952    0.4048  0\n",
      "  5    0.7297    0.2703  0    0.6858    0.3142  1    0.7503    0.2497  0    0.6506    0.3494  1    0.7095    0.2905  1    0.6334    0.3666  0    0.7049    0.2951  1    0.6562    0.3438  1    0.6579    0.3421  1    0.6527    0.3473  0\n",
      "\n",
      " 242 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   0.8311   1.663e-03   2.358e-03    0.8351 |  4.131e-06   0.45654   0.66684   0.76240   0.65314   0.69612 |   2.1427   1.037e-03   1.470e-03    2.1453 |1318.8 |\n",
      " 242 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   0.8926   1.002e-03   1.041e-03    0.8947 |  4.125e-06   0.45658   0.66560   0.76106   0.65203   0.69532 |   2.1396   1.027e-03   1.060e-03    2.1417 | 251.9 |\n",
      "\n",
      " ep:  242    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7236    0.2764  1    0.6932    0.3068  1    0.7045    0.2955  1    0.6969    0.3031  1    0.6772    0.3228  1    0.6534    0.3466  1    0.6364    0.3636  1    0.6217    0.3783  1    0.7056    0.2944  0    0.6013    0.3987  1\n",
      "  1    0.7632    0.2368  1    0.7905    0.2095  1    0.7318    0.2682  1    0.7062    0.2938  1    0.7334    0.2666  0    0.7125    0.2875  0    0.6510    0.3490  0    0.6556    0.3444  1    0.7885    0.2115  1    0.6552    0.3448  0\n",
      "  2    0.7932    0.2068  0    0.7746    0.2254  1    0.8209    0.1791  1    0.7298    0.2702  1    0.7692    0.2308  1    0.7515    0.2485  1    0.7194    0.2806  1    0.7732    0.2268  1    0.7559    0.2441  1    0.7379    0.2621  0\n",
      "  3    0.7008    0.2992  1    0.7367    0.2633  1    0.8000    0.2000  1    0.6615    0.3385  1    0.7235    0.2765  1    0.6734    0.3266  0    0.6367    0.3633  0    0.6671    0.3329  1    0.6869    0.3131  1    0.6995    0.3005  1\n",
      "  4    0.6676    0.3324  0    0.6940    0.3060  1    0.6399    0.3601  1    0.6965    0.3035  1    0.6655    0.3345  1    0.6450    0.3550  1    0.6495    0.3505  0    0.5696    0.4304  0    0.6381    0.3619  0    0.5940    0.4060  0\n",
      "  5    0.6819    0.3181  1    0.6844    0.3156  1    0.7362    0.2638  1    0.6485    0.3515  1    0.6848    0.3152  1    0.6266    0.3734  0    0.6928    0.3072  1    0.6512    0.3488  0    0.6451    0.3549  1    0.6345    0.3655  1\n",
      "\n",
      " 243 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   1.0960   1.647e-03   1.700e-03    1.0993 |  4.126e-06   0.45669   0.66570   0.76268   0.65217   0.69494 |   2.1402   1.027e-03   1.060e-03    2.1423 |1316.4 |\n",
      " 243 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   1.1727   9.998e-04   1.621e-03    1.1753 |  4.127e-06   0.45657   0.66524   0.76098   0.65175   0.69449 |   2.1408   1.025e-03   1.652e-03    2.1434 | 254.3 |\n",
      " decay gumbel temperature to 0.0003348414190687776\n",
      "\n",
      " ep:  243    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7212    0.2788  0    0.6950    0.3050  1    0.6958    0.3042  1    0.7158    0.2842  0    0.6448    0.3552  0    0.6560    0.3440  1    0.6405    0.3595  1    0.6241    0.3759  1    0.7468    0.2532  1    0.6057    0.3943  1\n",
      "  1    0.7516    0.2484  1    0.7878    0.2122  1    0.7303    0.2697  1    0.7374    0.2626  1    0.7528    0.2472  0    0.7106    0.2894  1    0.6631    0.3369  1    0.6538    0.3462  1    0.7843    0.2157  0    0.6539    0.3461  1\n",
      "  2    0.7899    0.2101  0    0.7813    0.2187  0    0.8266    0.1734  1    0.7273    0.2727  0    0.7551    0.2449  1    0.7538    0.2462  1    0.7164    0.2836  1    0.7680    0.2320  1    0.7528    0.2472  1    0.7231    0.2769  1\n",
      "  3    0.7058    0.2942  1    0.7254    0.2746  0    0.7902    0.2098  0    0.6754    0.3246  1    0.7200    0.2800  0    0.6712    0.3288  0    0.6324    0.3676  0    0.6642    0.3358  0    0.7107    0.2893  1    0.7055    0.2945  1\n",
      "  4    0.6622    0.3378  0    0.6895    0.3105  1    0.6297    0.3703  1    0.7012    0.2988  0    0.6771    0.3229  1    0.6447    0.3553  1    0.6488    0.3512  1    0.5877    0.4123  0    0.6164    0.3836  0    0.5943    0.4057  0\n",
      "  5    0.6920    0.3080  0    0.6740    0.3260  1    0.7277    0.2723  0    0.6404    0.3596  0    0.6821    0.3179  0    0.6050    0.3950  0    0.6770    0.3230  1    0.6407    0.3593  0    0.6177    0.3823  0    0.6248    0.3752  1\n",
      "\n",
      " 244 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.0677   1.643e-03   2.649e-03    1.0720 |  4.131e-06   0.45709   0.66612   0.76234   0.65267   0.69568 |   2.1427   1.025e-03   1.652e-03    2.1453 |1312.0 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 244 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.3442   1.001e-03   1.586e-03    1.3467 |  4.137e-06   0.45695   0.66586   0.76113   0.65263   0.69545 |   2.1460   1.026e-03   1.621e-03    2.1486 | 247.6 |\n",
      "\n",
      " ep:  244    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7093    0.2907  1    0.6983    0.3017  0    0.7193    0.2807  1    0.7179    0.2821  1    0.6873    0.3127  1    0.6614    0.3386  1    0.6679    0.3321  0    0.6311    0.3689  1    0.7406    0.2594  0    0.6127    0.3873  0\n",
      "  1    0.7444    0.2556  1    0.7910    0.2090  1    0.7758    0.2242  1    0.7428    0.2572  1    0.7479    0.2521  1    0.7420    0.2580  1    0.6713    0.3287  0    0.6607    0.3393  1    0.7874    0.2126  1    0.6617    0.3383  0\n",
      "  2    0.7864    0.2136  1    0.7713    0.2287  1    0.8237    0.1763  1    0.7152    0.2848  1    0.7509    0.2491  1    0.7256    0.2744  1    0.7109    0.2891  1    0.7634    0.2366  1    0.7464    0.2536  1    0.7067    0.2933  1\n",
      "  3    0.7014    0.2986  0    0.7221    0.2779  0    0.7732    0.2268  1    0.6730    0.3270  1    0.7175    0.2825  0    0.6654    0.3346  1    0.6300    0.3700  1    0.6905    0.3095  1    0.6959    0.3041  1    0.7038    0.2962  0\n",
      "  4    0.6553    0.3447  0    0.7130    0.2870  1    0.5973    0.4027  1    0.6956    0.3044  0    0.6799    0.3201  1    0.6448    0.3552  0    0.6657    0.3343  0    0.5829    0.4171  0    0.6236    0.3764  1    0.5948    0.4052  1\n",
      "  5    0.6882    0.3118  1    0.6667    0.3333  1    0.7221    0.2779  1    0.6389    0.3611  1    0.6779    0.3221  1    0.6165    0.3835  1    0.6627    0.3373  1    0.6344    0.3656  1    0.6136    0.3864  1    0.6186    0.3814  1\n",
      "\n",
      " 245 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.0938   1.645e-03   2.600e-03    1.0980 |  4.130e-06   0.45727   0.66656   0.76158   0.65329   0.69577 |   2.1423   1.026e-03   1.621e-03    2.1449 |1317.2 |\n",
      " 245 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.1661   1.009e-03   1.191e-03    1.1683 |  4.131e-06   0.45707   0.66685   0.76215   0.65346   0.69532 |   2.1426   1.034e-03   1.217e-03    2.1448 | 250.5 |\n",
      "\n",
      "[e] Policy training epoch:245  it:487305 -  Losses:   \t Task: 2.1426   \t Sparsity: 1.03430e-03    \t Sharing: 1.21653e-03    \t Total: 2.1448 \n",
      "\n",
      " ep:  245    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7082    0.2918  1    0.7118    0.2882  0    0.7163    0.2837  1    0.7209    0.2791  0    0.6859    0.3141  0    0.6627    0.3373  0    0.6666    0.3334  1    0.6382    0.3618  0    0.7390    0.2610  1    0.6172    0.3828  0\n",
      "  1    0.7649    0.2351  0    0.7969    0.2031  1    0.7675    0.2325  1    0.7453    0.2547  1    0.7536    0.2464  1    0.7405    0.2595  1    0.6798    0.3202  1    0.6929    0.3071  1    0.7891    0.2109  0    0.6794    0.3206  1\n",
      "  2    0.8029    0.1971  1    0.7623    0.2377  1    0.8194    0.1806  1    0.7067    0.2933  1    0.7737    0.2263  0    0.7102    0.2898  1    0.7044    0.2956  1    0.7568    0.2432  1    0.7386    0.2614  1    0.7040    0.2960  1\n",
      "  3    0.7003    0.2997  0    0.7220    0.2780  0    0.7735    0.2265  1    0.6769    0.3231  0    0.7194    0.2806  0    0.6609    0.3391  1    0.6506    0.3494  0    0.6864    0.3136  1    0.7097    0.2903  0    0.7069    0.2931  1\n",
      "  4    0.6174    0.3826  0    0.7032    0.2968  0    0.6138    0.3862  1    0.7325    0.2675  1    0.6768    0.3232  1    0.6521    0.3479  1    0.6648    0.3352  1    0.5837    0.4163  0    0.6235    0.3765  1    0.5947    0.4053  0\n",
      "  5    0.6906    0.3094  1    0.6651    0.3349  1    0.7418    0.2582  0    0.6423    0.3577  1    0.6799    0.3201  1    0.6197    0.3803  0    0.6600    0.3400  1    0.6423    0.3577  1    0.6309    0.3691  0    0.6192    0.3808  0\n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 246 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.3164   1.659e-03   1.951e-03    1.3200 |  4.148e-06   0.45724   0.66559   0.76008   0.65227   0.69449 |   2.1515   1.034e-03   1.217e-03    2.1537 |1319.0 |\n",
      "Ep:246 [policy] :  60%|████████████████████████                | 403/671 [01:31<00:57,  4.63it/s, it=489026, Lss=1.3142, Spr=9.3843e-04, Shr=7.9982e-04, lyr=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 247 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   0.9301   1.641e-03   2.323e-03    0.9341 |  4.127e-06   0.45676   0.66581   0.76292   0.65224   0.69544 |   2.1407   1.023e-03   1.448e-03    2.1431 |1325.6 |\n",
      "validation:  72%|███████████████████████████████████▎             | 487/675 [01:11<00:25,  7.31it/s, it=488, Lss=1.7573, Spr=9.3013e-04, Shr=1.2727e-03, lyr=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 248 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   1.0390   1.631e-03   2.232e-03    1.0428 |  4.121e-06   0.45608   0.66584   0.76264   0.65271   0.69533 |   2.1376   1.017e-03   1.391e-03    2.1400 |1329.8 |\n",
      " 248 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   1.1531   9.862e-04   1.193e-03    1.1553 |  4.122e-06   0.45629   0.66596   0.76296   0.65243   0.69548 |   2.1384   1.011e-03   1.235e-03    2.1406 | 258.0 |\n",
      "\n",
      " ep:  248    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6878    0.3122  1    0.7411    0.2589  1    0.6969    0.3031  1    0.7301    0.2699  0    0.6863    0.3137  0    0.6284    0.3716  1    0.6614    0.3386  0    0.6744    0.3256  1    0.7533    0.2467  1    0.6251    0.3749  1\n",
      "  1    0.7445    0.2555  1    0.7611    0.2389  1    0.7610    0.2390  1    0.7281    0.2719  1    0.7623    0.2377  1    0.7250    0.2750  0    0.6578    0.3422  1    0.6450    0.3550  1    0.7564    0.2436  1    0.6582    0.3418  0\n",
      "  2    0.7899    0.2101  1    0.7635    0.2365  1    0.8176    0.1824  1    0.7139    0.2861  0    0.7620    0.2380  1    0.7382    0.2618  0    0.6919    0.3081  1    0.7544    0.2456  0    0.7165    0.2835  1    0.6783    0.3217  1\n",
      "  3    0.6892    0.3108  1    0.7060    0.2940  1    0.7304    0.2696  0    0.6562    0.3438  0    0.6908    0.3092  1    0.6097    0.3903  1    0.6387    0.3613  0    0.6863    0.3137  1    0.6862    0.3138  1    0.6967    0.3033  1\n",
      "  4    0.6396    0.3604  0    0.6950    0.3050  1    0.6295    0.3705  1    0.7177    0.2823  1    0.6851    0.3149  1    0.6437    0.3563  0    0.6560    0.3440  0    0.5575    0.4425  1    0.6103    0.3897  1    0.6070    0.3930  0\n",
      "  5    0.6897    0.3103  1    0.6499    0.3501  0    0.7614    0.2386  0    0.6465    0.3535  1    0.6787    0.3213  0    0.6179    0.3821  0    0.6559    0.3441  0    0.6559    0.3441  0    0.6202    0.3798  0    0.6248    0.3752  0\n",
      "\n",
      " 249 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   0.7344   1.621e-03   1.981e-03    0.7380 |  4.133e-06   0.45711   0.66587   0.76145   0.65237   0.69554 |   2.1437   1.011e-03   1.235e-03    2.1460 |1319.1 |\n",
      "Epoch    99: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    99: reducing learning rate of group 1 to 1.2500e-04.\n",
      " 249 | 1.25e-04  1.25e-04  2.50e-03  2.51e-04 |   1.1799   9.773e-04   1.996e-03    1.1829 |  4.128e-06   0.45682   0.66537   0.76152   0.65216   0.69484 |   2.1412   1.002e-03   2.040e-03    2.1442 | 255.2 |\n",
      "Epoch    99: reducing learning rate of group 0 to 1.2500e-03.\n",
      " decay gumbel temperature to 0.0001883482982261874\n",
      "\n",
      " ep:  249    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6785    0.3215  1    0.7336    0.2664  1    0.7004    0.2996  1    0.7308    0.2692  1    0.6883    0.3117  1    0.6448    0.3552  0    0.6542    0.3458  0    0.6757    0.3243  1    0.7506    0.2494  0    0.6237    0.3763  1\n",
      "  1    0.7464    0.2536  1    0.7625    0.2375  1    0.7588    0.2412  1    0.7448    0.2552  0    0.7819    0.2181  1    0.7292    0.2708  1    0.6608    0.3392  1    0.6531    0.3469  1    0.7424    0.2576  1    0.6631    0.3369  0\n",
      "  2    0.7862    0.2138  0    0.7592    0.2408  1    0.8151    0.1849  0    0.6822    0.3178  0    0.7603    0.2397  1    0.7361    0.2639  1    0.6866    0.3134  1    0.7410    0.2590  1    0.7062    0.2938  0    0.6773    0.3227  1\n",
      "  3    0.6401    0.3599  1    0.7256    0.2744  1    0.7234    0.2766  1    0.6729    0.3271  1    0.6814    0.3186  1    0.6085    0.3915  1    0.6389    0.3611  0    0.6858    0.3142  1    0.6987    0.3013  1    0.6956    0.3044  1\n",
      "  4    0.6323    0.3677  1    0.6861    0.3139  1    0.6227    0.3773  0    0.6833    0.3167  1    0.6768    0.3232  1    0.6339    0.3661  0    0.6510    0.3490  1    0.5524    0.4476  1    0.5993    0.4007  1    0.5767    0.4233  0\n",
      "  5    0.6804    0.3196  1    0.6286    0.3714  1    0.7577    0.2423  0    0.6385    0.3615  0    0.7033    0.2967  1    0.6119    0.3881  0    0.6513    0.3487  1    0.6812    0.3188  1    0.5954    0.4046  1    0.6147    0.3853  0\n",
      "\n",
      " 250 | 1.25e-04  1.25e-04  1.25e-03  1.88e-04 |   0.9418   1.607e-03   3.272e-03    0.9467 |  4.131e-06   0.45678   0.66698   0.76186   0.65363   0.69670 |   2.1428   1.002e-03   2.040e-03    2.1458 |1317.1 |\n",
      " 250 | 1.25e-04  1.25e-04  1.25e-03  1.88e-04 |   1.1690   9.724e-04   1.353e-03    1.1713 |  4.130e-06   0.45660   0.66607   0.76266   0.65240   0.69550 |   2.1423   9.967e-04   1.384e-03    2.1447 | 254.7 |\n",
      "\n",
      "[e] Policy training epoch:250  it:497250 -  Losses:   \t Task: 2.1423   \t Sparsity: 9.96697e-04    \t Sharing: 1.38397e-03    \t Total: 2.1447 \n",
      "\n",
      " ep:  250    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6666    0.3334  0    0.7313    0.2687  1    0.6986    0.3014  1    0.7278    0.2722  1    0.6860    0.3140  1    0.6426    0.3574  0    0.6520    0.3480  1    0.6728    0.3272  0    0.7488    0.2512  1    0.6202    0.3798  1\n",
      "  1    0.7586    0.2414  1    0.7597    0.2403  0    0.7576    0.2424  0    0.7451    0.2549  1    0.7766    0.2234  1    0.7319    0.2681  1    0.6608    0.3392  0    0.6551    0.3449  1    0.7355    0.2645  1    0.6645    0.3355  1\n",
      "  2    0.7812    0.2188  1    0.7733    0.2267  0    0.8113    0.1887  1    0.6866    0.3134  1    0.7566    0.2434  1    0.7319    0.2681  1    0.6802    0.3198  1    0.7350    0.2650  0    0.6968    0.3032  1    0.6736    0.3264  1\n",
      "  3    0.6450    0.3550  0    0.7116    0.2884  1    0.7051    0.2949  1    0.6634    0.3366  0    0.6711    0.3289  1    0.6117    0.3883  0    0.6344    0.3656  1    0.6809    0.3191  1    0.6927    0.3073  0    0.6905    0.3095  0\n",
      "  4    0.6454    0.3546  1    0.6851    0.3149  1    0.6031    0.3969  0    0.6912    0.3088  1    0.6743    0.3257  1    0.6302    0.3698  1    0.6466    0.3534  0    0.5520    0.4480  1    0.6047    0.3953  1    0.5761    0.4239  1\n",
      "  5    0.6748    0.3252  1    0.6389    0.3611  0    0.7602    0.2398  1    0.6480    0.3520  1    0.6966    0.3034  1    0.6240    0.3760  0    0.6543    0.3457  1    0.6749    0.3251  1    0.6011    0.3989  0    0.6160    0.3840  1\n",
      "\n",
      " save train checkpoint  to :  model_train_last_ep_250\n",
      " save train metrics to     :  metrics_train_last_ep_250.pickle\n",
      "[Final] ep:250  it:497250 -  Losses:   \t Task: 2.1423   \t Sparsity: 9.96697e-04    \t Sharing: 1.38397e-03    \t Total: 2.1447 \n",
      "\n",
      " ep:  250   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.4905   -0.2025  1    0.4905   -0.5109  1    0.4905   -0.3503  1    0.4905   -0.4931  1    0.4905   -0.2912  1    0.4905   -0.0962  1    0.4905   -0.1371  1    0.4905   -0.2304  1    0.4905   -0.6016  1    0.4904    0.0001  1\n",
      "  1    0.6562   -0.4889  1    0.6286   -0.5225  1    0.6307   -0.5088  1    0.6285   -0.4442  1    0.6598   -0.5863  1    0.6285   -0.3759  1    0.6285   -0.0384  1    0.6285   -0.0130  1    0.5927   -0.4302  1    0.6285   -0.0548  1\n",
      "  2    0.7313   -0.5413  1    0.8301   -0.3967  1    0.7313   -0.7273  1    0.7045   -0.0796  1    0.7313   -0.4029  1    0.7313   -0.2730  1    0.7313   -0.0233  1    0.7313   -0.2888  1    0.7312   -0.1010  1    0.7313    0.0067  1\n",
      "  3    0.4608   -0.1363  1    0.4982   -0.4049  1    0.4801   -0.3918  1    0.4974   -0.1811  1    0.4972   -0.2160  1    0.4974    0.0430  1    0.4974   -0.0539  1    0.4974   -0.2606  1    0.4974   -0.3153  1    0.4974   -0.3050  1\n",
      "  4    0.3852   -0.2136  1    0.3630   -0.4141  1    0.3249   -0.0935  1    0.3630   -0.4427  1    0.3630   -0.3645  1    0.3631   -0.1701  1    0.3630   -0.2411  1    0.3631    0.1543  1    0.3630   -0.0622  1    0.3630    0.0564  1\n",
      "  5    0.4626   -0.2672  1    0.4626   -0.1080  1    0.4626   -0.6909  1    0.4627   -0.1478  1    0.4839   -0.3473  1    0.4626   -0.0438  1    0.4626   -0.1755  1    0.4775   -0.2531  1    0.4626    0.0528  1    0.4627   -0.0100  1\n",
      "\n",
      "\n",
      " ep:  250    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6666    0.3334  0    0.7313    0.2687  1    0.6986    0.3014  1    0.7278    0.2722  1    0.6860    0.3140  1    0.6426    0.3574  0    0.6520    0.3480  1    0.6728    0.3272  0    0.7488    0.2512  1    0.6202    0.3798  1\n",
      "  1    0.7586    0.2414  1    0.7597    0.2403  0    0.7576    0.2424  0    0.7451    0.2549  1    0.7766    0.2234  1    0.7319    0.2681  1    0.6608    0.3392  0    0.6551    0.3449  1    0.7355    0.2645  1    0.6645    0.3355  1\n",
      "  2    0.7812    0.2188  1    0.7733    0.2267  0    0.8113    0.1887  1    0.6866    0.3134  1    0.7566    0.2434  1    0.7319    0.2681  1    0.6802    0.3198  1    0.7350    0.2650  0    0.6968    0.3032  1    0.6736    0.3264  1\n",
      "  3    0.6450    0.3550  0    0.7116    0.2884  1    0.7051    0.2949  1    0.6634    0.3366  0    0.6711    0.3289  1    0.6117    0.3883  0    0.6344    0.3656  1    0.6809    0.3191  1    0.6927    0.3073  0    0.6905    0.3095  0\n",
      "  4    0.6454    0.3546  1    0.6851    0.3149  1    0.6031    0.3969  0    0.6912    0.3088  1    0.6743    0.3257  1    0.6302    0.3698  1    0.6466    0.3534  0    0.5520    0.4480  1    0.6047    0.3953  1    0.5761    0.4239  1\n",
      "  5    0.6748    0.3252  1    0.6389    0.3611  0    0.7602    0.2398  1    0.6480    0.3520  1    0.6966    0.3034  1    0.6240    0.3760  0    0.6543    0.3457  1    0.6749    0.3251  1    0.6011    0.3989  0    0.6160    0.3840  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight_policy_training(ns, opt, environ, dldrs, epochs = 10, display_policy = True, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67fef146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:34:18.944008Z",
     "start_time": "2022-09-02T14:34:18.870734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 250 | 1.25e-04  1.25e-04  1.25e-03  1.88e-04 |   1.1690   9.724e-04   1.353e-03    1.1713 |  4.130e-06   0.45660   0.66607   0.76266   0.65240   0.69550 |   2.1423   9.967e-04   1.384e-03    2.1447 |  -0.0 |\n",
      "\n",
      "[e] Last ep:250  it:497250  -  Losses:   \t Task: 2.1423   \t Sparsity: 9.96697e-04    \t Sharing: 1.38397e-03    \t Total: 2.1447 \n",
      "\n",
      "   best_epoch:   230   best iter: 456799   best_accuracy: 0.66728    best ROC auc: 0.76332\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45f834d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:34:23.755615Z",
     "start_time": "2022-09-02T14:34:23.716842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights Scheduler Parameters\n",
      "------------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0, 0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 4\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1219009263455093\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 100\n",
      "    _last_lr                 value: [0.000125, 0.000125]\n",
      "\n",
      "Policy Scheduler Parameters\n",
      "-----------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 4\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1244821764992645\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 100\n",
      "    _last_lr                 value: [0.00125]\n"
     ]
    }
   ],
   "source": [
    "print_underline('Weights Scheduler Parameters', verbose = True) \n",
    "for k,i in environ.schedulers['weights'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")\n",
    "\n",
    "print_underline('Policy Scheduler Parameters', verbose = True)\n",
    "for k,i in environ.schedulers['alphas'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3ae24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T13:38:02.096634Z",
     "start_time": "2022-08-29T13:38:02.065472Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.schedulers['alphas'].patience = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e42e43",
   "metadata": {},
   "source": [
    "### Close WandB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f6b31da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:34:33.774161Z",
     "start_time": "2022-09-02T14:34:28.596477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>▁▃▄▄▄▄▄▄▃▃▆▆▆▇▇▇▇▇█▇████▇███████████████</td></tr><tr><td>avg_prec_score</td><td>▁▃▄▄▄▄▄▄▄▃▆▆▆▇▇▇▇▇██████▇███████████████</td></tr><tr><td>bceloss</td><td>█▅▄▄▄▄▄▄▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>best_accuracy</td><td>▁▂▃▃▃▄▄▄▄▄▄▅▅▅▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>best_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>best_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>best_roc_auc</td><td>▁▂▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_max</td><td>▁▃▄▄▄▄▄▄▄▃▆▆▇▇▇▇▇▇██████▇▇▇█████████████</td></tr><tr><td>gumbel_temp</td><td>████████████████████████▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>kappa</td><td>▂▂▁▁▁▁▁▁▁▁▄▅▆▆▆▆▆▇▇▇████▇▇██████████████</td></tr><tr><td>kappa_max</td><td>▁▃▄▄▄▄▄▄▄▃▆▆▇▇▇▇▇███████▇▇██████████████</td></tr><tr><td>lambda_sharing</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_sparsity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██</td></tr><tr><td>lambda_tasks</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>logloss</td><td>█▄▄▃▃▄▃▄▃▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>lr_0</td><td>██████████▄▄▄▄▄▄▄▂▂▂▂▂▂▁███████▄▄▄▄▂▂▂▂▂</td></tr><tr><td>lr_1</td><td>██████████▄▄▄▄▄▄▄▂▂▂▂▂▂▁███████▄▄▄▄▂▂▂▂▂</td></tr><tr><td>p_f1_max</td><td>▆▆▆▇█▇███▇▇▆▇▆▆▆▆▆▅▄▅▃▄▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>p_kappa_max</td><td>█▄▃▅▆▅▆▅▆▅▆▆▆▄▆▅▆▆▅▄▅▃▄▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂</td></tr><tr><td>policy_lr</td><td>███████████████████████████████▃▃▃▃▁▁▁▁▁</td></tr><tr><td>roc_auc_score</td><td>▁▃▄▄▄▄▄▄▄▄▆▇▇▇▇▇▇▇██████▇███████████████</td></tr><tr><td>sc_loss</td><td>█▄▄▃▃▄▃▄▃▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train_layers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.6524</td></tr><tr><td>avg_prec_score</td><td>0.66607</td></tr><tr><td>bceloss</td><td>0.4566</td></tr><tr><td>best_roc_auc</td><td>0.76332</td></tr><tr><td>epoch</td><td>250</td></tr><tr><td>f1_max</td><td>0.6955</td></tr><tr><td>gumbel_temp</td><td>0.00019</td></tr><tr><td>kappa</td><td>0.26631</td></tr><tr><td>kappa_max</td><td>0.46433</td></tr><tr><td>lambda_sharing</td><td>0.05</td></tr><tr><td>lambda_sparsity</td><td>0.01</td></tr><tr><td>lambda_tasks</td><td>1.0</td></tr><tr><td>logloss</td><td>0.0</td></tr><tr><td>lr_0</td><td>0.00013</td></tr><tr><td>lr_1</td><td>0.00013</td></tr><tr><td>p_f1_max</td><td>0.33835</td></tr><tr><td>p_kappa_max</td><td>0.43202</td></tr><tr><td>policy_lr</td><td>0.00125</td></tr><tr><td>roc_auc_score</td><td>0.76266</td></tr><tr><td>sc_loss</td><td>0.00317</td></tr><tr><td>train_layers</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">0829_2050</strong>: <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/154b30qo\" target=\"_blank\">https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/154b30qo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220829_205037-154b30qo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b05db1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Misc Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb846fd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58eeb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:32.253924Z",
     "start_time": "2022-03-28T05:13:32.221329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ns.best_epoch = 0\n",
    "# from utils.notebook_modules import wrapup_phase\n",
    "# wrapup_phase(ns, opt, environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db994f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:32.307351Z",
     "start_time": "2022-03-28T05:13:32.262822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['policy_lr']       = 0.002\n",
    "# environ.opt['train']['lambda_sparsity'] = 0.05\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['lambda_tasks']    = 1.0\n",
    "# # environ.opt['train']['decay_temp_freq'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc42fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:50.458223Z",
     "start_time": "2022-01-27T00:58:50.430889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(opt['diff_sparsity_weights'])\n",
    "print(opt['is_sharing'])\n",
    "print(opt['diff_sparsity_weights'] and not opt['is_sharing'])\n",
    "print(environ.opt['train']['Lambda_sharing'])\n",
    "print(opt['train']['Lambda_sharing'])\n",
    "print(environ.opt['train']['Lambda_sparsity'])\n",
    "print(opt['train']['Lambda_sparsity'])\n",
    "print(environ.opt['train']['policy_lr'])\n",
    "print(opt['train']['policy_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8a246",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-12T07:35:36.625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print( f\" Backbone Learning Rate      : {environ.opt['train']['backbone_lr']}\\n\"\n",
    "       f\" Tasks    Learning Rate      : {environ.opt['train']['task_lr']}\\n\"\n",
    "       f\" Policy   Learning Rate      : {environ.opt['train']['policy_lr']}\\n\")\n",
    "\n",
    "print( f\" Sparsity regularization     : {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization     : {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization     : {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                 : {environ.gumbel_temperature:.4f}         \\n\" \n",
    "       f\" Gumbel Temp decay           : {environ.opt['train']['decay_temp_freq']}\\n\") \n",
    "\n",
    "print( f\" current_iters               : {ns.current_iter}   \\n\"\n",
    "       f\" current_epochs              : {ns.current_epoch}  \\n\" \n",
    "       f\" train_total_epochs          : {ns.training_epochs}\\n\" \n",
    "       f\" stop_epoch_training         : {ns.stop_epoch_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be2583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:20:45.610411Z",
     "start_time": "2022-08-18T12:20:45.568020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed489b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:24:06.014379Z",
     "start_time": "2022-08-18T12:24:05.974633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name, param in environ.networks['mtl-net'].named_parameters():\n",
    "    print(f\" {name:40s}  {param.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33193377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:25:02.887603Z",
     "start_time": "2022-08-18T12:25:02.846964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name, param in environ.networks['mtl-net'].backbone.named_parameters():\n",
    "        print(f\" {name:40s}  {param.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6d417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:34:17.484632Z",
     "start_time": "2022-08-18T12:34:17.320204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name, param in environ.networks['mtl-net'].named_parameters():\n",
    "    if 'task' in name and 'fc' in name:    \n",
    "        print(f\" {name:40s}  {param.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62568b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:30:31.940280Z",
     "start_time": "2022-01-26T19:30:31.910058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_blocks = 6\n",
    "num_policy_layers = 6\n",
    "gt =  torch.ones((num_blocks)).long()\n",
    "gt0 =  torch.zeros((num_blocks)).long()\n",
    "print(gt)\n",
    "print(gt0)\n",
    "\n",
    "loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61487657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:42:31.300891Z",
     "start_time": "2022-01-26T19:42:31.257774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if environ.opt['diff_sparsity_weights'] and not environ.opt['is_sharing']:\n",
    "    print(' cond 1')\n",
    "    ## Assign higher weights to higher layers \n",
    "    loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "    print(f\"{task_key} sparsity error:  {2 * (loss_weights[-num_blocks:] * environ.cross_entropy2(logits[-num_blocks:], gt)).mean()})\")\n",
    "    print_dbg(f\" loss_weights :  {loss_weights}\", verbose = True)\n",
    "    print_dbg(f\" cross_entropy:  {environ.cross_entropy2(logits[-num_blocks:], gt)}  \", verbose = True)\n",
    "    print_dbg(f\" loss[sparsity][{task_key}]: {self.losses['sparsity'][task_key] } \", verbose = True)\n",
    "\n",
    "else:\n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between \\n Logits   : \\n{logits[-num_blocks:]} \\n and gt: \\n{gt} \\n\", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-num_blocks:], gt)}\")\n",
    "    \n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt[-1:])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt0[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt0[-1:])} \\n\")\n",
    "    \n",
    "    print('\\n cond 3')    \n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt[0:1])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt0[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt0[0:1])} \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12352a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:02:21.600933Z",
     "start_time": "2022-04-28T10:02:21.561452Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\" ns.check_for_improvment_wait:  {ns.check_for_improvment_wait}\")\n",
    "print(\" ns.curriculum_epochs:          {ns.curriculum_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dad034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:14.782725Z",
     "start_time": "2022-04-28T11:09:14.692205Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.val_metrics)\n",
    "df = environ.val_metrics['task1']['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27ebed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:15.186827Z",
     "start_time": "2022-04-28T11:09:15.090906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df[pd.notna(df.roc_auc_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4e2d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:44.692326Z",
     "start_time": "2022-04-28T11:09:44.611694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[pd.notna(df.roc_auc_score)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a1a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:22.031664Z",
     "start_time": "2022-06-15T17:39:21.964660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.display_trained_policy(ns.current_epoch,out=[sys.stdout])\n",
    "# environ.num_tasks\n",
    "# print(environ.get_policy_prob().shape)\n",
    "# print(environ.val_data['task1'].keys())\n",
    "# print(environ.val_data['task1']['yc_ind'][0][:40])\n",
    "# print(environ.val_data['task1']['yc_ind'][1][:40])\n",
    "# print(environ.val_data['task1']['yc_data'][:40])\n",
    "# print(environ.val_data['task1']['yc_hat'][:40])\n",
    "# environ.display_trained_policy(ns.current_epoch,out=[sys.stdout])\n",
    "# environ.display_trained_logits(ns.current_epoch,out=[sys.stdout])\n",
    "batch = next(dldrs.warmup_trn_loader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f163b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:22.757684Z",
     "start_time": "2022-06-15T17:39:22.679466Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd33b3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf98b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:37:56.954474Z",
     "start_time": "2022-08-29T10:37:56.900806Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\" val_metric keys               : {ns.val_metrics.keys()}\")\n",
    "print(f\" aggreagted keys               : {ns.val_metrics['aggregated'].keys()}\")\n",
    "print(f\" task keys                     : {ns.val_metrics['task'].keys()}\")\n",
    "print(f\" task / task1 keys             : {ns.val_metrics['task']['task1']}\")\n",
    "print(f\" sparsity keys                 : {ns.val_metrics['sparsity'].keys()}\")\n",
    "print(f\" total keys                    : {ns.val_metrics['total'].keys()}\")\n",
    "print(f\" aggregated keys               : {ns.val_metrics['aggregated'].keys()}\")\n",
    "print()\n",
    "print(f\" task1 keys                    : {ns.val_metrics['task1'].keys()}\")\n",
    "print(f\" task1 classification keys     : {ns.val_metrics['task1']['classification'].keys()}\")\n",
    "print(f\" task1 classification_agg keys : {ns.val_metrics['task1']['classification_agg'].keys()}\")\n",
    "\n",
    "print()\n",
    "print(f\" task1 agg sc_loss             : {ns.val_metrics['task1']['classification_agg']['sc_loss']:5f}\")\n",
    "print(f\" task1 agg bce_loss            : {ns.val_metrics['task1']['classification_agg']['bceloss']:5f}\")\n",
    "print(f\" task1 agg bce_loss            : {ns.val_metrics['task1']['classification_agg']['logloss']:5f}\")\n",
    "print(f\" task-task1                    : {ns.val_metrics['task']['task1']:5f}\")\n",
    "print(f\" task-task1                    : \\n  {ns.val_metrics['task1']['classification']}\")\n",
    "print(f\" task-task1                    : \\n  {ns.val_metrics['task1']['classification_agg']}\")\n",
    "\n",
    "print()\n",
    "print(f\" task2                         : {ns.val_metrics['task2']['classification_agg']['sc_loss']:5f}\")\n",
    "print(f\" task3                         : {ns.val_metrics['task3']['classification_agg']['sc_loss']:5f}\")\n",
    "print(f\" loss                          : {ns.val_metrics['loss']['total']:5f}\")\n",
    "print(f\" train_time                    : {ns.val_metrics['train_time']:2f}\")\n",
    "print(f\" epoch                         : {ns.val_metrics['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c09f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:44:38.417573Z",
     "start_time": "2022-08-29T10:44:38.383420Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.batch_data['task1']['yc_trn_weights'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960c48b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:19:43.683550Z",
     "start_time": "2022-01-08T01:19:43.571450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = environ.get_loss_dict()\n",
    "print(tmp.keys())\n",
    "pp.pprint(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90151319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T09:12:32.452187Z",
     "start_time": "2022-04-13T09:12:32.420905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(ns.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c031eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:13:48.522436Z",
     "start_time": "2022-08-29T10:13:48.375346Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(ns.trn_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48149772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:16:26.685693Z",
     "start_time": "2022-08-29T10:16:26.367212Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6b5fa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc10ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:46:04.669904Z",
     "start_time": "2022-08-29T10:46:04.636155Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.val_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df018e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:51:15.390154Z",
     "start_time": "2022-08-29T10:51:15.214311Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.val_loader.dataset.y_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45fcf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:31:55.581510Z",
     "start_time": "2022-04-02T13:31:55.526855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(environ.val_data['task1']['yc_data'][0] == environ.val_data['task1']['yc_data']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02211ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T14:20:55.327255Z",
     "start_time": "2022-04-02T14:20:55.026238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.sparsechem_utils import compute_metrics, aggregate_results\n",
    "import pandas\n",
    "cc = compute_metrics(cols   = environ.val_data['task1']['yc_ind'][1], \n",
    "                     y_true = environ.val_data['task1']['yc_data'], \n",
    "                     y_score= environ.val_data['task1']['yc_hat'] ,\n",
    "                     num_tasks=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a5712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:34:57.196163Z",
     "start_time": "2022-04-02T13:34:57.130013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " df   = pd.DataFrame({\"task\"   : environ.val_data['task1']['yc_ind'][1], \n",
    "                      \"y_true\" : environ.val_data['task1']['yc_data'],  \n",
    "                      \"y_score\": environ.val_data['task1']['yc_hat']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde23676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:44:52.754320Z",
     "start_time": "2022-04-02T13:44:52.611945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for task, frame in df.groupby(\"task\", sort=True):\n",
    "    print(f\" task {task}\")\n",
    "    print(frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887a79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:46:29.715440Z",
     "start_time": "2022-04-02T13:46:29.640674Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df\n",
    "df.groupby(\"task\", sort=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862488b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:10:20.301689Z",
     "start_time": "2022-04-28T11:10:20.151621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e466147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T14:26:58.189057Z",
     "start_time": "2022-04-02T14:26:58.126134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.batch_data['task1']['yc_aggr_weights'])\n",
    "environ.batch['task1']['aggr_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007f28d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c2 = aggregate_results(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4570a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T17:11:11.578048Z",
     "start_time": "2022-04-02T17:11:11.535763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.trainset0.tasks_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82a453",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Post Warm-up Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f0235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:37:10.158440Z",
     "start_time": "2022-01-28T16:37:09.742327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_all_task_logits\n",
    "    \"p = environ.get_sample_policy(hard_sampling = False)\\n\"print(p)\n",
    "p = environ.get_policy_prob()\n",
    "print(p)\n",
    "p = environ.get_policy_logits()\n",
    "print(p)\n",
    "\n",
    "# p = environ.get_current_policy()\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bddd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:40:37.799917Z",
     "start_time": "2022-01-28T16:40:37.773177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = softmax([0.0, 1])\n",
    "print(a)\n",
    "sampled = np.random.choice((1, 0), p=a)\n",
    "print(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8f376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:13:16.205889Z",
     "start_time": "2022-01-28T16:13:16.179303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.optimizers['weights'])\n",
    "print(environ.schedulers['weights'].get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf9c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:46:50.411465Z",
     "start_time": "2022-02-01T12:46:50.020540Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('losses.keys      : ', environ.losses.keys())\n",
    "print('losses[task]keys : ', environ.losses['task1'].keys())\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T22:57:02.151169Z",
     "start_time": "2022-01-15T22:57:02.056562Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( environ.val_metrics.keys())\n",
    "# pp.pprint(val_metrics)\n",
    "print(type(environ.val_metrics['aggregated']))\n",
    "print()\n",
    "print(type(environ.val_metrics['task1']['classification_agg']))\n",
    "print()\n",
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2d510",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Policy / Logit stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb628497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:35:14.041577Z",
     "start_time": "2022-02-08T20:35:14.018303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.special          import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eed454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:30.103364Z",
     "start_time": "2022-02-08T20:00:30.068021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b5cc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_task_logits(n)` Get logits for task group n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed8b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:56:35.652087Z",
     "start_time": "2022-02-08T20:56:35.327406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task_logits = environ.get_task_logits(1)\n",
    "print(task_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66fa5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_arch_parameters()`: Get last used logits from network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85521e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:31.101960Z",
     "start_time": "2022-02-08T20:00:30.757064Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b0bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:06.054699Z",
     "start_time": "2022-02-09T15:43:05.689327Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871ee38",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_logits()`:  Get Policy Logits - returns same as `get_arch_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb40c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:24.972390Z",
     "start_time": "2022-02-09T15:43:24.636629Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = environ.get_policy_logits()\n",
    "for i in logs:\n",
    "    print(i, '\\n')\n",
    "# probs = softmax(logs, axis= -1)\n",
    "# for i in probs:\n",
    "#     print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080a364",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_prob()` : Gets the softmax of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c75af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:29.733732Z",
     "start_time": "2022-02-09T15:43:29.699600Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "for i in policy_softmaxs:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3160d9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = False)` : Calls test_sample_policy of network with random choices based on softmax of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:21:15.636722Z",
     "start_time": "2022-02-08T22:21:15.165456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "policies,logits = environ.get_sample_policy(hard_sampling = False)\n",
    "\n",
    "for l, p, s in zip(logits, policies, policy_softmaxs) :\n",
    "    for  l_row, p_row, s_row in zip(l, p, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802664ec",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = True)` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea65bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:59:40.790899Z",
     "start_time": "2022-02-08T20:59:40.726657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "hard_policies, logits = environ.get_sample_policy(hard_sampling = True)\n",
    "\n",
    "for p,l,s in zip(hard_policies, logits, policy_softmaxs) :\n",
    "    for  p_row, l_row, s_row in zip(p, l, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c39cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:00:47.452220Z",
     "start_time": "2022-02-08T21:00:47.422902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" Layer    task 1      task 2      task 3\")\n",
    "print(f\" -----    ------      ------      ------\")\n",
    "for idx, (l1, l2, l3) in enumerate(zip(hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1}       {l2}       {l3}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade0ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:39:39.936555Z",
     "start_time": "2022-02-08T22:39:39.911591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_trained_policy(iter):\n",
    "\n",
    "    policy_softmaxs = environ.get_policy_prob()\n",
    "    policy_argmaxs = 1-np.argmax(policy_softmaxs, axis = -1)\n",
    "    print(f\"  Trained polcies at iteration: {iter} \")\n",
    "    print(f\"                   task 1                           task 2                         task 3        \")\n",
    "    print(f\" Layer       softmax        select          softmax        select          softmax        select   \")\n",
    "    print(f\" -----    ---------------   ------       ---------------   ------       ---------------   ------   \")\n",
    "    for idx, (l1,l2,l3,  p1,p2,p3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2], policy_argmaxs[0], policy_argmaxs[1], policy_argmaxs[2]),1):\n",
    "        print(f\"   {idx}      {l1[0]:.4f}   {l1[1]:.4f}   {p1:4d}    {l2[0]:11.4f}   {l2[1]:.4f}   {p2:4d}    {l3[0]:11.4f}   {l3[1]:.4f}   {p3:4d}\")\n",
    "\n",
    "    print()\n",
    "# print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec208dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:42:12.650813Z",
     "start_time": "2022-02-08T22:42:12.330169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_trained_policy(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec517e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:07:30.836214Z",
     "start_time": "2022-02-08T22:07:30.804575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"                        POLICIES (SOFTMAX)                                       task 3          \")\n",
    "print(f\" Layer    task1              task2            task3 softmax         softmax         argmax         softmax         argmax   \")\n",
    "print(f\" -----    -------------     -------------     -------------   ------   \")\n",
    "for idx, (l1,l2,l3, h1,h2,h3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2],hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1[0]:.4f} {l1[1]:.4f}     {l2[0]:.4f} {l2[1]:.4f}     {l3[0]:.4f} {l3[1]:.4f}    {h3}\")\n",
    "    \n",
    "print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbeacb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:04:50.757406Z",
     "start_time": "2022-02-08T22:04:50.731736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(policy_softmaxs[2], np.argmax(1-policy_softmaxs[2], axis = -1))\n",
    "print(policy_softmaxs, np.argmax(policy_softmaxs, axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0240",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_current_logits()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb7240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:19:06.155425Z",
     "start_time": "2022-02-08T21:19:06.118640Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits  = (environ.get_current_logits())\n",
    "for i in logits:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84662",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_current_policy()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cfa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:40:06.543376Z",
     "start_time": "2022-02-08T20:40:06.230711Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pols  = (environ.get_current_policy())\n",
    "\n",
    "for i in pols:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f556a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:27:33.593255Z",
     "start_time": "2022-01-27T18:27:33.553141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792710e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `gumbel_softmax()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265490e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:28:17.107529Z",
     "start_time": "2022-02-08T21:28:17.084910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', floatmode = 'maxprec_equal')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb0087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:35:11.617269Z",
     "start_time": "2022-02-08T21:35:11.569599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.temp)\n",
    "# tau = environ.temp\n",
    "tau = 1\n",
    "for i in range(3): \n",
    "    logits_tensor = torch.tensor(logits[0])\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=tau, hard=False).cpu().numpy() \n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=tau, hard=True).cpu().numpy()\n",
    "    \n",
    "    for l, gs, gh in zip(lgts, gumbel_soft, gumbel_hard):\n",
    "        print(f\"   {l}   \\t {gs}            \\t {gh}\")\n",
    "#     print(lgts)\n",
    "#     print(gumbel_soft)\n",
    "#     print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5ef7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e0e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:21:35.524957Z",
     "start_time": "2022-02-08T21:21:35.488812Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for lgts in logits:\n",
    "    logits_tensor = torch.tensor(lgts)\n",
    "    print(lgts)\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=1, hard=False)\n",
    "    print(gumbel_soft)\n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=1, hard=True)\n",
    "    print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34a06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T20:49:37.643349Z",
     "start_time": "2022-02-03T20:49:37.580786Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "smax = scipy.special.softmax(logs, axis =1)\n",
    "# smax = np.array( \n",
    "# [[0.46973792, 0.530262  ],\n",
    "#  [0.45025694, 0.549743  ],\n",
    "#  [0.4443086 , 0.5556915 ],\n",
    "#  [0.4138397 , 0.58616036],\n",
    "#  [0.4140113 , 0.5859887 ],\n",
    "#  [0.42114905, 0.57885087]])\n",
    "\n",
    "print(smax.shape)\n",
    "print(smax)\n",
    "print(smax[0])\n",
    "print(smax[0].sum())\n",
    "print(np.random.choice((1,0), p =smax[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7de25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:15:19.893888Z",
     "start_time": "2022-02-08T21:15:19.870899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logs = np.array(\n",
    "[[0.33064184, 0.42053092],\n",
    " [0.3532089 , 0.52056104],\n",
    " [0.3888512 , 0.5680909 ],\n",
    " [0.42039296, 0.694217  ],\n",
    " [0.4519742 , 0.73311865],\n",
    " [0.48401102, 0.7522658 ]],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pyt-gpu]",
   "language": "python",
   "name": "conda-env-pyt-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "399px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "888d4fda4588b3bfc9793c8a97c6f83877963bb7385ca7ca0c08738cf63adc49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
