{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408f1261",
   "metadata": {},
   "source": [
    "## Initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55604c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:25.223269Z",
     "start_time": "2022-08-11T12:35:25.190666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0c686b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.246811Z",
     "start_time": "2022-08-11T12:35:25.225269Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:08.233990Z",
     "iopub.status.busy": "2022-01-07T22:44:08.233053Z",
     "iopub.status.idle": "2022-01-07T22:44:08.273284Z",
     "shell.execute_reply": "2022-01-07T22:44:08.271908Z",
     "shell.execute_reply.started": "2022-01-07T22:44:08.233943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "# sys.path.insert(0, '/home/kbardool/kusanagi/AdaSparseChem/src') ; print(sys.path)\n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "import types, copy, pprint\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "## Set visible GPU device \n",
    "##----------------------------------------------\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "# from pynvml import *\n",
    "from utils import (initialize, init_dataloaders, init_environment, init_wandb, training_initializations, model_initializations, \n",
    "                   check_for_resume_training, disp_dataloader_info, disp_info_1, warmup_phase, weight_policy_training, \n",
    "                   display_gpu_info, init_dataloaders_by_fold_id, print_separator, print_heading, \n",
    "                   timestring, print_loss, get_command_line_args, load_from_pickle) \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "pd.options.display.width = 132\n",
    "# torch.set_printoptions(precision=6, threshold=None, edgeitems=None, linewidth=132, profile=None, sci_mode=None)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Adashare_Train.ipynb\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33ac6e",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee652c57",
   "metadata": {},
   "source": [
    "### Parse Input Args  - Read YAML config file - wandb initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42bb98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.271726Z",
     "start_time": "2022-08-11T12:35:27.248889Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_args = \" --config yamls/chembl_3task_train.yaml \" \\\n",
    "#              \" --resume \" \\\n",
    "#              \" --exp_id      330i85cg\" \\\n",
    "#              \" --exp_name    0308_1204\" \\\n",
    "#              \" --lambda_sparsity  0.01\"\\\n",
    "#              \" --lambda_sharing   0.01\" \n",
    "#========================================================================\n",
    "#              \" --no_residual \"\n",
    "#              \" --exp_name       0410_1934 \" \\\n",
    "#              \" --hidden_size   100 100 100 100 100 100\" \\\n",
    "#              \" --tail_hidden_size  100 \" \\\n",
    "#              \" --decay_lr_rate      0.75\"  \\\n",
    "#              \" --decay_lr_freq       20\"  \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca1c17d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.340669Z",
     "start_time": "2022-08-11T12:35:27.273972Z"
    }
   },
   "outputs": [],
   "source": [
    "synthetic_1task_config = \"../yamls/chembl_synt_train_1task.yaml\"\n",
    "synthetic_3task_config = \"../yamls/chembl_synt_train_3task.yaml\"\n",
    "synthetic_5task_config = \"../yamls/chembl_synt_train_5task.yaml\"\n",
    "synthetic_config_file  = \"../yamls/chembl_synt_train.yaml\"\n",
    "# config_file      = \"../yamls/chembl_mini_train.yaml\"\n",
    "config_file      = \"../yamls/chembl_cb29_train.yaml\"\n",
    "config_file      = \"../yamls/chembl_cb29_train_10task.yaml\"\n",
    "batch_size=4098\n",
    "# batch_size=2048\n",
    "# RESUME_MODEL_CKPT = 'model_train_ep_25_seed_0088'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edb398",
   "metadata": {},
   "source": [
    "####   For Resume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c33f25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.383501Z",
     "start_time": "2022-08-11T12:35:27.342279Z"
    }
   },
   "outputs": [],
   "source": [
    "restart_input_args = f\" --config  {config_file} \" \\\n",
    "             \" --exp_desc     weight 105 bch/ep policy 105 bch/ep \" \\\n",
    "             \" --warmup_epochs        10 \"  \\\n",
    "             \" --hidden_size         4000 4000 4000 4000 4000\"  \\\n",
    "             \" --tail_hidden_size    4000 \"  \\\n",
    "             \" --first_dropout       0.80 \"  \\\n",
    "             \" --middle_dropout      0.80\"  \\\n",
    "             \" --last_dropout        0.80 \"  \\\n",
    "             \" --seed_idx              0 \"  \\\n",
    "             f\" --batch_size        {batch_size} \"  \\\n",
    "             \" --task_lr           0.001 \"  \\\n",
    "             \" --backbone_lr       0.001 \"  \\\n",
    "             \" --decay_lr_rate       0.3 \"  \\\n",
    "             \" --decay_lr_freq        10 \"  \\\n",
    "             \" --policy_lr         0.001 \"  \\\n",
    "             \" --lambda_sparsity    0.02 \"  \\\n",
    "             \" --lambda_sharing     0.01 \"  \\\n",
    "             \" --skip_hidden       False \"  \\\n",
    "             \" --skip_residual     False \"  \\\n",
    "             \" --pytorch_threads       4 \"  \\\n",
    "             \" --cuda_devices          2\"   \\\n",
    "             \" --gpu_ids               0 \"  \\\n",
    "             \" --resume\"                    \\\n",
    "             \" --resume_path        ../../experiments/AdaSparseChem-cb29/4000x5_0624_1938_lr0.001_do0.8_RESUME3/\" \\\n",
    "             \" --resume_ckpt        model_best_seed_model\" \\\n",
    "             \" --resume_metrics     metrics_best_seed.pickle\" \\\n",
    "             \" --exp_id             h05zsolg\" \\\n",
    "             \" --exp_name           0712_0950\" \\\n",
    "             \" --folder_sfx         RESUME4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68145e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T17:38:07.664270Z",
     "start_time": "2022-06-24T17:38:07.630274Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "source": [
    "####  For Initiating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3a7f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.466239Z",
     "start_time": "2022-08-11T12:35:27.385525Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [],
   "source": [
    "input_args = f\" --config  {config_file} \" \\\n",
    "             \" --exp_desc            weight 105 bch/ep policy 105 bch/ep \" \\\n",
    "             \" --warmup_epochs         10 \"  \\\n",
    "             \" --hidden_size         4000 4000 \"  \\\n",
    "             \" --tail_hidden_size    4000 \"  \\\n",
    "             \" --first_dropout       0.80 \"  \\\n",
    "             \" --middle_dropout      0.80\"  \\\n",
    "             \" --last_dropout        0.80 \"  \\\n",
    "             \" --seed_idx              0 \"  \\\n",
    "             f\" --batch_size        {batch_size} \"  \\\n",
    "             \" --task_lr           0.001 \"  \\\n",
    "             \" --backbone_lr       0.001 \"  \\\n",
    "             \" --decay_lr_rate       0.3 \"  \\\n",
    "             \" --decay_lr_freq        10 \"  \\\n",
    "             \" --policy_lr         0.001 \"  \\\n",
    "             \" --lambda_sparsity    0.02 \"  \\\n",
    "             \" --lambda_sharing     0.01 \"  \\\n",
    "             \" --skip_hidden       False \"  \\\n",
    "             \" --skip_residual     False \"  \\\n",
    "             \" --pytorch_threads       6 \"  \\\n",
    "             \" --cuda_devices          2\"   \\\n",
    "             \" --gpu_ids               0 \"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ffb30",
   "metadata": {},
   "source": [
    "### Read Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fcbe87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.583669Z",
     "start_time": "2022-08-11T12:35:27.467909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  command line parms : \n",
      "------------------------\n",
      " config...................  ../yamls/chembl_cb29_train_10task.yaml\n",
      " project_name.............  None\n",
      " exp_id...................  289yd31q\n",
      " exp_name.................  None\n",
      " folder_sfx...............  None\n",
      " exp_desc.................  weight 105 bch/ep policy 105 bch/ep\n",
      " hidden_sizes.............  [4000, 4000]\n",
      " tail_hidden_size.........  [4000]\n",
      " warmup_epochs............  10\n",
      " training_epochs..........  None\n",
      " seed_idx.................  0\n",
      " batch_size...............  4098\n",
      " first_dropout............  0.8\n",
      " middle_dropout...........  0.8\n",
      " last_dropout.............  0.8\n",
      " backbone_lr..............  0.001\n",
      " task_lr..................  0.001\n",
      " policy_lr................  0.001\n",
      " decay_lr_rate............  0.3\n",
      " decay_lr_freq............  10.0\n",
      " lambda_sparsity..........  0.02\n",
      " lambda_sharing...........  0.01\n",
      " cuda_devices.............  2\n",
      " gpu_ids..................  [0]\n",
      " pytorch_threads..........  6\n",
      " skip_residual............  False\n",
      " skip_hidden..............  False\n",
      " resume...................  False\n",
      " resume_path..............  None\n",
      " resume_ckpt..............  None\n",
      " resume_metrics...........  None\n",
      " cpu......................  False\n",
      " min_samples_class........  None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns = types.SimpleNamespace()\n",
    "input_args = input_args.split() if input_args is not None else input_args\n",
    "# input_args = restart_input_args.split() \n",
    "ns.args = get_command_line_args(input_args, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0c6844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:27.772636Z",
     "start_time": "2022-08-11T12:35:27.585276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CUDA Device(s) available\n",
      "--------------------------\n",
      " CUDA device count   :  1\n",
      " CUDA current device :  0   name:  NVIDIA TITAN Xp\n",
      " GPU Processes       :  GPU:0\n",
      "no processes are running\n",
      "\n",
      "\n",
      " GPU Device Info \n",
      "------------------\n",
      " Device : cuda:0\n",
      "   name:        NVIDIA TITAN Xp\n",
      "   capability:  (6, 1)\n",
      "   properties:  _CudaDeviceProperties(name='NVIDIA TITAN Xp', major=6, minor=1, total_memory=12196MB, multi_processor_count=30)\n",
      "   Allocated :  0\n",
      "   Reserved  :  0\n",
      "\n",
      "\n",
      " GPU Usage Stats \n",
      "------------------\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "|  1 | 40% | 87% |\n",
      "|  2 |  0% |  0% |\n",
      "\n",
      " torch.cuda.current-device():  0\n",
      " ids :  [2]\n",
      "\n",
      "\n",
      " nvml_gpu_id: 2\n",
      "----------------\n",
      " nvml handle: <pynvml.nvml.LP_struct_c_nvmlDevice_t object at 0x7fef0a9429c0>\n",
      "\n",
      " nvml Device Memory Info\n",
      "-------------------------\n",
      "c_nvmlMemory_t(total: 34087305216 B, free: 34083897344 B, used: 3407872 B)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=ns.args.cuda_devices\n",
    "display_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ddaf625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:33.003990Z",
     "start_time": "2022-08-11T12:35:27.774960Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "################### READ YAML ####################\n",
      "##################################################\n",
      " Pytorch thread count: 20\n",
      " Set Pytorch thread count to : 6\n",
      " Pytorch thread count set to : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkbardool\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kevin/WSL-projs/AdaSparseChem/notebooks/wandb/run-20220811_143528-289yd31q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/289yd31q\" target=\"_blank\">0811_1435</a></strong> to <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WandB Initialization -----------------------------------------------------------\n",
      " PROJECT NAME: AdaSparseChem-cb29-10Task\n",
      " RUN ID      : 289yd31q \n",
      " RUN NAME    : 0811_1435\n",
      " --------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " log_dir              create folder:  ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      " result_dir           folder exists:  ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      " checkpoint_dir       folder exists:  ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " experiment name       : 0811_1435 \n",
      " experiment id         : 289yd31q \n",
      " folder_name           : 4000x2_0811_1435_lr0.001_do0.8 \n",
      " experiment description: weight 105 bch/ep policy 105 bch/ep\n",
      " Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      " Random  seed used     : 88 \n",
      " log folder            : ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      " checkpoint folder     : ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      " Gpu ids               : [0]\n",
      " Seed index            : 0\n",
      " policy_iter           : best\n",
      " Data Split ratios     : [0.725, 0.225, 0.05]\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "        project_name : AdaSparseChem-cb29-10Task\n",
      "              exp_id : 289yd31q\n",
      "        exp_name_pfx : 0811_1435\n",
      "            exp_name : 0811_1435\n",
      "          exp_folder : 4000x2_0811_1435_lr0.001_do0.8\n",
      "     exp_description : weight 105 bch/ep policy 105 bch/ep\n",
      "          folder_sfx : None\n",
      "         random_seed : 88\n",
      "           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "              config : ../yamls/chembl_cb29_train_10task.yaml\n",
      "                 cpu : None\n",
      "             gpu_ids : [0]\n",
      "            backbone : SparseChem\n",
      "               tasks : ['class', 'class', 'class', 'class', 'class', 'class', 'class', 'class', 'class', 'class']\n",
      "     tasks_num_class : [472, 624, 688, 192, 620, 184, 224, 148, 344, 72]\n",
      "             lambdas : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "             verbose : False\n",
      "     input_size_freq : None\n",
      "          input_size : 32000\n",
      "        hidden_sizes : [4000, 4000]\n",
      "    tail_hidden_size : [4000]\n",
      " first_non_linearity : relu\n",
      "middle_non_linearity : relu\n",
      "  last_non_linearity : relu\n",
      "       first_dropout : 0.8\n",
      "      middle_dropout : 0.8\n",
      "        last_dropout : 0.8\n",
      "   class_output_size : None\n",
      "    regr_output_size : None\n",
      "              policy : True\n",
      "        policy_model : task-specific\n",
      "       skip_residual : False\n",
      "         skip_hidden : False\n",
      "           is_sparse : True\n",
      "          is_sharing : True\n",
      "diff_sparsity_weights : False\n",
      "          skip_layer : 0\n",
      "       is_curriculum : False\n",
      "    curriculum_speed : 3\n",
      "              fix_BN : False\n",
      "     retrain_from_pl : False\n",
      "\n",
      "train\n",
      "-----\n",
      "          batch_size : 128\n",
      "       warmup_epochs : 10\n",
      "     training_epochs : 250\n",
      "         total_iters : 25000\n",
      "       warm_up_iters : None\n",
      "             task_lr : 0.001\n",
      "         backbone_lr : 0.001\n",
      "    weight_optimizer : adam\n",
      "    policy_optimizer : adam\n",
      "       decay_lr_rate : 0.3\n",
      "       decay_lr_freq : 10.0\n",
      "   decay_lr_cooldown : 0\n",
      "           policy_lr : 0.001\n",
      "policy_decay_lr_rate : 0.75\n",
      "policy_decay_lr_freq : 50\n",
      "policy_decay_lr_cooldown : 0\n",
      "     lambda_sparsity : 0.02\n",
      "      lambda_sharing : 0.01\n",
      "        lambda_tasks : 1\n",
      "         init_method : random\n",
      "           init_temp : 4\n",
      "          decay_temp : 0.965\n",
      "     decay_temp_freq : 16\n",
      "     init_neg_logits : None\n",
      "       hard_sampling : False\n",
      "            val_freq : 500\n",
      "          print_freq : -1\n",
      "weight_iter_alternate : -1\n",
      "alpha_iter_alternate : -1\n",
      "           val_iters : -1\n",
      "              resume : False\n",
      "      retrain_resume : False\n",
      "         policy_iter : best\n",
      "          which_iter : warmup\n",
      "\n",
      "paths\n",
      "-----\n",
      "             log_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      "          result_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      "      checkpoint_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x2_0811_1435_lr0.001_do0.8\n",
      "\n",
      "dataload\n",
      "--------\n",
      "             dataset : Chembl29\n",
      "            dataroot : ../../MLDatasets/chembl29_10task\n",
      "                   x : chembl_29_X.npy\n",
      "      x_split_ratios : [0.725, 0.225, 0.05]\n",
      "             folding : chembl_29_folding.npy\n",
      "             y_tasks : ['chembl_29_Y_tg_0_cols_472.npy', 'chembl_29_Y_tg_1_cols_624.npy', 'chembl_29_Y_tg_6_cols_688.npy', 'chembl_29_Y_tg_10_cols_192.npy', 'chembl_29_Y_tg_11_cols_620.npy', 'chembl_29_Y_tg_643_cols_184.npy', 'chembl_29_Y_tg_836_cols_224.npy', 'chembl_29_Y_tg_1005_cols_148.npy', 'chembl_29_Y_tg_1028_cols_344.npy', 'chembl_29_Y_tg_1031_cols_72.npy']\n",
      "            y_censor : None\n",
      "         fold_inputs : 32000\n",
      "     input_transform : None\n",
      "       weights_class : None\n",
      "   min_samples_class : 1\n",
      "             fold_va : 0\n",
      "             fold_te : None\n",
      "\n",
      "SC\n",
      "--\n",
      "      normalize_loss : None\n",
      "     pytorch_threads : 6\n",
      "            seed_idx : 0\n",
      "         resume_path : None\n",
      "         resume_ckpt : None\n",
      "      resume_metrics : None\n"
     ]
    }
   ],
   "source": [
    "opt = initialize(ns, build_folders = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e71cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de876461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:33.035568Z",
     "start_time": "2022-08-11T12:35:33.006246Z"
    }
   },
   "outputs": [],
   "source": [
    "# print_heading(f\" experiment name       : {opt['exp_name']} \\n\"\n",
    "#               f\" experiment id         : {opt['exp_id']} \\n\"\n",
    "#               f\" folder_name           : {opt['exp_folder']} \\n\"\n",
    "#               f\" experiment description: {opt['exp_description']}\\n\"\n",
    "#               f\" Random seeds          : {opt['seed_list']}\\n\"\n",
    "#               f\" Seed index            : {opt['seed_idx']} \\n\"\n",
    "#               f\" Policy_iter           : {opt['train']['policy_iter']}\\n\"\n",
    "#               f\" Random  seed used     : {opt['random_seed']} \\n\"\n",
    "#               f\" log folder            : {opt['paths']['log_dir']}\\n\"\n",
    "#               f\" checkpoint folder     : {opt['paths']['checkpoint_dir']}\\n\"\n",
    "#               f\" Gpu ids               : {opt['gpu_ids']}\\n\"\n",
    "#               f\" X Split Ratios        : {opt['dataload']['x_split_ratios']}\"\n",
    "#               , verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58caea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T07:40:24.684944Z",
     "start_time": "2022-07-04T07:40:24.654093Z"
    }
   },
   "source": [
    "### Setup Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3a2edeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:43.565905Z",
     "start_time": "2022-08-11T12:35:37.835435Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      " 2022-08-11 14:35:37:875000  Create new  Chembl_23 instance \n",
      "---------------------------------------------------------------- \n",
      "\n",
      " verbose        : True\n",
      "\n",
      " FOLDS param provided - folds: [1, 2, 3, 4] \n",
      "\n",
      " Index shape: (423736,) # of entries 340803 \n",
      " [ True  True  True ...  True  True  True]\n",
      " X (ecfp[0]) file count non zero (post fold & transform) :79 \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_0_cols_472.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task1\n",
      " load_task_weights() - no weights file provided for y_task1, training_weights for all  472 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (472,)- total: 81937 - sum(axis=0):\n",
      "[ 570  264   58   11   45    4    0    0   13    1    0    0    6    2    0    0    2    0    0    0   20    4    0    0   80    9    0    0   12\n",
      "    1    0    0   62   19   11    0 1700 1238  752  391 1322  996  525  117    6    0    0    0   54    9    2    0  291  132   11    0 1552 1315\n",
      "  924  312  143   80   39   16 1636 1066  540  115 2304 1655  920  175 1294  793  327   90  458  191   82   10   19    0    0    0  662  101   40\n",
      "   22   11    0    0    0   46    8    0    0  122   19    0    0  399  180   36   20   55    5    0    0  107   33    0    0  780  528   90    7\n",
      "   45    4    2    0  321  204   65    0  109   50   14    4   10    3    0    0   37    4    2    0  228  179  131   25    8    0    0    0  117\n",
      "   57    9    0   73   35    5    1 1336  895  326   63 1831 1414  512   64  573  260    0    0   14    1    0    0  438   72   19    2  169  111\n",
      "   59    1 1015  710  297   72  532  201   17    0   73   29   17    8  153   95   24    5   79    4    1    0  106   36    9    1 1806 1474 1189\n",
      "  748   49   21    1    0  113    7    0    0 1182  781  399   98  147   69   28   10  268  134   86   48   74    2    0    0  108   15    1    0\n",
      "  173   34    1    0    2    0    0    0  595  385  161   64   96   80   43   18    8    0    0    0  346  243   43    4  383  285  163   37  142\n",
      "  114   59   14  851  710  477   77  326  241  141   25  136   33   14    7   90   49   11    0  354  262  120   31  646  563  385  132  150  104\n",
      "   38    7   78   50    8    1   13    2    0    0 1275  802  356   97  127   60   39    4    0    0    0    0    3    2    0    0  753  399  110\n",
      "   17   85   49   20    4  129  110   42   16  121   53   24    0  193  119   56    5 1133  989  687  350  118   82   41    6   60   10    1    1\n",
      "  658  492  253   54   47   10    0    0   56   17    7    1   57   24    4    0  132   30    7    0   24    8    1    0  140  110   26    5   39\n",
      "    1    0    0  214  214  171   44  349  349  349  344  100   44    3    0  252   79   10    0  268  246  198   22  148   44    4    0  161  154\n",
      "  101   44  141   94   51   12  143  110   49    4  220  162   92   27  255  215  125   10   36   19    7    0  144  144  144  140 1201  988  271\n",
      "   59  336  136    0    0   83   35   19    6    0    0    0    0  600  445  268    3   64    1    0    0   68   45    5    0   68    9    1    0\n",
      "   85   64   31    9   59   16   15    4] \n",
      "\n",
      "\n",
      " fold_pos: (5, 472) \n",
      " [[ 22   7   0 ...   0   0   0]\n",
      " [ 94  55  15 ...   0   0   0]\n",
      " [102  49  12 ...   0   0   0]\n",
      " [301 137  22 ...  16  15   4]\n",
      " [ 51  16   9 ...   0   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (472,)- total: 188511 - sum(axis=0):\n",
      "[ 405  711  917  965  113  154  158  158  623  635  636  636  630  634  635  635  616  618  618  618 1010 1024 1028 1028   51  122  131  131  108\n",
      "  119  121  121   73  135  143  154  930 1416 1906 2268  520  842 1309 1671  175  181  181  181   98  143  150  152  176  283  394  405  191  501\n",
      "  891 1496  166  229  270  293  402 1000 1544 1974  485 1241 2009 2767  645 1198 1683 1926  581  850  959 1031  133  146  146  146  987 1536 1597\n",
      " 1615 1397 1408 1408 1408  191  231  239  239  250  353  369  369  481  701  841  857  428  478  483  483  790  879  910  910  255  527  965 1041\n",
      "  213  254  256  258  191  311  449  501   76  112  146  165  437  461  488  490  191  224  228  232  244  298  346  452   89   97   97   97  286\n",
      "  346  394  403   69  111  143  147  563 1032 1599 1859  328  759 1661 2099  315  419  421  421  209  225  225  225  844 1014 1067 1084   42  101\n",
      "  153  211  502  786 1197 1398  311  650  833  850   39   79   90   99  214  274  325  331   98  117  120  121  248  325  352  360  313  560  809\n",
      " 1232  145  173  193  194  378  484  491  491  446  877 1259 1560  211  289  330  348  217  351  399  437  167  240  242  242   42  137  153  155\n",
      "  689  836  875  878  323  325  325  325  333  543  767  864   11   27   67   92  216  224  224  224  123  231  429  466  425  550  672  798   86\n",
      "  114  169  214  222  366  599  999  261  346  447  564  239  342  361  369   59  100  138  149  190  304  446  535   46  134  312  565   38   92\n",
      "  158  189   79  109  151  157  126  137  139  139  216  899 1345 1603   60  129  150  185  114  114  114  114  114  115  117  117  161  606  896\n",
      "  989   77  113  142  158   24   43  111  137   90  155  183  207  117  191  254  305  118  272  590  926  233  277  318  353  192  237  246  246\n",
      "  139  277  480  673  166  203  213  213  172  211  221  227  161  194  214  217  192  294  317  324   91  107  114  115   21   53  137  158   61\n",
      "   99  100  100   21   21   64  191    0    0    0    5   94  152  193  196  184  357  426  436    9   42   90  266   11   53   93   97    5   13\n",
      "   80  136  129  176  219  257    8   41  102  147   30   93  163  228   23   64  154  269   89  160  179  189    0    0    0    4  532  755 1449\n",
      " 1661   24  224  360  360   49   97  113  126  204  204  204  204  130  354  528  621   74  137  138  138   21   50  102  107  162  221  229  230\n",
      "   20   41   74   96  149  192  193  204] \n",
      "\n",
      "\n",
      " fold_neg: (5, 472) \n",
      " [[ 81  96 103 ...   1   1   1]\n",
      " [ 47  86 126 ...   0   0   0]\n",
      " [ 99 152 189 ...   0   0   0]\n",
      " [142 306 421 ... 148 149 160]\n",
      " [ 36  71  78 ...  43  43  43]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (472,) sum: 199.0\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "  Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      "  Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 472  Y rows with populated labels: 27224  non zero cols: 67476\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_1_cols_624.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task2\n",
      " load_task_weights() - no weights file provided for y_task2, training_weights for all  624 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (624,)- total: 90665 - sum(axis=0):\n",
      "[ 106   72   29    3  587  494  345   97 2738 1689  792  256  133   68   40   19  412  247  133   36  153   98   31    3  143   83   42   11   59\n",
      "   47   24    3  407  244   26    1 1063  803  533  303   77   35    9    3  111   32    5    0   61   10    0    0   65   23    5    0 1030  315\n",
      "   66    9   64    7    1    0    6    0    0    0  162   24    0    0   16   10    4    0  315  178   68    6   12    0    0    0   33    0    0\n",
      "    0   28    0    0    0   87   61   16    1  187   62   18    1  121   49    8    0   72    5    0    0 1276  949  718  336    8    1    0    0\n",
      "    9    2    0    0   57    0    0    0   38    2    0    0   34   17    2    0   67   11    2    0  103    6    0    0    7    1    0    0  105\n",
      "   58   33    6    0    0    0    0  174   27    2    0   84    8    0    0  284  158   51    2  209  112   43    6   76   42   13    0  266   70\n",
      "   21    2  628  399  219  101  181    0    0    0  203    0    0    0  500  393  186   16  197   19    1    0  215   41   16    0   30    2    0\n",
      "    0  117    0    0    0   33    0    0    0 1044  797  479  174   92   67   37    1 1404 1109  784  451  663  563  373  121  446  404  197   24\n",
      " 3488 3085 2184 1007   56   33   16    4   57   29   11    5    8    6    5    4   76   16    4    1 1370  770  377  112  113   62   19    1  480\n",
      "  279  111   23   36   12    3    0  116   15    2    1    1    0    0    0  180   58   10    0  434  279   91    9   82   12    0    0  587  443\n",
      "  291  141   51    4    0    0  123  102   59   45   27    1    0    0   87   43    7    0   20    6    2    0  163  125   82   34   75   14    4\n",
      "    0  253  189   80   25  149   18    0    0   64    5    1    0    4    0    0    0   17   10    6    0  184   78   26    1  127   70   37   12\n",
      "   84   56   31    6   79   35   15    4  150  102   60   17  123   89   37   21   48   21    6    0   22   12    0    0  160   64    7    0  100\n",
      "   82   32   10  323  235  116   48  431  184    8    0  425  242   98   41  132   26    0    0  375  307  135   26   73   45   15    5  393  308\n",
      "  206  116   22    8    0    0  754  683  488   89  143  100   57   19  510  427  289  136  126   57   19    4   16    5    1    1   70   45   37\n",
      "   22  137  109   67   19  141  121   49   17 2237 1959 1151  178   56   45    2    0  353  268  146   55   39    0    0    0  105   82   61   37\n",
      "   66   35   16    6  103   76   41   11  123  103   47    8  700  521  293   38  119   83   48    6  105   61   13    1  195   77   12    1  130\n",
      "   95   33    5  153  116   62    7  245  167   79   24  299  207   90   13  257  136   14    0  314  234  160    6  124  111    1    0  210  210\n",
      "   88    0    7    0    0    0  118  112   89   51  193  135    0    0  575  476   34   10  115   97   44   12   88   32    2    0   82   42   14\n",
      "    6  341  226   79   11 2942 1595  239   34  589  481  248   54  483  297  158   35   21   11    6    1   20    1    0    0   12    1    0    0\n",
      "   57   28    4    0  118   63   36   10   49   32   28   28  124   95   67    8  198  141   41    1  397    4    0    0  393    1    0    0   13\n",
      "    9    5    0  186  179  144   90  340  340   12    7  157   66    8    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 624) \n",
      " [[ 0  0  0 ...  5  0  0]\n",
      " [24 15 12 ... 11  3  0]\n",
      " [44 30  8 ... 28  3  0]\n",
      " [18 13  5 ... 17  0  0]\n",
      " [20 14  4 ...  5  2  0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (624,)- total: 219244 - sum(axis=0):\n",
      "[  14   48   91  117  205  298  447  694 2275 3375 4272 4806  253  340  368  389  302  514  628  718  284  339  406  434  118  181  222  253   60\n",
      "   73  100  121   93  217  347  372  356  629  899 1128  109  152  178  184  387  466  493  498  311  362  372  372  456  499  516  521 2428 3143\n",
      " 3392 3448  357  414  420  421  175  181  181  181   92  230  254  254  326  332  338  342  218  355  465  527  138  150  150  150  263  296  296\n",
      "  296  186  214  214  214   23   49   94  109  599  724  768  785  369  441  482  490  460  525  530  530  296  624  851 1232  106  125  127  127\n",
      "  119  126  128  128 1101 1158 1158 1158  694  729  730  730  135  155  170  172  164  220  229  231  215  312  318  318  162  170  171  171  367\n",
      "  421  446  473  157  157  157  157  526  670  692  694  355  425  432  432  194  334  441  490  112  209  278  315   48   98  126  140   38  420\n",
      "  469  488  370  604  784  902  658  839  839  839  811 1014 1014 1014  188  317  524  694  483  646  662  663  604  778  802  819   82  110  112\n",
      "  112  894 1011 1011 1011  634  667  667  667  519  781 1097 1402   32   57   87  122  489  800 1137 1470  306  418  608  857  135  202  410  583\n",
      "  189  609 1496 2663   58  102  119  131   38   73   91   97   79  104  105  106  187  245  257  260 1503 2111 2505 2769  119  170  213  231  170\n",
      "  371  539  627   73   97  106  109  177  267  279  280  137  138  138  138  686  808  856  866   90  245  433  515  195  266  278  278   66  192\n",
      "  319  466  288  335  338  338   28   49   92  106  132  158  159  159   22   66  102  109  110  124  128  130   32   84  136  184  220  281  291\n",
      "  295   26   90  200  255  431  562  580  580  284  343  347  348  178  182  182  182  138  145  149  155  278  384  436  461   74  131  164  189\n",
      "   76  104  129  154  434  480  497  511   31   81  123  166  129  163  215  231   94  121  136  142  106  116  128  128   28  124  181  188   94\n",
      "  116  167  189  197  285  410  478  314  561  737  745  382  565  714  771  540  646  672  672   34  102  274  356   37   65   95  105   56  145\n",
      "  248  338  104  118  126  126   49  120  315  517  157  200  243  281  105  239  377  528   98  181  216  231   93  108  112  112   83  113  121\n",
      "  136   47   75  117  165    3   24   96  128  106  429 1237 2210   56   67   90   90  177  277  399  499  103  142  142  142   24   47   68   92\n",
      "   54   85  104  114   22   55   91  121   48   80  136  175  204  386  583  838   29   66  101  143   22   67  115  127   94  212  277  288   41\n",
      "   76  138  165    5   26   53  104   30  108  196  251   28  131  248  325  133  254  376  390   68  148  222  376    0    0    0    1    2   21\n",
      "  143  212  142  149  149  149   64   70   93  131   14   72  184  184   37  136  446  470   15   33   86  118   43   99  129  131   66  106  134\n",
      "  142  170  285  432  500  244 1594 2950 3155   63  188  420  612  175  374  514  634  156  166  171  176   90  109  110  110  107  118  119  119\n",
      "   59   87  111  115   72  127  154  180   80   97  101  101   30   60   88  147   63  120  220  260    0    0    0    0    0    0    0    0  110\n",
      "  114  118  123    0    7   42   96    0   71  249  254   62  163  221  229] \n",
      "\n",
      "\n",
      " fold_neg: (5, 624) \n",
      " [[ 0  0  0 ... 45 50 50]\n",
      " [ 0  9 12 ... 23 31 34]\n",
      " [ 9 23 45 ... 34 59 62]\n",
      " [ 2  7 15 ... 37 54 54]\n",
      " [ 3  9 19 ... 24 27 29]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (624,) sum: 258.0\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "  Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 624  Y rows with populated labels: 31481  non zero cols: 75211\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_6_cols_688.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task3\n",
      " load_task_weights() - no weights file provided for y_task3, training_weights for all  688 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (688,)- total: 320094 - sum(axis=0):\n",
      "[1271  905  543  194 1109  958  566  112  352  191   59    3 1482 1150  696  218 2242 1643  975  450 1745 1365  760  258  420  309  156   42  440\n",
      "  317  177   81  282  189  109   25  104   83   55   27  272  219  138   37  964  905  720  237  262  154   45    2   96   51   23    1  590  527\n",
      "  403  191 1398 1223  833  209 6995 5174 2414  514 1467 1196  855  343 1270  996  756  446  736  447  199   41 2269 1830  844  389  739  471   74\n",
      "   14 1207  470  221   30 1067  680  259   24  105   87   65   36 3100 2536 1499  575 3493 2694 1437  423 1831 1181  555  134 2120 1478  832  233\n",
      "  136   76   18    4 2549 2136 1202  291 1663 1358  886  385 1195  798  421  112  743  475  178   26  877  615  298   57  220   81   20    0 2742\n",
      " 2280 1490  442 2748 2510 2057 1202  197  162  107   46 5767 4236 2258  911 1948 1457  974  315  108   71   38    9 2326 2057 1553  799 2832 2069\n",
      " 1157  619 3688 3373 2712 1188 4792 3893 2211  784  668  455  175   42  807  304   79   18  442  335  135    5 1788 1272  442   78  134   65   27\n",
      "   11 1290  997  480  260  978  699  371   90 1143  890  410   63  335  262  161   33  373  255    8    2   89   74   53   23 2595 2188 1604  856\n",
      "  539  332  148   24  547  331  122   17  810  647  323   67  641  506  261   67   72   41   20    5  417  199    7    2   49   25   15    8   81\n",
      "   62   32    6 1109  715  455  175 1356 1267 1010  410  128   77   33   18  391  299  211  101  297  185   98   23  119   65   29    2  639  494\n",
      "  264   59  427  267  168  111  414  310  121   16  170  142   85   28  604  442  241   76  302  208  131   27  333  233  120   27  101   82   38\n",
      "    3  106   48   23   10  502  363  190    6  993  644  205   52  196  134   62    8  679  521  261   33  191  159   86   26  450  362  211   88\n",
      " 1993 1694 1170  446  108   91   72   54   76   26    8    2  191  147   53    8  218  133   57   14  639  575  395  165  145   62   24    4  965\n",
      "  892  597  139  136  112   93   60  183  113   58    7  684  477  252   75  138  110   27    0   58   45   31   11   64   17    5    1 2556 1951\n",
      " 1144  351  125   47    2    0  801  642  390   57   79   34   11    1 1339 1117  750  270 1877 1566 1039  460  125  120  106   14   52   19    5\n",
      "    3  800  732  633  276  190  190  142    1  122   85   37   16  132   98   50    6 1578 1285  885  289   59   37    6    0  285  138   66   19\n",
      "  223  146  109   34  373  244   98   24  120   58   19    5  148   88   32    0  176  129   54    8  401  340  237   54  109   85   44   12  176\n",
      "  121   71   17 3985 3382 2176  933  719  535  104   28   67   46    1    1  184  136   98   31  244  154   51    6  169  143   78    6  228  180\n",
      "  123   37  931  843  633  277  596  452  234   24  276  198  106   11  237  195   62    0  277  181   68   14  149  116   88   65   88   70   52\n",
      "   19   53   10    0    0   72   62    1    1  131   67   31    4  229  226  169   64  193  167  113   64  150  102   45    1    4    2    1    0\n",
      "   25    1    0    0   82   51   21    3  265  244  186   73   47   30   14    2  541  360  112    1  159    2    0    0  131   89   36    8  129\n",
      "  104   57    7 1021  936  740  271 3414 2826 1678  632  788  463  166   57  186  139  101   80   33    7    0    0  209  184  128   38   94   80\n",
      "   64   35  695  601  221   31   99   57   27    1  914  752  515  124  326  305  231   35  120   68   26    4  227  163   96   21  140   59    7\n",
      "    0  283  236  129   23  190  159  102   23  135   14    0    0  140  117   44    2  581  338   95    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 688) \n",
      " [[275 201 124 ... 150  26   0]\n",
      " [144  94  51 ...   8   3   0]\n",
      " [233 156  83 ...   1   0   0]\n",
      " [439 332 207 ... 136  66   0]\n",
      " [180 122  78 ...  43   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (688,)- total: 382164 - sum(axis=0):\n",
      "[ 824 1225 1583 1926   72  243  635 1088  155  316  448  508  416  761 1211 1598  726 1384 2047 2495 1016 1472 2049 2544  142  261  417  530  226\n",
      "  359  498  597  113  209  292  376   14   38   66   94   22   75  156  257   84  166  352  833  159  269  378  422   27   72  100  120   68  131\n",
      "  255  467  109  326  713 1213 2049 4123 6900 8779  313  637  976 1492  503  813 1042 1346  382  730  974 1135  619 1090 1694 2149  552  840  990\n",
      " 1050  444  729  966 1158  420  824 1247 1482  111  133  155  182  330  906 1943 2870  823 1778 3032 4044  996 1686 2311 2735 1570 2250 2886 3471\n",
      "  197  266  324  342  290  732 1655 2563  282  603 1077 1571  942 1389 1768 2072  670  971 1266 1417  252  533  850 1089  244  384  445  465  695\n",
      " 1244 2046 3101  432  696 1152 2012  145  196  249  308 2653 4220 5720 7056  169  503  801 1449   37   75  108  137  233  527 1040 1782  650 1668\n",
      " 2601 3088  265  626 1287 2720  554 1849 3624 5157  590  820 1100 1233  263  458  684  745  171  281  481  610  703 1171 1813 2171  163  236  274\n",
      "  291  226  526  806  966  576  893 1220 1502  323  612 1103 1448  117  196  297  427  105  237  433  441   72   89  110  141  316  742 1324 2075\n",
      "  463  678  862  990  380  602  811  914  120  286  609  847  207  350  596  791   41   73   94  109  203  422  568  573   83  110  120  127   44\n",
      "   68   98  124  413  793 1001 1204  115  230  487 1086   31   82  125  140  119  216  306  417  161  297  384  456   64  123  159  185  174  323\n",
      "  553  758  452  635  740  799   99  211  406  511   55   90  146  203  423  596  796  962  138  252  334  442   89  214  327  421  105  136  180\n",
      "  215  187  248  270  283  158  269  369  427  246  758 1197 1354  148  214  299  353  219  387  641  868   19   60  140  197  153  260  410  533\n",
      "  669 1023 1552 2276   39   56   75   93   92  145  166  171   88  136  230  275  155  242  313  354   94  172  363  593  113  195  232  250   64\n",
      "  181  478  935   36   59   78  111   90  160  215  266  247  495  720  895   65   94  177  204   64   79   93  113   51   99  111  115  693 1320\n",
      " 2131 2927   88  174  219  221  172  383  635  968  108  154  177  191  147  415  782 1253  209  555 1082 1663    8   11   25  117   50   83   97\n",
      "   99   47  115  214  575   11   12   13   13   43   80  128  147   59   93  141  188  462  763 1172 1771   42   67   98  103   68  215  287  334\n",
      "   83  164  201  258  171  325  471  543   43  105  144  158  105  169  228  261  139  187  262  308   58  119  220  403   36   59  100  131   44\n",
      "  130  180  233  528 1203 2163 3326  192  417  715  790   38   59   91   91  102  151  187  251   87  187  291  336   17   55  120  191   53  105\n",
      "  163  249   40  130  340  695   44  173  315  396  183  258  294  318   57   99  232  294   63  164  277  330   52   89  117  140   57   85  103\n",
      "  133  118  161  171  171   35   45   84   84   34   98  134  161   13   17   74  179   19   47  101  150   85  133  190  234   54   57   58   59\n",
      "  150  174  175  175   35   66   96  114    8   45  103  216   68   86  101  112  153  335  583  698   14   35   37   37   94  136  189  217   15\n",
      "   43   90  138   48  136  334  807  700 1351 2490 3534  365  707  878  987  170  217  256  277   86  112  119  123   27   53  109  199   22   37\n",
      "   53   86   65  160  539  728   60  106  137  155  127  300  536  918   26   51  125  321  167  220  262  284  212  281  348  421   36  117  169\n",
      "  176   52  104  211  321   23   53  110  189   41  900  914  914   19   42  115  143  132  395  638  733] \n",
      "\n",
      "\n",
      " fold_neg: (5, 688) \n",
      " [[147 227 303 ...  93 217 243]\n",
      " [162 212 255 ...  28  33  36]\n",
      " [178 256 329 ...   6   7   7]\n",
      " [189 320 442 ...  11  81 147]\n",
      " [148 210 254 ... 257 300 300]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (688,) sum: 524.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 0.]\n",
      "\n",
      "  Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 688  Y rows with populated labels: 71831  non zero cols: 248001\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_10_cols_192.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task4\n",
      " load_task_weights() - no weights file provided for y_task4, training_weights for all  192 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (192,)- total: 44576 - sum(axis=0):\n",
      "[ 756  267   91   17  563  169   31   10  231  162   65    9 1083  666  166   17 1949 1057  399  106  575  365  149   18  495  272  119   33  448\n",
      "  330  160   43  176  110   41   10 1539  722  148   12 2398 1373  534  103 2921 2246 1450  532    8    0    0    0  221  113   42    1   11    3\n",
      "    0    0   44    9    1    0  277   64    9    0  187  163  138   52    9    6    4    2   68   21    1    0   39   10    3    0   77   52   39\n",
      "    0  137   70   21    8  179   83   21    2   18    1    1    0  113   77   28    2  162  117   59    0  575  372  173   23  171  138   56   19\n",
      "  592  314  141   24  265  196   98   24  111   99   74   22  437  296   98    0  262  200  111   23  377  307  246  172   56    3    0    0  565\n",
      "  499    9    0   94   45   24    7  459  290  166   68  253  135   87   55  194  126   34    0 1490 1162  265   28   69   43    6    0  290  283\n",
      "   16    1   17    0    0    0  154  125   88   43 1006  865  729  437  172  134   59    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 192) \n",
      " [[121  45  16   6 115  31   8   3  35  26  12   6 189 124  29   2 423 242 110  48  92  58  21   3  99  45   3   0  61  23   4   0  37  24   7   0\n",
      "  246  99  19   3 638 394 171  30 669 437 251  87   0   0   0   0  40  13   9   1   2   0   0   0   3   1   1   0  65  12   0   0  43  37  26   9\n",
      "    3   1   0   0  30   9   0   0   7   3   1   0  21  18  18   0   9   5   0   0  46  13   0   0   0   0   0   0  17   9   2   0  71  60  33   0\n",
      "  126  91  40   3  41  38  25   5  66  33  13   1  62  41   3   0   7   6   6   5 107  91  52   0  56  50  36  14  37  29  20   7   3   0   0   0\n",
      "  105  83   0   0  19  10   8   3  51  23   9   2   1   1   1   1  32  16   2   0 218 183  59   7  34  24   2   0  53  52   7   0  12   0   0   0\n",
      "   19  14   7   1 163 141  91  41  29  13   5   0]\n",
      " [146  59  22   2 110  35   8   2  15   8   4   0 231 116  16   1 278 124  31   6 259 177  80   7  45  14   2   0  58  27   3   0  35  23  11   2\n",
      "  363 169  36   6 320 145  31   3 772 626 432 188   4   0   0   0  46  25   7   0   5   3   0   0   7   0   0   0  70  23   5   0  26  20  13   6\n",
      "    0   0   0   0   1   0   0   0   2   0   0   0  14  10  10   0  26  17   0   0  38  26   9   2  12   0   0   0   8   6   0   0  33  17   0   0\n",
      "   93  61  32   8  39  33   2   0 173  94  39   4  44  29  11   2  27  26  22   1 137  72  19   0  58  49  37   2 149 141 132 116   4   1   0   0\n",
      "   68  55   0   0  19   4   0   0  92  62  42   7  30  27  25   7  37  28   3   0 149 123  27   0   5   4   2   0  49  44   1   0   2   0   0   0\n",
      "   33  28  20  12  57  41  29  13  15  13   3   0]\n",
      " [179  72  36   7 101  39  11   3   0   0   0   0 197 121  47   9 446 233  92  17  51  38   7   0 162  94  44  20 172 156  78  11  28  19   8   2\n",
      "  294 154  20   2 392 181  89  27 465 349 189  55   4   0   0   0  50  31  13   0   1   0   0   0  12   2   0   0  48  15   1   0  53  53  50  32\n",
      "    4   4   4   2  11   6   1   0   7   6   2   0  18   5   0   0  41  20   9   5  45  29  12   0   3   1   1   0  18   5   1   0  22   7   1   0\n",
      "  128  78  42   6  26  18   4   1 112  53  24   8  39  21   8   3  57  54  36  10  16   7   1   0  46  25   5   0  47  42  28  14   0   0   0   0\n",
      "  180 167   8   0  17   6   3   2 123  89  80  50 160  95  55  46  40  22   3   0 199 176  14   7  15  11   2   0  83  83   6   1   3   0   0   0\n",
      "   32  25  16   9 189 163 143  69  30  23   6   0]\n",
      " [181  57  13   2 111  28   1   0  12   5   1   0 139  87  26   1 438 273  93  18  91  48  26   2  95  76  47   3 105  84  58  27  31  20   6   1\n",
      "  349 172  50   0 323 168  62  16 528 465 303  91   0   0   0   0  52  30  11   0   3   0   0   0   6   5   0   0  44  13   3   0  36  31  29   1\n",
      "    0   0   0   0  17   2   0   0  20   1   0   0  19  15  10   0  47  23   7   0  20   5   0   0   0   0   0   0  36  33  21   1   3   1   1   0\n",
      "  166 109  43   3  29  17   5   0 154  90  53   9  75  72  65  16   6   4   4   3  71  48   8   0  55  43  11   0  65  40  28  23   6   0   0   0\n",
      "  136 125   1   0  21  15  13   2  67  27   5   1  33   1   1   0  34  23  10   0 637 460 106   7  12   4   0   0  54  53   1   0   0   0   0   0\n",
      "   37  30  25   9  61  60  48   9  63  53  31   0]\n",
      " [129  34   4   0 126  36   3   2 169 123  48   3 327 218  48   4 364 185  73  17  82  44  15   6  94  43  23  10  52  40  17   5  45  24   9   5\n",
      "  287 128  23   1 725 485 181  27 487 369 275 111   0   0   0   0  33  14   2   0   0   0   0   0  16   1   0   0  50   1   0   0  29  22  20   4\n",
      "    2   1   0   0   9   4   0   0   3   0   0   0   5   4   1   0  14   5   5   3  30  10   0   0   3   0   0   0  34  24   4   1  33  32  24   0\n",
      "   62  33  16   3  36  32  20  13  87  44  12   2  45  33  11   3  14   9   6   3 106  78  18   0  47  33  22   7  79  55  38  12  43   2   0   0\n",
      "   76  69   0   0  18  10   0   0 126  89  30   8  29  11   5   1  51  37  16   0 287 220  59   7   3   0   0   0  51  51   1   0   0   0   0   0\n",
      "   33  28  20  12 536 460 418 305  35  32  14   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (192,)- total: 110611 - sum(axis=0):\n",
      "[2161 2677 2853 2927 2052 2446 2574 2595  185  254  351  407  621 1111 1414 1558 1993 2934 3594 3887  257  467  683  810   70  293  446  532  107\n",
      "  233  403  520  110  176  245  276  640 1462 2036 2172 1934 2964 3786 4225  341 1049 1857 2775  685  693  693  693   71  179  254  295  846  854\n",
      "  857  857   92  127  135  136  118  400  455  464   51   78  103  188  155  159  166  168  343  391  411  412  169  198  211  214   57   82   95\n",
      "  134  296  363  412  425  307  403  465  475  122  134  134  135   31   67  116  141   57  102  160  219  528  778  978 1128   27   60  142  179\n",
      "  351  632  805  920  139  207  305  379   26   38   63  115  217  378  576  667   41  110  201  289   34  119  192  266   53   94   95   95   31\n",
      "  130  318  327   44   93  114  131  519  688  809  902  118  236  284  316   93  161  254  288  280  660 1276 1464   67   93  130  136   22   35\n",
      "   81   87   91  108  108  108   28   55   92  137   88  229  365  657   23   61  136  195] \n",
      "\n",
      "\n",
      " fold_neg: (5, 192) \n",
      " [[ 435  511  542  552  611  693  712  717   22   31   45   51  143  235  300  327  395  582  714  776   63   97  134  151   14   68  110  113   40\n",
      "    81  100  104   21   34   51   58  109  257  337  353  470  716  939 1089   66  307  497  661  108  108  108  108   19   46   51   59  163  165\n",
      "   165  165   22   24   24   25    6   84   96   96    5   11   22   39   17   19   23   23   60   81   90   90   26   30   35   36    6    9    9\n",
      "    27   78   82   87   87   53   86   99   99   28   28   28   28    3   11   18   20    8   19   46   79   99  134  185  222    0    3   16   36\n",
      "    62   95  115  127   19   40   78   81    5    6    6    7   17   35   74  122    5   11   27   49    0   19   41   54   17   20   20   20   12\n",
      "    43   68   68    7   16   18   23   40   68   80   87    0    0    0    0   25   41   55   57   17   56  142  184    1   11   33   35    2    4\n",
      "    13   16    7   19   19   19   17   22   29   35   37   59  109  159   11   27   35   40]\n",
      " [ 391  483  519  539  339  414  441  447   12   19   23   27  128  255  319  334  442  597  690  715   62  144  241  312   14   45   57   59   18\n",
      "    52   76   79   23   35   47   56  160  345  478  508  401  572  684  715   61  212  410  654  168  172  172  172   13   34   52   59  212  214\n",
      "   217  217   21   28   28   28   15   72   90   95   19   25   32   39   47   47   47   47   76   77   77   77   47   49   49   49    5    9    9\n",
      "    19   61   70   87   87   63   75   92   99   16   25   25   25    9   11   17   17   14   30   47   47   99  158  187  211    1    7   38   40\n",
      "    73  152  207  241   37   52   70   79    2    3    7   28   57  128  180  196    3   14   26   61    3   14   23   39    7   11   12   12    5\n",
      "    25   68   68    9   24   28   28   64   94  114  146    4    7    9   27   20   29   54   57   43   85  147  167   20   21   23   25   12   17\n",
      "    26   26   19   21   21   21    3    8   16   24    9   25   37   53    3    5   15   18]\n",
      " [ 459  587  622  651  267  331  359  367    2    2    2    2   80  160  210  248  360  594  737  812   43   56   87   94   12   80  130  154   19\n",
      "    35  113  180   30   39   50   56  124  269  403  421  330  545  637  699   68  191  352  486  175  179  179  179   10   29   47   60  140  141\n",
      "   141  141   16   26   28   28   36   63   77   78    1    1    4   22   30   30   30   32   77   82   87   88   32   33   37   39    9   22   27\n",
      "    27   46   67   78   82   54   70   87   90   25   27   27   28   12   25   29   29   23   38   44   45   55  105  141  177   14   22   36   39\n",
      "    59  118  147  163   39   57   70   75    0    3   21   47   50   60   66   67   14   37   57   62    8   13   27   41   11   11   11   11    3\n",
      "    22   60   68   11   22   25   26  140  174  183  213   66  131  171  180   17   35   54   57   45   86  155  161   11   15   24   26    0    3\n",
      "    21   22   15   18   18   18    4   11   20   27   12   38   58  132    4   11   28   34]\n",
      " [ 454  578  622  633  267  350  377  378   23   30   34   35  125  196  237  262  445  617  797  872   45   88  110  133   11   30   59  103    6\n",
      "    28   54   85   23   34   48   53  151  333  455  505  266  421  527  573   87  157  320  532  116  116  116  116    5   27   49   60  181  184\n",
      "   184  184   21   22   27   27   36   86   96   99   10   15   17   44   32   32   32   32   47   62   64   64   26   45   46   46   24   28   33\n",
      "    43   40   64   80   87   60   75   80   80   27   27   27   27    0    3   15   35    4    6    6    7  201  278  345  385   11   23   35   40\n",
      "    94  161  198  241    8   11   18   67   15   17   17   18   22   54   95  103    7   19   51   62   19   44   55   60   14   17   17   17    3\n",
      "    14   65   66    7   13   15   26  147  187  209  213   36   68   68   69   23   34   48   58   99  290  574  651   20   28   32   32    1    4\n",
      "    12   13   34   34   34   34    1    6   11   27    5    6   18   57    2   12   34   65]\n",
      " [ 422  518  548  552  568  658  685  686  126  172  247  292  145  265  348  387  351  544  656  712   44   82  111  120   19   70   90  103   24\n",
      "    37   60   72   13   34   49   53   96  258  363  385  467  710  999 1149   59  182  278  442  118  118  118  118   24   43   55   57  150  150\n",
      "   150  150   12   27   28   28   25   95   96   96   16   26   28   44   29   31   34   34   83   89   93   93   38   41   44   44   13   14   17\n",
      "    18   71   80   80   82   77   97  107  107   26   27   27   27    7   17   37   40    8    9   17   41   74  103  120  133    1    5   17   24\n",
      "    63  106  138  148   36   47   69   77    4    9   12   15   71  101  161  179   12   29   40   55    4   29   46   72    4   35   35   35    8\n",
      "    26   57   57   10   18   28   28  128  165  223  243   12   30   36   40    8   22   43   59   76  143  258  301   15   18   18   18    7    7\n",
      "     9   10   16   16   16   16    3    8   16   24   25  101  143  256    3    6   24   38]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (192,) sum: 111.0\n",
      " [1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "\n",
      "  Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 192  Y rows with populated labels: 16038  non zero cols: 36170\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_11_cols_620.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task5\n",
      " load_task_weights() - no weights file provided for y_task5, training_weights for all  620 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (620,)- total: 142158 - sum(axis=0):\n",
      "[ 441  321  180   66  561  309  162   86 1156  912  572  233  657  568  449  255  466  285  135   36   28    0    0    0  255  160   63   16  240\n",
      "  204  143   29  205  174  124   60  194   82   25   13  174   92   40   21   74   11    2    1  106   61   36   16  110   41   16    5  420  254\n",
      "  139   53  367  147   68   28  574  380  191   48  198  146   65   21  194  125   47   13  917  643  277   76  710  370  106   23  233  160   87\n",
      "   32  270  207  113   21  189  158  107   37  734  465  266  109  281  176  111   38  739  594  436  222  650  356  202   86  334  206   78    6\n",
      "  702  549  420  243 1628 1440 1059  671  344  256  131   24  262  179   78   15  329  272  168   61  773  527  269   77  140   63   15    2   34\n",
      "   21   12    2  187    4    0    0  196  116   69   21  260  113   34    7  487  423  341  197   97   14    0    0  716  554  280   49   87   49\n",
      "   27   14  729  555  291   98   40   25   13    0  681  520  236   33 1741 1500 1047  531   24   21   20   13  127   72   24    0   58    4    0\n",
      "    0  655  465  292   83  659  537  370  211  261  252  197    0 1386 1023  578  145 1162  909  459  193   96   78   45    6  230  122   20    3\n",
      "  321  295  255  133   12    2    0    0  150   31    7    0  853  452  229   58 1549  971  498  130  136   20    3    0  295  230  144   70  198\n",
      "  149   84   33  196  126   53   19  452  330  122   14 1131  878  581  252  187   66   26   10  676  518  228   53  632  366  108   31  219  137\n",
      "   77   26  219  194  139   82   96   80   43   18 1507 1100  503   86  287  244  167  107   72   14    4    1  219   95   24    5  253  160   75\n",
      "   13  321  269  176   80  126   27    5    0 1981 1689 1009  241  447  248  135   65 1388  913  357   64  543  390  202   43  288  249  185   92\n",
      "   95   36   13    3  272  193  109   13  584  431  260  105  151  137   94   30  302  212  108   18  165  131   90   47  415  321  172   64  104\n",
      "   95   82   54  341  273  135   29  640  577  190   16  252  202   61    7  516  429  276  136  280  184  130   76  418  291  158   56   63   57\n",
      "   43   11  631  343  140   34   73   48   23    3   92   58   16    0  490  395  295  173  364  198   70   24  331  229  125   37 1961 1683 1200\n",
      "  373  217  158  102   32  123   95   74   54  109   33    2    0  751  529  308  130  256  168   82    8  411  355  253  149  791  638  322  112\n",
      "  122   74   30    4  334  241  134   43   63   36    7    0  627  573  344   22  518  421  248   59  255  225  154   57   54   24    8    2  299\n",
      "  276  233   97   99   42   14    2   65    8    0    0  116  115  113   99  710  609  407  202  842  747  558  316  315  213  111   24   66   11\n",
      "    1    0   92   36    2    0  168  122   92   56   74   40   19    2  450  358  233  141  475  270   81   12  643  431  222   38  105   88   78\n",
      "   31  188  143   56    6  152  119   54    0  131   88   41    7  110   91   66   21  158  153  121   21  709  540  166   16  162  119   43    2\n",
      "   57   26    6    0  201  105   57   34  144  107   71   52  629  496  355  166  294  143   24    0  330  302  203  195  362  287  201  138    0\n",
      "    0    0    0   95   84   70   23  125   33    4    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 620) \n",
      " [[96 65 25 ...  5  0  0]\n",
      " [79 59 39 ...  2  0  0]\n",
      " [90 63 37 ...  7  0  0]\n",
      " [82 57 24 ... 11  0  0]\n",
      " [94 77 55 ...  8  4  0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (620,)- total: 193933 - sum(axis=0):\n",
      "[ 144  315  464  559  294  547  694  762  517  795 1141 1480  440  526  645  840  467  643  793  892  186  214  214  214   84  184  281  326   30\n",
      "   78  140  251   24   57  108  172   96  212  269  281   55  148  200  219   54  118  126  127   30   80  105  125   86  163  188  199  132  303\n",
      "  418  495  287  511  591  631  390  618  810  952   69  127  209  252   30   99  177  209  158  458  824 1025  315  685  949 1032   67  152  226\n",
      "  282   77  155  249  341  117  158  210  279  463  753  952 1109   75  184  249  321  635  795  953 1170  524  817  970 1086  145  287  415  487\n",
      "  170  331  466  601  125  327  714 1102   63  164  289  396  132  230  333  396   84  143  247  350  100  349  619  808   83  160  208  221  310\n",
      "  323  332  341  388  563  566  566  146  263  310  358   41  199  278  305  164  242  324  468  358  443  457  457  121  287  561  792   34   81\n",
      "  103  116  305  498  762  955   63   78   90  103  245  443  727  929  289  568 1023 1539   91   94   95  102  257  313  359  382   50  103  106\n",
      "  106  229  418  591  761   94  231  398  557   11   22   30   30  273  659 1104 1537  115  370  820 1086   13   33   66  105    8   60   80   97\n",
      "   10   50   90  212  110  120  122  122  219  329  352  359  602 1037 1260 1444  254  833 1306 1676   84  213  232  235  279  351  438  512  291\n",
      "  351  416  467   52  130  203  237  359  571  780  888  235  492  790 1120  123  244  284  300   77  244  546  720   59  157  237  282   89  204\n",
      "  264  315   28   56  111  168   11   27   67   92  413  820 1417 1831   39   94  170  230  139  188  198  201   79  211  293  312   33  273  357\n",
      "  419   27  227  320  416  213  312  334  339  359  732 1413 2181  239  441  554  626  498 1056 1612 1905  275  452  640  799   22   62  126  218\n",
      "   75  137  160  170   92  178  262  358  154  354  525  680   30   55   98  162  151  263  367  460   33   99  140  183   60  236  385  493   13\n",
      "   23   36   64  105  179  317  423   29  105  500  674   40   92  233  287   33  283  437  577   27  271  328  382   16  287  426  528   39   45\n",
      "   59   91  414  746  953 1059   45   71   96  116   87  129  171  187   95  195  294  416  207  374  502  548  107  217  322  410   60  354  838\n",
      " 1665   24   85  140  210   86  117  138  158   22  105  136  138  458  696  919 1097   34  123  209  283   25   84  187  291  125  314  631  841\n",
      "   37   86  130  156  123  228  336  427   63   90  119  126   91  147  376  698   90  166  341  530  103  136  212  309  104  138  154  160  100\n",
      "  124  167  303   43  104  132  142   60  117  125  125    0    1    3   17   42  182  384  589   44  146  374  616   70  175  277  364   51  106\n",
      "  116  117   81  137  171  173   59  104  136  172   33   67   88  105   79  174  299  391  238  446  635  703  181  393  603  787   16   33   43\n",
      "   90   75  130  216  266    2   35  100  154   38   81  128  162   10   29   57  102    4   11   64  164   51  269  558  705   10   64  140  180\n",
      "   56   88  108  114   83  198  246  269   12   49   85  104   25  179  319  508   27  178  297  321    4   53  152  160  179  254  340  403  204\n",
      "  204  204  204    6   17   31   78   36  128  157  161] \n",
      "\n",
      "\n",
      " fold_neg: (5, 620) \n",
      " [[ 65 117 158 ...  17  22  22]\n",
      " [ 28  52  73 ...  12  14  14]\n",
      " [ 24  55  81 ...  43  50  50]\n",
      " [ 10  47  80 ...  35  46  46]\n",
      " [ 17  44  72 ...  21  25  29]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (620,) sum: 389.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "  Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 620  Y rows with populated labels: 40992  non zero cols: 113901\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_643_cols_184.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task6\n",
      " load_task_weights() - no weights file provided for y_task6, training_weights for all  184 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (184,)- total: 41813 - sum(axis=0):\n",
      "[2552 2041 1193  453 1951 1139  390   72  248   84   13    2 1457  864  468  130 3934 2633 1093  412   48   10    1    0  172   92   42    3   60\n",
      "   40   23    8    9    3    0    0   29    0    0    0 1845 1436  767  151  858  667  181    2  582  468  274   48  678  508  269   46  164   59\n",
      "   18    1  482  444   35    0   77    5    0    0   77    0    0    0   56   10    1    0  270  103   17    2    9    4    2    1   50    2    0\n",
      "    0   68    3    0    0  189  143   86   46  211  130   38    4   95   68   19    4  122  103   55   22  239  134   61   16   70   23    0    0\n",
      "  231   91   12    1  120   93   63   10 1095  922  450  104  221  163   60    7  182    3    2    0   67   21    3    0  437  360  224   60  186\n",
      "  169   42    2   42   13    6    1  239  180   75    6   94   49   12    1   98   82   54   21  246  226    0    0   64   33    9    0   90   58\n",
      "   16    0   81   69   37   13  126   50   23   11] \n",
      "\n",
      "\n",
      " fold_pos: (5, 184) \n",
      " [[ 461  367  247  124  407  219   79   28   79   39    7    1  357  271  188   52  454  334  158   52    5    0    0    0   42   24   10    0   10\n",
      "     3    2    0    2    0    0    0    6    0    0    0  530  426  281   83  263  185   18    0  104   86   52    7  207  155   74    9   16    1\n",
      "     0    0   47   44    3    0   14    1    0    0   22    0    0    0   16    3    1    0   63   22    1    0    0    0    0    0   15    2    0\n",
      "     0   18    0    0    0   38   17    8    5   33   20    9    0   17   13    0    0   13   11    8    4   51   27   10    1   17    9    0    0\n",
      "    67   41    6    1    4    2    0    0  173  147   94   26    3    2    0    0  166    0    0    0   21    1    0    0  178  139   79   20   18\n",
      "    15   11    0   40   13    6    1   41   25    7    0    6    2    0    0   13    4    0    0    0    0    0    0   20    9    0    0   13    6\n",
      "     3    0   18   18    7    0    2    0    0    0]\n",
      " [ 290  202   97   19  212   86   14    0   52   16    2    1  208   94   43   18  535  265   81    7   18    8    1    0   17    2    1    0    3\n",
      "     1    1    0    2    1    0    0    3    0    0    0  362  271  131   10  380  313  116    0  135   91   34    8   58   39   27   13   29    2\n",
      "     0    0   51   44    6    0    2    0    0    0   14    0    0    0    0    0    0    0   51   10    0    0    2    1    1    0   24    0    0\n",
      "     0    7    0    0    0   45   38   31   25   39   20    2    1   22   16    5    0   29   24   11    5   43   26    8    1    8    1    0    0\n",
      "    49   12    0    0    4    0    0    0  234  178   66    8   16    8    4    2    2    1    1    0   14    7    2    0   36   26   17    3   12\n",
      "     3    1    1    1    0    0    0   62   50   19    1   30   20    6    0    4    3    1    1   59   45    0    0    5    2    0    0   37   24\n",
      "    11    0   11   10    7    4   24    3    3    3]\n",
      " [ 618  513  352  115  371  241  126   25   41   12    0    0  249  129   64   15  846  533  189   68   11    0    0    0   52   48   23    2   16\n",
      "    13    9    3    4    1    0    0    3    0    0    0  263  208  100    5   16   14    9    2  161  147  119   20  203  166  116   10   34    7\n",
      "     0    0   45   40    8    0    6    2    0    0   14    0    0    0    6    1    0    0   49   14    4    1    3    2    0    0    1    0    0\n",
      "     0    6    1    0    0   36   28   19   10   34   24    6    1   18   11    5    1   10    8    6    4   35    5    0    0   13    3    0    0\n",
      "    35   22    5    0   17   10    1    0  104   80   28    7  129   96   34    5    2    0    0    0    0    0    0    0   56   48   33   14    1\n",
      "     1    1    0    0    0    0    0   61   48   27    5   19   11    4    1   21   19   18   10    0    0    0    0    2    0    0    0   22   12\n",
      "     1    0   19   16   13    8   29    0    0    0]\n",
      " [ 609  489  198   65  510  301   63   16   41   12    4    0  437  278  133   37 1350  973  395  157    6    1    0    0   41    9    7    1   18\n",
      "    15    5    1    1    1    0    0    9    0    0    0  352  267  102   24  122  108   24    0  108   80   39    2  125   78   18    2   56   40\n",
      "    16    1  291  273   14    0   46    2    0    0   12    0    0    0    1    0    0    0   46   34    8    0    0    0    0    0    9    0    0\n",
      "     0   18    1    0    0   28   22    3    2   62   35   11    0   25   18    8    3   64   54   26    7   54   30    9    2   13    2    0    0\n",
      "    27    4    1    0    2    1    0    0  464  414  229   62   44   36   16    0   10    1    0    0   20   10    0    0   83   66   39   14  148\n",
      "   143   26    1    1    0    0    0   43   38   15    0   24   11    0    0   12   12    5    1  176  170    0    0   33   22    9    0    3    3\n",
      "     0    0   14    9    4    1   44   30    8    1]\n",
      " [ 574  470  299  130  451  292  108    3   35    5    0    0  206   92   40    8  749  528  270  128    8    1    0    0   20    9    1    0   13\n",
      "     8    6    4    0    0    0    0    8    0    0    0  338  264  153   29   77   47   14    0   74   64   30   11   85   70   34   12   29    9\n",
      "     2    0   48   43    4    0    9    0    0    0   15    0    0    0   33    6    0    0   61   23    4    1    4    1    1    1    1    0    0\n",
      "     0   19    1    0    0   42   38   25    4   43   31   10    2   13   10    1    0    6    6    4    2   56   46   34   12   19    8    0    0\n",
      "    53   12    0    0   93   80   62   10  120  103   33    1   29   21    6    0    2    1    1    0   12    3    1    0   84   81   56    9    7\n",
      "     7    3    0    0    0    0    0   32   19    7    0   15    5    2    0   48   44   30    9   11   11    0    0    4    0    0    0   15   13\n",
      "     1    0   19   16    6    0   27   17   12    7]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (184,)- total: 69820 - sum(axis=0):\n",
      "[ 445 1016 1698 2409  815 1655 2266 2582  415  579  650  661  572 1337 1708 2045 1052 2557 4098 4775   93  131  140  141  136  217  267  306   60\n",
      "   81   98  113  110  116  119  119  311  340  340  340  443  874 1572 2177  198  386  865 1044   64  180  367  592  102  273  512  735   76  182\n",
      "  223  239   12   48  457  492  410  482  487  487  118  195  195  195   66  112  121  122  193  360  446  461   99  104  106  107   99  147  149\n",
      "  149   60  125  128  128  117  163  219  266   74  171  263  296   20   47   96  111   14   33   81  114  156  263  336  381  127  175  198  198\n",
      "  254  404  480  491   56   83  113  166  123  309  734 1044   48  106  209  262  125  190  191  193   52   98  116  119   35  112  248  412    9\n",
      "   26  153  193   54   83   90  102  151  231  336  405   32   77  114  125    4   20   48   81    0   20   56   56   50   81  105  114   33   65\n",
      "  107  123   25   37   69   93  139  226  253  265] \n",
      "\n",
      "\n",
      " fold_neg: (5, 184) \n",
      " [[ 109  215  330  449  199  376  483  534   72  112  144  150  105  209  292  428  175  307  485  590   24   29   29   29   20   38   52   62   12\n",
      "    19   20   22   23   25   25   25   62   68   68   68   98  204  344  532   51  129  296  314   14   32   66  110   26   78  159  224   22   37\n",
      "    38   38    2    5   46   49   33   46   47   47   18   40   40   40    6   19   21   22   29   70   91   92    5    5    5    5   16   29   31\n",
      "    31   17   35   35   35   24   45   54   57   14   36   47   56    6   10   23   23    6    8   11   15   28   53   70   79   22   30   39   39\n",
      "    31   57   92   97   12   14   16   16   41   67  106  152   13   14   16   16  101  160  160  160    6   26   27   27   17   56  116  175    0\n",
      "     3    7   18   17   44   51   63   33   58   76   83   10   14   16   16    0    9   13   13    0    0    0    0    5   16   25   25   11   18\n",
      "    21   24    4    4   15   22   48   56   56   56]\n",
      " [  91  177  282  338  113  238  310  323   88  124  138  139  124  243  294  319  249  524  708  781   11   21   28   29   48   63   64   65   22\n",
      "    24   24   25   17   18   19   19   66   69   69   69   59  165  311  431   62  129  326  442   25   70  127  153   11   30   42   56   16   43\n",
      "    45   45    0    7   45   51   49   51   51   51   24   38   38   38   20   20   20   20   42   83   93   93   43   44   44   45   44   68   68\n",
      "    68    8   15   15   15   16   23   25   40   18   38   56   57    4   10   21   26    0    5   18   24   36   53   71   78   31   39   40   40\n",
      "    50   83   92   92   14   18   18   18   27   95  207  264   10   18   22   24    2    3    3    4    8   15   20   22    8   18   27   41    1\n",
      "    10   12   12   11   12   12   12   16   31   62   80    8   18   32   38    0    1    3    3    0   14   22   22   17   20   22   22    5   18\n",
      "    31   42    7    8   11   14   32   53   53   53]\n",
      " [  56  193  346  583  125  256  371  472   99  128  140  140   91  257  324  373  159  487  830  951   14   25   25   25    7   12   37   58    9\n",
      "    13   17   23   15   18   19   19   65   68   68   68  120  180  306  401   11   13   18   25    7   22   50  149   26   64  114  220   11   39\n",
      "    46   46    2    7   39   47   42   46   48   48   25   39   39   39   12   17   18   18   44   79   89   92   16   17   19   19   13   14   14\n",
      "    14   20   25   26   26   30   38   42   49   23   33   51   55    5   12   18   22    0    2    4    6   44   74   79   79   26   36   39   39\n",
      "    63   78   95  100   17   24   33   34   19   48  100  121   10   43  105  134    2    2    2    2   21   21   21   21    3   11   26   45    0\n",
      "     0    0    1    0    0    0    0   18   36   57   79    5   13   20   23    0    2    3   11    0    0    0    0    9   11   11   11    8   18\n",
      "    29   30    3    6    9   14   25   54   54   54]\n",
      " [ 114  241  393  526  233  474  598  645   73  102  110  114  132  371  490  586  264  774 1349 1586   23   28   29   29   21   53   55   61    6\n",
      "     9   19   23   17   17   18   18   59   68   68   68  105  190  355  433   48   62  146  170    8   36   70  107   26   73  133  149   10   26\n",
      "    50   64    4   22  281  295  247  291  293  293   27   39   39   39   23   24   24   24   47   59   85   93   14   14   14   14   12   21   21\n",
      "    21    8   25   26   26   35   41   60   61    8   35   59   70    3   10   20   25    6   16   44   63   25   50   71   78   27   38   40   40\n",
      "    66  101  104  105   11   12   13   13   32   82  261  415   10   18   38   54   12   16   17   17   13   23   33   33    7   24   51   76    8\n",
      "    13  130  155   26   27   27   27   38   44   67   82    5   18   29   29    1    1    8   12    0    6   34   34    4   15   28   37    2    2\n",
      "     5    5    8   13   18   21    7   26   48   55]\n",
      " [  75  190  347  513  145  311  504  608   83  113  118  118  120  257  308  339  205  465  726  867   21   28   29   29   40   51   59   60   11\n",
      "    16   18   20   38   38   38   38   59   67   67   67   61  135  256  380   26   53   79   93   10   20   54   73   13   28   64   86   17   37\n",
      "    44   46    4    7   46   50   39   48   48   48   24   39   39   39    5   32   38   38   31   69   88   91   21   24   24   24   14   15   15\n",
      "    15    7   25   26   26   12   16   38   59   11   29   50   58    2    5   14   15    2    2    4    6   23   33   45   67   21   32   40   40\n",
      "    44   85   97   97    2   15   33   85    4   17   60   92    5   13   28   34    8    9    9   10    4   13   15   16    0    3   28   75    0\n",
      "     0    4    7    0    0    0    0   46   62   74   81    4   14   17   19    3    7   21   42    0    0    0    0   15   19   19   19    7    9\n",
      "    21   22    3    6   16   22   27   37   42   47]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (184,) sum: 92.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "\n",
      "  Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      "  Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 184  Y rows with populated labels: 12337  non zero cols: 33237\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_836_cols_224.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task7\n",
      " load_task_weights() - no weights file provided for y_task7, training_weights for all  224 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      " All y values are 0, 1, or -1.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (224,)- total: 38227 - sum(axis=0):\n",
      "[1848 1158  386   47  323  134   40    5   84   60   27    4  139   88   31    9   44   24   17    0   25   15    0    0  104   33   11    0  171\n",
      "   61   11    1    1    0    0    0  114   39    5    2  290  197   50    3  295  211   64    2  279  154   78   17   22   10    2    0   61   33\n",
      "    7    0  247  116   48   19   81   27    3    0  139   63   15    2   84   45    6    0  121   46   10    0   52   24   11    0  122   53   34\n",
      "   13 2609 1928 1074  277  108   53   18    2 3875 2588 1082  228   96   61   17    3 1142  725  271   89 1131  616  211   48  101   50   21    3\n",
      " 1257  507   78   10  189  103   45   14  484  307  124   26  224  164   79   19   58   46   33   21    5    1    1    1   53   27   10    0   66\n",
      "   17    3    0  560  340  124   39   18    2    0    0   75   46    1    0    0    0    0    0  381  332   57    2   64   34    0    0   46    3\n",
      "    1    0    4    1    1    0  121   84   50    1   41    6    1    0  106   76   14    0  170  114    8    0  462  256   20    1  433  398   56\n",
      "    9  460  460  308   11  443  379  259   67    0    0    0    0   42   25    0    0  311  288    0    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 224) \n",
      " [[252 186  50 ...  35   0   0]\n",
      " [380 263 113 ...  59   0   0]\n",
      " [498 313 105 ...  63   0   0]\n",
      " [299 206  44 ...  78   0   0]\n",
      " [419 190  74 ...  53   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (224,)- total: 91904 - sum(axis=0):\n",
      "[1057 1753 2514 2840  649  839  933  968   39   63   96  119   73  124  181  205  147  166  173  190  188  198  213  213  201  275  297  308  600\n",
      "  715  765  775  122  124  124  124  271  346  380  383  118  212  359  406   77  161  308  369  145  274  350  411   93  105  113  115  122  150\n",
      "  176  183  250  381  449  478  120  174  198  201   75  150  198  212   39   80  119  125   63  138  174  184  159  188  201  212  170  244  264\n",
      "  285  905 1617 2472 3267  242  303  339  355 1379 2708 4216 5053  171  214  259  273  505  968 1421 1603  551 1119 1532 1693  241  297  327  344\n",
      "  733 1510 1943 2009  153  273  332  363  375  561  748  845  136  196  281  341   58   68   80   91  100  104  104  104   62   90  107  117   90\n",
      "  139  153  156  585  817 1023 1107  106  122  124  124   41   70  115  116  120  120  120  120  187  235  338  385   49   79  104  104  188  234\n",
      "  239  240  233  240  240  241   45   83  117  165  138  176  181  182   56   84  144  158  135  190  274  282  218  422  575  591   21   55  186\n",
      "  222    8    8  160  457   87  155  278  453  129  129  129  129   65   83   92   92   11   38  105  105] \n",
      "\n",
      "\n",
      " fold_neg: (5, 224) \n",
      " [[214 282 417 ...  16  24  24]\n",
      " [205 322 472 ...   5  23  23]\n",
      " [203 390 598 ...   4  16  16]\n",
      " [161 257 417 ...   5  16  16]\n",
      " [274 502 610 ...   8  26  26]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (224,) sum: 109.0\n",
      " [1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "  Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      "  Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 224  Y rows with populated labels: 9469  non zero cols: 30630\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      "filename : chembl_29_Y_tg_1005_cols_148.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task8\n",
      " load_task_weights() - no weights file provided for y_task8, training_weights for all  148 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (148,)- total: 45065 - sum(axis=0):\n",
      "[2742  763  271   77   62   18    4    2  389  117   13    2 2345 1740  714  104   43   19    4    0  385  262  166  127  486  391  215   56  959\n",
      "  694  254   23  374  313  175   35  314  209  116   56  206   47    0    0  393  222   90    9  140   45    7    1 5011 2970 1201  199  583  305\n",
      "   14    0   39   24   10    0   69   46   35   16  692  542  349   92   86   51   20    0   81   33   19    5   78   48   15    2  588  466  110\n",
      "    1   89   29   16    7  362  230   75    0   58   20    9    0  328  276  222  152  129  128  127  123  501  262   31    4  172   78   35    8\n",
      "  233  169  115   29 1495 1118  452   24  288   60   21    8 2824 2211 1175  234  444  238   86   20   43   14    2    0   88   66   32    2  103\n",
      "    1    0    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 148) \n",
      " [[ 472  146   79   26   10    5    3    1   31    4    1    0  570  413  178   17   10    4    2    0   20    4    0    0   11    4    0    0  244\n",
      "   202   94    6   40   32   13    1   26    9    1    0    1    0    0    0   77   27   15    2    6    3    1    0  629  335   90    7   41   12\n",
      "     0    0    8    5    3    0   19    7    5    4   79   30    2    0   18    8    0    0   14    1    1    0   35   27    9    1   71   53    7\n",
      "     0   18    4    1    1   67   49   12    0    5    0    0    0   60   48   41   20    0    0    0    0   25   16    9    1   31   25   17    4\n",
      "    50   46   31    0  198  143   57    6   54    1    0    0  570  442  240   50   83   43   13    0    4    3    1    0    0    0    0    0    6\n",
      "     0    0    0]\n",
      " [ 643  177   37    6   25    6    0    0  102   40    6    1  321  213   68   20    5    1    0    0   18    7    0    0   89   55    8    3  159\n",
      "   101   21    0   67   47   19    1   68   62   39   19   18    7    0    0   73   27    3    1  133   42    6    1 1736 1068  451   76  158   73\n",
      "     4    0    7    7    3    0    9    6    0    0   30    7    2    2   17   11    6    0    9    3    3    0    8    1    0    0  221  173   48\n",
      "     1   21    8    5    2   74   48   10    0    0    0    0    0   46   32   16    1    1    0    0    0   27   16    1    0   28    1    0    0\n",
      "    41   25   18    6  315  220   83    1   62    9    2    0  505  367  189   33  102   37   14    3    2    0    0    0    8    0    0    0   29\n",
      "     0    0    0]\n",
      " [ 477  112   44   10   15    3    0    0   97   47    4    1  611  482  186   34   19    9    1    0  253  196  143  125  119   95   62    3  130\n",
      "    84   14    0   54   37   11    0   67   24    2    0    9    0    0    0   73   45   13    0    0    0    0    0  920  675  359   63   98   62\n",
      "     3    0    8    7    1    0   19   14   14    1   93   77   44   14   18   12    5    0   20    6    1    1   11    6    0    0   47   37   23\n",
      "     0   15    8    7    3   81   40   14    0   34    4    0    0   74   62   62   58  127  127  127  123    1    0    0    0   41   16    3    0\n",
      "    37   22   17    8  250  198   82    3   65   19    2    0  426  359  205   31   84   50   18    6   14    6    0    0    0    0    0    0    3\n",
      "     0    0    0]\n",
      " [ 749  218   67   20    3    0    0    0   68   11    0    0  487  351  145   10    3    2    0    0   53   42   23    2  254  228  142   50  167\n",
      "   107   43   14   70   62   38    6   11   10    3    2   13    3    0    0   79   40    7    0    1    0    0    0  871  450  158   37   97   62\n",
      "     3    0   15    5    3    0    1    1    0    0  298  253  144   47   20   13    6    0   21   10    5    1    3    0    0    0  182  151    6\n",
      "     0   15    4    3    1   62   42   15    0    6    5    4    0   71   57   53   36    1    1    0    0  275  154   16    3   47   18    6    2\n",
      "    49   35   20    2  403  291  106    4   43    8    0    0  731  596  312   64   77   50   15    1    1    0    0    0   80   66   32    2   49\n",
      "     1    0    0]\n",
      " [ 401  110   44   15    9    4    1    1   91   15    2    0  356  281  137   23    6    3    1    0   41   13    0    0   13    9    3    0  259\n",
      "   200   82    3  143  135   94   27  142  104   71   35  165   37    0    0   91   83   52    6    0    0    0    0  855  442  143   16  189   96\n",
      "     4    0    1    0    0    0   21   18   16   11  192  175  157   29   13    7    3    0   17   13    9    3   21   14    6    1   67   52   26\n",
      "     0   20    5    0    0   78   51   24    0   13   11    5    0   77   77   50   37    0    0    0    0  173   76    5    0   25   18    9    2\n",
      "    56   41   29   13  329  266  124   10   64   23   17    8  592  447  229   56   98   58   26   10   22    5    1    0    0    0    0    0   16\n",
      "     0    0    0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (148,)- total: 104361 - sum(axis=0):\n",
      "[6762 8795 9264 9481  209  253  267  269 1517 1792 1896 1907  666 1292 2280 2889   56   81   96  100  121  242  338  382  253  362  529  682  390\n",
      "  657 1097 1328  161  224  362  502  105  243  336  396  166  351  392  392  163  332  463  544  145  244  282  287 1165 3232 5002 6001  196  493\n",
      "  785  799  108  126  140  148   31   54   65   97   87  241  433  681   19   54   85  105  109  158  172  186  114  143  175  188   22  149  505\n",
      "  614   68  129  142  150   92  224  379  454   69  107  117  126  135  203  276  345   18   19   21   40   32  277  508  535   58  167  210  238\n",
      "   47  111  165  251   80  458 1124 1552   95  323  362  375  221  869 1903 2841  113  320  472  538   64   93  105  107   32   54   88  118  252\n",
      "  354  355  355] \n",
      "\n",
      "\n",
      " fold_neg: (5, 148) \n",
      " [[1171 1507 1575 1630   49   54   56   58  253  281  284  285  128  287  522  683   11   18   20   22   29   44   48   48   36   43   47   47   38\n",
      "    80  188  276   26   35   54   66   23   45   53   54   13   17   17   17   38   86   98  111    5    9   11   12  191  486  731  814    9   39\n",
      "    51   51   24   29   31   33   18   30   32   61   25   74  102  104    4   14   22   22   24   37   37   38    4   12   30   38    0   18   64\n",
      "    71   16   31   34   34   12   30   67   79   20   25   25   25   36   51   58   79    3    3    3    3    3   13   20   28   17   23   31   44\n",
      "     6   10   25   56   16   72  158  209   23   76   77   77   47  178  380  570   24   64   94  107   17   18   20   21    0    0    0    0   58\n",
      "    64   64   64]\n",
      " [1481 1947 2086 2121   33   52   58   58  512  576  610  615  129  240  384  432   11   15   16   16   18   28   35   35   42   76  115  120  108\n",
      "   166  246  267   28   49   77   95   25   32   55   75   13   17   18   18   38   84  108  110  111  205  241  245  367 1040 1658 2030   88  179\n",
      "   248  252   24   24   28   30   10   13   19   19   26   49   54   54    4   10   15   21   25   31   31   34   30   37   38   38    4   54  179\n",
      "   226   11   24   27   29   19   45   83   93    3    3    3    3   48   65   81   95    0    1    1    1    6   22   37   38   19   48   49   50\n",
      "    11   27   34   46   13  108  245  327   15   68   75   77   43  176  354  510   38  103  126  137   18   20   20   20   16   24   24   24   30\n",
      "    59   59   59]\n",
      " [1287 1646 1713 1760   40   52   55   55  180  229  272  275  123  260  519  670    9   19   27   28   58  115  168  191   75   99  131  189   69\n",
      "   117  187  201   21   38   64   75   15   58   80   82   68   77   77   77   33   61   93  106   20   20   20   20  144  403  719 1015   39   75\n",
      "   134  137   22   23   29   30    1    6    6    6   12   28   60   90    3    9   16   21   19   33   38   38   24   29   34   34   13   23   37\n",
      "    60   12   19   20   24   12   53   79   93   30   60   63   63   20   38   38   42   14   14   15   34    2    3    3    3    4   35   48   51\n",
      "    16   31   36   45    9   61  177  256   10   56   73   75   32  103  257  430   28   63   95  107    5   13   19   19    0    0    0    0   11\n",
      "    14   14   14]\n",
      " [1497 2033 2164 2212   41   44   44   44  293  351  362  362   96  233  439  574    8    9   11   11    6   17   36   57   52   83  169  256   86\n",
      "   146  210  239   15   23   47   79    5    6   13   14   12   22   25   25   34   73  106  113    5    6    6    6  229  651  943 1064   25   60\n",
      "   119  122    9   20   22   25    1    1    2    2   16   65  174  262    1    8   15   21   18   30   35   39   36   38   38   38    3   34  179\n",
      "   185   17   28   29   31   37   57   84   99   12   13   14   18   28   44   48   65    1    1    2    2   13  134  272  285   12   46   58   62\n",
      "    10   24   39   57   30  142  327  429   34   69   77   77   49  214  496  742   10   37   72   86   11   12   12   12    0   14   48   78  127\n",
      "   175  176  176]\n",
      " [1326 1662 1726 1758   46   51   54   54  279  355  368  370  190  272  416  530   17   20   22   23   10   38   51   51   48   61   67   70   89\n",
      "   148  266  345   71   79  120  187   37  102  135  171   60  218  255  255   20   28   58  104    4    4    4    4  234  652  951 1078   35  140\n",
      "   233  237   29   30   30   30    1    4    6    9    8   25   43  171    7   13   17   20   23   27   31   37   20   27   35   40    2   20   46\n",
      "    72   12   27   32   32   12   39   66   90    4    6   12   17    3    5   51   64    0    0    0    0    8  105  176  181    6   15   24   31\n",
      "     4   19   31   47   12   75  217  331   13   54   60   69   50  198  416  589   13   53   85  101   13   30   34   35   16   16   16   16   26\n",
      "    42   42   42]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (148,) sum: 80.0\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0.]\n",
      "\n",
      "  Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 148  Y rows with populated labels: 18026  non zero cols: 38238\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      "filename : chembl_29_Y_tg_1028_cols_344.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task9\n",
      " load_task_weights() - no weights file provided for y_task9, training_weights for all  344 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (344,)- total: 110249 - sum(axis=0):\n",
      "[ 439  342  217   81 1277  760  396  108 1700 1397  956  408   99    1    0    0  157    0    0    0  121    1    0    0  832  585  298   70  579\n",
      "  434  263   71 5382 3822 2049  506  845  505  248   65  461  234   61    9 1471 1242  944  560   67   17    9    0   44    5    0    0 2099 1536\n",
      "  980  410 1221  990  601  256  993  701  406   76 1539 1189  814  323  445  351  199   45  356  197   97   21   13    6    0    0 1519 1210  804\n",
      "  417   30    1    0    0  603  491  223   15  740  318  111   22 3153 2584 1456  421  528  369  177   35  206  128   75   20 2107 1835 1354  677\n",
      "  269  207  123   37  170   88   44    5 1213  960  516  204  250  106   58   20  328  117   48    8  328  248  159   69 1369  808  297   76  238\n",
      "  159   50    7  101   65   32   18  374  143   68   38  316   99   30    1  103   50   16    3  222   78   21    5  348  228  161   88  359  269\n",
      "  141   43  587  491  298  148  216  128   40   19  162   80   50   12   73   37   14    4  321  205   99   22  149   82   28    0   84   27    8\n",
      "    2 2004 1500  940  344   69   56    6    3   62   33    5    1  118   85   42   10  555  280   51    2  198   66   36   30  325  231  143   76\n",
      "   44   15    2    1   31    9    0    0  119   72   35    9  146  102   23    1  303  228  142   46   26    4    0    0  302  206   90    2   82\n",
      "   35   11    2 1017  744  314   22  694  633  456   85 2654 2272 1703  991   73   27    8    1   69   49   24    6   50   36   17    6  134  116\n",
      "   54    0  201  122   79   31  623  329  128   21  183  128   79   38   35   12    0    0   40    7    0    0  338  293  198   83  151  109   64\n",
      "   16   30   24   19    3  242  200    0    0  106   83   33    6  685  614   91   15   50   25   13    1   47   23    1    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 344) \n",
      " [[ 43  26  10 ...   0   0   0]\n",
      " [ 96  68  37 ...   0   0   0]\n",
      " [ 99  83  52 ...   1   0   0]\n",
      " [ 91  77  59 ...   0   0   0]\n",
      " [110  88  59 ...  22   1   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (344,)- total: 213195 - sum(axis=0):\n",
      "[ 233  347  472  608 1637 2198 2564 2853  285  618 1063 1603  889  986  987  987  831  986  986  986  867  983  984  984 1273 1521 1808 2036  227\n",
      "  384  555  733 1681 3177 4949 6489  777 1164 1420 1601  769 1039 1212 1261  507  739 1033 1416  255  305  313  322  401  440  444  444 1491 2108\n",
      " 2663 3226  268  531  921 1265  549  855 1151 1482  743 1121 1500 1992  147  255  407  561  447  609  710  786  706  713  719  719  219  573  983\n",
      " 1371  154  183  184  184  137  249  512  717 1442 1867 2074 2160  929 1542 2679 3713  165  324  516  658  286  364  420  475  373  650 1137 1795\n",
      "  193  257  341  427  299  386  432  471  676  954 1399 1707  208  361  407  445  155  367  436  477  114  217  305  395 1324 1940 2446 2674  102\n",
      "  196  306  349   81  116  147  161 1266 1494 1569 1599 1307 1518 1591 1620  360  413  447  460  951 1095 1152 1168  277  398  465  538  155  246\n",
      "  374  479  241  339  532  679  111  199  288  309  239  321  351  404  174  213  236  259  105  224  330  407  197  267  321  349  195  254  273\n",
      "  279  597 1124 1686 2282   49   62  112  115   44   73  101  105   54   88  131  163  332  607  836  885  346  512  544  553  235  335  425  493\n",
      "  210  260  275  276  281  313  322  322   92  139  176  202   46   95  174  196  114  189  275  371  128  152  158  158   94  223  339  427   77\n",
      "  124  148  157  134  407  837 1129   38  100  278  650  368  763 1338 2049  175  221  240  247   54   76  101  116   81   95  114  124   47   67\n",
      "  129  183   84  174  217  266  265  559  760  868   52  116  165  206   78  101  113  113  130  164  171  171   59  104  199  314   52   94  139\n",
      "  187  188  194  199  215    4   46  163  163   25   48   98  125   73  132  268  341   61   86   98  110   84  108  130  131] \n",
      "\n",
      "\n",
      " fold_neg: (5, 344) \n",
      " [[ 73  94 110 ...   8   8   8]\n",
      " [ 47  84 115 ...   1   1   1]\n",
      " [ 39  57  88 ...  19  20  20]\n",
      " [ 30  45  63 ...   6   6   6]\n",
      " [ 44  67  96 ...  74  95  96]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (344,) sum: 226.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "  Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      "  Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 344  Y rows with populated labels: 28144  non zero cols: 85070\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      "filename : chembl_29_Y_tg_1031_cols_72.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task10\n",
      " load_task_weights() - no weights file provided for y_task10, training_weights for all  72 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (72,)- total: 18631 - sum(axis=0):\n",
      "[ 595  244   46    2 1032  295   50    8  766  176   35    8 1925  634  129   42 1259 1057  656  134 1214  677  264  101  769  592  260   33  387\n",
      "   98   15    0   99   20    2    1   21    9    0    0  240  148   59    4  264  211  102   31  100   25    2    0  793  629  380   25  140   90\n",
      "   46    9  830  492  192   27   81   31    6    1   16    2    0    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 72) \n",
      " [[120  33   6   0 188  66   9   1 126  28   7   1 346  92  18   5 231 195 111  36 283 131  40  19 214 158  80  16  86  29   3   0  18   1   0   0\n",
      "    3   0   0   0  32  12   5   0  40  31  15   4  16   1   1   0  77  50  32   4  41  38  22   4 208 152  58   3  15   2   0   0   0   0   0   0]\n",
      " [139  52  11   1 225  55   9   2 166  30   8   1 337 126  25   7 432 387 254  33 335 193  92  51 123  91  27   3  67  19   5   0  27   5   1   0\n",
      "    9   5   0   0  51  26  10   1  53  39  10   4  27   7   1   0 252 207 125  13  20   4   4   0  96  39  15   8   7   4   2   1  10   2   0   0]\n",
      " [103  48   9   1 205  65  14   3 126  32   7   3 443 126  17   4 160 129  73  11 136  65  20   6 220 191  73   7  68  16   4   0  13   0   0   0\n",
      "    3   1   0   0  50  35   8   0  53  35  15   5  20   6   0   0 150 134  88   2  23  14   4   3 162  92  37   2  31  16   4   0   1   0   0   0]\n",
      " [102  49  10   0 171  41   8   1 162  41   3   0 433 180  24   3 208 174 107  37 289 185  67  16  52  18  10   1  55  11   2   0  27  11   1   1\n",
      "    1   0   0   0  51  27  13   2  55  48  19   9  13   2   0   0 101  62  35   5  28  24  12   1 170  97  45  13   7   1   0   0   1   0   0   0]\n",
      " [131  62  10   0 243  68  10   1 186  45  10   3 366 110  45  23 228 172 111  17 171 103  45   9 160 134  70   6 111  23   1   0  14   3   0   0\n",
      "    5   3   0   0  56  48  23   1  63  58  43   9  24   9   0   0 213 176 100   1  28  10   4   1 194 112  37   1  21   8   0   0   4   0   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (72,)- total: 107922 - sum(axis=0):\n",
      "[2497 2888 3055 3105 4162 4871 5081 5133 3600 4213 4344 4373 5694 6972 7433 7533   58  298  699 1221  803 1371 1790 1953  173  366  703  927 2314\n",
      " 2615 2685 2701  471  553  571  572  326  342  351  351   78  175  264  319   56  110  219  290  237  310  333  335  149  322  582  771   48  104\n",
      "  148  186  229  584  881 1046   47   98  123  128  134  148  150  150] \n",
      "\n",
      "\n",
      " fold_neg: (5, 72) \n",
      " [[ 592  681  702  707  878  981 1017 1024  725  830  851  856 1079 1337 1393 1406   15   58  142  217  194  360  451  472   58  115  198  262  495\n",
      "   553  572  575   98  115  116  116   65   68   68   68   32   52   59   64   24   33   49   60   51   66   66   67   27   56   74   97    0    3\n",
      "    19   37   43  105  199  254    9   22   24   24   30   30   30   30]\n",
      " [ 419  511  544  558  774  944  986  995  630  772  790  797 1076 1289 1385 1410   12   68  201  422  182  327  430  471   13   52  116  140  384\n",
      "   439  453  460   88  112  116  117   57   64   69   69   13   39   55   64   10   24   53   59   41   60   66   67   42   90  176  226   16   32\n",
      "    32   36   73  136  160  167    6    9   11   12   22   30   32   32]\n",
      " [ 481  541  575  586  922 1063 1109 1123  703  798  823  829 1306 1609 1713 1731   13   48  104  166  144  216  261  275   28   58  176  239  414\n",
      "   468  478  481   97  110  111  111   70   72   73   73   14   29   56   64   12   30   50   60   48   61   67   67   13   30   81  135   14   24\n",
      "    34   35   43  113  168  203   18   33   45   49   30   31   31   31]\n",
      " [ 496  571  610  621  789  919  950  958  721  851  887  890  999 1256 1398 1421    9   46  113  183  162  279  401  452   48   83   91  100  468\n",
      "   518  528  531   88  105  114  114   68   70   70   70   11   39   53   64    9   17   46   56   54   65   67   67   40   81  110  132    8   17\n",
      "    29   41   37  114  166  198   10   16   17   17   26   27   27   27]\n",
      " [ 509  584  624  633  799  964 1019 1033  821  962  993 1001 1234 1481 1544 1565    9   78  139  233  121  189  247  283   26   58  122  186  553\n",
      "   637  654  654  100  111  114  114   66   68   71   71    8   16   41   63    1    6   21   55   43   58   67   67   27   65  141  181   10   28\n",
      "    34   37   33  116  188  224    4   18   26   26   26   30   30   30]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (72,) sum: 52.0\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "  Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      "  Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  340803     # Features per Sample: 32000 \n",
      "Y file : # Samples :  340803     # Labels per Sample  : 72  Y rows with populated labels: 6391  non zero cols: 15068\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "--------------------------------------------------\n",
      "Chembl_23 Create complete\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      " 2022-08-11 14:35:40:923864  Create new  Chembl_23 instance \n",
      "---------------------------------------------------------------- \n",
      "\n",
      " verbose        : True\n",
      "\n",
      " FOLDS param provided - folds: [0] \n",
      "\n",
      " Index shape: (423736,) # of entries 82933 \n",
      " [False False False ... False False False]\n",
      " X (ecfp[0]) file count non zero (post fold & transform) :79 \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_0_cols_472.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task1\n",
      " load_task_weights() - no weights file provided for y_task1, training_weights for all  472 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (472,)- total: 81937 - sum(axis=0):\n",
      "[ 570  264   58   11   45    4    0    0   13    1    0    0    6    2    0    0    2    0    0    0   20    4    0    0   80    9    0    0   12\n",
      "    1    0    0   62   19   11    0 1700 1238  752  391 1322  996  525  117    6    0    0    0   54    9    2    0  291  132   11    0 1552 1315\n",
      "  924  312  143   80   39   16 1636 1066  540  115 2304 1655  920  175 1294  793  327   90  458  191   82   10   19    0    0    0  662  101   40\n",
      "   22   11    0    0    0   46    8    0    0  122   19    0    0  399  180   36   20   55    5    0    0  107   33    0    0  780  528   90    7\n",
      "   45    4    2    0  321  204   65    0  109   50   14    4   10    3    0    0   37    4    2    0  228  179  131   25    8    0    0    0  117\n",
      "   57    9    0   73   35    5    1 1336  895  326   63 1831 1414  512   64  573  260    0    0   14    1    0    0  438   72   19    2  169  111\n",
      "   59    1 1015  710  297   72  532  201   17    0   73   29   17    8  153   95   24    5   79    4    1    0  106   36    9    1 1806 1474 1189\n",
      "  748   49   21    1    0  113    7    0    0 1182  781  399   98  147   69   28   10  268  134   86   48   74    2    0    0  108   15    1    0\n",
      "  173   34    1    0    2    0    0    0  595  385  161   64   96   80   43   18    8    0    0    0  346  243   43    4  383  285  163   37  142\n",
      "  114   59   14  851  710  477   77  326  241  141   25  136   33   14    7   90   49   11    0  354  262  120   31  646  563  385  132  150  104\n",
      "   38    7   78   50    8    1   13    2    0    0 1275  802  356   97  127   60   39    4    0    0    0    0    3    2    0    0  753  399  110\n",
      "   17   85   49   20    4  129  110   42   16  121   53   24    0  193  119   56    5 1133  989  687  350  118   82   41    6   60   10    1    1\n",
      "  658  492  253   54   47   10    0    0   56   17    7    1   57   24    4    0  132   30    7    0   24    8    1    0  140  110   26    5   39\n",
      "    1    0    0  214  214  171   44  349  349  349  344  100   44    3    0  252   79   10    0  268  246  198   22  148   44    4    0  161  154\n",
      "  101   44  141   94   51   12  143  110   49    4  220  162   92   27  255  215  125   10   36   19    7    0  144  144  144  140 1201  988  271\n",
      "   59  336  136    0    0   83   35   19    6    0    0    0    0  600  445  268    3   64    1    0    0   68   45    5    0   68    9    1    0\n",
      "   85   64   31    9   59   16   15    4] \n",
      "\n",
      "\n",
      " fold_pos: (5, 472) \n",
      " [[ 22   7   0 ...   0   0   0]\n",
      " [ 94  55  15 ...   0   0   0]\n",
      " [102  49  12 ...   0   0   0]\n",
      " [301 137  22 ...  16  15   4]\n",
      " [ 51  16   9 ...   0   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (472,)- total: 188511 - sum(axis=0):\n",
      "[ 405  711  917  965  113  154  158  158  623  635  636  636  630  634  635  635  616  618  618  618 1010 1024 1028 1028   51  122  131  131  108\n",
      "  119  121  121   73  135  143  154  930 1416 1906 2268  520  842 1309 1671  175  181  181  181   98  143  150  152  176  283  394  405  191  501\n",
      "  891 1496  166  229  270  293  402 1000 1544 1974  485 1241 2009 2767  645 1198 1683 1926  581  850  959 1031  133  146  146  146  987 1536 1597\n",
      " 1615 1397 1408 1408 1408  191  231  239  239  250  353  369  369  481  701  841  857  428  478  483  483  790  879  910  910  255  527  965 1041\n",
      "  213  254  256  258  191  311  449  501   76  112  146  165  437  461  488  490  191  224  228  232  244  298  346  452   89   97   97   97  286\n",
      "  346  394  403   69  111  143  147  563 1032 1599 1859  328  759 1661 2099  315  419  421  421  209  225  225  225  844 1014 1067 1084   42  101\n",
      "  153  211  502  786 1197 1398  311  650  833  850   39   79   90   99  214  274  325  331   98  117  120  121  248  325  352  360  313  560  809\n",
      " 1232  145  173  193  194  378  484  491  491  446  877 1259 1560  211  289  330  348  217  351  399  437  167  240  242  242   42  137  153  155\n",
      "  689  836  875  878  323  325  325  325  333  543  767  864   11   27   67   92  216  224  224  224  123  231  429  466  425  550  672  798   86\n",
      "  114  169  214  222  366  599  999  261  346  447  564  239  342  361  369   59  100  138  149  190  304  446  535   46  134  312  565   38   92\n",
      "  158  189   79  109  151  157  126  137  139  139  216  899 1345 1603   60  129  150  185  114  114  114  114  114  115  117  117  161  606  896\n",
      "  989   77  113  142  158   24   43  111  137   90  155  183  207  117  191  254  305  118  272  590  926  233  277  318  353  192  237  246  246\n",
      "  139  277  480  673  166  203  213  213  172  211  221  227  161  194  214  217  192  294  317  324   91  107  114  115   21   53  137  158   61\n",
      "   99  100  100   21   21   64  191    0    0    0    5   94  152  193  196  184  357  426  436    9   42   90  266   11   53   93   97    5   13\n",
      "   80  136  129  176  219  257    8   41  102  147   30   93  163  228   23   64  154  269   89  160  179  189    0    0    0    4  532  755 1449\n",
      " 1661   24  224  360  360   49   97  113  126  204  204  204  204  130  354  528  621   74  137  138  138   21   50  102  107  162  221  229  230\n",
      "   20   41   74   96  149  192  193  204] \n",
      "\n",
      "\n",
      " fold_neg: (5, 472) \n",
      " [[ 81  96 103 ...   1   1   1]\n",
      " [ 47  86 126 ...   0   0   0]\n",
      " [ 99 152 189 ...   0   0   0]\n",
      " [142 306 421 ... 148 149 160]\n",
      " [ 36  71  78 ...  43  43  43]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (472,) sum: 199.0\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "  Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      "  Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 472  Y rows with populated labels: 5642  non zero cols: 14461\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_1_cols_624.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task2\n",
      " load_task_weights() - no weights file provided for y_task2, training_weights for all  624 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (624,)- total: 90665 - sum(axis=0):\n",
      "[ 106   72   29    3  587  494  345   97 2738 1689  792  256  133   68   40   19  412  247  133   36  153   98   31    3  143   83   42   11   59\n",
      "   47   24    3  407  244   26    1 1063  803  533  303   77   35    9    3  111   32    5    0   61   10    0    0   65   23    5    0 1030  315\n",
      "   66    9   64    7    1    0    6    0    0    0  162   24    0    0   16   10    4    0  315  178   68    6   12    0    0    0   33    0    0\n",
      "    0   28    0    0    0   87   61   16    1  187   62   18    1  121   49    8    0   72    5    0    0 1276  949  718  336    8    1    0    0\n",
      "    9    2    0    0   57    0    0    0   38    2    0    0   34   17    2    0   67   11    2    0  103    6    0    0    7    1    0    0  105\n",
      "   58   33    6    0    0    0    0  174   27    2    0   84    8    0    0  284  158   51    2  209  112   43    6   76   42   13    0  266   70\n",
      "   21    2  628  399  219  101  181    0    0    0  203    0    0    0  500  393  186   16  197   19    1    0  215   41   16    0   30    2    0\n",
      "    0  117    0    0    0   33    0    0    0 1044  797  479  174   92   67   37    1 1404 1109  784  451  663  563  373  121  446  404  197   24\n",
      " 3488 3085 2184 1007   56   33   16    4   57   29   11    5    8    6    5    4   76   16    4    1 1370  770  377  112  113   62   19    1  480\n",
      "  279  111   23   36   12    3    0  116   15    2    1    1    0    0    0  180   58   10    0  434  279   91    9   82   12    0    0  587  443\n",
      "  291  141   51    4    0    0  123  102   59   45   27    1    0    0   87   43    7    0   20    6    2    0  163  125   82   34   75   14    4\n",
      "    0  253  189   80   25  149   18    0    0   64    5    1    0    4    0    0    0   17   10    6    0  184   78   26    1  127   70   37   12\n",
      "   84   56   31    6   79   35   15    4  150  102   60   17  123   89   37   21   48   21    6    0   22   12    0    0  160   64    7    0  100\n",
      "   82   32   10  323  235  116   48  431  184    8    0  425  242   98   41  132   26    0    0  375  307  135   26   73   45   15    5  393  308\n",
      "  206  116   22    8    0    0  754  683  488   89  143  100   57   19  510  427  289  136  126   57   19    4   16    5    1    1   70   45   37\n",
      "   22  137  109   67   19  141  121   49   17 2237 1959 1151  178   56   45    2    0  353  268  146   55   39    0    0    0  105   82   61   37\n",
      "   66   35   16    6  103   76   41   11  123  103   47    8  700  521  293   38  119   83   48    6  105   61   13    1  195   77   12    1  130\n",
      "   95   33    5  153  116   62    7  245  167   79   24  299  207   90   13  257  136   14    0  314  234  160    6  124  111    1    0  210  210\n",
      "   88    0    7    0    0    0  118  112   89   51  193  135    0    0  575  476   34   10  115   97   44   12   88   32    2    0   82   42   14\n",
      "    6  341  226   79   11 2942 1595  239   34  589  481  248   54  483  297  158   35   21   11    6    1   20    1    0    0   12    1    0    0\n",
      "   57   28    4    0  118   63   36   10   49   32   28   28  124   95   67    8  198  141   41    1  397    4    0    0  393    1    0    0   13\n",
      "    9    5    0  186  179  144   90  340  340   12    7  157   66    8    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 624) \n",
      " [[ 0  0  0 ...  5  0  0]\n",
      " [24 15 12 ... 11  3  0]\n",
      " [44 30  8 ... 28  3  0]\n",
      " [18 13  5 ... 17  0  0]\n",
      " [20 14  4 ...  5  2  0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (624,)- total: 219244 - sum(axis=0):\n",
      "[  14   48   91  117  205  298  447  694 2275 3375 4272 4806  253  340  368  389  302  514  628  718  284  339  406  434  118  181  222  253   60\n",
      "   73  100  121   93  217  347  372  356  629  899 1128  109  152  178  184  387  466  493  498  311  362  372  372  456  499  516  521 2428 3143\n",
      " 3392 3448  357  414  420  421  175  181  181  181   92  230  254  254  326  332  338  342  218  355  465  527  138  150  150  150  263  296  296\n",
      "  296  186  214  214  214   23   49   94  109  599  724  768  785  369  441  482  490  460  525  530  530  296  624  851 1232  106  125  127  127\n",
      "  119  126  128  128 1101 1158 1158 1158  694  729  730  730  135  155  170  172  164  220  229  231  215  312  318  318  162  170  171  171  367\n",
      "  421  446  473  157  157  157  157  526  670  692  694  355  425  432  432  194  334  441  490  112  209  278  315   48   98  126  140   38  420\n",
      "  469  488  370  604  784  902  658  839  839  839  811 1014 1014 1014  188  317  524  694  483  646  662  663  604  778  802  819   82  110  112\n",
      "  112  894 1011 1011 1011  634  667  667  667  519  781 1097 1402   32   57   87  122  489  800 1137 1470  306  418  608  857  135  202  410  583\n",
      "  189  609 1496 2663   58  102  119  131   38   73   91   97   79  104  105  106  187  245  257  260 1503 2111 2505 2769  119  170  213  231  170\n",
      "  371  539  627   73   97  106  109  177  267  279  280  137  138  138  138  686  808  856  866   90  245  433  515  195  266  278  278   66  192\n",
      "  319  466  288  335  338  338   28   49   92  106  132  158  159  159   22   66  102  109  110  124  128  130   32   84  136  184  220  281  291\n",
      "  295   26   90  200  255  431  562  580  580  284  343  347  348  178  182  182  182  138  145  149  155  278  384  436  461   74  131  164  189\n",
      "   76  104  129  154  434  480  497  511   31   81  123  166  129  163  215  231   94  121  136  142  106  116  128  128   28  124  181  188   94\n",
      "  116  167  189  197  285  410  478  314  561  737  745  382  565  714  771  540  646  672  672   34  102  274  356   37   65   95  105   56  145\n",
      "  248  338  104  118  126  126   49  120  315  517  157  200  243  281  105  239  377  528   98  181  216  231   93  108  112  112   83  113  121\n",
      "  136   47   75  117  165    3   24   96  128  106  429 1237 2210   56   67   90   90  177  277  399  499  103  142  142  142   24   47   68   92\n",
      "   54   85  104  114   22   55   91  121   48   80  136  175  204  386  583  838   29   66  101  143   22   67  115  127   94  212  277  288   41\n",
      "   76  138  165    5   26   53  104   30  108  196  251   28  131  248  325  133  254  376  390   68  148  222  376    0    0    0    1    2   21\n",
      "  143  212  142  149  149  149   64   70   93  131   14   72  184  184   37  136  446  470   15   33   86  118   43   99  129  131   66  106  134\n",
      "  142  170  285  432  500  244 1594 2950 3155   63  188  420  612  175  374  514  634  156  166  171  176   90  109  110  110  107  118  119  119\n",
      "   59   87  111  115   72  127  154  180   80   97  101  101   30   60   88  147   63  120  220  260    0    0    0    0    0    0    0    0  110\n",
      "  114  118  123    0    7   42   96    0   71  249  254   62  163  221  229] \n",
      "\n",
      "\n",
      " fold_neg: (5, 624) \n",
      " [[ 0  0  0 ... 45 50 50]\n",
      " [ 0  9 12 ... 23 31 34]\n",
      " [ 9 23 45 ... 34 59 62]\n",
      " [ 2  7 15 ... 37 54 54]\n",
      " [ 3  9 19 ... 24 27 29]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (624,) sum: 258.0\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "  Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 624  Y rows with populated labels: 6650  non zero cols: 15454\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_6_cols_688.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task3\n",
      " load_task_weights() - no weights file provided for y_task3, training_weights for all  688 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (688,)- total: 320094 - sum(axis=0):\n",
      "[1271  905  543  194 1109  958  566  112  352  191   59    3 1482 1150  696  218 2242 1643  975  450 1745 1365  760  258  420  309  156   42  440\n",
      "  317  177   81  282  189  109   25  104   83   55   27  272  219  138   37  964  905  720  237  262  154   45    2   96   51   23    1  590  527\n",
      "  403  191 1398 1223  833  209 6995 5174 2414  514 1467 1196  855  343 1270  996  756  446  736  447  199   41 2269 1830  844  389  739  471   74\n",
      "   14 1207  470  221   30 1067  680  259   24  105   87   65   36 3100 2536 1499  575 3493 2694 1437  423 1831 1181  555  134 2120 1478  832  233\n",
      "  136   76   18    4 2549 2136 1202  291 1663 1358  886  385 1195  798  421  112  743  475  178   26  877  615  298   57  220   81   20    0 2742\n",
      " 2280 1490  442 2748 2510 2057 1202  197  162  107   46 5767 4236 2258  911 1948 1457  974  315  108   71   38    9 2326 2057 1553  799 2832 2069\n",
      " 1157  619 3688 3373 2712 1188 4792 3893 2211  784  668  455  175   42  807  304   79   18  442  335  135    5 1788 1272  442   78  134   65   27\n",
      "   11 1290  997  480  260  978  699  371   90 1143  890  410   63  335  262  161   33  373  255    8    2   89   74   53   23 2595 2188 1604  856\n",
      "  539  332  148   24  547  331  122   17  810  647  323   67  641  506  261   67   72   41   20    5  417  199    7    2   49   25   15    8   81\n",
      "   62   32    6 1109  715  455  175 1356 1267 1010  410  128   77   33   18  391  299  211  101  297  185   98   23  119   65   29    2  639  494\n",
      "  264   59  427  267  168  111  414  310  121   16  170  142   85   28  604  442  241   76  302  208  131   27  333  233  120   27  101   82   38\n",
      "    3  106   48   23   10  502  363  190    6  993  644  205   52  196  134   62    8  679  521  261   33  191  159   86   26  450  362  211   88\n",
      " 1993 1694 1170  446  108   91   72   54   76   26    8    2  191  147   53    8  218  133   57   14  639  575  395  165  145   62   24    4  965\n",
      "  892  597  139  136  112   93   60  183  113   58    7  684  477  252   75  138  110   27    0   58   45   31   11   64   17    5    1 2556 1951\n",
      " 1144  351  125   47    2    0  801  642  390   57   79   34   11    1 1339 1117  750  270 1877 1566 1039  460  125  120  106   14   52   19    5\n",
      "    3  800  732  633  276  190  190  142    1  122   85   37   16  132   98   50    6 1578 1285  885  289   59   37    6    0  285  138   66   19\n",
      "  223  146  109   34  373  244   98   24  120   58   19    5  148   88   32    0  176  129   54    8  401  340  237   54  109   85   44   12  176\n",
      "  121   71   17 3985 3382 2176  933  719  535  104   28   67   46    1    1  184  136   98   31  244  154   51    6  169  143   78    6  228  180\n",
      "  123   37  931  843  633  277  596  452  234   24  276  198  106   11  237  195   62    0  277  181   68   14  149  116   88   65   88   70   52\n",
      "   19   53   10    0    0   72   62    1    1  131   67   31    4  229  226  169   64  193  167  113   64  150  102   45    1    4    2    1    0\n",
      "   25    1    0    0   82   51   21    3  265  244  186   73   47   30   14    2  541  360  112    1  159    2    0    0  131   89   36    8  129\n",
      "  104   57    7 1021  936  740  271 3414 2826 1678  632  788  463  166   57  186  139  101   80   33    7    0    0  209  184  128   38   94   80\n",
      "   64   35  695  601  221   31   99   57   27    1  914  752  515  124  326  305  231   35  120   68   26    4  227  163   96   21  140   59    7\n",
      "    0  283  236  129   23  190  159  102   23  135   14    0    0  140  117   44    2  581  338   95    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 688) \n",
      " [[275 201 124 ... 150  26   0]\n",
      " [144  94  51 ...   8   3   0]\n",
      " [233 156  83 ...   1   0   0]\n",
      " [439 332 207 ... 136  66   0]\n",
      " [180 122  78 ...  43   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (688,)- total: 382164 - sum(axis=0):\n",
      "[ 824 1225 1583 1926   72  243  635 1088  155  316  448  508  416  761 1211 1598  726 1384 2047 2495 1016 1472 2049 2544  142  261  417  530  226\n",
      "  359  498  597  113  209  292  376   14   38   66   94   22   75  156  257   84  166  352  833  159  269  378  422   27   72  100  120   68  131\n",
      "  255  467  109  326  713 1213 2049 4123 6900 8779  313  637  976 1492  503  813 1042 1346  382  730  974 1135  619 1090 1694 2149  552  840  990\n",
      " 1050  444  729  966 1158  420  824 1247 1482  111  133  155  182  330  906 1943 2870  823 1778 3032 4044  996 1686 2311 2735 1570 2250 2886 3471\n",
      "  197  266  324  342  290  732 1655 2563  282  603 1077 1571  942 1389 1768 2072  670  971 1266 1417  252  533  850 1089  244  384  445  465  695\n",
      " 1244 2046 3101  432  696 1152 2012  145  196  249  308 2653 4220 5720 7056  169  503  801 1449   37   75  108  137  233  527 1040 1782  650 1668\n",
      " 2601 3088  265  626 1287 2720  554 1849 3624 5157  590  820 1100 1233  263  458  684  745  171  281  481  610  703 1171 1813 2171  163  236  274\n",
      "  291  226  526  806  966  576  893 1220 1502  323  612 1103 1448  117  196  297  427  105  237  433  441   72   89  110  141  316  742 1324 2075\n",
      "  463  678  862  990  380  602  811  914  120  286  609  847  207  350  596  791   41   73   94  109  203  422  568  573   83  110  120  127   44\n",
      "   68   98  124  413  793 1001 1204  115  230  487 1086   31   82  125  140  119  216  306  417  161  297  384  456   64  123  159  185  174  323\n",
      "  553  758  452  635  740  799   99  211  406  511   55   90  146  203  423  596  796  962  138  252  334  442   89  214  327  421  105  136  180\n",
      "  215  187  248  270  283  158  269  369  427  246  758 1197 1354  148  214  299  353  219  387  641  868   19   60  140  197  153  260  410  533\n",
      "  669 1023 1552 2276   39   56   75   93   92  145  166  171   88  136  230  275  155  242  313  354   94  172  363  593  113  195  232  250   64\n",
      "  181  478  935   36   59   78  111   90  160  215  266  247  495  720  895   65   94  177  204   64   79   93  113   51   99  111  115  693 1320\n",
      " 2131 2927   88  174  219  221  172  383  635  968  108  154  177  191  147  415  782 1253  209  555 1082 1663    8   11   25  117   50   83   97\n",
      "   99   47  115  214  575   11   12   13   13   43   80  128  147   59   93  141  188  462  763 1172 1771   42   67   98  103   68  215  287  334\n",
      "   83  164  201  258  171  325  471  543   43  105  144  158  105  169  228  261  139  187  262  308   58  119  220  403   36   59  100  131   44\n",
      "  130  180  233  528 1203 2163 3326  192  417  715  790   38   59   91   91  102  151  187  251   87  187  291  336   17   55  120  191   53  105\n",
      "  163  249   40  130  340  695   44  173  315  396  183  258  294  318   57   99  232  294   63  164  277  330   52   89  117  140   57   85  103\n",
      "  133  118  161  171  171   35   45   84   84   34   98  134  161   13   17   74  179   19   47  101  150   85  133  190  234   54   57   58   59\n",
      "  150  174  175  175   35   66   96  114    8   45  103  216   68   86  101  112  153  335  583  698   14   35   37   37   94  136  189  217   15\n",
      "   43   90  138   48  136  334  807  700 1351 2490 3534  365  707  878  987  170  217  256  277   86  112  119  123   27   53  109  199   22   37\n",
      "   53   86   65  160  539  728   60  106  137  155  127  300  536  918   26   51  125  321  167  220  262  284  212  281  348  421   36  117  169\n",
      "  176   52  104  211  321   23   53  110  189   41  900  914  914   19   42  115  143  132  395  638  733] \n",
      "\n",
      "\n",
      " fold_neg: (5, 688) \n",
      " [[147 227 303 ...  93 217 243]\n",
      " [162 212 255 ...  28  33  36]\n",
      " [178 256 329 ...   6   7   7]\n",
      " [189 320 442 ...  11  81 147]\n",
      " [148 210 254 ... 257 300 300]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (688,) sum: 524.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 0.]\n",
      "\n",
      "  Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 688  Y rows with populated labels: 19594  non zero cols: 72093\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_10_cols_192.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task4\n",
      " load_task_weights() - no weights file provided for y_task4, training_weights for all  192 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (192,)- total: 44576 - sum(axis=0):\n",
      "[ 756  267   91   17  563  169   31   10  231  162   65    9 1083  666  166   17 1949 1057  399  106  575  365  149   18  495  272  119   33  448\n",
      "  330  160   43  176  110   41   10 1539  722  148   12 2398 1373  534  103 2921 2246 1450  532    8    0    0    0  221  113   42    1   11    3\n",
      "    0    0   44    9    1    0  277   64    9    0  187  163  138   52    9    6    4    2   68   21    1    0   39   10    3    0   77   52   39\n",
      "    0  137   70   21    8  179   83   21    2   18    1    1    0  113   77   28    2  162  117   59    0  575  372  173   23  171  138   56   19\n",
      "  592  314  141   24  265  196   98   24  111   99   74   22  437  296   98    0  262  200  111   23  377  307  246  172   56    3    0    0  565\n",
      "  499    9    0   94   45   24    7  459  290  166   68  253  135   87   55  194  126   34    0 1490 1162  265   28   69   43    6    0  290  283\n",
      "   16    1   17    0    0    0  154  125   88   43 1006  865  729  437  172  134   59    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 192) \n",
      " [[121  45  16   6 115  31   8   3  35  26  12   6 189 124  29   2 423 242 110  48  92  58  21   3  99  45   3   0  61  23   4   0  37  24   7   0\n",
      "  246  99  19   3 638 394 171  30 669 437 251  87   0   0   0   0  40  13   9   1   2   0   0   0   3   1   1   0  65  12   0   0  43  37  26   9\n",
      "    3   1   0   0  30   9   0   0   7   3   1   0  21  18  18   0   9   5   0   0  46  13   0   0   0   0   0   0  17   9   2   0  71  60  33   0\n",
      "  126  91  40   3  41  38  25   5  66  33  13   1  62  41   3   0   7   6   6   5 107  91  52   0  56  50  36  14  37  29  20   7   3   0   0   0\n",
      "  105  83   0   0  19  10   8   3  51  23   9   2   1   1   1   1  32  16   2   0 218 183  59   7  34  24   2   0  53  52   7   0  12   0   0   0\n",
      "   19  14   7   1 163 141  91  41  29  13   5   0]\n",
      " [146  59  22   2 110  35   8   2  15   8   4   0 231 116  16   1 278 124  31   6 259 177  80   7  45  14   2   0  58  27   3   0  35  23  11   2\n",
      "  363 169  36   6 320 145  31   3 772 626 432 188   4   0   0   0  46  25   7   0   5   3   0   0   7   0   0   0  70  23   5   0  26  20  13   6\n",
      "    0   0   0   0   1   0   0   0   2   0   0   0  14  10  10   0  26  17   0   0  38  26   9   2  12   0   0   0   8   6   0   0  33  17   0   0\n",
      "   93  61  32   8  39  33   2   0 173  94  39   4  44  29  11   2  27  26  22   1 137  72  19   0  58  49  37   2 149 141 132 116   4   1   0   0\n",
      "   68  55   0   0  19   4   0   0  92  62  42   7  30  27  25   7  37  28   3   0 149 123  27   0   5   4   2   0  49  44   1   0   2   0   0   0\n",
      "   33  28  20  12  57  41  29  13  15  13   3   0]\n",
      " [179  72  36   7 101  39  11   3   0   0   0   0 197 121  47   9 446 233  92  17  51  38   7   0 162  94  44  20 172 156  78  11  28  19   8   2\n",
      "  294 154  20   2 392 181  89  27 465 349 189  55   4   0   0   0  50  31  13   0   1   0   0   0  12   2   0   0  48  15   1   0  53  53  50  32\n",
      "    4   4   4   2  11   6   1   0   7   6   2   0  18   5   0   0  41  20   9   5  45  29  12   0   3   1   1   0  18   5   1   0  22   7   1   0\n",
      "  128  78  42   6  26  18   4   1 112  53  24   8  39  21   8   3  57  54  36  10  16   7   1   0  46  25   5   0  47  42  28  14   0   0   0   0\n",
      "  180 167   8   0  17   6   3   2 123  89  80  50 160  95  55  46  40  22   3   0 199 176  14   7  15  11   2   0  83  83   6   1   3   0   0   0\n",
      "   32  25  16   9 189 163 143  69  30  23   6   0]\n",
      " [181  57  13   2 111  28   1   0  12   5   1   0 139  87  26   1 438 273  93  18  91  48  26   2  95  76  47   3 105  84  58  27  31  20   6   1\n",
      "  349 172  50   0 323 168  62  16 528 465 303  91   0   0   0   0  52  30  11   0   3   0   0   0   6   5   0   0  44  13   3   0  36  31  29   1\n",
      "    0   0   0   0  17   2   0   0  20   1   0   0  19  15  10   0  47  23   7   0  20   5   0   0   0   0   0   0  36  33  21   1   3   1   1   0\n",
      "  166 109  43   3  29  17   5   0 154  90  53   9  75  72  65  16   6   4   4   3  71  48   8   0  55  43  11   0  65  40  28  23   6   0   0   0\n",
      "  136 125   1   0  21  15  13   2  67  27   5   1  33   1   1   0  34  23  10   0 637 460 106   7  12   4   0   0  54  53   1   0   0   0   0   0\n",
      "   37  30  25   9  61  60  48   9  63  53  31   0]\n",
      " [129  34   4   0 126  36   3   2 169 123  48   3 327 218  48   4 364 185  73  17  82  44  15   6  94  43  23  10  52  40  17   5  45  24   9   5\n",
      "  287 128  23   1 725 485 181  27 487 369 275 111   0   0   0   0  33  14   2   0   0   0   0   0  16   1   0   0  50   1   0   0  29  22  20   4\n",
      "    2   1   0   0   9   4   0   0   3   0   0   0   5   4   1   0  14   5   5   3  30  10   0   0   3   0   0   0  34  24   4   1  33  32  24   0\n",
      "   62  33  16   3  36  32  20  13  87  44  12   2  45  33  11   3  14   9   6   3 106  78  18   0  47  33  22   7  79  55  38  12  43   2   0   0\n",
      "   76  69   0   0  18  10   0   0 126  89  30   8  29  11   5   1  51  37  16   0 287 220  59   7   3   0   0   0  51  51   1   0   0   0   0   0\n",
      "   33  28  20  12 536 460 418 305  35  32  14   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (192,)- total: 110611 - sum(axis=0):\n",
      "[2161 2677 2853 2927 2052 2446 2574 2595  185  254  351  407  621 1111 1414 1558 1993 2934 3594 3887  257  467  683  810   70  293  446  532  107\n",
      "  233  403  520  110  176  245  276  640 1462 2036 2172 1934 2964 3786 4225  341 1049 1857 2775  685  693  693  693   71  179  254  295  846  854\n",
      "  857  857   92  127  135  136  118  400  455  464   51   78  103  188  155  159  166  168  343  391  411  412  169  198  211  214   57   82   95\n",
      "  134  296  363  412  425  307  403  465  475  122  134  134  135   31   67  116  141   57  102  160  219  528  778  978 1128   27   60  142  179\n",
      "  351  632  805  920  139  207  305  379   26   38   63  115  217  378  576  667   41  110  201  289   34  119  192  266   53   94   95   95   31\n",
      "  130  318  327   44   93  114  131  519  688  809  902  118  236  284  316   93  161  254  288  280  660 1276 1464   67   93  130  136   22   35\n",
      "   81   87   91  108  108  108   28   55   92  137   88  229  365  657   23   61  136  195] \n",
      "\n",
      "\n",
      " fold_neg: (5, 192) \n",
      " [[ 435  511  542  552  611  693  712  717   22   31   45   51  143  235  300  327  395  582  714  776   63   97  134  151   14   68  110  113   40\n",
      "    81  100  104   21   34   51   58  109  257  337  353  470  716  939 1089   66  307  497  661  108  108  108  108   19   46   51   59  163  165\n",
      "   165  165   22   24   24   25    6   84   96   96    5   11   22   39   17   19   23   23   60   81   90   90   26   30   35   36    6    9    9\n",
      "    27   78   82   87   87   53   86   99   99   28   28   28   28    3   11   18   20    8   19   46   79   99  134  185  222    0    3   16   36\n",
      "    62   95  115  127   19   40   78   81    5    6    6    7   17   35   74  122    5   11   27   49    0   19   41   54   17   20   20   20   12\n",
      "    43   68   68    7   16   18   23   40   68   80   87    0    0    0    0   25   41   55   57   17   56  142  184    1   11   33   35    2    4\n",
      "    13   16    7   19   19   19   17   22   29   35   37   59  109  159   11   27   35   40]\n",
      " [ 391  483  519  539  339  414  441  447   12   19   23   27  128  255  319  334  442  597  690  715   62  144  241  312   14   45   57   59   18\n",
      "    52   76   79   23   35   47   56  160  345  478  508  401  572  684  715   61  212  410  654  168  172  172  172   13   34   52   59  212  214\n",
      "   217  217   21   28   28   28   15   72   90   95   19   25   32   39   47   47   47   47   76   77   77   77   47   49   49   49    5    9    9\n",
      "    19   61   70   87   87   63   75   92   99   16   25   25   25    9   11   17   17   14   30   47   47   99  158  187  211    1    7   38   40\n",
      "    73  152  207  241   37   52   70   79    2    3    7   28   57  128  180  196    3   14   26   61    3   14   23   39    7   11   12   12    5\n",
      "    25   68   68    9   24   28   28   64   94  114  146    4    7    9   27   20   29   54   57   43   85  147  167   20   21   23   25   12   17\n",
      "    26   26   19   21   21   21    3    8   16   24    9   25   37   53    3    5   15   18]\n",
      " [ 459  587  622  651  267  331  359  367    2    2    2    2   80  160  210  248  360  594  737  812   43   56   87   94   12   80  130  154   19\n",
      "    35  113  180   30   39   50   56  124  269  403  421  330  545  637  699   68  191  352  486  175  179  179  179   10   29   47   60  140  141\n",
      "   141  141   16   26   28   28   36   63   77   78    1    1    4   22   30   30   30   32   77   82   87   88   32   33   37   39    9   22   27\n",
      "    27   46   67   78   82   54   70   87   90   25   27   27   28   12   25   29   29   23   38   44   45   55  105  141  177   14   22   36   39\n",
      "    59  118  147  163   39   57   70   75    0    3   21   47   50   60   66   67   14   37   57   62    8   13   27   41   11   11   11   11    3\n",
      "    22   60   68   11   22   25   26  140  174  183  213   66  131  171  180   17   35   54   57   45   86  155  161   11   15   24   26    0    3\n",
      "    21   22   15   18   18   18    4   11   20   27   12   38   58  132    4   11   28   34]\n",
      " [ 454  578  622  633  267  350  377  378   23   30   34   35  125  196  237  262  445  617  797  872   45   88  110  133   11   30   59  103    6\n",
      "    28   54   85   23   34   48   53  151  333  455  505  266  421  527  573   87  157  320  532  116  116  116  116    5   27   49   60  181  184\n",
      "   184  184   21   22   27   27   36   86   96   99   10   15   17   44   32   32   32   32   47   62   64   64   26   45   46   46   24   28   33\n",
      "    43   40   64   80   87   60   75   80   80   27   27   27   27    0    3   15   35    4    6    6    7  201  278  345  385   11   23   35   40\n",
      "    94  161  198  241    8   11   18   67   15   17   17   18   22   54   95  103    7   19   51   62   19   44   55   60   14   17   17   17    3\n",
      "    14   65   66    7   13   15   26  147  187  209  213   36   68   68   69   23   34   48   58   99  290  574  651   20   28   32   32    1    4\n",
      "    12   13   34   34   34   34    1    6   11   27    5    6   18   57    2   12   34   65]\n",
      " [ 422  518  548  552  568  658  685  686  126  172  247  292  145  265  348  387  351  544  656  712   44   82  111  120   19   70   90  103   24\n",
      "    37   60   72   13   34   49   53   96  258  363  385  467  710  999 1149   59  182  278  442  118  118  118  118   24   43   55   57  150  150\n",
      "   150  150   12   27   28   28   25   95   96   96   16   26   28   44   29   31   34   34   83   89   93   93   38   41   44   44   13   14   17\n",
      "    18   71   80   80   82   77   97  107  107   26   27   27   27    7   17   37   40    8    9   17   41   74  103  120  133    1    5   17   24\n",
      "    63  106  138  148   36   47   69   77    4    9   12   15   71  101  161  179   12   29   40   55    4   29   46   72    4   35   35   35    8\n",
      "    26   57   57   10   18   28   28  128  165  223  243   12   30   36   40    8   22   43   59   76  143  258  301   15   18   18   18    7    7\n",
      "     9   10   16   16   16   16    3    8   16   24   25  101  143  256    3    6   24   38]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (192,) sum: 111.0\n",
      " [1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "\n",
      "  Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      "  Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 192  Y rows with populated labels: 3986  non zero cols: 8406\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_11_cols_620.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task5\n",
      " load_task_weights() - no weights file provided for y_task5, training_weights for all  620 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (620,)- total: 142158 - sum(axis=0):\n",
      "[ 441  321  180   66  561  309  162   86 1156  912  572  233  657  568  449  255  466  285  135   36   28    0    0    0  255  160   63   16  240\n",
      "  204  143   29  205  174  124   60  194   82   25   13  174   92   40   21   74   11    2    1  106   61   36   16  110   41   16    5  420  254\n",
      "  139   53  367  147   68   28  574  380  191   48  198  146   65   21  194  125   47   13  917  643  277   76  710  370  106   23  233  160   87\n",
      "   32  270  207  113   21  189  158  107   37  734  465  266  109  281  176  111   38  739  594  436  222  650  356  202   86  334  206   78    6\n",
      "  702  549  420  243 1628 1440 1059  671  344  256  131   24  262  179   78   15  329  272  168   61  773  527  269   77  140   63   15    2   34\n",
      "   21   12    2  187    4    0    0  196  116   69   21  260  113   34    7  487  423  341  197   97   14    0    0  716  554  280   49   87   49\n",
      "   27   14  729  555  291   98   40   25   13    0  681  520  236   33 1741 1500 1047  531   24   21   20   13  127   72   24    0   58    4    0\n",
      "    0  655  465  292   83  659  537  370  211  261  252  197    0 1386 1023  578  145 1162  909  459  193   96   78   45    6  230  122   20    3\n",
      "  321  295  255  133   12    2    0    0  150   31    7    0  853  452  229   58 1549  971  498  130  136   20    3    0  295  230  144   70  198\n",
      "  149   84   33  196  126   53   19  452  330  122   14 1131  878  581  252  187   66   26   10  676  518  228   53  632  366  108   31  219  137\n",
      "   77   26  219  194  139   82   96   80   43   18 1507 1100  503   86  287  244  167  107   72   14    4    1  219   95   24    5  253  160   75\n",
      "   13  321  269  176   80  126   27    5    0 1981 1689 1009  241  447  248  135   65 1388  913  357   64  543  390  202   43  288  249  185   92\n",
      "   95   36   13    3  272  193  109   13  584  431  260  105  151  137   94   30  302  212  108   18  165  131   90   47  415  321  172   64  104\n",
      "   95   82   54  341  273  135   29  640  577  190   16  252  202   61    7  516  429  276  136  280  184  130   76  418  291  158   56   63   57\n",
      "   43   11  631  343  140   34   73   48   23    3   92   58   16    0  490  395  295  173  364  198   70   24  331  229  125   37 1961 1683 1200\n",
      "  373  217  158  102   32  123   95   74   54  109   33    2    0  751  529  308  130  256  168   82    8  411  355  253  149  791  638  322  112\n",
      "  122   74   30    4  334  241  134   43   63   36    7    0  627  573  344   22  518  421  248   59  255  225  154   57   54   24    8    2  299\n",
      "  276  233   97   99   42   14    2   65    8    0    0  116  115  113   99  710  609  407  202  842  747  558  316  315  213  111   24   66   11\n",
      "    1    0   92   36    2    0  168  122   92   56   74   40   19    2  450  358  233  141  475  270   81   12  643  431  222   38  105   88   78\n",
      "   31  188  143   56    6  152  119   54    0  131   88   41    7  110   91   66   21  158  153  121   21  709  540  166   16  162  119   43    2\n",
      "   57   26    6    0  201  105   57   34  144  107   71   52  629  496  355  166  294  143   24    0  330  302  203  195  362  287  201  138    0\n",
      "    0    0    0   95   84   70   23  125   33    4    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 620) \n",
      " [[96 65 25 ...  5  0  0]\n",
      " [79 59 39 ...  2  0  0]\n",
      " [90 63 37 ...  7  0  0]\n",
      " [82 57 24 ... 11  0  0]\n",
      " [94 77 55 ...  8  4  0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (620,)- total: 193933 - sum(axis=0):\n",
      "[ 144  315  464  559  294  547  694  762  517  795 1141 1480  440  526  645  840  467  643  793  892  186  214  214  214   84  184  281  326   30\n",
      "   78  140  251   24   57  108  172   96  212  269  281   55  148  200  219   54  118  126  127   30   80  105  125   86  163  188  199  132  303\n",
      "  418  495  287  511  591  631  390  618  810  952   69  127  209  252   30   99  177  209  158  458  824 1025  315  685  949 1032   67  152  226\n",
      "  282   77  155  249  341  117  158  210  279  463  753  952 1109   75  184  249  321  635  795  953 1170  524  817  970 1086  145  287  415  487\n",
      "  170  331  466  601  125  327  714 1102   63  164  289  396  132  230  333  396   84  143  247  350  100  349  619  808   83  160  208  221  310\n",
      "  323  332  341  388  563  566  566  146  263  310  358   41  199  278  305  164  242  324  468  358  443  457  457  121  287  561  792   34   81\n",
      "  103  116  305  498  762  955   63   78   90  103  245  443  727  929  289  568 1023 1539   91   94   95  102  257  313  359  382   50  103  106\n",
      "  106  229  418  591  761   94  231  398  557   11   22   30   30  273  659 1104 1537  115  370  820 1086   13   33   66  105    8   60   80   97\n",
      "   10   50   90  212  110  120  122  122  219  329  352  359  602 1037 1260 1444  254  833 1306 1676   84  213  232  235  279  351  438  512  291\n",
      "  351  416  467   52  130  203  237  359  571  780  888  235  492  790 1120  123  244  284  300   77  244  546  720   59  157  237  282   89  204\n",
      "  264  315   28   56  111  168   11   27   67   92  413  820 1417 1831   39   94  170  230  139  188  198  201   79  211  293  312   33  273  357\n",
      "  419   27  227  320  416  213  312  334  339  359  732 1413 2181  239  441  554  626  498 1056 1612 1905  275  452  640  799   22   62  126  218\n",
      "   75  137  160  170   92  178  262  358  154  354  525  680   30   55   98  162  151  263  367  460   33   99  140  183   60  236  385  493   13\n",
      "   23   36   64  105  179  317  423   29  105  500  674   40   92  233  287   33  283  437  577   27  271  328  382   16  287  426  528   39   45\n",
      "   59   91  414  746  953 1059   45   71   96  116   87  129  171  187   95  195  294  416  207  374  502  548  107  217  322  410   60  354  838\n",
      " 1665   24   85  140  210   86  117  138  158   22  105  136  138  458  696  919 1097   34  123  209  283   25   84  187  291  125  314  631  841\n",
      "   37   86  130  156  123  228  336  427   63   90  119  126   91  147  376  698   90  166  341  530  103  136  212  309  104  138  154  160  100\n",
      "  124  167  303   43  104  132  142   60  117  125  125    0    1    3   17   42  182  384  589   44  146  374  616   70  175  277  364   51  106\n",
      "  116  117   81  137  171  173   59  104  136  172   33   67   88  105   79  174  299  391  238  446  635  703  181  393  603  787   16   33   43\n",
      "   90   75  130  216  266    2   35  100  154   38   81  128  162   10   29   57  102    4   11   64  164   51  269  558  705   10   64  140  180\n",
      "   56   88  108  114   83  198  246  269   12   49   85  104   25  179  319  508   27  178  297  321    4   53  152  160  179  254  340  403  204\n",
      "  204  204  204    6   17   31   78   36  128  157  161] \n",
      "\n",
      "\n",
      " fold_neg: (5, 620) \n",
      " [[ 65 117 158 ...  17  22  22]\n",
      " [ 28  52  73 ...  12  14  14]\n",
      " [ 24  55  81 ...  43  50  50]\n",
      " [ 10  47  80 ...  35  46  46]\n",
      " [ 17  44  72 ...  21  25  29]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (620,) sum: 389.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "\n",
      "  Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 620  Y rows with populated labels: 10009  non zero cols: 28257\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_643_cols_184.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task6\n",
      " load_task_weights() - no weights file provided for y_task6, training_weights for all  184 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (184,)- total: 41813 - sum(axis=0):\n",
      "[2552 2041 1193  453 1951 1139  390   72  248   84   13    2 1457  864  468  130 3934 2633 1093  412   48   10    1    0  172   92   42    3   60\n",
      "   40   23    8    9    3    0    0   29    0    0    0 1845 1436  767  151  858  667  181    2  582  468  274   48  678  508  269   46  164   59\n",
      "   18    1  482  444   35    0   77    5    0    0   77    0    0    0   56   10    1    0  270  103   17    2    9    4    2    1   50    2    0\n",
      "    0   68    3    0    0  189  143   86   46  211  130   38    4   95   68   19    4  122  103   55   22  239  134   61   16   70   23    0    0\n",
      "  231   91   12    1  120   93   63   10 1095  922  450  104  221  163   60    7  182    3    2    0   67   21    3    0  437  360  224   60  186\n",
      "  169   42    2   42   13    6    1  239  180   75    6   94   49   12    1   98   82   54   21  246  226    0    0   64   33    9    0   90   58\n",
      "   16    0   81   69   37   13  126   50   23   11] \n",
      "\n",
      "\n",
      " fold_pos: (5, 184) \n",
      " [[ 461  367  247  124  407  219   79   28   79   39    7    1  357  271  188   52  454  334  158   52    5    0    0    0   42   24   10    0   10\n",
      "     3    2    0    2    0    0    0    6    0    0    0  530  426  281   83  263  185   18    0  104   86   52    7  207  155   74    9   16    1\n",
      "     0    0   47   44    3    0   14    1    0    0   22    0    0    0   16    3    1    0   63   22    1    0    0    0    0    0   15    2    0\n",
      "     0   18    0    0    0   38   17    8    5   33   20    9    0   17   13    0    0   13   11    8    4   51   27   10    1   17    9    0    0\n",
      "    67   41    6    1    4    2    0    0  173  147   94   26    3    2    0    0  166    0    0    0   21    1    0    0  178  139   79   20   18\n",
      "    15   11    0   40   13    6    1   41   25    7    0    6    2    0    0   13    4    0    0    0    0    0    0   20    9    0    0   13    6\n",
      "     3    0   18   18    7    0    2    0    0    0]\n",
      " [ 290  202   97   19  212   86   14    0   52   16    2    1  208   94   43   18  535  265   81    7   18    8    1    0   17    2    1    0    3\n",
      "     1    1    0    2    1    0    0    3    0    0    0  362  271  131   10  380  313  116    0  135   91   34    8   58   39   27   13   29    2\n",
      "     0    0   51   44    6    0    2    0    0    0   14    0    0    0    0    0    0    0   51   10    0    0    2    1    1    0   24    0    0\n",
      "     0    7    0    0    0   45   38   31   25   39   20    2    1   22   16    5    0   29   24   11    5   43   26    8    1    8    1    0    0\n",
      "    49   12    0    0    4    0    0    0  234  178   66    8   16    8    4    2    2    1    1    0   14    7    2    0   36   26   17    3   12\n",
      "     3    1    1    1    0    0    0   62   50   19    1   30   20    6    0    4    3    1    1   59   45    0    0    5    2    0    0   37   24\n",
      "    11    0   11   10    7    4   24    3    3    3]\n",
      " [ 618  513  352  115  371  241  126   25   41   12    0    0  249  129   64   15  846  533  189   68   11    0    0    0   52   48   23    2   16\n",
      "    13    9    3    4    1    0    0    3    0    0    0  263  208  100    5   16   14    9    2  161  147  119   20  203  166  116   10   34    7\n",
      "     0    0   45   40    8    0    6    2    0    0   14    0    0    0    6    1    0    0   49   14    4    1    3    2    0    0    1    0    0\n",
      "     0    6    1    0    0   36   28   19   10   34   24    6    1   18   11    5    1   10    8    6    4   35    5    0    0   13    3    0    0\n",
      "    35   22    5    0   17   10    1    0  104   80   28    7  129   96   34    5    2    0    0    0    0    0    0    0   56   48   33   14    1\n",
      "     1    1    0    0    0    0    0   61   48   27    5   19   11    4    1   21   19   18   10    0    0    0    0    2    0    0    0   22   12\n",
      "     1    0   19   16   13    8   29    0    0    0]\n",
      " [ 609  489  198   65  510  301   63   16   41   12    4    0  437  278  133   37 1350  973  395  157    6    1    0    0   41    9    7    1   18\n",
      "    15    5    1    1    1    0    0    9    0    0    0  352  267  102   24  122  108   24    0  108   80   39    2  125   78   18    2   56   40\n",
      "    16    1  291  273   14    0   46    2    0    0   12    0    0    0    1    0    0    0   46   34    8    0    0    0    0    0    9    0    0\n",
      "     0   18    1    0    0   28   22    3    2   62   35   11    0   25   18    8    3   64   54   26    7   54   30    9    2   13    2    0    0\n",
      "    27    4    1    0    2    1    0    0  464  414  229   62   44   36   16    0   10    1    0    0   20   10    0    0   83   66   39   14  148\n",
      "   143   26    1    1    0    0    0   43   38   15    0   24   11    0    0   12   12    5    1  176  170    0    0   33   22    9    0    3    3\n",
      "     0    0   14    9    4    1   44   30    8    1]\n",
      " [ 574  470  299  130  451  292  108    3   35    5    0    0  206   92   40    8  749  528  270  128    8    1    0    0   20    9    1    0   13\n",
      "     8    6    4    0    0    0    0    8    0    0    0  338  264  153   29   77   47   14    0   74   64   30   11   85   70   34   12   29    9\n",
      "     2    0   48   43    4    0    9    0    0    0   15    0    0    0   33    6    0    0   61   23    4    1    4    1    1    1    1    0    0\n",
      "     0   19    1    0    0   42   38   25    4   43   31   10    2   13   10    1    0    6    6    4    2   56   46   34   12   19    8    0    0\n",
      "    53   12    0    0   93   80   62   10  120  103   33    1   29   21    6    0    2    1    1    0   12    3    1    0   84   81   56    9    7\n",
      "     7    3    0    0    0    0    0   32   19    7    0   15    5    2    0   48   44   30    9   11   11    0    0    4    0    0    0   15   13\n",
      "     1    0   19   16    6    0   27   17   12    7]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (184,)- total: 69820 - sum(axis=0):\n",
      "[ 445 1016 1698 2409  815 1655 2266 2582  415  579  650  661  572 1337 1708 2045 1052 2557 4098 4775   93  131  140  141  136  217  267  306   60\n",
      "   81   98  113  110  116  119  119  311  340  340  340  443  874 1572 2177  198  386  865 1044   64  180  367  592  102  273  512  735   76  182\n",
      "  223  239   12   48  457  492  410  482  487  487  118  195  195  195   66  112  121  122  193  360  446  461   99  104  106  107   99  147  149\n",
      "  149   60  125  128  128  117  163  219  266   74  171  263  296   20   47   96  111   14   33   81  114  156  263  336  381  127  175  198  198\n",
      "  254  404  480  491   56   83  113  166  123  309  734 1044   48  106  209  262  125  190  191  193   52   98  116  119   35  112  248  412    9\n",
      "   26  153  193   54   83   90  102  151  231  336  405   32   77  114  125    4   20   48   81    0   20   56   56   50   81  105  114   33   65\n",
      "  107  123   25   37   69   93  139  226  253  265] \n",
      "\n",
      "\n",
      " fold_neg: (5, 184) \n",
      " [[ 109  215  330  449  199  376  483  534   72  112  144  150  105  209  292  428  175  307  485  590   24   29   29   29   20   38   52   62   12\n",
      "    19   20   22   23   25   25   25   62   68   68   68   98  204  344  532   51  129  296  314   14   32   66  110   26   78  159  224   22   37\n",
      "    38   38    2    5   46   49   33   46   47   47   18   40   40   40    6   19   21   22   29   70   91   92    5    5    5    5   16   29   31\n",
      "    31   17   35   35   35   24   45   54   57   14   36   47   56    6   10   23   23    6    8   11   15   28   53   70   79   22   30   39   39\n",
      "    31   57   92   97   12   14   16   16   41   67  106  152   13   14   16   16  101  160  160  160    6   26   27   27   17   56  116  175    0\n",
      "     3    7   18   17   44   51   63   33   58   76   83   10   14   16   16    0    9   13   13    0    0    0    0    5   16   25   25   11   18\n",
      "    21   24    4    4   15   22   48   56   56   56]\n",
      " [  91  177  282  338  113  238  310  323   88  124  138  139  124  243  294  319  249  524  708  781   11   21   28   29   48   63   64   65   22\n",
      "    24   24   25   17   18   19   19   66   69   69   69   59  165  311  431   62  129  326  442   25   70  127  153   11   30   42   56   16   43\n",
      "    45   45    0    7   45   51   49   51   51   51   24   38   38   38   20   20   20   20   42   83   93   93   43   44   44   45   44   68   68\n",
      "    68    8   15   15   15   16   23   25   40   18   38   56   57    4   10   21   26    0    5   18   24   36   53   71   78   31   39   40   40\n",
      "    50   83   92   92   14   18   18   18   27   95  207  264   10   18   22   24    2    3    3    4    8   15   20   22    8   18   27   41    1\n",
      "    10   12   12   11   12   12   12   16   31   62   80    8   18   32   38    0    1    3    3    0   14   22   22   17   20   22   22    5   18\n",
      "    31   42    7    8   11   14   32   53   53   53]\n",
      " [  56  193  346  583  125  256  371  472   99  128  140  140   91  257  324  373  159  487  830  951   14   25   25   25    7   12   37   58    9\n",
      "    13   17   23   15   18   19   19   65   68   68   68  120  180  306  401   11   13   18   25    7   22   50  149   26   64  114  220   11   39\n",
      "    46   46    2    7   39   47   42   46   48   48   25   39   39   39   12   17   18   18   44   79   89   92   16   17   19   19   13   14   14\n",
      "    14   20   25   26   26   30   38   42   49   23   33   51   55    5   12   18   22    0    2    4    6   44   74   79   79   26   36   39   39\n",
      "    63   78   95  100   17   24   33   34   19   48  100  121   10   43  105  134    2    2    2    2   21   21   21   21    3   11   26   45    0\n",
      "     0    0    1    0    0    0    0   18   36   57   79    5   13   20   23    0    2    3   11    0    0    0    0    9   11   11   11    8   18\n",
      "    29   30    3    6    9   14   25   54   54   54]\n",
      " [ 114  241  393  526  233  474  598  645   73  102  110  114  132  371  490  586  264  774 1349 1586   23   28   29   29   21   53   55   61    6\n",
      "     9   19   23   17   17   18   18   59   68   68   68  105  190  355  433   48   62  146  170    8   36   70  107   26   73  133  149   10   26\n",
      "    50   64    4   22  281  295  247  291  293  293   27   39   39   39   23   24   24   24   47   59   85   93   14   14   14   14   12   21   21\n",
      "    21    8   25   26   26   35   41   60   61    8   35   59   70    3   10   20   25    6   16   44   63   25   50   71   78   27   38   40   40\n",
      "    66  101  104  105   11   12   13   13   32   82  261  415   10   18   38   54   12   16   17   17   13   23   33   33    7   24   51   76    8\n",
      "    13  130  155   26   27   27   27   38   44   67   82    5   18   29   29    1    1    8   12    0    6   34   34    4   15   28   37    2    2\n",
      "     5    5    8   13   18   21    7   26   48   55]\n",
      " [  75  190  347  513  145  311  504  608   83  113  118  118  120  257  308  339  205  465  726  867   21   28   29   29   40   51   59   60   11\n",
      "    16   18   20   38   38   38   38   59   67   67   67   61  135  256  380   26   53   79   93   10   20   54   73   13   28   64   86   17   37\n",
      "    44   46    4    7   46   50   39   48   48   48   24   39   39   39    5   32   38   38   31   69   88   91   21   24   24   24   14   15   15\n",
      "    15    7   25   26   26   12   16   38   59   11   29   50   58    2    5   14   15    2    2    4    6   23   33   45   67   21   32   40   40\n",
      "    44   85   97   97    2   15   33   85    4   17   60   92    5   13   28   34    8    9    9   10    4   13   15   16    0    3   28   75    0\n",
      "     0    4    7    0    0    0    0   46   62   74   81    4   14   17   19    3    7   21   42    0    0    0    0   15   19   19   19    7    9\n",
      "    21   22    3    6   16   22   27   37   42   47]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (184,) sum: 92.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
      "\n",
      "  Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      "  Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 184  Y rows with populated labels: 3206  non zero cols: 8576\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      "filename : chembl_29_Y_tg_836_cols_224.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task7\n",
      " load_task_weights() - no weights file provided for y_task7, training_weights for all  224 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (224,)- total: 38227 - sum(axis=0):\n",
      "[1848 1158  386   47  323  134   40    5   84   60   27    4  139   88   31    9   44   24   17    0   25   15    0    0  104   33   11    0  171\n",
      "   61   11    1    1    0    0    0  114   39    5    2  290  197   50    3  295  211   64    2  279  154   78   17   22   10    2    0   61   33\n",
      "    7    0  247  116   48   19   81   27    3    0  139   63   15    2   84   45    6    0  121   46   10    0   52   24   11    0  122   53   34\n",
      "   13 2609 1928 1074  277  108   53   18    2 3875 2588 1082  228   96   61   17    3 1142  725  271   89 1131  616  211   48  101   50   21    3\n",
      " 1257  507   78   10  189  103   45   14  484  307  124   26  224  164   79   19   58   46   33   21    5    1    1    1   53   27   10    0   66\n",
      "   17    3    0  560  340  124   39   18    2    0    0   75   46    1    0    0    0    0    0  381  332   57    2   64   34    0    0   46    3\n",
      "    1    0    4    1    1    0  121   84   50    1   41    6    1    0  106   76   14    0  170  114    8    0  462  256   20    1  433  398   56\n",
      "    9  460  460  308   11  443  379  259   67    0    0    0    0   42   25    0    0  311  288    0    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 224) \n",
      " [[252 186  50 ...  35   0   0]\n",
      " [380 263 113 ...  59   0   0]\n",
      " [498 313 105 ...  63   0   0]\n",
      " [299 206  44 ...  78   0   0]\n",
      " [419 190  74 ...  53   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (224,)- total: 91904 - sum(axis=0):\n",
      "[1057 1753 2514 2840  649  839  933  968   39   63   96  119   73  124  181  205  147  166  173  190  188  198  213  213  201  275  297  308  600\n",
      "  715  765  775  122  124  124  124  271  346  380  383  118  212  359  406   77  161  308  369  145  274  350  411   93  105  113  115  122  150\n",
      "  176  183  250  381  449  478  120  174  198  201   75  150  198  212   39   80  119  125   63  138  174  184  159  188  201  212  170  244  264\n",
      "  285  905 1617 2472 3267  242  303  339  355 1379 2708 4216 5053  171  214  259  273  505  968 1421 1603  551 1119 1532 1693  241  297  327  344\n",
      "  733 1510 1943 2009  153  273  332  363  375  561  748  845  136  196  281  341   58   68   80   91  100  104  104  104   62   90  107  117   90\n",
      "  139  153  156  585  817 1023 1107  106  122  124  124   41   70  115  116  120  120  120  120  187  235  338  385   49   79  104  104  188  234\n",
      "  239  240  233  240  240  241   45   83  117  165  138  176  181  182   56   84  144  158  135  190  274  282  218  422  575  591   21   55  186\n",
      "  222    8    8  160  457   87  155  278  453  129  129  129  129   65   83   92   92   11   38  105  105] \n",
      "\n",
      "\n",
      " fold_neg: (5, 224) \n",
      " [[214 282 417 ...  16  24  24]\n",
      " [205 322 472 ...   5  23  23]\n",
      " [203 390 598 ...   4  16  16]\n",
      " [161 257 417 ...   5  16  16]\n",
      " [274 502 610 ...   8  26  26]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (224,) sum: 109.0\n",
      " [1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "  Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      "  Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 224  Y rows with populated labels: 2320  non zero cols: 7597\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      "filename : chembl_29_Y_tg_1005_cols_148.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task8\n",
      " load_task_weights() - no weights file provided for y_task8, training_weights for all  148 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (148,)- total: 45065 - sum(axis=0):\n",
      "[2742  763  271   77   62   18    4    2  389  117   13    2 2345 1740  714  104   43   19    4    0  385  262  166  127  486  391  215   56  959\n",
      "  694  254   23  374  313  175   35  314  209  116   56  206   47    0    0  393  222   90    9  140   45    7    1 5011 2970 1201  199  583  305\n",
      "   14    0   39   24   10    0   69   46   35   16  692  542  349   92   86   51   20    0   81   33   19    5   78   48   15    2  588  466  110\n",
      "    1   89   29   16    7  362  230   75    0   58   20    9    0  328  276  222  152  129  128  127  123  501  262   31    4  172   78   35    8\n",
      "  233  169  115   29 1495 1118  452   24  288   60   21    8 2824 2211 1175  234  444  238   86   20   43   14    2    0   88   66   32    2  103\n",
      "    1    0    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 148) \n",
      " [[ 472  146   79   26   10    5    3    1   31    4    1    0  570  413  178   17   10    4    2    0   20    4    0    0   11    4    0    0  244\n",
      "   202   94    6   40   32   13    1   26    9    1    0    1    0    0    0   77   27   15    2    6    3    1    0  629  335   90    7   41   12\n",
      "     0    0    8    5    3    0   19    7    5    4   79   30    2    0   18    8    0    0   14    1    1    0   35   27    9    1   71   53    7\n",
      "     0   18    4    1    1   67   49   12    0    5    0    0    0   60   48   41   20    0    0    0    0   25   16    9    1   31   25   17    4\n",
      "    50   46   31    0  198  143   57    6   54    1    0    0  570  442  240   50   83   43   13    0    4    3    1    0    0    0    0    0    6\n",
      "     0    0    0]\n",
      " [ 643  177   37    6   25    6    0    0  102   40    6    1  321  213   68   20    5    1    0    0   18    7    0    0   89   55    8    3  159\n",
      "   101   21    0   67   47   19    1   68   62   39   19   18    7    0    0   73   27    3    1  133   42    6    1 1736 1068  451   76  158   73\n",
      "     4    0    7    7    3    0    9    6    0    0   30    7    2    2   17   11    6    0    9    3    3    0    8    1    0    0  221  173   48\n",
      "     1   21    8    5    2   74   48   10    0    0    0    0    0   46   32   16    1    1    0    0    0   27   16    1    0   28    1    0    0\n",
      "    41   25   18    6  315  220   83    1   62    9    2    0  505  367  189   33  102   37   14    3    2    0    0    0    8    0    0    0   29\n",
      "     0    0    0]\n",
      " [ 477  112   44   10   15    3    0    0   97   47    4    1  611  482  186   34   19    9    1    0  253  196  143  125  119   95   62    3  130\n",
      "    84   14    0   54   37   11    0   67   24    2    0    9    0    0    0   73   45   13    0    0    0    0    0  920  675  359   63   98   62\n",
      "     3    0    8    7    1    0   19   14   14    1   93   77   44   14   18   12    5    0   20    6    1    1   11    6    0    0   47   37   23\n",
      "     0   15    8    7    3   81   40   14    0   34    4    0    0   74   62   62   58  127  127  127  123    1    0    0    0   41   16    3    0\n",
      "    37   22   17    8  250  198   82    3   65   19    2    0  426  359  205   31   84   50   18    6   14    6    0    0    0    0    0    0    3\n",
      "     0    0    0]\n",
      " [ 749  218   67   20    3    0    0    0   68   11    0    0  487  351  145   10    3    2    0    0   53   42   23    2  254  228  142   50  167\n",
      "   107   43   14   70   62   38    6   11   10    3    2   13    3    0    0   79   40    7    0    1    0    0    0  871  450  158   37   97   62\n",
      "     3    0   15    5    3    0    1    1    0    0  298  253  144   47   20   13    6    0   21   10    5    1    3    0    0    0  182  151    6\n",
      "     0   15    4    3    1   62   42   15    0    6    5    4    0   71   57   53   36    1    1    0    0  275  154   16    3   47   18    6    2\n",
      "    49   35   20    2  403  291  106    4   43    8    0    0  731  596  312   64   77   50   15    1    1    0    0    0   80   66   32    2   49\n",
      "     1    0    0]\n",
      " [ 401  110   44   15    9    4    1    1   91   15    2    0  356  281  137   23    6    3    1    0   41   13    0    0   13    9    3    0  259\n",
      "   200   82    3  143  135   94   27  142  104   71   35  165   37    0    0   91   83   52    6    0    0    0    0  855  442  143   16  189   96\n",
      "     4    0    1    0    0    0   21   18   16   11  192  175  157   29   13    7    3    0   17   13    9    3   21   14    6    1   67   52   26\n",
      "     0   20    5    0    0   78   51   24    0   13   11    5    0   77   77   50   37    0    0    0    0  173   76    5    0   25   18    9    2\n",
      "    56   41   29   13  329  266  124   10   64   23   17    8  592  447  229   56   98   58   26   10   22    5    1    0    0    0    0    0   16\n",
      "     0    0    0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (148,)- total: 104361 - sum(axis=0):\n",
      "[6762 8795 9264 9481  209  253  267  269 1517 1792 1896 1907  666 1292 2280 2889   56   81   96  100  121  242  338  382  253  362  529  682  390\n",
      "  657 1097 1328  161  224  362  502  105  243  336  396  166  351  392  392  163  332  463  544  145  244  282  287 1165 3232 5002 6001  196  493\n",
      "  785  799  108  126  140  148   31   54   65   97   87  241  433  681   19   54   85  105  109  158  172  186  114  143  175  188   22  149  505\n",
      "  614   68  129  142  150   92  224  379  454   69  107  117  126  135  203  276  345   18   19   21   40   32  277  508  535   58  167  210  238\n",
      "   47  111  165  251   80  458 1124 1552   95  323  362  375  221  869 1903 2841  113  320  472  538   64   93  105  107   32   54   88  118  252\n",
      "  354  355  355] \n",
      "\n",
      "\n",
      " fold_neg: (5, 148) \n",
      " [[1171 1507 1575 1630   49   54   56   58  253  281  284  285  128  287  522  683   11   18   20   22   29   44   48   48   36   43   47   47   38\n",
      "    80  188  276   26   35   54   66   23   45   53   54   13   17   17   17   38   86   98  111    5    9   11   12  191  486  731  814    9   39\n",
      "    51   51   24   29   31   33   18   30   32   61   25   74  102  104    4   14   22   22   24   37   37   38    4   12   30   38    0   18   64\n",
      "    71   16   31   34   34   12   30   67   79   20   25   25   25   36   51   58   79    3    3    3    3    3   13   20   28   17   23   31   44\n",
      "     6   10   25   56   16   72  158  209   23   76   77   77   47  178  380  570   24   64   94  107   17   18   20   21    0    0    0    0   58\n",
      "    64   64   64]\n",
      " [1481 1947 2086 2121   33   52   58   58  512  576  610  615  129  240  384  432   11   15   16   16   18   28   35   35   42   76  115  120  108\n",
      "   166  246  267   28   49   77   95   25   32   55   75   13   17   18   18   38   84  108  110  111  205  241  245  367 1040 1658 2030   88  179\n",
      "   248  252   24   24   28   30   10   13   19   19   26   49   54   54    4   10   15   21   25   31   31   34   30   37   38   38    4   54  179\n",
      "   226   11   24   27   29   19   45   83   93    3    3    3    3   48   65   81   95    0    1    1    1    6   22   37   38   19   48   49   50\n",
      "    11   27   34   46   13  108  245  327   15   68   75   77   43  176  354  510   38  103  126  137   18   20   20   20   16   24   24   24   30\n",
      "    59   59   59]\n",
      " [1287 1646 1713 1760   40   52   55   55  180  229  272  275  123  260  519  670    9   19   27   28   58  115  168  191   75   99  131  189   69\n",
      "   117  187  201   21   38   64   75   15   58   80   82   68   77   77   77   33   61   93  106   20   20   20   20  144  403  719 1015   39   75\n",
      "   134  137   22   23   29   30    1    6    6    6   12   28   60   90    3    9   16   21   19   33   38   38   24   29   34   34   13   23   37\n",
      "    60   12   19   20   24   12   53   79   93   30   60   63   63   20   38   38   42   14   14   15   34    2    3    3    3    4   35   48   51\n",
      "    16   31   36   45    9   61  177  256   10   56   73   75   32  103  257  430   28   63   95  107    5   13   19   19    0    0    0    0   11\n",
      "    14   14   14]\n",
      " [1497 2033 2164 2212   41   44   44   44  293  351  362  362   96  233  439  574    8    9   11   11    6   17   36   57   52   83  169  256   86\n",
      "   146  210  239   15   23   47   79    5    6   13   14   12   22   25   25   34   73  106  113    5    6    6    6  229  651  943 1064   25   60\n",
      "   119  122    9   20   22   25    1    1    2    2   16   65  174  262    1    8   15   21   18   30   35   39   36   38   38   38    3   34  179\n",
      "   185   17   28   29   31   37   57   84   99   12   13   14   18   28   44   48   65    1    1    2    2   13  134  272  285   12   46   58   62\n",
      "    10   24   39   57   30  142  327  429   34   69   77   77   49  214  496  742   10   37   72   86   11   12   12   12    0   14   48   78  127\n",
      "   175  176  176]\n",
      " [1326 1662 1726 1758   46   51   54   54  279  355  368  370  190  272  416  530   17   20   22   23   10   38   51   51   48   61   67   70   89\n",
      "   148  266  345   71   79  120  187   37  102  135  171   60  218  255  255   20   28   58  104    4    4    4    4  234  652  951 1078   35  140\n",
      "   233  237   29   30   30   30    1    4    6    9    8   25   43  171    7   13   17   20   23   27   31   37   20   27   35   40    2   20   46\n",
      "    72   12   27   32   32   12   39   66   90    4    6   12   17    3    5   51   64    0    0    0    0    8  105  176  181    6   15   24   31\n",
      "     4   19   31   47   12   75  217  331   13   54   60   69   50  198  416  589   13   53   85  101   13   30   34   35   16   16   16   16   26\n",
      "    42   42   42]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (148,) sum: 80.0\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0.]\n",
      "\n",
      "  Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      "  Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 148  Y rows with populated labels: 3434  non zero cols: 6827\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      "filename : chembl_29_Y_tg_1028_cols_344.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task9\n",
      " load_task_weights() - no weights file provided for y_task9, training_weights for all  344 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (344,)- total: 110249 - sum(axis=0):\n",
      "[ 439  342  217   81 1277  760  396  108 1700 1397  956  408   99    1    0    0  157    0    0    0  121    1    0    0  832  585  298   70  579\n",
      "  434  263   71 5382 3822 2049  506  845  505  248   65  461  234   61    9 1471 1242  944  560   67   17    9    0   44    5    0    0 2099 1536\n",
      "  980  410 1221  990  601  256  993  701  406   76 1539 1189  814  323  445  351  199   45  356  197   97   21   13    6    0    0 1519 1210  804\n",
      "  417   30    1    0    0  603  491  223   15  740  318  111   22 3153 2584 1456  421  528  369  177   35  206  128   75   20 2107 1835 1354  677\n",
      "  269  207  123   37  170   88   44    5 1213  960  516  204  250  106   58   20  328  117   48    8  328  248  159   69 1369  808  297   76  238\n",
      "  159   50    7  101   65   32   18  374  143   68   38  316   99   30    1  103   50   16    3  222   78   21    5  348  228  161   88  359  269\n",
      "  141   43  587  491  298  148  216  128   40   19  162   80   50   12   73   37   14    4  321  205   99   22  149   82   28    0   84   27    8\n",
      "    2 2004 1500  940  344   69   56    6    3   62   33    5    1  118   85   42   10  555  280   51    2  198   66   36   30  325  231  143   76\n",
      "   44   15    2    1   31    9    0    0  119   72   35    9  146  102   23    1  303  228  142   46   26    4    0    0  302  206   90    2   82\n",
      "   35   11    2 1017  744  314   22  694  633  456   85 2654 2272 1703  991   73   27    8    1   69   49   24    6   50   36   17    6  134  116\n",
      "   54    0  201  122   79   31  623  329  128   21  183  128   79   38   35   12    0    0   40    7    0    0  338  293  198   83  151  109   64\n",
      "   16   30   24   19    3  242  200    0    0  106   83   33    6  685  614   91   15   50   25   13    1   47   23    1    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 344) \n",
      " [[ 43  26  10 ...   0   0   0]\n",
      " [ 96  68  37 ...   0   0   0]\n",
      " [ 99  83  52 ...   1   0   0]\n",
      " [ 91  77  59 ...   0   0   0]\n",
      " [110  88  59 ...  22   1   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (344,)- total: 213195 - sum(axis=0):\n",
      "[ 233  347  472  608 1637 2198 2564 2853  285  618 1063 1603  889  986  987  987  831  986  986  986  867  983  984  984 1273 1521 1808 2036  227\n",
      "  384  555  733 1681 3177 4949 6489  777 1164 1420 1601  769 1039 1212 1261  507  739 1033 1416  255  305  313  322  401  440  444  444 1491 2108\n",
      " 2663 3226  268  531  921 1265  549  855 1151 1482  743 1121 1500 1992  147  255  407  561  447  609  710  786  706  713  719  719  219  573  983\n",
      " 1371  154  183  184  184  137  249  512  717 1442 1867 2074 2160  929 1542 2679 3713  165  324  516  658  286  364  420  475  373  650 1137 1795\n",
      "  193  257  341  427  299  386  432  471  676  954 1399 1707  208  361  407  445  155  367  436  477  114  217  305  395 1324 1940 2446 2674  102\n",
      "  196  306  349   81  116  147  161 1266 1494 1569 1599 1307 1518 1591 1620  360  413  447  460  951 1095 1152 1168  277  398  465  538  155  246\n",
      "  374  479  241  339  532  679  111  199  288  309  239  321  351  404  174  213  236  259  105  224  330  407  197  267  321  349  195  254  273\n",
      "  279  597 1124 1686 2282   49   62  112  115   44   73  101  105   54   88  131  163  332  607  836  885  346  512  544  553  235  335  425  493\n",
      "  210  260  275  276  281  313  322  322   92  139  176  202   46   95  174  196  114  189  275  371  128  152  158  158   94  223  339  427   77\n",
      "  124  148  157  134  407  837 1129   38  100  278  650  368  763 1338 2049  175  221  240  247   54   76  101  116   81   95  114  124   47   67\n",
      "  129  183   84  174  217  266  265  559  760  868   52  116  165  206   78  101  113  113  130  164  171  171   59  104  199  314   52   94  139\n",
      "  187  188  194  199  215    4   46  163  163   25   48   98  125   73  132  268  341   61   86   98  110   84  108  130  131] \n",
      "\n",
      "\n",
      " fold_neg: (5, 344) \n",
      " [[ 73  94 110 ...   8   8   8]\n",
      " [ 47  84 115 ...   1   1   1]\n",
      " [ 39  57  88 ...  19  20  20]\n",
      " [ 30  45  63 ...   6   6   6]\n",
      " [ 44  67  96 ...  74  95  96]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (344,) sum: 226.0\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "  Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 344  Y rows with populated labels: 7852  non zero cols: 25179\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      "filename : chembl_29_Y_tg_1031_cols_72.npy  type: <class 'scipy.sparse.csr.csr_matrix'> . . .\n",
      " load_task_weights() - filename: None label: y_task10\n",
      " load_task_weights() - no weights file provided for y_task10, training_weights for all  72 classes set to 1 \n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      " All y values are 0, 1, or -1.\n",
      "\n",
      " tasks_class.aggregation_weight WAS NOT passed \n",
      " min_samples_class: 1\n",
      "\n",
      " Class fold counts: \n",
      "---------------------\n",
      " fold_pos (72,)- total: 18631 - sum(axis=0):\n",
      "[ 595  244   46    2 1032  295   50    8  766  176   35    8 1925  634  129   42 1259 1057  656  134 1214  677  264  101  769  592  260   33  387\n",
      "   98   15    0   99   20    2    1   21    9    0    0  240  148   59    4  264  211  102   31  100   25    2    0  793  629  380   25  140   90\n",
      "   46    9  830  492  192   27   81   31    6    1   16    2    0    0] \n",
      "\n",
      "\n",
      " fold_pos: (5, 72) \n",
      " [[120  33   6   0 188  66   9   1 126  28   7   1 346  92  18   5 231 195 111  36 283 131  40  19 214 158  80  16  86  29   3   0  18   1   0   0\n",
      "    3   0   0   0  32  12   5   0  40  31  15   4  16   1   1   0  77  50  32   4  41  38  22   4 208 152  58   3  15   2   0   0   0   0   0   0]\n",
      " [139  52  11   1 225  55   9   2 166  30   8   1 337 126  25   7 432 387 254  33 335 193  92  51 123  91  27   3  67  19   5   0  27   5   1   0\n",
      "    9   5   0   0  51  26  10   1  53  39  10   4  27   7   1   0 252 207 125  13  20   4   4   0  96  39  15   8   7   4   2   1  10   2   0   0]\n",
      " [103  48   9   1 205  65  14   3 126  32   7   3 443 126  17   4 160 129  73  11 136  65  20   6 220 191  73   7  68  16   4   0  13   0   0   0\n",
      "    3   1   0   0  50  35   8   0  53  35  15   5  20   6   0   0 150 134  88   2  23  14   4   3 162  92  37   2  31  16   4   0   1   0   0   0]\n",
      " [102  49  10   0 171  41   8   1 162  41   3   0 433 180  24   3 208 174 107  37 289 185  67  16  52  18  10   1  55  11   2   0  27  11   1   1\n",
      "    1   0   0   0  51  27  13   2  55  48  19   9  13   2   0   0 101  62  35   5  28  24  12   1 170  97  45  13   7   1   0   0   1   0   0   0]\n",
      " [131  62  10   0 243  68  10   1 186  45  10   3 366 110  45  23 228 172 111  17 171 103  45   9 160 134  70   6 111  23   1   0  14   3   0   0\n",
      "    5   3   0   0  56  48  23   1  63  58  43   9  24   9   0   0 213 176 100   1  28  10   4   1 194 112  37   1  21   8   0   0   4   0   0   0]] \n",
      "\n",
      "\n",
      "\n",
      " fold_neg (72,)- total: 107922 - sum(axis=0):\n",
      "[2497 2888 3055 3105 4162 4871 5081 5133 3600 4213 4344 4373 5694 6972 7433 7533   58  298  699 1221  803 1371 1790 1953  173  366  703  927 2314\n",
      " 2615 2685 2701  471  553  571  572  326  342  351  351   78  175  264  319   56  110  219  290  237  310  333  335  149  322  582  771   48  104\n",
      "  148  186  229  584  881 1046   47   98  123  128  134  148  150  150] \n",
      "\n",
      "\n",
      " fold_neg: (5, 72) \n",
      " [[ 592  681  702  707  878  981 1017 1024  725  830  851  856 1079 1337 1393 1406   15   58  142  217  194  360  451  472   58  115  198  262  495\n",
      "   553  572  575   98  115  116  116   65   68   68   68   32   52   59   64   24   33   49   60   51   66   66   67   27   56   74   97    0    3\n",
      "    19   37   43  105  199  254    9   22   24   24   30   30   30   30]\n",
      " [ 419  511  544  558  774  944  986  995  630  772  790  797 1076 1289 1385 1410   12   68  201  422  182  327  430  471   13   52  116  140  384\n",
      "   439  453  460   88  112  116  117   57   64   69   69   13   39   55   64   10   24   53   59   41   60   66   67   42   90  176  226   16   32\n",
      "    32   36   73  136  160  167    6    9   11   12   22   30   32   32]\n",
      " [ 481  541  575  586  922 1063 1109 1123  703  798  823  829 1306 1609 1713 1731   13   48  104  166  144  216  261  275   28   58  176  239  414\n",
      "   468  478  481   97  110  111  111   70   72   73   73   14   29   56   64   12   30   50   60   48   61   67   67   13   30   81  135   14   24\n",
      "    34   35   43  113  168  203   18   33   45   49   30   31   31   31]\n",
      " [ 496  571  610  621  789  919  950  958  721  851  887  890  999 1256 1398 1421    9   46  113  183  162  279  401  452   48   83   91  100  468\n",
      "   518  528  531   88  105  114  114   68   70   70   70   11   39   53   64    9   17   46   56   54   65   67   67   40   81  110  132    8   17\n",
      "    29   41   37  114  166  198   10   16   17   17   26   27   27   27]\n",
      " [ 509  584  624  633  799  964 1019 1033  821  962  993 1001 1234 1481 1544 1565    9   78  139  233  121  189  247  283   26   58  122  186  553\n",
      "   637  654  654  100  111  114  114   66   68   71   71    8   16   41   63    1    6   21   55   43   58   67   67   27   65  141  181   10   28\n",
      "    34   37   33  116  188  224    4   18   26   26   26   30   30   30]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  task_weights.aggregation_weight: \n",
      "------------------------------------\n",
      " fold_pos >= 1 and  fold_neg >= 1\n",
      " shape: (72,) sum: 52.0\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "  Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      "  Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  82933     # Features per Sample: 32000 \n",
      "Y file : # Samples :  82933     # Labels per Sample  : 72  Y rows with populated labels: 1444  non zero cols: 3563\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "--------------------------------------------------\n",
      "Chembl_23 Create complete\n",
      "-------------------------------------------------- \n",
      "\n",
      " dataloader preparation - set number of batches per weight training epoch to: 2663\n",
      " dataloader preparation - set number of batches per policy training epoch to: 2663\n",
      " dataloader preparation - set number of batches per validation to           : 648\n",
      "\n",
      " trainset.y_class                                   :  [(340803, 472), (340803, 624), (340803, 688), (340803, 192), (340803, 620), (340803, 184), (340803, 224), (340803, 148), (340803, 344), (340803, 72)] \n",
      " trainset1.y_class                                  :  [(340803, 472), (340803, 624), (340803, 688), (340803, 192), (340803, 620), (340803, 184), (340803, 224), (340803, 148), (340803, 344), (340803, 72)] \n",
      " trainset2.y_class                                  :  [(340803, 472), (340803, 624), (340803, 688), (340803, 192), (340803, 620), (340803, 184), (340803, 224), (340803, 148), (340803, 344), (340803, 72)] \n",
      " valset.y_class                                     :  [(82933, 472), (82933, 624), (82933, 688), (82933, 192), (82933, 620), (82933, 184), (82933, 224), (82933, 148), (82933, 344), (82933, 72)]  \n",
      "\n",
      "                                Total                :  1105342 \n",
      "\n",
      "\n",
      " Training dataset :\n",
      "--------------------\n",
      "  Size of training set 0 (warm up)                   :  340803 \n",
      "  Number of batches in training 0 (warm up)          :  2663 \n",
      "  Size of training set 1 (network parms)             :  340803 \n",
      "  Number of batches in training 1 (network parms)    :  2663 \n",
      "  Size of training set 2 (policy weights)            :  340803 \n",
      "  Number of batches in training 2 (policy weights)   :  2663 \n",
      "  training set num of positive                       :  18631 \n",
      "  training set num of negative                       :  107922 \n",
      "  task_weights_list[0].aggregation_weight sum        :  199.0\n",
      "\n",
      "\n",
      " Validation dataset :\n",
      "----------------------\n",
      "  Rows in dataset                                    : 82933\n",
      "  Number of batches in dataset                       : 648\n",
      "  validation set num of positive                     : 18631\n",
      "  validation set num of negative                     : 107922\n",
      "  task_weights_list[0].aggregation_weight sum        : 199.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dldrs = init_dataloaders(opt, verbose = False)\n",
    "dldrs = init_dataloaders_by_fold_id(opt, verbose = True)\n",
    "disp_dataloader_info(dldrs)\n",
    "# folding  = np.load(os.path.join('/mnt/f/Chembl29', opt['dataload']['folding']))\n",
    "# print(folding.shape)\n",
    "# print(folding.max())\n",
    "# for i in zip(dldrs.valset.num_pos, dldrs.valset.num_neg, dldrs.trainset0.num_pos, dldrs.trainset0.num_neg):\n",
    "#     print(f\" {i[0]:4d}  {i[1]:4d}    trianing: {i[2]:4d}   {i[3]:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff214af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:35:57.930979Z",
     "start_time": "2022-08-11T12:35:57.881120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[472, 624, 688, 192, 620, 184, 224, 148, 344, 72]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dldrs.valset.class_output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bfa8d",
   "metadata": {},
   "source": [
    "## Setup Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62717fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:36:02.652331Z",
     "start_time": "2022-08-11T12:35:59.798578Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:16.229028Z",
     "iopub.status.busy": "2022-01-07T22:44:16.227544Z",
     "iopub.status.idle": "2022-01-07T22:44:16.659397Z",
     "shell.execute_reply": "2022-01-07T22:44:16.658348Z",
     "shell.execute_reply.started": "2022-01-07T22:44:16.228966Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "############# CREATE THE ENVIRONMENT #############\n",
      "##################################################\n",
      " device is  cuda:0\n",
      "--------------------------------------------------\n",
      " SparseChem_Backbone  Ver: 1.0 Init() Start \n",
      "-------------------------------------------------- \n",
      "\n",
      " layer config        : [1, 1] \n",
      " skip residual layers: False   skip hidden layers  : False\n",
      " SparseChem_BackBone() Input Layer  - Input: 32000  output: 4000  non-linearity:<class 'torch.nn.modules.activation.ReLU'>\n",
      " Hidden layer 0 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Final Hidden layer 0 : Input size: 4000   output size:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Module List \n",
      "--------------------------------------------------\n",
      " Initialize weights \n",
      "-------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " SparseChem Backbone -- Final configuration(2) : \n",
      "                     self.blocks: <class 'torch.nn.modules.container.ModuleList'>  len:2\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "\n",
      "  Input_Layer  type:<class 'torch.nn.modules.container.Sequential'>  \n",
      "----------------------------------------------------------------------\n",
      "self.Input_layer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layers/Blocks    : <class 'torch.nn.modules.container.ModuleList'>   len:2 \n",
      "Resdiual layers  : <class 'torch.nn.modules.container.ModuleList'>   len:2\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 1  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 1  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 2  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 2  type:<class 'NoneType'> \n",
      " None\n",
      "\n",
      "\n",
      "\n",
      " SparseChem_Backbone Init() End \n",
      "----------------------------------------------------\n",
      "* SparseChemEnv environment successfully created\n",
      "---------------------------------------------------- \n",
      "\n",
      " \n",
      "\n",
      "SparseChemEnv  Configuration       \n",
      "---------------------------------------- \n",
      "\n",
      "----------------\n",
      "networks       :\n",
      "----------------\n",
      " {'mtl-net': MTL3(\n",
      "  (backbone): SparseChem_Backbone(\n",
      "    (Input_Layer): Sequential(\n",
      "      (linear): SparseLinear(in_features=32000, out_features=4000, bias=True)\n",
      "      (non_linear): ReLU()\n",
      "      (dropout): Dropout(p=0.8, inplace=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (1): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (residuals): ModuleList(\n",
      "      (0): None\n",
      "      (1): None\n",
      "    )\n",
      "  )\n",
      "  (task1_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=472, bias=True)\n",
      "  )\n",
      "  (task2_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=624, bias=True)\n",
      "  )\n",
      "  (task3_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=688, bias=True)\n",
      "  )\n",
      "  (task4_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=192, bias=True)\n",
      "  )\n",
      "  (task5_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=620, bias=True)\n",
      "  )\n",
      "  (task6_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=184, bias=True)\n",
      "  )\n",
      "  (task7_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=224, bias=True)\n",
      "  )\n",
      "  (task8_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=148, bias=True)\n",
      "  )\n",
      "  (task9_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=344, bias=True)\n",
      "  )\n",
      "  (task10_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=72, bias=True)\n",
      "  )\n",
      ")}\n",
      "\n",
      "----------------\n",
      "optimizers     :\n",
      "----------------\n",
      " {}\n",
      "\n",
      "----------------\n",
      "schedulers     :\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "environ = init_environment(ns, opt, is_train = True, policy_learning = False, display_cfg = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb86f4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T10:36:12.133969Z",
     "start_time": "2022-07-28T10:36:12.104681Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.optimizers['weights'].param_groups[0]\n",
    "# print(environ.print_configuration())\n",
    "# print(environ.networks['mtl-net']._arch_parameters)\n",
    "# print()\n",
    "# pp.pprint(environ.networks['mtl-net'].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7feeb05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T10:36:12.186759Z",
     "start_time": "2022-07-28T10:36:12.135366Z"
    }
   },
   "outputs": [],
   "source": [
    "# d = environ.get_task_logits(0,verbose=True)\n",
    "# print(d.data)\n",
    "# print(type(d))\n",
    "# print(d.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738062",
   "metadata": {},
   "source": [
    "## Initiate / Resume Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba16a95c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:36:03.505677Z",
     "start_time": "2022-08-11T12:36:03.456849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt['train']['which_iter'] :  warmup\n",
      "##################################################\n",
      "######## Initiate Training from scratch  #########\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "check_for_resume_training(ns, opt, environ, epoch = 0 , iter = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964c4940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T10:36:12.295870Z",
     "start_time": "2022-07-28T10:36:12.273549Z"
    }
   },
   "outputs": [],
   "source": [
    "# loaded_metrics\n",
    "# ns.val_metrics\n",
    "# ns.best_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73116275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T10:36:12.318851Z",
     "start_time": "2022-07-28T10:36:12.297146Z"
    }
   },
   "outputs": [],
   "source": [
    "# a = environ.load_checkpoint('warmup_ep_40_seed_0088', path = '../experiments/AdaSparseChem/50x6_0304_1549_plr0.01_sp0.0001_sh0.01/')\n",
    "# current_iter = environ.load_checkpoint('latest_weights_policy')\n",
    "# priRant('Evaluating the snapshot saved at %d iter' % current_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278d6d1",
   "metadata": {},
   "source": [
    "## Warmup Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa99797",
   "metadata": {},
   "source": [
    "### Warmup Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7ce6a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:36:08.121400Z",
     "start_time": "2022-08-11T12:36:08.088485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model optimizers defined . . . policy_learning: {policy_learning}\n",
      " Model schedulers defined . . . policy_learning: {policy_learning}\n",
      " Metrics CSV file header written . . . \n",
      " Update weights -- fix alpha\n",
      " Model initializations complete . . . \n"
     ]
    }
   ],
   "source": [
    "model_initializations(ns, opt, environ, phase = 'update_weights', policy_learning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33100539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:36:09.676047Z",
     "start_time": "2022-08-11T12:36:09.309063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training preparation: - check for CUDA - cuda available as device id: [0]\n",
      " training preparation: - set print_freq to                                 : 2663 \n",
      " training preparation: - set number of batches per weight training epoch to: 2663\n",
      " training preparation: - set number of batches per policy training epoch to: 2663\n",
      " training preparation: - set number of batches per validation to           : 648\n",
      " training preparation complete . . .\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    folder: 4000x2_0811_1435_lr0.001_do0.8\n",
      "    layers: 2 [4000, 4000] \n",
      "    \n",
      "    first dropout          : 0.8\n",
      "    middle dropout         : 0.8\n",
      "    last dropout           : 0.8\n",
      "    diff_sparsity_weights  : False\n",
      "    skip_layer             : 0\n",
      "    is_curriculum          : False\n",
      "    curriculum_speed       : 3\n",
      "    \n",
      "    task_lr                : 0.001\n",
      "    backbone_lr            : 0.001\n",
      "    decay_lr_rate          : 0.3\n",
      "    decay_lr_freq          : 10.0\n",
      "    \n",
      "    policy_lr              : 0.001\n",
      "    policy_decay_lr_rate   : 0.75\n",
      "    policy_decay_lr_freq   : 50\n",
      "    lambda_sparsity        : 0.02\n",
      "    lambda_sharing         : 0.01\n",
      "    lambda_tasks           : 1\n",
      "    \n",
      "    Gumbel init_temp       : 4\n",
      "    Gumbel decay_temp      : 0.965\n",
      "    Gumbel decay_temp_freq : 16\n",
      "    Logit init_method      : random\n",
      "    Logit init_neg_logits  : None\n",
      "    Logit hard_sampling    : False\n",
      "    Warm-up epochs         : 10\n",
      "    training epochs        : 250\n",
      "    Data split ratios      : [0.725, 0.225, 0.05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_initializations(ns, opt, environ, dldrs, phase='update_weights', warmup = True)\n",
    "# print('-'*80)\n",
    "# disp_info_1(ns, opt, environ)\n",
    "print('-'*80)\n",
    "print(environ.disp_for_excel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73de409d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:45:01.929833Z",
     "start_time": "2022-08-01T14:45:01.886327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "ns.eval_iters = 10\n",
    "ns.trn_iters_w = 64\n",
    "print(ns.eval_iters )\n",
    "print(ns.trn_iters_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92380a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T12:36:17.729589Z",
     "start_time": "2022-08-11T12:36:17.697386Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  2 - Run epochs 1 to 2\n",
      "---------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# environ.display_trained_policy(ns.current_epoch,out=sys.stdout)\n",
    "# ns.check_for_improvment_wait = 0\n",
    "ns.warmup_epochs = 2\n",
    "print_heading(f\" Last Epoch: {ns.current_epoch}   # of warm-up epochs to do:  {ns.warmup_epochs} - Run epochs {ns.current_epoch+1} to {ns.current_epoch + ns.warmup_epochs}\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd29e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T13:15:28.775150Z",
     "start_time": "2022-08-11T12:36:18.733285Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  2 - Run epochs 1 to 2\n",
      "---------------------------------------------------------------------- \n",
      "\n",
      " Warmup Epoch 1/2:  61%|██████████████████████████████████▊                      | 1629/2663 [10:38<03:05,  5.58it/s, curr_iter=1629, Loss=2.2205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   2.1847   2.070e-03   1.643e-05    2.1868 |   0.00000   0.48258   0.63846   0.73509   0.62571   0.67832 |   2.3640   1.084e-03   8.602e-06    2.3651 |1153.4 |\n",
      "Previous best_epoch:     1   best iter:  2663   best_accuracy: 0.62256    best ROC auc: 0.71330\n",
      "Previous best_epoch:     2   best iter:  5326   best_accuracy: 0.63846    best ROC auc: 0.73509\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " save warmup checkpoint  to :  model_warmup_ep_2\n",
      " save warmup metrics to     :  metrics_warmup_ep_2.pickle\n",
      "[Final] ep:2  it:5326 -  Total Loss: 2.3651     \n",
      "Task: 2.3640   Sparsity: 1.08359e-03    Sharing: 8.60202e-06 \n",
      "\n",
      " ep:    2   softmax      s        softmax      s        softmax      s        softmax      s        softmax      s        softmax      s        softmax      s        softmax      s        softmax      s        softmax      s        \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.5004    0.4996  1    0.5003    0.4997  1    0.4998    0.5002  0    0.5008    0.4992  1    0.5000    0.5000  1    0.4995    0.5005  0    0.5000    0.5000  1    0.5005    0.4995  1    0.5003    0.4997  1    0.5002    0.4998  1\n",
      "  1    0.5000    0.5000  1    0.4997    0.5003  0    0.5003    0.4997  1    0.4994    0.5006  0    0.5003    0.4997  1    0.4995    0.5005  0    0.5005    0.4995  1    0.5002    0.4998  1    0.5001    0.4999  1    0.5006    0.4994  1\n",
      "\n",
      "\n",
      " ep:    2   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.0004   -0.0014  1    0.0024    0.0014  1   -0.0005    0.0001  0    0.0019   -0.0013  1    0.0008    0.0008  1   -0.0002    0.0018  0    0.0002    0.0001  1    0.0007   -0.0012  1   -0.0001   -0.0014  1    0.0006   -0.0000  1\n",
      "  1    0.0011    0.0009  1   -0.0009    0.0002  0    0.0010   -0.0003  1   -0.0012    0.0013  0   -0.0016   -0.0026  1   -0.0023   -0.0005  0   -0.0000   -0.0022  1    0.0001   -0.0008  1    0.0000   -0.0002  1    0.0009   -0.0014  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warmup_phase(ns,opt, environ, dldrs, verbose = False, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c6e1c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T13:25:16.005711Z",
     "start_time": "2022-08-11T13:25:15.957456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'auc_pr': 0.6257101393279701,\n",
      "    'avg_prec_score': 0.6384607356703277,\n",
      "    'bceloss': 0.4825799150894597,\n",
      "    'f1_max': 0.6783152058445592,\n",
      "    'kappa': 0.20420264749386563,\n",
      "    'kappa_max': 0.43279052734175666,\n",
      "    'logloss': 4.4676439319816735e-06,\n",
      "    'p_f1_max': 0.3924611335648817,\n",
      "    'p_kappa_max': 0.4617277694425509,\n",
      "    'roc_auc_score': 0.7350875468856584,\n",
      "    'sc_loss': 0.0036481139454649053}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(environ.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "952bb1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:20:29.338273Z",
     "start_time": "2022-07-28T11:20:23.964832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>▁█</td></tr><tr><td>avg_prec_score</td><td>▁█</td></tr><tr><td>bceloss</td><td>█▁</td></tr><tr><td>best_accuracy</td><td>▁█</td></tr><tr><td>best_epoch</td><td>▁█</td></tr><tr><td>best_iter</td><td>▁█</td></tr><tr><td>best_roc_auc</td><td>▁█</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>f1_max</td><td>▁█</td></tr><tr><td>gumbel_temp</td><td>▁▁</td></tr><tr><td>kappa</td><td>▁█</td></tr><tr><td>kappa_max</td><td>▁█</td></tr><tr><td>lambda_sharing</td><td>▁▁</td></tr><tr><td>lambda_sparsity</td><td>▁▁</td></tr><tr><td>lambda_tasks</td><td>▁▁</td></tr><tr><td>logloss</td><td>█▁</td></tr><tr><td>lr_0</td><td>▁▁</td></tr><tr><td>lr_1</td><td>▁▁</td></tr><tr><td>p_f1_max</td><td>█▁</td></tr><tr><td>p_kappa_max</td><td>█▁</td></tr><tr><td>policy</td><td>▁▁</td></tr><tr><td>policy_lr</td><td>▁▁</td></tr><tr><td>roc_auc_score</td><td>▁█</td></tr><tr><td>sc_loss</td><td>█▁</td></tr><tr><td>task</td><td>█▁</td></tr><tr><td>task1</td><td>▁▁</td></tr><tr><td>task10</td><td>▁▁</td></tr><tr><td>task2</td><td>▁▁</td></tr><tr><td>task3</td><td>▁▁</td></tr><tr><td>task4</td><td>▁▁</td></tr><tr><td>task5</td><td>▁▁</td></tr><tr><td>task6</td><td>▁▁</td></tr><tr><td>task7</td><td>▁▁</td></tr><tr><td>task8</td><td>▁▁</td></tr><tr><td>task9</td><td>▁▁</td></tr><tr><td>total</td><td>▁▁</td></tr><tr><td>train_layers</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.6974</td></tr><tr><td>avg_prec_score</td><td>0.72131</td></tr><tr><td>bceloss</td><td>0.43003</td></tr><tr><td>best_roc_auc</td><td>0.79359</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>f1_max</td><td>0.7534</td></tr><tr><td>gumbel_temp</td><td>4</td></tr><tr><td>kappa</td><td>0.30549</td></tr><tr><td>kappa_max</td><td>0.57316</td></tr><tr><td>lambda_sharing</td><td>0.01</td></tr><tr><td>lambda_sparsity</td><td>0.02</td></tr><tr><td>lambda_tasks</td><td>1</td></tr><tr><td>logloss</td><td>1e-05</td></tr><tr><td>lr_0</td><td>0.001</td></tr><tr><td>lr_1</td><td>0.001</td></tr><tr><td>p_f1_max</td><td>0.43669</td></tr><tr><td>p_kappa_max</td><td>0.49471</td></tr><tr><td>policy</td><td>0.00109</td></tr><tr><td>policy_lr</td><td>0.001</td></tr><tr><td>roc_auc_score</td><td>0.79359</td></tr><tr><td>sc_loss</td><td>0.01063</td></tr><tr><td>task</td><td>1.93528</td></tr><tr><td>task1</td><td>0.00011</td></tr><tr><td>task10</td><td>0.00011</td></tr><tr><td>task2</td><td>0.00011</td></tr><tr><td>task3</td><td>0.00011</td></tr><tr><td>task4</td><td>0.00011</td></tr><tr><td>task5</td><td>0.00011</td></tr><tr><td>task6</td><td>0.00011</td></tr><tr><td>task7</td><td>0.00011</td></tr><tr><td>task8</td><td>0.00011</td></tr><tr><td>task9</td><td>0.00011</td></tr><tr><td>total</td><td>0.00108</td></tr><tr><td>total_mean</td><td>nan</td></tr><tr><td>train_layers</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">0728_1236</strong>: <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/3ev7hn83\" target=\"_blank\">https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/3ev7hn83</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220728_123600-3ev7hn83/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns.wandb_run.finish()\n",
    "# ns.wandb_run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c03a5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7ffbb96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:38:21.068022Z",
     "start_time": "2022-08-02T08:38:20.916022Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      roc_auc_score  auc_pr  avg_prec_score  f1_max  p_f1_max  kappa  kappa_max  p_kappa_max  bceloss\n",
      "task                                                                                                 \n",
      "0               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "1               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "2               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "3               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "4               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "...             ...     ...             ...     ...       ...    ...        ...          ...      ...\n",
      "619             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "620             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "621             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "622             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "623             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "\n",
      "[624 rows x 9 columns]\n",
      "{'roc_auc_score': 0.7021453617607464, 'auc_pr': 0.6567980589430293, 'avg_prec_score': 0.7368993282084998, 'f1_max': 0.8135136940807991, 'p_f1_max': 0.4633618478591625, 'kappa': 0.0822039072039072, 'kappa_max': 0.6198916913993331, 'p_kappa_max': 0.5026694165590482, 'bceloss': 0.6972301713167093, 'sc_loss': 0.025541066036169013, 'logloss': 0.0002915646807781851}\n",
      " wsum: 167.0   df.shape: (624, 9)   df2: (624, 9)  df2.sum(axis=0): \n",
      " roc_auc_score     39.0\n",
      "auc_pr            39.0\n",
      "avg_prec_score    39.0\n",
      "f1_max            39.0\n",
      "p_f1_max          39.0\n",
      "kappa             39.0\n",
      "kappa_max         39.0\n",
      "p_kappa_max       39.0\n",
      "bceloss           39.0\n",
      "dtype: float64\n",
      "\n",
      "  DIVISOR \n",
      "-----------\n",
      "roc_auc_score     0.025641\n",
      "auc_pr            0.025641\n",
      "avg_prec_score    0.025641\n",
      "f1_max            0.025641\n",
      "p_f1_max          0.025641\n",
      "kappa             0.025641\n",
      "kappa_max         0.025641\n",
      "p_kappa_max       0.025641\n",
      "bceloss           0.025641\n",
      "dtype: float64\n",
      "\n",
      "  DF \n",
      "------\n",
      "      roc_auc_score  auc_pr  avg_prec_score  f1_max  p_f1_max  kappa  kappa_max  p_kappa_max  bceloss\n",
      "task                                                                                                 \n",
      "0               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "1               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "2               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "3               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "4               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "...             ...     ...             ...     ...       ...    ...        ...          ...      ...\n",
      "619             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "620             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "621             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "622             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "623             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "\n",
      "[624 rows x 9 columns]\n",
      "\n",
      "  DF2 \n",
      "-------\n",
      "      roc_auc_score  auc_pr  avg_prec_score  f1_max  p_f1_max  kappa  kappa_max  p_kappa_max  bceloss\n",
      "task                                                                                                 \n",
      "0               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "1               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "2               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "3               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "4               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "...             ...     ...             ...     ...       ...    ...        ...          ...      ...\n",
      "619             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "620             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "621             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "622             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "623             NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "\n",
      "[624 rows x 9 columns]\n",
      "\n",
      "  RESULT \n",
      "----------\n",
      "roc_auc_score     0.702145\n",
      "auc_pr            0.656798\n",
      "avg_prec_score    0.736899\n",
      "f1_max            0.813514\n",
      "p_f1_max          0.463362\n",
      "kappa             0.082204\n",
      "kappa_max         0.619892\n",
      "p_kappa_max       0.502669\n",
      "bceloss           0.697230\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from utils             import censored_mse_loss, censored_mae_loss, aggregate_results\n",
    "task_key = 'task2'\n",
    "print(environ.val_metrics[task_key]['classification'])\n",
    "# print(environ.val_metrics[task_key]['classification'].sum())\n",
    "print(environ.val_metrics[task_key]['classification_agg'])\n",
    "# print(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "# print((environ.batch_data[task_key]['yc_aggr_weights']==environ.val_data[task_key]['yc_aggr_weights']).all())\n",
    "\n",
    "\n",
    "tmp = aggregate_results(environ.val_metrics[task_key][\"classification\"], \n",
    "                      environ.val_data[task_key]['yc_aggr_weights'],\n",
    "                      verbose = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5ed7b40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:16.735975Z",
     "start_time": "2022-08-02T08:53:16.662665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del all_tgs, all_tgs2\n",
    "del con,con2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d52214fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T16:01:47.159463Z",
     "start_time": "2022-08-01T16:01:47.041852Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 task1  shape:  (472,) classifiaction: (472, 10)\n",
      "roc_auc_score      60.0\n",
      "auc_pr             60.0\n",
      "avg_prec_score     60.0\n",
      "f1_max             60.0\n",
      "p_f1_max           60.0\n",
      "kappa              60.0\n",
      "kappa_max          60.0\n",
      "p_kappa_max        60.0\n",
      "bceloss            60.0\n",
      "task_group        472.0\n",
      "dtype: float64\n",
      "initialize (472,) (472, 10)\n",
      "2 task2  shape:  (624,) classifiaction: (624, 10)\n",
      "roc_auc_score      46.0\n",
      "auc_pr             46.0\n",
      "avg_prec_score     46.0\n",
      "f1_max             46.0\n",
      "p_f1_max           46.0\n",
      "kappa              46.0\n",
      "kappa_max          46.0\n",
      "p_kappa_max        46.0\n",
      "bceloss            46.0\n",
      "task_group        624.0\n",
      "dtype: float64\n",
      "concatenate:  task2      (1096,) (1096, 10)\n",
      "3 task3  shape:  (688,) classifiaction: (688, 10)\n",
      "roc_auc_score     126.0\n",
      "auc_pr            126.0\n",
      "avg_prec_score    126.0\n",
      "f1_max            126.0\n",
      "p_f1_max          126.0\n",
      "kappa             126.0\n",
      "kappa_max         126.0\n",
      "p_kappa_max       126.0\n",
      "bceloss           126.0\n",
      "task_group        688.0\n",
      "dtype: float64\n",
      "concatenate:  task3      (1784,) (1784, 10)\n",
      "4 task4  shape:  (192,) classifiaction: (192, 10)\n",
      "roc_auc_score      29.0\n",
      "auc_pr             29.0\n",
      "avg_prec_score     29.0\n",
      "f1_max             29.0\n",
      "p_f1_max           29.0\n",
      "kappa              29.0\n",
      "kappa_max          29.0\n",
      "p_kappa_max        29.0\n",
      "bceloss            29.0\n",
      "task_group        192.0\n",
      "dtype: float64\n",
      "concatenate:  task4      (1976,) (1976, 10)\n",
      "5 task5  shape:  (620,) classifiaction: (620, 10)\n",
      "roc_auc_score     107.0\n",
      "auc_pr            107.0\n",
      "avg_prec_score    107.0\n",
      "f1_max            107.0\n",
      "p_f1_max          107.0\n",
      "kappa             107.0\n",
      "kappa_max         107.0\n",
      "p_kappa_max       107.0\n",
      "bceloss           107.0\n",
      "task_group        620.0\n",
      "dtype: float64\n",
      "concatenate:  task5      (2596,) (2596, 10)\n",
      "6 task6  shape:  (184,) classifiaction: (184, 10)\n",
      "roc_auc_score      27.0\n",
      "auc_pr             27.0\n",
      "avg_prec_score     27.0\n",
      "f1_max             27.0\n",
      "p_f1_max           27.0\n",
      "kappa              27.0\n",
      "kappa_max          27.0\n",
      "p_kappa_max        27.0\n",
      "bceloss            27.0\n",
      "task_group        184.0\n",
      "dtype: float64\n",
      "concatenate:  task6      (2780,) (2780, 10)\n",
      "7 task7  shape:  (224,) classifiaction: (224, 10)\n",
      "roc_auc_score      27.0\n",
      "auc_pr             27.0\n",
      "avg_prec_score     27.0\n",
      "f1_max             27.0\n",
      "p_f1_max           27.0\n",
      "kappa              27.0\n",
      "kappa_max          27.0\n",
      "p_kappa_max        27.0\n",
      "bceloss            27.0\n",
      "task_group        224.0\n",
      "dtype: float64\n",
      "concatenate:  task7      (3004,) (3004, 10)\n",
      "8 task8  shape:  (148,) classifiaction: (148, 10)\n",
      "roc_auc_score      24.0\n",
      "auc_pr             24.0\n",
      "avg_prec_score     24.0\n",
      "f1_max             24.0\n",
      "p_f1_max           24.0\n",
      "kappa              24.0\n",
      "kappa_max          24.0\n",
      "p_kappa_max        24.0\n",
      "bceloss            24.0\n",
      "task_group        148.0\n",
      "dtype: float64\n",
      "concatenate:  task8      (3152,) (3152, 10)\n",
      "9 task9  shape:  (344,) classifiaction: (344, 10)\n",
      "roc_auc_score      57.0\n",
      "auc_pr             57.0\n",
      "avg_prec_score     57.0\n",
      "f1_max             57.0\n",
      "p_f1_max           57.0\n",
      "kappa              57.0\n",
      "kappa_max          57.0\n",
      "p_kappa_max        57.0\n",
      "bceloss            57.0\n",
      "task_group        344.0\n",
      "dtype: float64\n",
      "concatenate:  task9      (3496,) (3496, 10)\n",
      "10 task10  shape:  (72,) classifiaction: (72, 10)\n",
      "roc_auc_score     22.0\n",
      "auc_pr            22.0\n",
      "avg_prec_score    22.0\n",
      "f1_max            22.0\n",
      "p_f1_max          22.0\n",
      "kappa             22.0\n",
      "kappa_max         22.0\n",
      "p_kappa_max       22.0\n",
      "bceloss           22.0\n",
      "task_group        72.0\n",
      "dtype: float64\n",
      "concatenate:  task10      (3568,) (3568, 10)\n",
      "ttl :  3568 con.shape: (3568,) all_tgs.shape (3568, 10)\n"
     ]
    }
   ],
   "source": [
    "# del con\n",
    "ttl = 0\n",
    "\n",
    "# con = np.ndarray()\n",
    "appd_df = []\n",
    "for i in range(1,11):\n",
    "    task_key = f\"task{i}\"\n",
    "    print(i, task_key, ' shape: ', environ.val_data[task_key]['yc_aggr_weights'].shape,  'classifiaction:', environ.val_metrics[task_key]['classification'].shape)\n",
    "    tmp_df = environ.val_metrics[task_key]['classification'].where(pd.isnull,1)\n",
    "    print(tmp_df.sum(axis=0))\n",
    "    \n",
    "    if i == 1:\n",
    "        con = np.copy(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "        all_tgs = environ.val_metrics[task_key]['classification'].copy()\n",
    "        print(\"initialize\", con.shape, all_tgs.shape)\n",
    "    else:\n",
    "        con = np.hstack((con, environ.val_data[task_key]['yc_aggr_weights']))\n",
    "        all_tgs = all_tgs.append(environ.val_metrics[task_key]['classification'])\n",
    "        print(\"concatenate: \",task_key, \"    \", con.shape, all_tgs.shape)\n",
    "        \n",
    "    ttl += environ.val_data[task_key]['yc_aggr_weights'].shape[0]\n",
    "    \n",
    "print('ttl : ', ttl,  'con.shape:', con.shape, 'all_tgs.shape', all_tgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a22870bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:22.328820Z",
     "start_time": "2022-08-02T08:53:22.185202Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3568 entries, 0 to 71\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   roc_auc_score   525 non-null    float64\n",
      " 1   auc_pr          525 non-null    float64\n",
      " 2   avg_prec_score  525 non-null    float64\n",
      " 3   f1_max          525 non-null    float64\n",
      " 4   p_f1_max        525 non-null    float32\n",
      " 5   kappa           525 non-null    float64\n",
      " 6   kappa_max       525 non-null    float64\n",
      " 7   p_kappa_max     525 non-null    float32\n",
      " 8   bceloss         525 non-null    float64\n",
      "dtypes: float32(2), float64(7)\n",
      "memory usage: 250.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>auc_pr</th>\n",
       "      <th>avg_prec_score</th>\n",
       "      <th>f1_max</th>\n",
       "      <th>p_f1_max</th>\n",
       "      <th>kappa</th>\n",
       "      <th>kappa_max</th>\n",
       "      <th>p_kappa_max</th>\n",
       "      <th>bceloss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.320208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587646</td>\n",
       "      <td>0.780148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>0.575516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max  kappa  kappa_max  p_kappa_max   bceloss\n",
       "task                                                                                                      \n",
       "0               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "1          0.000000  0.250000        0.500000  0.666667  0.320208    0.0        0.0     0.587646  0.780148\n",
       "2               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "3               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "4          0.333333  0.166667        0.333333  0.500000  0.328739    0.0        0.2     0.328739  0.575516\n",
       "5               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "6               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "7               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "8               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "9               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "10              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "11              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "12              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "13              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "14              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "15              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "16              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "17              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "18              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "19              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tgs2 = pd.concat(environ.val_metrics[f\"task{i}\"]['classification'] for i in range(1,11))\n",
    "\n",
    "all_tgs2.info()\n",
    "all_tgs2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8bfb4137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:27.145197Z",
     "start_time": "2022-08-02T08:53:27.076862Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3568,)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con2 = np.hstack([ environ.val_data[f\"task{i}\"]['yc_aggr_weights'] for i in range(1,11)])\n",
    "con2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9ecbee89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:51:56.737396Z",
     "start_time": "2022-08-02T08:51:56.667161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_tgs.index = range(all_tgs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b8334f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:52:05.606811Z",
     "start_time": "2022-08-02T08:52:05.540830Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(all_tgs2[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ac56e5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:38.557041Z",
     "start_time": "2022-08-02T08:53:38.479724Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "roc_auc_score     448.0\n",
       "auc_pr            448.0\n",
       "avg_prec_score    448.0\n",
       "f1_max            448.0\n",
       "p_f1_max          448.0\n",
       "kappa             448.0\n",
       "kappa_max         448.0\n",
       "p_kappa_max       448.0\n",
       "bceloss           448.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tgs2_mod = all_tgs2.where(pd.isnull, 1) * con2[:,None]\n",
    "all_tgs2_mod.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "71ba25bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:40:54.050290Z",
     "start_time": "2022-08-02T08:40:53.982441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# con3 = pd.concat([environ.val_metrics['task1']['classification'],environ.val_metrics['task2']['classification'] ])\n",
    "# print(con3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4e3cc76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:46.438201Z",
     "start_time": "2022-08-02T08:53:46.318451Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wsum: 1314.0   df.shape: (3568, 9)   df2: (3568, 9)  df2.sum(axis=0): \n",
      " roc_auc_score     448.0\n",
      "auc_pr            448.0\n",
      "avg_prec_score    448.0\n",
      "f1_max            448.0\n",
      "p_f1_max          448.0\n",
      "kappa             448.0\n",
      "kappa_max         448.0\n",
      "p_kappa_max       448.0\n",
      "bceloss           448.0\n",
      "dtype: float64\n",
      "\n",
      "  DIVISOR \n",
      "-----------\n",
      "roc_auc_score     0.002232\n",
      "auc_pr            0.002232\n",
      "avg_prec_score    0.002232\n",
      "f1_max            0.002232\n",
      "p_f1_max          0.002232\n",
      "kappa             0.002232\n",
      "kappa_max         0.002232\n",
      "p_kappa_max       0.002232\n",
      "bceloss           0.002232\n",
      "dtype: float64\n",
      "\n",
      "  DF \n",
      "------\n",
      "      roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max  kappa  kappa_max  p_kappa_max   bceloss\n",
      "task                                                                                                      \n",
      "0               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "1          0.000000  0.250000        0.500000  0.666667  0.320208    0.0        0.0     0.587646  0.780148\n",
      "2               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "3               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "4          0.333333  0.166667        0.333333  0.500000  0.328739    0.0        0.2     0.328739  0.575516\n",
      "...             ...       ...             ...       ...       ...    ...        ...          ...       ...\n",
      "67              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "68              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "69              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "70              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "71              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "\n",
      "[3568 rows x 9 columns]\n",
      "\n",
      "  DF2 \n",
      "-------\n",
      "      roc_auc_score  auc_pr  avg_prec_score  f1_max  p_f1_max  kappa  kappa_max  p_kappa_max  bceloss\n",
      "task                                                                                                 \n",
      "0               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "1               1.0     1.0             1.0     1.0       1.0    1.0        1.0          1.0      1.0\n",
      "2               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "3               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "4               1.0     1.0             1.0     1.0       1.0    1.0        1.0          1.0      1.0\n",
      "...             ...     ...             ...     ...       ...    ...        ...          ...      ...\n",
      "67              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "68              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "69              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "70              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "71              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "\n",
      "[3568 rows x 9 columns]\n",
      "\n",
      "  RESULT \n",
      "----------\n",
      "roc_auc_score     0.685598\n",
      "auc_pr            0.681871\n",
      "avg_prec_score    0.744206\n",
      "f1_max            0.799004\n",
      "p_f1_max          0.484420\n",
      "kappa             0.080060\n",
      "kappa_max         0.582045\n",
      "p_kappa_max       0.532944\n",
      "bceloss           0.636030\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tmp2 = aggregate_results(all_tgs2, con2, verbose = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f8aa6cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:20:37.470235Z",
     "start_time": "2022-08-02T09:20:37.391995Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'auc_pr': 0.6818710653441948,\n",
      "    'avg_prec_score': 0.7442062851089334,\n",
      "    'bceloss': 0.6360303774875189,\n",
      "    'f1_max': 0.7990044449679128,\n",
      "    'kappa': 0.08006046274015079,\n",
      "    'kappa_max': 0.5820449674723311,\n",
      "    'logloss': 0.00032448763622636064,\n",
      "    'p_f1_max': 0.48441957962599447,\n",
      "    'p_kappa_max': 0.532944236640885,\n",
      "    'roc_auc_score': 0.6855984778924409,\n",
      "    'sc_loss': 0.24956344102169398}\n",
      "0.24956344102169398\n",
      "0.00032448763622636064\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(environ.val_metrics['aggregated'])\n",
    "print(environ.val_metrics['aggregated']['sc_loss'] )\n",
    "print(environ.val_metrics['aggregated'][\"logloss\"] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "46f21fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:02:49.573973Z",
     "start_time": "2022-08-02T09:02:49.497930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score     0.685598\n",
      "auc_pr            0.681871\n",
      "avg_prec_score    0.744206\n",
      "f1_max            0.799004\n",
      "p_f1_max          0.484420\n",
      "kappa             0.080060\n",
      "kappa_max         0.582045\n",
      "p_kappa_max       0.532944\n",
      "bceloss           0.636030\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tmp2)\n",
    "pp.pprint(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c58a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f0711df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:56.393951Z",
     "start_time": "2022-08-02T08:53:56.231370Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 task1  shape:  (472,) classifiaction: (472, 9)\n",
      "roc_auc_score     60.0\n",
      "auc_pr            60.0\n",
      "avg_prec_score    60.0\n",
      "f1_max            60.0\n",
      "p_f1_max          60.0\n",
      "kappa             60.0\n",
      "kappa_max         60.0\n",
      "p_kappa_max       60.0\n",
      "bceloss           60.0\n",
      "dtype: float64\n",
      "2 task2  shape:  (624,) classifiaction: (624, 9)\n",
      "roc_auc_score     46.0\n",
      "auc_pr            46.0\n",
      "avg_prec_score    46.0\n",
      "f1_max            46.0\n",
      "p_f1_max          46.0\n",
      "kappa             46.0\n",
      "kappa_max         46.0\n",
      "p_kappa_max       46.0\n",
      "bceloss           46.0\n",
      "dtype: float64\n",
      "3 task3  shape:  (688,) classifiaction: (688, 9)\n",
      "roc_auc_score     126.0\n",
      "auc_pr            126.0\n",
      "avg_prec_score    126.0\n",
      "f1_max            126.0\n",
      "p_f1_max          126.0\n",
      "kappa             126.0\n",
      "kappa_max         126.0\n",
      "p_kappa_max       126.0\n",
      "bceloss           126.0\n",
      "dtype: float64\n",
      "4 task4  shape:  (192,) classifiaction: (192, 9)\n",
      "roc_auc_score     29.0\n",
      "auc_pr            29.0\n",
      "avg_prec_score    29.0\n",
      "f1_max            29.0\n",
      "p_f1_max          29.0\n",
      "kappa             29.0\n",
      "kappa_max         29.0\n",
      "p_kappa_max       29.0\n",
      "bceloss           29.0\n",
      "dtype: float64\n",
      "5 task5  shape:  (620,) classifiaction: (620, 9)\n",
      "roc_auc_score     107.0\n",
      "auc_pr            107.0\n",
      "avg_prec_score    107.0\n",
      "f1_max            107.0\n",
      "p_f1_max          107.0\n",
      "kappa             107.0\n",
      "kappa_max         107.0\n",
      "p_kappa_max       107.0\n",
      "bceloss           107.0\n",
      "dtype: float64\n",
      "6 task6  shape:  (184,) classifiaction: (184, 9)\n",
      "roc_auc_score     27.0\n",
      "auc_pr            27.0\n",
      "avg_prec_score    27.0\n",
      "f1_max            27.0\n",
      "p_f1_max          27.0\n",
      "kappa             27.0\n",
      "kappa_max         27.0\n",
      "p_kappa_max       27.0\n",
      "bceloss           27.0\n",
      "dtype: float64\n",
      "7 task7  shape:  (224,) classifiaction: (224, 9)\n",
      "roc_auc_score     27.0\n",
      "auc_pr            27.0\n",
      "avg_prec_score    27.0\n",
      "f1_max            27.0\n",
      "p_f1_max          27.0\n",
      "kappa             27.0\n",
      "kappa_max         27.0\n",
      "p_kappa_max       27.0\n",
      "bceloss           27.0\n",
      "dtype: float64\n",
      "8 task8  shape:  (148,) classifiaction: (148, 9)\n",
      "roc_auc_score     24.0\n",
      "auc_pr            24.0\n",
      "avg_prec_score    24.0\n",
      "f1_max            24.0\n",
      "p_f1_max          24.0\n",
      "kappa             24.0\n",
      "kappa_max         24.0\n",
      "p_kappa_max       24.0\n",
      "bceloss           24.0\n",
      "dtype: float64\n",
      "9 task9  shape:  (344,) classifiaction: (344, 9)\n",
      "roc_auc_score     57.0\n",
      "auc_pr            57.0\n",
      "avg_prec_score    57.0\n",
      "f1_max            57.0\n",
      "p_f1_max          57.0\n",
      "kappa             57.0\n",
      "kappa_max         57.0\n",
      "p_kappa_max       57.0\n",
      "bceloss           57.0\n",
      "dtype: float64\n",
      "10 task10  shape:  (72,) classifiaction: (72, 9)\n",
      "roc_auc_score     22.0\n",
      "auc_pr            22.0\n",
      "avg_prec_score    22.0\n",
      "f1_max            22.0\n",
      "p_f1_max          22.0\n",
      "kappa             22.0\n",
      "kappa_max         22.0\n",
      "p_kappa_max       22.0\n",
      "bceloss           22.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_tasks_classification_metrics = []\n",
    "all_tasks_aggregation_weights    = [] \n",
    "\n",
    "for i in range(1,11):\n",
    "    task_key = f\"task{i}\"\n",
    "    print(i, task_key, ' shape: ', environ.val_data[task_key]['yc_aggr_weights'].shape,  'classifiaction:', environ.val_metrics[task_key]['classification'].shape)\n",
    "    tmp_df = environ.val_metrics[task_key]['classification'].where(pd.isnull,1)\n",
    "    print(tmp_df.sum(axis=0))\n",
    "    \n",
    "    all_tasks_classification_metrics.append(environ.val_metrics[task_key]['classification'])\n",
    "    all_tasks_aggregation_weights.append(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9616c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:50:55.310369Z",
     "start_time": "2022-08-02T08:50:55.205109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs3 = pd.concat(all_tasks_classification_metrics)\n",
    "con3 = np.concatenate(all_tasks_aggregation_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e9cdf412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:54:37.322219Z",
     "start_time": "2022-08-02T08:54:37.193928Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3568 entries, 0 to 71\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   roc_auc_score   525 non-null    float64\n",
      " 1   auc_pr          525 non-null    float64\n",
      " 2   avg_prec_score  525 non-null    float64\n",
      " 3   f1_max          525 non-null    float64\n",
      " 4   p_f1_max        525 non-null    float32\n",
      " 5   kappa           525 non-null    float64\n",
      " 6   kappa_max       525 non-null    float64\n",
      " 7   p_kappa_max     525 non-null    float32\n",
      " 8   bceloss         525 non-null    float64\n",
      "dtypes: float32(2), float64(7)\n",
      "memory usage: 250.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>auc_pr</th>\n",
       "      <th>avg_prec_score</th>\n",
       "      <th>f1_max</th>\n",
       "      <th>p_f1_max</th>\n",
       "      <th>kappa</th>\n",
       "      <th>kappa_max</th>\n",
       "      <th>p_kappa_max</th>\n",
       "      <th>bceloss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.320208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587646</td>\n",
       "      <td>0.780148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>0.575516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max  kappa  kappa_max  p_kappa_max   bceloss\n",
       "task                                                                                                      \n",
       "0               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "1          0.000000  0.250000        0.500000  0.666667  0.320208    0.0        0.0     0.587646  0.780148\n",
       "2               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "3               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "4          0.333333  0.166667        0.333333  0.500000  0.328739    0.0        0.2     0.328739  0.575516\n",
       "5               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "6               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "7               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "8               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "9               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "10              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "11              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "12              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "13              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "14              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "15              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "16              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "17              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "18              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
       "19              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tgs3.info()\n",
    "all_tgs3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9f29ee1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:02:16.745152Z",
     "start_time": "2022-08-02T09:02:16.608823Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " wsum: 1314.0   df.shape: (3568, 9)   df2: (3568, 9)  df2.sum(axis=0): \n",
      " roc_auc_score     448.0\n",
      "auc_pr            448.0\n",
      "avg_prec_score    448.0\n",
      "f1_max            448.0\n",
      "p_f1_max          448.0\n",
      "kappa             448.0\n",
      "kappa_max         448.0\n",
      "p_kappa_max       448.0\n",
      "bceloss           448.0\n",
      "dtype: float64\n",
      "\n",
      "  DIVISOR \n",
      "-----------\n",
      "roc_auc_score     0.002232\n",
      "auc_pr            0.002232\n",
      "avg_prec_score    0.002232\n",
      "f1_max            0.002232\n",
      "p_f1_max          0.002232\n",
      "kappa             0.002232\n",
      "kappa_max         0.002232\n",
      "p_kappa_max       0.002232\n",
      "bceloss           0.002232\n",
      "dtype: float64\n",
      "\n",
      "  DF \n",
      "------\n",
      "      roc_auc_score    auc_pr  avg_prec_score    f1_max  p_f1_max  kappa  kappa_max  p_kappa_max   bceloss\n",
      "task                                                                                                      \n",
      "0               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "1          0.000000  0.250000        0.500000  0.666667  0.320208    0.0        0.0     0.587646  0.780148\n",
      "2               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "3               NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "4          0.333333  0.166667        0.333333  0.500000  0.328739    0.0        0.2     0.328739  0.575516\n",
      "...             ...       ...             ...       ...       ...    ...        ...          ...       ...\n",
      "67              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "68              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "69              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "70              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "71              NaN       NaN             NaN       NaN       NaN    NaN        NaN          NaN       NaN\n",
      "\n",
      "[3568 rows x 9 columns]\n",
      "\n",
      "  DF2 \n",
      "-------\n",
      "      roc_auc_score  auc_pr  avg_prec_score  f1_max  p_f1_max  kappa  kappa_max  p_kappa_max  bceloss\n",
      "task                                                                                                 \n",
      "0               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "1               1.0     1.0             1.0     1.0       1.0    1.0        1.0          1.0      1.0\n",
      "2               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "3               NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "4               1.0     1.0             1.0     1.0       1.0    1.0        1.0          1.0      1.0\n",
      "...             ...     ...             ...     ...       ...    ...        ...          ...      ...\n",
      "67              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "68              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "69              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "70              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "71              NaN     NaN             NaN     NaN       NaN    NaN        NaN          NaN      NaN\n",
      "\n",
      "[3568 rows x 9 columns]\n",
      "\n",
      "  RESULT \n",
      "----------\n",
      "roc_auc_score     0.685598\n",
      "auc_pr            0.681871\n",
      "avg_prec_score    0.744206\n",
      "f1_max            0.799004\n",
      "p_f1_max          0.484420\n",
      "kappa             0.080060\n",
      "kappa_max         0.582045\n",
      "p_kappa_max       0.532944\n",
      "bceloss           0.636030\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tmp3 = aggregate_results( all_tgs3, con3, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e5c12369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:59:51.408133Z",
     "start_time": "2022-08-02T08:59:51.294153Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task\n",
      "0   NaN\n",
      "Name: roc_auc_score, dtype: float64\n",
      "task\n",
      "0   NaN\n",
      "Name: roc_auc_score, dtype: float64\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_tgs2[0:1]['roc_auc_score'])\n",
    "print(all_tgs3[0:1]['roc_auc_score'])\n",
    "print((all_tgs2[0:1]['roc_auc_score'] == all_tgs3[0:1]['roc_auc_score']).all())\n",
    "all_tgs2.compare(all_tgs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "399eae39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T15:18:30.544168Z",
     "start_time": "2022-08-01T15:18:30.511229Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344,) (344,)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(environ.val_data['task9']['yc_aggr_weights'].shape, con[3152:3496].shape)\n",
    "print((environ.val_data['task9']['yc_aggr_weights'] == con[3152:3496]).all())a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8183a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:53:09.749812Z",
     "start_time": "2022-08-01T14:53:09.749793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76406f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T13:56:23.922805Z",
     "start_time": "2022-07-08T13:56:23.891800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch :       3\n",
      "Best Iteration :   7977 \n",
      "Best ROC AUC   :   0.73690\n",
      "Best Precision :   0.64175\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60564cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:50:02.608053Z",
     "start_time": "2022-07-07T13:50:02.553468Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch :       1\n",
      "Best Iteration :   2659 \n",
      "Best ROC AUC   :   0.71991\n",
      "Best Precision :   0.63031\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a8071",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bc9c724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:20:10.987625Z",
     "start_time": "2022-07-28T11:20:10.957009Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch :       2\n",
      "Best Iteration :   6258 \n",
      "Best ROC AUC   :   0.79359\n",
      "Best Precision :   0.72131\n",
      "\n",
      "\n",
      "roc_auc_score           0.7936\n",
      "auc_pr                  0.6974\n",
      "avg_prec_score          0.7213\n",
      "f1_max                  0.7534\n",
      "p_f1_max                0.4367\n",
      "kappa                   0.3055\n",
      "kappa_max               0.5732\n",
      "p_kappa_max             0.4947\n",
      "bceloss                 0.4300\n",
      "sc_loss                 0.0106\n",
      "logloss                 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()\n",
    "for key in environ.val_metrics['aggregated']:\n",
    "    print(f\"{key:20s}    {environ.val_metrics['aggregated'][key]:0.4f}\")\n",
    "# pp.pprint(environ.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48528a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb49645c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Weight & Policy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aade16",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Weight/Policy Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f750fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T08:19:07.607828Z",
     "start_time": "2022-07-12T08:19:07.576657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(wandb.run)\n",
    "# init_wandb(ns, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe24a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T08:19:09.527706Z",
     "start_time": "2022-07-12T08:19:09.499023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ns.flag_warmup = True\n",
    "# num_train_layers = None \n",
    "# environ.opt['is_curriculum'] = True\n",
    "# environ.opt['curriculum_speed'] = 4\n",
    "# ns.num_train_layers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b1ace9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T08:19:41.113261Z",
     "start_time": "2022-07-12T08:19:41.077227Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Update weights -- fix alpha\n",
      " training preparation: - check for CUDA - cuda available as device id: [0]\n",
      " training preparation: - set print_freq to length of train loader: 2659\n",
      " training preparation: - set eval_iters to length of val loader : 652\n",
      " training preparation: - set number of batches per weight training epoch to: 2659\n",
      " training preparation: - set number of batches per policy training epoch to: 2659\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "** 2022-07-12 10:19:41:109439 \n",
      "** Training epoch: 10 iter: 26590   flag: update_weights \n",
      "** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\n",
      "** Switch from Warm Up training to Alternate training Weights & Policy \n",
      "** Take checkpoint and block gradient flow through Policy net\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns.flag = 'update_weights'\n",
    "ns.flag_warmup = False\n",
    "model_initializations(ns, opt, environ, phase = ns.flag, policy_learning = True)\n",
    "training_initializations(ns, opt, environ, dldrs, phase='update_weights', weight_iterations = 10, policy_iterations = 10, warmup = False)\n",
    "\n",
    "print_heading( f\"** {timestring()} \\n\"\n",
    "               f\"** Training epoch: {ns.current_epoch} iter: {ns.current_iter}   flag: {ns.flag} \\n\"\n",
    "               f\"** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\\n\"\n",
    "               f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "               f\"** Take checkpoint and block gradient flow through Policy net\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66affd0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T08:19:44.087283Z",
     "start_time": "2022-07-12T08:19:44.058214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ns.training_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a037fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:14:21.604719Z",
     "start_time": "2022-07-12T09:14:21.526746Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ns.flag_warmup                 :      False\n",
      " ns.flag                        :      update_weights\n",
      " num_train_layers               :      5\n",
      " environ.opt['is_curriculum']   :      False\n",
      " environ.opt['curriculum_speed']:      3\n",
      "\n",
      " Backbone Initial LR            :      0.001000      current LR : 0.001 \n",
      " Tasks    Initial LR            :      0.001000      current LR : 1e-05    \n",
      " Policy   Initial LR            :      0.001000      current LR : 1e-05  \n",
      "\n",
      " Sparsity regularization        :      0.02\n",
      " Sharing  regularization        :      0.01 \n",
      "\n",
      " Tasks    regularization        :      1   \n",
      " Gumbel Temp                    :      4.0000         \n",
      " Gumbel Temp decay              :      16 \n",
      "\n",
      " ns.current_epoch               :      12\n",
      " ns.training_epochs             :      2 \n",
      "\n",
      " ns.current_iters               :      26630\n",
      " Batches in weight epoch        :      10\n",
      " Batches in policy epoch        :      10\n",
      " num_train_layers               :      5 \n",
      "\n",
      "[e] Last ep:12  it:26630  -  Total Loss: 2.3150     \n",
      "Task: 2.3149   Sparsity: 1.05423e-04    Sharing: 0.00000e+00 \n",
      "\n",
      "\n",
      " ep:   12   softmax      s        \n",
      " ----- ----------------- -    \n",
      "  0    0.4903    0.5097  0\n",
      "  1    0.4908    0.5092  0\n",
      "  2    0.4907    0.5093  0\n",
      "  3    0.4905    0.5095  0\n",
      "  4    0.4908    0.5092  0\n",
      "\n",
      "\n",
      " ep:   12   logits       s         \n",
      " ----- ----------------- -    \n",
      "  0   -0.0192    0.0197  0\n",
      "  1   -0.0181    0.0185  0\n",
      "  2   -0.0189    0.0181  0\n",
      "  3   -0.0196    0.0186  0\n",
      "  4   -0.0179    0.0190  0\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 12       # of epochs to run:  2 -->  epochs 13 to 14\n",
      " policy_learning rate : 0.001 \n",
      " lambda_sparsity      : 0.02\n",
      " lambda_sharing       : 0.01\n",
      " curriculum training  : False     cirriculum speed: 3     num_training_layers : 5\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\" ns.flag_warmup                 :      {ns.flag_warmup}\")\n",
    "print( f\" ns.flag                        :      {ns.flag}\")\n",
    "print( f\" num_train_layers               :      {ns.num_train_layers}\")\n",
    "print( f\" environ.opt['is_curriculum']   :      {environ.opt['is_curriculum']}\")\n",
    "print( f\" environ.opt['curriculum_speed']:      {environ.opt['curriculum_speed']}\\n\")\n",
    "print( f\" Backbone Initial LR            :      {environ.opt['train']['backbone_lr']:4f}      current LR : {environ.optimizers['alphas'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            :      {environ.opt['train']['task_lr']:4f}      current LR : {environ.optimizers['weights'].param_groups[0]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            :      {environ.opt['train']['policy_lr']:4f}      current LR : {environ.optimizers['weights'].param_groups[1]['lr']}  \\n\")\n",
    "print( f\" Sparsity regularization        :      {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization        :      {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization        :      {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                    :      {environ.gumbel_temperature:.4f}         \\n\" #\n",
    "       f\" Gumbel Temp decay              :      {environ.opt['train']['decay_temp_freq']} \\n\") #\n",
    "print( f\" ns.current_epoch               :      {ns.current_epoch}\")\n",
    "print( f\" ns.training_epochs             :      {ns.training_epochs} \\n\") \n",
    "print( f\" ns.current_iters               :      {ns.current_iter}\")  \n",
    "print( f\" Batches in weight epoch        :      {ns.stop_iter_w}\")\n",
    "print( f\" Batches in policy epoch        :      {ns.stop_iter_a}\")\n",
    "print( f\" num_train_layers               :      {ns.num_train_layers} \\n\")\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "environ.display_trained_policy(ns.current_epoch)\n",
    "environ.display_trained_logits(ns.current_epoch)\n",
    "print_heading(f\" Last Epoch Completed : {ns.current_epoch}       # of epochs to run:  {ns.training_epochs} -->  epochs {ns.current_epoch+1} to {ns.training_epochs + ns.current_epoch}\"\n",
    "              f\"\\n policy_learning rate : {environ.opt['train']['policy_lr']} \"\n",
    "              f\"\\n lambda_sparsity      : {environ.opt['train']['lambda_sparsity']}\"\n",
    "              f\"\\n lambda_sharing       : {environ.opt['train']['lambda_sharing']}\"\n",
    "              f\"\\n curriculum training  : {opt['is_curriculum']}     cirriculum speed: {opt['curriculum_speed']}     num_training_layers : {ns.num_train_layers}\", \n",
    "              verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c71af",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Weight/Policy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d10ad7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T09:25:43.201399Z",
     "start_time": "2022-07-12T09:14:28.642350Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 12   # of epochs to run:  2 -->  epochs 13 to 14    \n",
      " policy_learning rate : 0.001      \n",
      " lambda_sparsity      : 0.02\n",
      " lambda_sharing       : 0.01 \n",
      " curriculum training  : False     cirriculum speed: 3     num_training_layers : 5\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |   logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total | time |\n",
      "  13 | 1.00e-05  1.00e-05  1.00e-03  4.00e+00 |   1.8621   1.054e-04   0.000e+00    1.8622 |   0.00000   0.46781   0.65082   0.74756   0.63897   0.68645 |   2.3151   1.054e-04   0.000e+00    2.3152 |174.0 |\n",
      "  13 | 1.00e-05  1.00e-05  1.00e-03  4.00e+00 |   1.5790   1.044e-04   0.000e+00    1.5791 |   0.00000   0.46781   0.65076   0.74756   0.63893   0.68644 |   2.3149   1.043e-04   0.000e+00    2.3150 |171.9 |\n",
      "  14 | 1.00e-05  1.00e-05  1.00e-03  4.00e+00 |   1.5118   1.043e-04   0.000e+00    1.5119 |   0.00000   0.46781   0.65079   0.74758   0.63895   0.68644 |   2.3149   1.043e-04   0.000e+00    2.3150 |163.7 |\n",
      "  14 | 1.00e-05  1.00e-05  1.00e-03  4.00e+00 |   2.0780   1.037e-04   0.000e+00    2.0781 |   0.00000   0.46781   0.65072   0.74756   0.63894   0.68639 |   2.3149   1.037e-04   0.000e+00    2.3150 |161.5 |\n",
      " save train checkpoint  to :  model_train_ep_14\n",
      " save train metrics to     :  metrics_train_ep_14.pickle\n",
      "[Final] ep:14  it:26670 -  Total Loss: 2.3150     \n",
      "Task: 2.3149   Sparsity: 1.03683e-04    Sharing: 0.00000e+00 \n",
      "\n",
      " ep:   14   softmax      s        \n",
      " ----- ----------------- -    \n",
      "  0    0.4847    0.5153  0\n",
      "  1    0.4850    0.5150  0\n",
      "  2    0.4850    0.5150  0\n",
      "  3    0.4848    0.5152  0\n",
      "  4    0.4850    0.5150  0\n",
      "\n",
      "\n",
      " ep:   14   logits       s         \n",
      " ----- ----------------- -    \n",
      "  0   -0.0304    0.0307  0\n",
      "  1   -0.0298    0.0301  0\n",
      "  2   -0.0303    0.0298  0\n",
      "  3   -0.0306    0.0301  0\n",
      "  4   -0.0297    0.0303  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight_policy_training(ns, opt, environ, dldrs, epochs = 100)\n",
    "weight_policy_training(ns, opt, environ, dldrs, disable_tqdm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27779c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:28.065212Z",
     "start_time": "2022-03-28T05:13:27.897193Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3dcb54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.display_trained_policy(ns.current_epoch)\n",
    "environ.display_trained_logits(ns.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e42e43",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Close WandB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f6b31da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-09T13:11:52.877662Z",
     "start_time": "2022-07-09T13:11:48.152254Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.63525</td></tr><tr><td>avg_prec_score</td><td>0.64684</td></tr><tr><td>bceloss</td><td>0.46873</td></tr><tr><td>best_roc_auc</td><td>0.73857</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>f1_max</td><td>0.68205</td></tr><tr><td>gumbel_temp</td><td>4</td></tr><tr><td>kappa</td><td>0.20695</td></tr><tr><td>kappa_max</td><td>0.42927</td></tr><tr><td>lambda_sharing</td><td>0.01</td></tr><tr><td>lambda_sparsity</td><td>0.02</td></tr><tr><td>lambda_tasks</td><td>1</td></tr><tr><td>logloss</td><td>0.0</td></tr><tr><td>lr_0</td><td>1e-05</td></tr><tr><td>lr_1</td><td>1e-05</td></tr><tr><td>p_f1_max</td><td>0.37759</td></tr><tr><td>p_kappa_max</td><td>0.45006</td></tr><tr><td>policy</td><td>0.0001</td></tr><tr><td>policy_lr</td><td>0.001</td></tr><tr><td>roc_auc_score</td><td>0.73857</td></tr><tr><td>sc_loss</td><td>0.00351</td></tr><tr><td>task</td><td>2.28529</td></tr><tr><td>task1</td><td>0.0001</td></tr><tr><td>total</td><td>0.0001</td></tr><tr><td>total_mean</td><td>0.00298</td></tr><tr><td>train_layers</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">0707_1506_RESUME2</strong>: <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29/runs/1872m4ic\" target=\"_blank\">https://wandb.ai/kbardool/AdaSparseChem-cb29/runs/1872m4ic</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220709_145655-1872m4ic/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab259da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:32.253924Z",
     "start_time": "2022-03-28T05:13:32.221329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ns.best_epoch = 0\n",
    "# from utils.notebook_modules import wrapup_phase\n",
    "# wrapup_phase(ns, opt, environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb718c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:32.307351Z",
     "start_time": "2022-03-28T05:13:32.262822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['policy_lr']       = 0.002\n",
    "# environ.opt['train']['lambda_sparsity'] = 0.05\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['lambda_tasks']    = 1.0\n",
    "# # environ.opt['train']['decay_temp_freq'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accefdf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-12T07:35:36.625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print( f\" Backbone Learning Rate      : {environ.opt['train']['backbone_lr']}\\n\"\n",
    "       f\" Tasks    Learning Rate      : {environ.opt['train']['task_lr']}\\n\"\n",
    "       f\" Policy   Learning Rate      : {environ.opt['train']['policy_lr']}\\n\")\n",
    "\n",
    "print( f\" Sparsity regularization     : {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization     : {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization     : {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                 : {environ.gumbel_temperature:.4f}         \\n\" \n",
    "       f\" Gumbel Temp decay           : {environ.opt['train']['decay_temp_freq']}\\n\") \n",
    "\n",
    "print( f\" current_iters               : {ns.current_iter}   \\n\"\n",
    "       f\" current_epochs              : {ns.current_epoch}  \\n\" \n",
    "       f\" train_total_epochs          : {ns.training_epochs}\\n\" \n",
    "       f\" stop_epoch_training         : {ns.stop_epoch_training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b05db1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Post Weight + Policy Training Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de4040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T16:19:42.220730Z",
     "start_time": "2022-04-05T16:19:41.747365Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.losses)\n",
    "# pp.pprint(environ.val_metrics)\n",
    "# environ.num_layers, environ.networks['mtl-net'].num_layers\n",
    "# environ.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ca92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T16:22:55.037557Z",
     "start_time": "2022-04-05T16:22:54.888980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e5cec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T17:44:36.218784Z",
     "start_time": "2022-02-21T17:44:36.063411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_loss(environ.val_metrics, title = f\"[Final] ep:{current_epoch}  it:{current_iter}\",)\n",
    "# environ.display_trained_policy(current_epoch)\n",
    "# environ.display_trained_logits(current_epoch)\n",
    "# environ.log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c1c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T00:32:52.580865Z",
     "start_time": "2022-03-06T00:32:52.554112Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_label   = 'model_train_ep_%d_seed_%04d' % (current_epoch, opt['random_seed'])\n",
    "# metrics_label = 'metrics_train_ep_%d_seed_%04d.pickle' % (current_epoch, opt['random_seed'])\n",
    "# environ.save_checkpoint(model_label, current_iter, current_epoch) \n",
    "# save_to_pickle(environ.val_metrics, environ.opt['paths']['checkpoint_dir'], metrics_label)\n",
    "# print_loss(environ.val_metrics, title = f\"[Final] ep:{current_epoch}  it:{current_iter}\",)\n",
    "# environ.display_trained_policy(current_epoch,out=[sys.stdout, environ.log_file])\n",
    "# environ.display_trained_logits(current_epoch)\n",
    "# environ.log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad3a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T22:48:27.014120Z",
     "start_time": "2022-02-20T22:48:26.982535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_loss(current_iter, environ.losses, title = f\"[e] Policy training epoch:{current_epoch}    iter:\")\n",
    "# print()\n",
    "# print_loss(current_iter, trn_losses, title = f\"[e] Policy training epoch:{current_epoch}    iter:\")\n",
    "# print()\n",
    "# print_loss(current_iter, environ.val_metrics, title = f\"[e] Policy training epoch:{current_epoch}    iter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d5db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T20:31:45.254334Z",
     "start_time": "2022-03-01T20:31:45.116895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.losses\n",
    "# environ.val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4dd81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T20:47:29.582501Z",
     "start_time": "2022-03-01T20:47:29.492581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.batch_data\n",
    "# print_metrics_cr(current_epoch, time.time() - start_time, trn_losses, environ.val_metrics, 0, out=[sys.stdout])\n",
    "# environ.display_parameters()\n",
    "\n",
    "# with np.printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', precision=7, formatter={'float': lambda x: f\"{x:12.5e}\"}):\n",
    "#     environ.print_logit_grads('gradients')\n",
    "\n",
    "# environ_params = environ.get_task_specific_parameters()\n",
    "# environ_params = environ.get_arch_parameters()\n",
    "# environ_params = environ.get_backbone_parameters()\n",
    "# print(environ_params)\n",
    "# for param in environ_params:\n",
    "#     print(param.grad.shape, '\\n', param.grad)\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c80c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T21:12:46.806056Z",
     "start_time": "2022-03-11T21:12:46.471801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.display_trained_logits(ns.current_epoch)\n",
    "# environ.display_trained_policy(ns.current_epoch)\n",
    "\n",
    "# environ.display_test_sample_policy(ns.current_epoch, hard_sampling = True)\n",
    "# environ.display_train_sample_policy(ns.current_epoch, hard_sampling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b8e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T23:10:03.751132Z",
     "start_time": "2022-03-05T23:10:03.724538Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# opt['exp_instance'] = '0218_1358'     \n",
    "# folder_name=  f\"{opt['exp_instance']}_bs{opt['train']['batch_size']:03d}_{opt['train']['decay_lr_rate']:3.2f}_{opt['train']['decay_lr_freq']}\"\n",
    "# print()\n",
    "# opt['exp_instance'] = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "# opt['exp_description'] = f\"No Alternating Weight/Policy - training all done with both weights and policy\"\n",
    "# folder_name=  f\"{opt['exp_instance']}_bs{opt['train']['batch_size']:03d}_{opt['train']['decay_lr_rate']:3.2f}_{opt['train']['decay_lr_freq']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2affee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T21:12:20.322227Z",
     "start_time": "2022-03-11T21:12:20.285961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527bd00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T10:48:54.253046Z",
     "start_time": "2022-04-30T10:48:54.143506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils import load_sparse\n",
    "ecfp     = load_sparse(opt['dataload']['dataroot'], opt['dataload']['x'])\n",
    "folding  = np.load(os.path.join(opt['dataload']['dataroot'], opt['dataload']['folding']))\n",
    "\n",
    "print(ecfp.shape, folding.shape)\n",
    "\n",
    "fold_va = opt['dataload']['fold_va']\n",
    "idx_tr  = np.where(folding != fold_va)[0]\n",
    "idx_va  = np.where(folding == fold_va)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dafb74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T10:51:34.176972Z",
     "start_time": "2022-04-30T10:51:34.093419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ecfp_tr = ecfp[idx_tr]\n",
    "ecfp_tr2 = ecfp[idx_tr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b1f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T10:51:48.263281Z",
     "start_time": "2022-04-30T10:51:48.191211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ecfp_tr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde09b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T11:07:11.516007Z",
     "start_time": "2022-04-30T11:07:11.441791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = (ecfp_tr != ecfp_tr2)\n",
    "print(res.shape)\n",
    "print(res[:10,:10].nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1552a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T10:58:24.177162Z",
     "start_time": "2022-04-30T10:58:23.283605Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "equal = 0\n",
    "for i in range(14633):\n",
    "    for j in range(1):\n",
    "        if (ecfp_tr[i,j] != ecfp_tr2[i,j]):\n",
    "            print(i, j, '     ', ecfp_tr[i,j], '    !=    ', ecfp_tr2[i,j])\n",
    "        else:\n",
    "            equal+=1 \n",
    "print(\"total eaqual = \", equal)\n",
    "# print(ecfp_tr2[0,:100])\n",
    "# print(ecfp_tr[:10,:10] != ecfp_tr2[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0f60f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74828d5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c3a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:22:20.634818Z",
     "start_time": "2022-01-27T00:22:20.444566Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.networks['mtl-net'].backbone.layer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62568b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:30:31.940280Z",
     "start_time": "2022-01-26T19:30:31.910058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_blocks = 6\n",
    "num_policy_layers = 6\n",
    "gt =  torch.ones((num_blocks)).long()\n",
    "gt0 =  torch.zeros((num_blocks)).long()\n",
    "print(gt)\n",
    "print(gt0)\n",
    "\n",
    "loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61487657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:42:31.300891Z",
     "start_time": "2022-01-26T19:42:31.257774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if environ.opt['diff_sparsity_weights'] and not environ.opt['is_sharing']:\n",
    "    print(' cond 1')\n",
    "    ## Assign higher weights to higher layers \n",
    "    loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "    print(f\"{task_key} sparsity error:  {2 * (loss_weights[-num_blocks:] * environ.cross_entropy2(logits[-num_blocks:], gt)).mean()})\")\n",
    "    print_dbg(f\" loss_weights :  {loss_weights}\", verbose = True)\n",
    "    print_dbg(f\" cross_entropy:  {environ.cross_entropy2(logits[-num_blocks:], gt)}  \", verbose = True)\n",
    "    print_dbg(f\" loss[sparsity][{task_key}]: {self.losses['sparsity'][task_key] } \", verbose = True)\n",
    "\n",
    "else:\n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between \\n Logits   : \\n{logits[-num_blocks:]} \\n and gt: \\n{gt} \\n\", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-num_blocks:], gt)}\")\n",
    "    \n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt[-1:])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt0[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt0[-1:])} \\n\")\n",
    "    \n",
    "    print('\\n cond 3')    \n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt[0:1])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt0[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt0[0:1])} \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b9e72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00d0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:14:30.155045Z",
     "start_time": "2022-01-26T00:14:30.107095Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# flag = 'update_w'\n",
    "# environ.fix_alpha\n",
    "# environ.free_w(opt['fix_BN'])\n",
    "\n",
    "flag = 'update_alpha'\n",
    "environ.fix_weights()\n",
    "environ.free_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5cfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:31.184285Z",
     "start_time": "2022-01-25T23:43:31.159229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.networks['mtl-net'].num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f717b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:14:34.993711Z",
     "start_time": "2022-01-26T00:14:34.968623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"current_epochs           : {current_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") \n",
    "\n",
    "train_total_epochs += 5\n",
    "\n",
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"current_epochs           : {current_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49551ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T17:15:02.216665Z",
     "start_time": "2022-01-25T17:15:01.848081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_metrics_cr(current_epoch, time.time() - t0, None, environ.val_metrics , num_prints)      \n",
    "\n",
    "# num_prints += 1\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Take check point\n",
    "# environ.save_checkpoint('latest', current_iter)\n",
    "# environ.train()\n",
    "# #-------------------------------------------------------\n",
    "# # END validation process\n",
    "# #-------------------------------------------------------       \n",
    "# flag = 'update_alpha'\n",
    "# environ.fix_w()\n",
    "# environ.free_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea51d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:08:57.366231Z",
     "start_time": "2022-01-08T01:08:57.295445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dilation = 2\n",
    "# kernel_size = np.asarray((3, 3))\n",
    "# upsampled_kernel_size = (kernel_size - 1) * (dilation - 1) + kernel_size\n",
    "# print(upsampled_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d878cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:39.763599Z",
     "start_time": "2022-01-25T23:43:39.728402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.optimizers['weights'].param_groups[0]\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38850b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:41.628847Z",
     "start_time": "2022-01-25T23:43:41.602238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.schedulers['weights'].get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c2d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:36:17.129535Z",
     "start_time": "2022-01-08T01:36:16.006144Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_state = {}\n",
    "for k, v in environ.optimizers.items():\n",
    "    print(f'state dict for {k} = {v}')\n",
    "    current_state[k] = v.state_dict()\n",
    "pp.pprint(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790a540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:40:22.032247Z",
     "start_time": "2022-01-08T01:40:22.006953Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_state = {}\n",
    "for k, v in environ.schedulers.items():\n",
    "    print(f'state dict for {k} = {v}')\n",
    "    print(v.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57103a9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12352a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:02:21.600933Z",
     "start_time": "2022-04-28T10:02:21.561452Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\" ns.check_for_improvment_wait:  {ns.check_for_improvment_wait}\")\n",
    "print(\" ns.curriculum_epochs:          {ns.curriculum_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dad034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:14.782725Z",
     "start_time": "2022-04-28T11:09:14.692205Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.val_metrics)\n",
    "df = environ.val_metrics['task1']['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27ebed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:15.186827Z",
     "start_time": "2022-04-28T11:09:15.090906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df[pd.notna(df.roc_auc_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4e2d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:44.692326Z",
     "start_time": "2022-04-28T11:09:44.611694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[pd.notna(df.roc_auc_score)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a1a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:22.031664Z",
     "start_time": "2022-06-15T17:39:21.964660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.display_trained_policy(ns.current_epoch,out=[sys.stdout])\n",
    "# environ.num_tasks\n",
    "# print(environ.get_policy_prob().shape)\n",
    "# print(environ.val_data['task1'].keys())\n",
    "# print(environ.val_data['task1']['yc_ind'][0][:40])\n",
    "# print(environ.val_data['task1']['yc_ind'][1][:40])\n",
    "# print(environ.val_data['task1']['yc_data'][:40])\n",
    "# print(environ.val_data['task1']['yc_hat'][:40])\n",
    "# environ.display_trained_policy(ns.current_epoch,out=[sys.stdout])\n",
    "# environ.display_trained_logits(ns.current_epoch,out=[sys.stdout])\n",
    "batch = next(dldrs.warmup_trn_loader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f163b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:22.757684Z",
     "start_time": "2022-06-15T17:39:22.679466Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6662777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:23.282940Z",
     "start_time": "2022-06-15T17:39:23.218734Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(batch['x_ind'].shape)\n",
    "print(batch['x_ind'][:,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5dc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:43:06.352600Z",
     "start_time": "2022-06-15T17:43:06.288637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(type(batch['x_data']))\n",
    "print(batch['x_data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9cb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:45.873056Z",
     "start_time": "2022-06-15T17:39:45.812606Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(batch['row_id']))\n",
    "print(batch['row_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08467352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:44:07.505660Z",
     "start_time": "2022-06-15T17:44:07.448666Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(batch['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de083fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T09:25:02.534297Z",
     "start_time": "2022-04-13T09:25:02.503118Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.val_metrics['task1'])\n",
    "pp.pprint(environ.val_metrics)\n",
    "# pp.pprint(ns.val_metrics)\n",
    "# print((environ.val_data['task1']['yc_data']).sum())\n",
    "# print(len(environ.val_data['task1']['yc_ind'][1]))\n",
    "# print(len(environ.val_data['task1']['yc_data']))\n",
    "# print(len(environ.val_data['task1']['yc_hat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90151319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T09:12:32.452187Z",
     "start_time": "2022-04-13T09:12:32.420905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(ns.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c031eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:47:30.057623Z",
     "start_time": "2022-04-13T08:47:29.588103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(ns.trn_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45fcf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:31:55.581510Z",
     "start_time": "2022-04-02T13:31:55.526855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(environ.val_data['task1']['yc_data'][0] == environ.val_data['task1']['yc_data']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02211ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T14:20:55.327255Z",
     "start_time": "2022-04-02T14:20:55.026238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.sparsechem_utils import compute_metrics, aggregate_results\n",
    "import pandas\n",
    "cc = compute_metrics(cols   = environ.val_data['task1']['yc_ind'][1], \n",
    "                     y_true = environ.val_data['task1']['yc_data'], \n",
    "                     y_score= environ.val_data['task1']['yc_hat'] ,\n",
    "                     num_tasks=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a5712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:34:57.196163Z",
     "start_time": "2022-04-02T13:34:57.130013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " df   = pd.DataFrame({\"task\"   : environ.val_data['task1']['yc_ind'][1], \n",
    "                      \"y_true\" : environ.val_data['task1']['yc_data'],  \n",
    "                      \"y_score\": environ.val_data['task1']['yc_hat']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde23676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:44:52.754320Z",
     "start_time": "2022-04-02T13:44:52.611945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for task, frame in df.groupby(\"task\", sort=True):\n",
    "    print(f\" task {task}\")\n",
    "    print(frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887a79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:46:29.715440Z",
     "start_time": "2022-04-02T13:46:29.640674Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df\n",
    "df.groupby(\"task\", sort=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862488b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:10:20.301689Z",
     "start_time": "2022-04-28T11:10:20.151621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e466147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T14:26:58.189057Z",
     "start_time": "2022-04-02T14:26:58.126134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.batch_data['task1']['yc_aggr_weights'])\n",
    "environ.batch['task1']['aggr_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007f28d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c2 = aggregate_results(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4570a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T17:11:11.578048Z",
     "start_time": "2022-04-02T17:11:11.535763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.trainset0.tasks_weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32a63e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T04:02:47.803657Z",
     "start_time": "2022-03-28T04:02:47.736497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# environ.opt['train']['policy_lr'] = 0.01\n",
    "# opt['train']['policy_lr']         = 0.01\n",
    "# environ.opt['train']['lambda_sparsity'] = 0.1\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['lambda_tasks']    = 1.0\n",
    "# environ.opt['train']['decay_temp_freq'] = 2\n",
    "# print(environ.optimizers['alphas'].param_groups)\n",
    "# print(environ.optimizers['weights'].param_groups)\n",
    "# print('current lr: ', environ.optimizers['alphas'].param_groups[0]['lr'],)\n",
    "# print('current lr: ', environ.optimizers['weights'].param_groups[0]['lr'])\n",
    "# print('current lr: ', environ.optimizers['weights'].param_groups[1]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82a453",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Post Warm-up Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb74c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:45:43.999959Z",
     "start_time": "2022-02-01T12:45:43.862475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b47da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:46:36.700361Z",
     "start_time": "2022-02-01T12:46:36.367037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.networks['mtl-net'].arch_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f0235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:37:10.158440Z",
     "start_time": "2022-01-28T16:37:09.742327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = environ.get_sample_policy(hard_sampling = False)\n",
    "print(p)\n",
    "p = environ.get_policy_prob()\n",
    "print(p)\n",
    "p = environ.get_policy_logits()\n",
    "print(p)\n",
    "\n",
    "# p = environ.get_current_policy()\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bddd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:40:37.799917Z",
     "start_time": "2022-01-28T16:40:37.773177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = softmax([0.0, 1])\n",
    "print(a)\n",
    "sampled = np.random.choice((1, 0), p=a)\n",
    "print(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8f376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:13:16.205889Z",
     "start_time": "2022-01-28T16:13:16.179303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.optimizers['weights'])\n",
    "print(environ.schedulers['weights'].get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf9c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:46:50.411465Z",
     "start_time": "2022-02-01T12:46:50.020540Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('losses.keys      : ', environ.losses.keys())\n",
    "print('losses[task]keys : ', environ.losses['task1'].keys())\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T22:57:02.151169Z",
     "start_time": "2022-01-15T22:57:02.056562Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( environ.val_metrics.keys())\n",
    "# pp.pprint(val_metrics)\n",
    "print(type(environ.val_metrics['aggregated']))\n",
    "print()\n",
    "print(type(environ.val_metrics['task1']['classification_agg']))\n",
    "print()\n",
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6e311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.266303Z",
     "start_time": "2022-01-14T18:57:26.166878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"val_metrics.pkl\", mode= 'wb') as f:\n",
    "#         pickle.dump(val_metrics, f)\n",
    "    \n",
    "# with open('val_metrics.pkl', 'rb') as f:    \n",
    "#     tst_val_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00a8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.297444Z",
     "start_time": "2022-01-14T18:57:26.269323Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(environ.input.shape) \n",
    "# a = getattr(environ, 'task1_pred')\n",
    "# yc_data = environ.batch['task1_data']\n",
    "# print(yc_data.shape)\n",
    "# yc_ind = environ.batch['task1_ind']\n",
    "# print(yc_ind.shape)\n",
    "# yc_hat_all = getattr(environ, 'task1_pred')\n",
    "# print(yc_hat_all.shape)\n",
    "# yc_hat  = yc_hat_all[yc_ind[0], yc_ind[1]]\n",
    "# print(yc_hat_all.shape, yc_hat.shape)\n",
    "\n",
    "# \n",
    "# environ.losses\n",
    "# loss = {}\n",
    "# for key in environ.losses.keys():\n",
    "#     loss[key] = {}\n",
    "#     for subkey, v in environ.losses[key].items():\n",
    "#         print(f\" key:  {key}   subkey: {subkey} \")\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             loss[key][subkey] = v.data\n",
    "#             print(f\" Tensor  -  key:  {key}   subkey: {subkey}           value type: {type(v)}  value: {v:.4f}\")\n",
    "#         else:\n",
    "#             loss[key][subkey] = v\n",
    "#             print(f\" integer -  key:  {key}   subkey: {subkey}           value type: {type(v)}  value: {v:.4f}\")\n",
    "# pp.pprint(tst_val_metrics)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987f89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.340792Z",
     "start_time": "2022-01-14T18:57:26.302528Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print('metrics.keys: ', environ.metrics.keys())\n",
    "# print('metrics[task].keys: ', environ.metrics['task1'].keys())\n",
    "# pp.pprint(environ.metrics['task1'])\n",
    "# pp.pprint(environ.losses['task1']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ae417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.373399Z",
     "start_time": "2022-01-14T18:57:26.345065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# title='Iteration'\n",
    "# for t_id, _ in enumerate(environ.tasks):\n",
    "#     task_key = f\"task{t_id+1}\"\n",
    "# #     print_heading(f\"{title}  {current_iter}  {task_key} : {val_metrics[task_key]['classification_agg']}\", verbose = True)\n",
    "\n",
    "#     for key, _  in val_metrics[task_key]['classification_agg'].items():\n",
    "#         print('%s/%-20s'%(task_key, key), val_metrics[task_key]['classification_agg'][key], current_iter)\n",
    "#         print(f\"{task_key:s}/{key:20s}\", val_metrics[task_key]['classification_agg'][key], current_iter)\n",
    "#         print()\n",
    "#             # print_current_errors(os.path.join(self.log_dir, 'loss.txt'), current_iter,key, loss[key], time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b73b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:52:48.162261Z",
     "start_time": "2022-01-07T22:52:48.140423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.print_loss(current_iter, start_time, metrics = val_metrics['loss'], verbose=True)\n",
    "# print(opt['lambdas'])\n",
    "# p = (opt['lambdas'][0] * environ.losses['tasks']['task1'])\n",
    "# print(p)\n",
    "\n",
    "# environ.print_val_metrics(current_iter, start_time, val_metrics , title='validation', verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850378b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:52:47.661019Z",
     "start_time": "2022-01-07T22:52:47.639094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter)\n",
    "# print_metrics_cr(current_iter, t1 - t0, None, val_metrics , True)\n",
    "# environ.print_val_metrics(current_iter, start_time, val_metrics, title='validation', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a80b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T23:14:58.034384Z",
     "start_time": "2022-01-07T23:14:58.004850Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\" val_metric keys               : {val_metrics.keys()}\")\n",
    "print(f\" loss keys                     : {val_metrics['loss'].keys()}\")\n",
    "print(f\" task1 keys                    : {val_metrics['task1'].keys()}\")\n",
    "print(f\" task1 classification keys     : {val_metrics['task1']['classification'].keys()}\")\n",
    "print(f\" task1 classification_agg keys : {val_metrics['task1']['classification_agg'].keys()}\")\n",
    "print()\n",
    "print(f\" task1                       : {val_metrics['task1']['classification_agg']['loss']:5f}\")\n",
    "print(f\" task2                       : {val_metrics['task2']['classification_agg']['loss']:5f}\")\n",
    "print(f\" task3                       : {val_metrics['task3']['classification_agg']['loss']:5f}\")\n",
    "print(f\" loss                        : {val_metrics['loss']['total']:5f}\")\n",
    "print(f\" train_time                  : {val_metrics['train_time']:2f}\")\n",
    "print(f\" epoch                       : {val_metrics['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd497e72",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66169a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:34:44.555617Z",
     "start_time": "2022-02-09T16:34:44.507417Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_losses = environ.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb8234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:35:11.758429Z",
     "start_time": "2022-02-10T03:35:11.278211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_metrics_cr(current_epoch, time.time() - start_time, trn_losses, environ.val_metrics , num_prints)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0301f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:21:49.107793Z",
     "start_time": "2022-02-10T03:21:49.084484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_metrics_cr(current_epoch, time.time() - start_time, trn_losses, environ.val_metrics , num_prints)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498b6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:20:32.866010Z",
     "start_time": "2022-02-10T03:20:32.442919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.losses)\n",
    "pp.pprint(trn_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe30724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:34:21.945701Z",
     "start_time": "2022-02-10T03:34:21.411234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d81167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:49.364883Z",
     "start_time": "2022-01-27T00:58:49.342931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['Lambda_sharing'] = 0.5\n",
    "# opt['train']['Lambda_sharing'] = 0.5\n",
    "\n",
    "# environ.opt['train']['policy_lr'] = 0.001\n",
    "# opt['train']['policy_lr'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73aa06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T01:00:41.996410Z",
     "start_time": "2022-01-26T01:00:41.559006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.losses.keys()\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ece74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:19:43.683550Z",
     "start_time": "2022-01-08T01:19:43.571450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = environ.get_loss_dict()\n",
    "print(tmp.keys())\n",
    "pp.pprint(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac0256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:50.458223Z",
     "start_time": "2022-01-27T00:58:50.430889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(opt['diff_sparsity_weights'])\n",
    "print(opt['is_sharing'])\n",
    "print(opt['diff_sparsity_weights'] and not opt['is_sharing'])\n",
    "print(environ.opt['train']['Lambda_sharing'])\n",
    "print(opt['train']['Lambda_sharing'])\n",
    "print(environ.opt['train']['Lambda_sparsity'])\n",
    "print(opt['train']['Lambda_sparsity'])\n",
    "print(environ.opt['train']['policy_lr'])\n",
    "print(opt['train']['policy_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2d510",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Policy / Logit stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb628497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:35:14.041577Z",
     "start_time": "2022-02-08T20:35:14.018303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.special          import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eed454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:30.103364Z",
     "start_time": "2022-02-08T20:00:30.068021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b5cc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_task_logits(n)` Get logits for task group n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed8b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:56:35.652087Z",
     "start_time": "2022-02-08T20:56:35.327406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task_logits = environ.get_task_logits(1)\n",
    "print(task_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66fa5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_arch_parameters()`: Get last used logits from network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85521e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:31.101960Z",
     "start_time": "2022-02-08T20:00:30.757064Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b0bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:06.054699Z",
     "start_time": "2022-02-09T15:43:05.689327Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea1743",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_logits()`:  Get Policy Logits - returns same as `get_arch_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb40c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:24.972390Z",
     "start_time": "2022-02-09T15:43:24.636629Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = environ.get_policy_logits()\n",
    "for i in logs:\n",
    "    print(i, '\\n')\n",
    "# probs = softmax(logs, axis= -1)\n",
    "# for i in probs:\n",
    "#     print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080a364",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_prob()` : Gets the softmax of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c75af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:29.733732Z",
     "start_time": "2022-02-09T15:43:29.699600Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "for i in policy_softmaxs:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3160d9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = False)` : Calls test_sample_policy of network with random choices based on softmax of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:21:15.636722Z",
     "start_time": "2022-02-08T22:21:15.165456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "policies,logits = environ.get_sample_policy(hard_sampling = False)\n",
    "\n",
    "for l, p, s in zip(logits, policies, policy_softmaxs) :\n",
    "    for  l_row, p_row, s_row in zip(l, p, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802664ec",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = True)` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea65bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:59:40.790899Z",
     "start_time": "2022-02-08T20:59:40.726657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "hard_policies, logits = environ.get_sample_policy(hard_sampling = True)\n",
    "\n",
    "for p,l,s in zip(hard_policies, logits, policy_softmaxs) :\n",
    "    for  p_row, l_row, s_row in zip(p, l, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c39cf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:00:47.452220Z",
     "start_time": "2022-02-08T21:00:47.422902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" Layer    task 1      task 2      task 3\")\n",
    "print(f\" -----    ------      ------      ------\")\n",
    "for idx, (l1, l2, l3) in enumerate(zip(hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1}       {l2}       {l3}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade0ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:39:39.936555Z",
     "start_time": "2022-02-08T22:39:39.911591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_trained_policy(iter):\n",
    "\n",
    "    policy_softmaxs = environ.get_policy_prob()\n",
    "    policy_argmaxs = 1-np.argmax(policy_softmaxs, axis = -1)\n",
    "    print(f\"  Trained polcies at iteration: {iter} \")\n",
    "    print(f\"                   task 1                           task 2                         task 3        \")\n",
    "    print(f\" Layer       softmax        select          softmax        select          softmax        select   \")\n",
    "    print(f\" -----    ---------------   ------       ---------------   ------       ---------------   ------   \")\n",
    "    for idx, (l1,l2,l3,  p1,p2,p3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2], policy_argmaxs[0], policy_argmaxs[1], policy_argmaxs[2]),1):\n",
    "        print(f\"   {idx}      {l1[0]:.4f}   {l1[1]:.4f}   {p1:4d}    {l2[0]:11.4f}   {l2[1]:.4f}   {p2:4d}    {l3[0]:11.4f}   {l3[1]:.4f}   {p3:4d}\")\n",
    "\n",
    "    print()\n",
    "# print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec208dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:42:12.650813Z",
     "start_time": "2022-02-08T22:42:12.330169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_trained_policy(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec517e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:07:30.836214Z",
     "start_time": "2022-02-08T22:07:30.804575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"                        POLICIES (SOFTMAX)                                       task 3          \")\n",
    "print(f\" Layer    task1              task2            task3 softmax         softmax         argmax         softmax         argmax   \")\n",
    "print(f\" -----    -------------     -------------     -------------   ------   \")\n",
    "for idx, (l1,l2,l3, h1,h2,h3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2],hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1[0]:.4f} {l1[1]:.4f}     {l2[0]:.4f} {l2[1]:.4f}     {l3[0]:.4f} {l3[1]:.4f}    {h3}\")\n",
    "    \n",
    "print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbeacb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:04:50.757406Z",
     "start_time": "2022-02-08T22:04:50.731736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(policy_softmaxs[2], np.argmax(1-policy_softmaxs[2], axis = -1))\n",
    "print(policy_softmaxs, np.argmax(policy_softmaxs, axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0240",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_current_logits()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb7240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:19:06.155425Z",
     "start_time": "2022-02-08T21:19:06.118640Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits  = (environ.get_current_logits())\n",
    "for i in logits:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84662",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_current_policy()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cfa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:40:06.543376Z",
     "start_time": "2022-02-08T20:40:06.230711Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pols  = (environ.get_current_policy())\n",
    "\n",
    "for i in pols:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f556a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:27:33.593255Z",
     "start_time": "2022-01-27T18:27:33.553141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792710e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `gumbel_softmax()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265490e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:28:17.107529Z",
     "start_time": "2022-02-08T21:28:17.084910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', floatmode = 'maxprec_equal')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb0087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:35:11.617269Z",
     "start_time": "2022-02-08T21:35:11.569599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.temp)\n",
    "# tau = environ.temp\n",
    "tau = 1\n",
    "for i in range(3): \n",
    "    logits_tensor = torch.tensor(logits[0])\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=tau, hard=False).cpu().numpy() \n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=tau, hard=True).cpu().numpy()\n",
    "    \n",
    "    for l, gs, gh in zip(lgts, gumbel_soft, gumbel_hard):\n",
    "        print(f\"   {l}   \\t {gs}            \\t {gh}\")\n",
    "#     print(lgts)\n",
    "#     print(gumbel_soft)\n",
    "#     print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5ef7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e0e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:21:35.524957Z",
     "start_time": "2022-02-08T21:21:35.488812Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for lgts in logits:\n",
    "    logits_tensor = torch.tensor(lgts)\n",
    "    print(lgts)\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=1, hard=False)\n",
    "    print(gumbel_soft)\n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=1, hard=True)\n",
    "    print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34a06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T20:49:37.643349Z",
     "start_time": "2022-02-03T20:49:37.580786Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "smax = scipy.special.softmax(logs, axis =1)\n",
    "# smax = np.array( \n",
    "# [[0.46973792, 0.530262  ],\n",
    "#  [0.45025694, 0.549743  ],\n",
    "#  [0.4443086 , 0.5556915 ],\n",
    "#  [0.4138397 , 0.58616036],\n",
    "#  [0.4140113 , 0.5859887 ],\n",
    "#  [0.42114905, 0.57885087]])\n",
    "\n",
    "print(smax.shape)\n",
    "print(smax)\n",
    "print(smax[0])\n",
    "print(smax[0].sum())\n",
    "print(np.random.choice((1,0), p =smax[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7de25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:15:19.893888Z",
     "start_time": "2022-02-08T21:15:19.870899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logs = np.array(\n",
    "[[0.33064184, 0.42053092],\n",
    " [0.3532089 , 0.52056104],\n",
    " [0.3888512 , 0.5680909 ],\n",
    " [0.42039296, 0.694217  ],\n",
    " [0.4519742 , 0.73311865],\n",
    " [0.48401102, 0.7522658 ]],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pyt-gpu]",
   "language": "python",
   "name": "conda-env-pyt-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "399px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "888d4fda4588b3bfc9793c8a97c6f83877963bb7385ca7ca0c08738cf63adc49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
