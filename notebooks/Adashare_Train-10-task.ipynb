{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408f1261",
   "metadata": {},
   "source": [
    "## Initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55604c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:47.901620Z",
     "start_time": "2022-09-06T20:55:47.866179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0c686b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:49.901661Z",
     "start_time": "2022-09-06T20:55:47.904093Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:08.233990Z",
     "iopub.status.busy": "2022-01-07T22:44:08.233053Z",
     "iopub.status.idle": "2022-01-07T22:44:08.273284Z",
     "shell.execute_reply": "2022-01-07T22:44:08.271908Z",
     "shell.execute_reply.started": "2022-01-07T22:44:08.233943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "# sys.path.insert(0, '/home/kbardool/kusanagi/AdaSparseChem/src') ; print(sys.path)\n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "import types, copy, pprint\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from utils import (initialize, init_dataloaders, init_environment, init_wandb, training_initializations, model_initializations, \n",
    "                   check_for_resume_training, disp_dataloader_info, disp_info_1, warmup_phase, weight_policy_training, \n",
    "                   display_gpu_info, init_dataloaders_by_fold_id, print_separator, print_heading, print_underline,\n",
    "                   timestring, print_loss, print_metrics_cr, get_command_line_args, load_from_pickle) \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "pd.options.display.width = 132\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Adashare_Train.ipynb\"\n",
    "\n",
    "## Set visible GPU device \n",
    "##----------------------------------------------\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33ac6e",
   "metadata": {},
   "source": [
    "# Initialization and  Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee652c57",
   "metadata": {},
   "source": [
    "### Parse Input Args  - Read YAML config file - wandb initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca1c17d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:49.930349Z",
     "start_time": "2022-09-06T20:55:49.904206Z"
    }
   },
   "outputs": [],
   "source": [
    "# synthetic_config_file  = \"../yamls/chembl_synt_train.yaml\"\n",
    "# config_file      = \"../yamls/chembl_mini_train.yaml\"\n",
    "config_file      = \"../yamls/chembl_cb29_train_1task.yaml\"\n",
    "config_file      = \"../yamls/chembl_cb29_train_10task.yaml\"\n",
    "batch_size=4096\n",
    "# batch_size=2048\n",
    "# RESUME_MODEL_CKPT = 'model_train_ep_25_seed_0088'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edb398",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####   For Resume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c33f25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:49.967802Z",
     "start_time": "2022-09-06T20:55:49.933479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "restart_input_args = f\" --config  {config_file} \" \\\n",
    "             f\" --batch_size       {batch_size} \"  \\\n",
    "             \" --exp_desc            10-task warmup with policy training \" \\\n",
    "             \" --hidden_size             4000 4000 4000 4000 4000 4000 \"  \\\n",
    "             \" --warmup_epochs             50 \"  \\\n",
    "             \" --tail_hidden_size        4000 \"  \\\n",
    "             \" --first_dropout           0.80 \"  \\\n",
    "             \" --middle_dropout          0.80 \"  \\\n",
    "             \" --last_dropout            0.80 \"  \\\n",
    "             \" --seed_idx                   0 \"  \\\n",
    "             \" --task_lr                0.001 \"  \\\n",
    "             \" --backbone_lr            0.001 \"  \\\n",
    "             \" --decay_lr_rate            0.5 \"  \\\n",
    "             \" --decay_lr_freq             40 \"  \\\n",
    "             \" --decay_lr_cooldown         10 \"  \\\n",
    "             \" --policy_lr               0.01 \"  \\\n",
    "             \" --policy_decay_lr_rate     0.5 \"  \\\n",
    "             \" --policy_decay_lr_freq      40 \"  \\\n",
    "             \" --policy_decay_lr_cooldown  10 \"  \\\n",
    "             \" --lambda_tasks             1.0 \"  \\\n",
    "             \" --lambda_sparsity        0.001 \"  \\\n",
    "             \" --lambda_sharing          0.05 \"  \\\n",
    "             \" --pytorch_threads            7 \"  \\\n",
    "             \" --cuda_devices               2\"   \\\n",
    "             \" --gpu_ids                    0 \"  \\\n",
    "             \" --resume\"                       \\\n",
    "             \" --resume_path        ../../experiments/AdaSparseChem-cb29-10task/4000x6_0822_1755_lr0.001_do0.8\" \\\n",
    "             \" --resume_ckpt        model_warmup_last_ep_10\" \\\n",
    "             \" --resume_metrics     metrics_warmup_last_ep_10.pickle\" \\\n",
    "             \" --exp_id             1x50t0va\" \\\n",
    "             \" --exp_name           0822_1755 \" \\\n",
    "             \" --folder_sfx         RESUME_2 \"\n",
    "\n",
    "#              \" --resume_ckpt        model_best_model\" \\\n",
    "#              \" --resume_metrics     metrics_best.pickle\" \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68145e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T17:38:07.664270Z",
     "start_time": "2022-06-24T17:38:07.630274Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "source": [
    "####  For Initiating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3a7f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:50.059322Z",
     "start_time": "2022-09-06T20:55:49.970012Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [],
   "source": [
    "input_args = f\" --config          {config_file} \" \\\n",
    "             f\" --batch_size       {batch_size} \"  \\\n",
    "             \" --exp_desc            10-task warmup with policy training \" \\\n",
    "             \" --hidden_size             4000 4000 4000 4000 4000 4000 \"  \\\n",
    "             \" --tail_hidden_size        4000 \"  \\\n",
    "             \" --warmup_epochs             20 \"  \\\n",
    "             \" --first_dropout           0.80 \"  \\\n",
    "             \" --middle_dropout          0.80 \"  \\\n",
    "             \" --last_dropout            0.80 \"  \\\n",
    "             \" --seed_idx                   0 \"  \\\n",
    "             \" --task_lr                0.001 \"  \\\n",
    "             \" --backbone_lr            0.001 \"  \\\n",
    "             \" --decay_lr_rate            0.5 \"  \\\n",
    "             \" --decay_lr_freq             20 \"  \\\n",
    "             \" --decay_lr_cooldown          5 \"  \\\n",
    "             \" --policy_lr               0.01 \"  \\\n",
    "             \" --policy_decay_lr_rate     0.5 \"  \\\n",
    "             \" --policy_decay_lr_freq      20 \"  \\\n",
    "             \" --policy_decay_lr_cooldown   5 \"  \\\n",
    "             \" --lambda_tasks             1.0 \"  \\\n",
    "             \" --lambda_sparsity        0.005 \"  \\\n",
    "             \" --lambda_sharing          0.05 \"  \\\n",
    "             \" --pytorch_threads            7 \"  \\\n",
    "             \" --cuda_devices               2\"   \\\n",
    "             \" --gpu_ids                    0 \"  \\\n",
    "\n",
    "#              \" --decay_lr_rate       0.3 \"  \\\n",
    "#              \" --decay_lr_freq        10 \"  \\\n",
    "#              \" --policy_lr         0.001 \"  \\\n",
    "#              \" --lambda_sparsity    0.02 \"  \\\n",
    "#              \" --lambda_sharing     0.01 \"  \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ffb30",
   "metadata": {},
   "source": [
    "### Read yaml Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fcbe87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:50.163685Z",
     "start_time": "2022-09-06T20:55:50.061757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " command line parms : \n",
      "------------------------\n",
      " config...................  ../yamls/chembl_cb29_train_10task.yaml\n",
      " project_name.............  None\n",
      " exp_id...................  2jeue03f\n",
      " exp_name.................  None\n",
      " folder_sfx...............  None\n",
      " exp_desc.................  10-task warmup with policy training\n",
      " hidden_sizes.............  [4000, 4000, 4000, 4000, 4000, 4000]\n",
      " tail_hidden_size.........  [4000]\n",
      " warmup_epochs............  20\n",
      " training_epochs..........  None\n",
      " seed_idx.................  0\n",
      " batch_size...............  4096\n",
      " first_dropout............  0.8\n",
      " middle_dropout...........  0.8\n",
      " last_dropout.............  0.8\n",
      " backbone_lr..............  0.001\n",
      " task_lr..................  0.001\n",
      " policy_lr................  0.01\n",
      " decay_lr_rate............  0.5\n",
      " decay_lr_freq............  20\n",
      " decay_lr_cooldown........  5\n",
      " policy_decay_lr_rate.....  0.5\n",
      " policy_decay_lr_freq.....  20\n",
      " policy_decay_lr_cooldown.  5\n",
      " lambda_tasks.............  1.0\n",
      " lambda_sparsity..........  0.005\n",
      " lambda_sharing...........  0.05\n",
      " cuda_devices.............  2\n",
      " gpu_ids..................  [0]\n",
      " pytorch_threads..........  7\n",
      " skip_residual............  False\n",
      " skip_hidden..............  False\n",
      " resume...................  False\n",
      " resume_path..............  None\n",
      " resume_ckpt..............  None\n",
      " resume_metrics...........  None\n",
      " cpu......................  False\n",
      " min_samples_class........  None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ns = types.SimpleNamespace()\n",
    "input_args = input_args.split() if input_args is not None else input_args\n",
    "# input_args = restart_input_args.split() \n",
    "ns.args = get_command_line_args(input_args, display = True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=ns.args.cuda_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0c6844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:50.192419Z",
     "start_time": "2022-09-06T20:55:50.165510Z"
    }
   },
   "outputs": [],
   "source": [
    "# display_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddaf625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:55:54.818818Z",
     "start_time": "2022-09-06T20:55:50.194497Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "################### READ YAML ####################\n",
      "##################################################\n",
      " Pytorch thread count: 20\n",
      " Set Pytorch thread count to : 7\n",
      " Pytorch thread count set to : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkbardool\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kevin/WSL-projs/AdaSparseChem/notebooks/wandb/run-20220906_225550-2jeue03f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/2jeue03f\" target=\"_blank\">0906_2255</a></strong> to <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WandB Initialization -----------------------------------------------------------\n",
      " PROJECT NAME: AdaSparseChem-cb29-10Task\n",
      " RUN ID      : 2jeue03f \n",
      " RUN NAME    : 0906_2255\n",
      " --------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " log_dir              create folder:  ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      " result_dir           folder exists:  ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      " checkpoint_dir       folder exists:  ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " experiment name       : 0906_2255 \n",
      " experiment id         : 2jeue03f \n",
      " folder_name           : 4000x6_0906_2255_lr0.001_do0.8 \n",
      " experiment description: 10-task warmup with policy training\n",
      " Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      " Random  seed used     : 88 \n",
      " log folder            : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      " checkpoint folder     : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      " Gpu ids               : [0]\n",
      " Seed index            : 0\n",
      " policy_iter           : best\n",
      " Data Split ratios     : [0.725, 0.225, 0.05]\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "        project_name : AdaSparseChem-cb29-10Task\n",
      "              exp_id : 2jeue03f\n",
      "        exp_name_pfx : 0906_2255\n",
      "            exp_name : 0906_2255\n",
      "          exp_folder : 4000x6_0906_2255_lr0.001_do0.8\n",
      "     exp_description : 10-task warmup with policy training\n",
      "          folder_sfx : None\n",
      "         random_seed : 88\n",
      "           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "              config : ../yamls/chembl_cb29_train_10task.yaml\n",
      "                 cpu : None\n",
      "             gpu_ids : [0]\n",
      "            backbone : SparseChem\n",
      "               tasks : ['class', 'class', 'class', 'class', 'class', 'class', 'class', 'class', 'class', 'class']\n",
      "     tasks_num_class : [472, 624, 688, 192, 620, 184, 224, 148, 344, 72]\n",
      "             lambdas : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "             verbose : False\n",
      "     input_size_freq : None\n",
      "          input_size : 32000\n",
      "        hidden_sizes : [4000, 4000, 4000, 4000, 4000, 4000]\n",
      "    tail_hidden_size : [4000]\n",
      " first_non_linearity : relu\n",
      "middle_non_linearity : relu\n",
      "  last_non_linearity : relu\n",
      "       first_dropout : 0.8\n",
      "      middle_dropout : 0.8\n",
      "        last_dropout : 0.8\n",
      "   class_output_size : None\n",
      "    regr_output_size : None\n",
      "              policy : True\n",
      "        policy_model : task-specific\n",
      "       skip_residual : False\n",
      "         skip_hidden : False\n",
      "           is_sparse : True\n",
      "diff_sparsity_weights : False\n",
      "          is_sharing : True\n",
      "diff_sharing_weights : False\n",
      "          skip_layer : 0\n",
      "       is_curriculum : False\n",
      "    curriculum_speed : 3\n",
      "              fix_BN : False\n",
      "     retrain_from_pl : False\n",
      "\n",
      "train\n",
      "-----\n",
      "       warmup_epochs : 20\n",
      "     training_epochs : 250\n",
      "         total_iters : 25000\n",
      "       warm_up_iters : None\n",
      "             task_lr : 0.001\n",
      "         backbone_lr : 0.001\n",
      "    weight_optimizer : adam\n",
      "    policy_optimizer : adam\n",
      "       decay_lr_rate : 0.5\n",
      "       decay_lr_freq : 20\n",
      "   decay_lr_cooldown : 5\n",
      "           policy_lr : 0.01\n",
      "policy_decay_lr_rate : 0.5\n",
      "policy_decay_lr_freq : 20\n",
      "policy_decay_lr_cooldown : 5\n",
      "     lambda_sparsity : 0.005\n",
      "      lambda_sharing : 0.05\n",
      "        lambda_tasks : 1.0\n",
      "         init_method : random\n",
      "           init_temp : 2.5\n",
      "          decay_temp : 0.75\n",
      "     decay_temp_freq : 3\n",
      "     init_neg_logits : None\n",
      "       hard_sampling : False\n",
      "            val_freq : 500\n",
      "          print_freq : -1\n",
      "warmup_iter_alternate : -1\n",
      "weight_iter_alternate : -1\n",
      "alpha_iter_alternate : -1\n",
      "           val_iters : -1\n",
      "              resume : False\n",
      "      retrain_resume : False\n",
      "         policy_iter : best\n",
      "          which_iter : warmup\n",
      "\n",
      "paths\n",
      "-----\n",
      "             log_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      "          result_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      "      checkpoint_dir : ../../experiments/AdaSparseChem-cb29-10task/4000x6_0906_2255_lr0.001_do0.8\n",
      "\n",
      "dataload\n",
      "--------\n",
      "             dataset : Chembl29\n",
      "            dataroot : ../../MLDatasets/chembl29_10task\n",
      "                   x : chembl_29_X.npy\n",
      "      x_split_ratios : [0.725, 0.225, 0.05]\n",
      "             folding : chembl_29_folding.npy\n",
      "             y_tasks : ['chembl_29_Y_tg_0_cols_472.npy', 'chembl_29_Y_tg_1_cols_624.npy', 'chembl_29_Y_tg_6_cols_688.npy', 'chembl_29_Y_tg_10_cols_192.npy', 'chembl_29_Y_tg_11_cols_620.npy', 'chembl_29_Y_tg_643_cols_184.npy', 'chembl_29_Y_tg_836_cols_224.npy', 'chembl_29_Y_tg_1005_cols_148.npy', 'chembl_29_Y_tg_1028_cols_344.npy', 'chembl_29_Y_tg_1031_cols_72.npy']\n",
      "            y_censor : None\n",
      "         fold_inputs : 32000\n",
      "     input_transform : None\n",
      "       weights_class : None\n",
      "   min_samples_class : 1\n",
      "           fold_test : [0]\n",
      "             fold_va : [1]\n",
      "         fold_warmup : [2, 3, 4]\n",
      "        fold_weights : [2, 3]\n",
      "         fold_policy : [4]\n",
      "\n",
      "SC\n",
      "--\n",
      "      normalize_loss : None\n",
      "\n",
      "test\n",
      "----\n",
      "          test_iters : -1\n",
      "     pytorch_threads : 7\n",
      "            seed_idx : 0\n",
      "          batch_size : 4096\n",
      "         resume_path : None\n",
      "         resume_ckpt : None\n",
      "      resume_metrics : None\n"
     ]
    }
   ],
   "source": [
    "opt = initialize(ns, build_folders = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f8e71cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T21:03:25.112240Z",
     "start_time": "2022-09-06T21:03:20.707601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">0906_2255</strong>: <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/2jeue03f\" target=\"_blank\">https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/2jeue03f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220906_225550-2jeue03f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ns.wandb_run.finish()\n",
    "ns.wandb_run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58caea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T07:40:24.684944Z",
     "start_time": "2022-07-04T07:40:24.654093Z"
    }
   },
   "source": [
    "### Setup Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a2edeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:02.007705Z",
     "start_time": "2022-09-06T20:55:54.867425Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Warmup folds    : [2, 3, 4]\n",
      " Weights folds   : [2, 3]\n",
      " Policy folds    : [4]\n",
      " Validation folds: [1]\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 472  Y rows with populated labels: 21581  non zero cols: 53309\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 624  Y rows with populated labels: 22820  non zero cols: 55015\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 688  Y rows with populated labels: 53858  non zero cols: 186792\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 192  Y rows with populated labels: 12262  non zero cols: 27798\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 620  Y rows with populated labels: 30988  non zero cols: 86678\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 184  Y rows with populated labels: 9818  non zero cols: 27152\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 224  Y rows with populated labels: 7090  non zero cols: 22638\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 148  Y rows with populated labels: 13262  non zero cols: 28925\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 344  Y rows with populated labels: 20684  non zero cols: 63517\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  254529     # Features per Sample: 32000 \n",
      "Y file : # Samples :  254529     # Labels per Sample  : 72  Y rows with populated labels: 4475  non zero cols: 10677\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 472  Y rows with populated labels: 15323  non zero cols: 38126\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 624  Y rows with populated labels: 15572  non zero cols: 38310\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 688  Y rows with populated labels: 34821  non zero cols: 121231\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 192  Y rows with populated labels: 7752  non zero cols: 17657\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 620  Y rows with populated labels: 20914  non zero cols: 58596\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 184  Y rows with populated labels: 7179  non zero cols: 19397\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 224  Y rows with populated labels: 4476  non zero cols: 14840\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 148  Y rows with populated labels: 8900  non zero cols: 19704\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 344  Y rows with populated labels: 12593  non zero cols: 39248\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  168649     # Features per Sample: 32000 \n",
      "Y file : # Samples :  168649     # Labels per Sample  : 72  Y rows with populated labels: 2897  non zero cols: 6734\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 472  Y rows with populated labels: 6258  non zero cols: 15183\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 624  Y rows with populated labels: 7248  non zero cols: 16705\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 688  Y rows with populated labels: 19037  non zero cols: 65561\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n",
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 192  Y rows with populated labels: 4510  non zero cols: 10141\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 620  Y rows with populated labels: 10074  non zero cols: 28082\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 184  Y rows with populated labels: 2639  non zero cols: 7755\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 224  Y rows with populated labels: 2614  non zero cols: 7798\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 148  Y rows with populated labels: 4362  non zero cols: 9221\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 344  Y rows with populated labels: 8091  non zero cols: 24269\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  85880     # Features per Sample: 32000 \n",
      "Y file : # Samples :  85880     # Labels per Sample  : 72  Y rows with populated labels: 1578  non zero cols: 3943\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 1 - task group chembl_29_Y_tg_0_cols_472.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 1 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      81937 \n",
      "    Total   -1  Labels :     188511 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     270448\n",
      "\n",
      " Task 1 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 472  Y rows with populated labels: 32866  non zero cols: 81937\n",
      "\n",
      " Task 1 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 472  Y rows with populated labels: 5643  non zero cols: 14167\n",
      "\n",
      "Using 199 of 472 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 2 - task group chembl_29_Y_tg_1_cols_624.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 2 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      90665 \n",
      "    Total   -1  Labels :     219244 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     309909\n",
      "\n",
      " Task 2 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 624  Y rows with populated labels: 38131  non zero cols: 90665\n",
      "\n",
      " Task 2 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 624  Y rows with populated labels: 8661  non zero cols: 20196\n",
      "\n",
      "Using 258 of 624 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "---------------------------------------------------------------------------\n",
      "Load label/Y file for task 3 - task group chembl_29_Y_tg_6_cols_688.npy\n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 3 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     320094 \n",
      "    Total   -1  Labels :     382164 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     702258\n",
      "\n",
      " Task 3 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 688  Y rows with populated labels: 91425  non zero cols: 320094\n",
      "\n",
      " Task 3 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 688  Y rows with populated labels: 17973  non zero cols: 61209\n",
      "\n",
      "Using 524 of 688 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 4 - task group chembl_29_Y_tg_10_cols_192.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 4 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      44576 \n",
      "    Total   -1  Labels :     110611 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     155187\n",
      "\n",
      " Task 4 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 192  Y rows with populated labels: 20024  non zero cols: 44576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task 4 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 192  Y rows with populated labels: 3776  non zero cols: 8372\n",
      "\n",
      "Using 111 of 192 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "----------------------------------------------------------------------------\n",
      "Load label/Y file for task 5 - task group chembl_29_Y_tg_11_cols_620.npy\n",
      "---------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 5 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     142158 \n",
      "    Total   -1  Labels :     193933 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     336091\n",
      "\n",
      " Task 5 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 620  Y rows with populated labels: 51001  non zero cols: 142158\n",
      "\n",
      " Task 5 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 620  Y rows with populated labels: 10004  non zero cols: 27223\n",
      "\n",
      "Using 389 of 620 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 6 - task group chembl_29_Y_tg_643_cols_184.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 6 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      41813 \n",
      "    Total   -1  Labels :      69820 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     111633\n",
      "\n",
      " Task 6 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 184  Y rows with populated labels: 15543  non zero cols: 41813\n",
      "\n",
      " Task 6 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 184  Y rows with populated labels: 2519  non zero cols: 6085\n",
      "\n",
      "Using 92 of 184 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "-----------------------------------------------------------------------------\n",
      "Load label/Y file for task 7 - task group chembl_29_Y_tg_836_cols_224.npy\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 7 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      38227 \n",
      "    Total   -1  Labels :      91904 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     130131\n",
      "\n",
      " Task 7 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 224  Y rows with populated labels: 11789  non zero cols: 38227\n",
      "\n",
      " Task 7 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 224  Y rows with populated labels: 2379  non zero cols: 7992\n",
      "\n",
      "Using 109 of 224 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 8 - task group chembl_29_Y_tg_1005_cols_148.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 8 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      45065 \n",
      "    Total   -1  Labels :     104361 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     149426\n",
      "\n",
      " Task 8 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 148  Y rows with populated labels: 21460  non zero cols: 45065\n",
      "\n",
      " Task 8 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 148  Y rows with populated labels: 4764  non zero cols: 9313\n",
      "\n",
      "Using 80 of 148 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 9 - task group chembl_29_Y_tg_1028_cols_344.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 9 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :     110249 \n",
      "    Total   -1  Labels :     213195 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     323444\n",
      "\n",
      " Task 9 files pre-filtering : \n",
      "--------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 344  Y rows with populated labels: 35996  non zero cols: 110249\n",
      "\n",
      " Task 9 files post-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 344  Y rows with populated labels: 7460  non zero cols: 21553\n",
      "\n",
      "Using 226 of 344 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      "------------------------------------------------------------------------------\n",
      "Load label/Y file for task 10 - task group chembl_29_Y_tg_1031_cols_72.npy\n",
      "------------------------------------------------------------------------------ \n",
      "\n",
      " Number of non-zero features in ecfp[0]:79\n",
      "\n",
      " Task 10 label file: \n",
      "    Total > +1  Labels :          0 \n",
      "    Total   +1  Labels :      18631 \n",
      "    Total   -1  Labels :     107922 \n",
      "    Total < -1  Labels :          0 \n",
      "    Total != 0  Labels :     126553\n",
      "\n",
      " Task 10 files pre-filtering : \n",
      "---------------------------------\n",
      "X file : # Samples :  423736     # Features per Sample: 32000   \n",
      "Y file : # Samples :  423736     # Labels per Sample  : 72  Y rows with populated labels: 7835  non zero cols: 18631\n",
      "\n",
      " Task 10 files post-filtering : \n",
      "----------------------------------\n",
      "X file : # Samples :  86274     # Features per Sample: 32000 \n",
      "Y file : # Samples :  86274     # Labels per Sample  : 72  Y rows with populated labels: 1916  non zero cols: 4391\n",
      "\n",
      "Using 52 of 72 classification tasks for calculating aggregated metrics (AUCROC, F1_max, etc).\n",
      " dataloader preparation - set number of batches per warmup training epoch to: 63\n",
      " dataloader preparation - set number of batches per weight training epoch to: 42\n",
      " dataloader preparation - set number of batches per policy training epoch to: 21\n",
      " dataloader preparation - set number of batches per validation to           : 22\n",
      "\n",
      " trainset.y_class                                   :  [(254529, 472), (254529, 624), (254529, 688), (254529, 192), (254529, 620), (254529, 184), (254529, 224), (254529, 148), (254529, 344), (254529, 72)] \n",
      " trainset1.y_class                                  :  [(168649, 472), (168649, 624), (168649, 688), (168649, 192), (168649, 620), (168649, 184), (168649, 224), (168649, 148), (168649, 344), (168649, 72)] \n",
      " trainset2.y_class                                  :  [(85880, 472), (85880, 624), (85880, 688), (85880, 192), (85880, 620), (85880, 184), (85880, 224), (85880, 148), (85880, 344), (85880, 72)] \n",
      " valset.y_class                                     :  [(86274, 472), (86274, 624), (86274, 688), (86274, 192), (86274, 620), (86274, 184), (86274, 224), (86274, 148), (86274, 344), (86274, 72)]  \n",
      "\n",
      "                                Total                :  595332 \n",
      "\n",
      "\n",
      "Training dataset :\n",
      "--------------------\n",
      "  Size of training set 0 (warm up)                   :  254529 \n",
      "  Number of batches in training 0 (warm up)          :  63 \n",
      "  Size of training set 1 (network parms)             :  168649 \n",
      "  Number of batches in training 1 (network parms)    :  42 \n",
      "  Size of training set 2 (policy weights)            :  85880 \n",
      "  Number of batches in training 2 (policy weights)   :  21 \n",
      "  training set num of positive                       :  18631 \n",
      "  training set num of negative                       :  107922 \n",
      "  task_weights_list[0].aggregation_weight sum        :  199.0\n",
      "\n",
      "\n",
      "Validation dataset :\n",
      "----------------------\n",
      "  Rows in dataset                                    : 86274\n",
      "  Number of batches in dataset                       : 22\n",
      "  validation set num of positive                     : 18631\n",
      "  validation set num of negative                     : 107922\n",
      "  task_weights_list[0].aggregation_weight sum        : 199.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dldrs = init_dataloaders(opt, verbose = False)\n",
    "dldrs = init_dataloaders_by_fold_id(opt, verbose = False)\n",
    "disp_dataloader_info(dldrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bfa8d",
   "metadata": {},
   "source": [
    "### Setup Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e8437e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:06.146761Z",
     "start_time": "2022-09-06T20:56:02.011979Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:16.229028Z",
     "iopub.status.busy": "2022-01-07T22:44:16.227544Z",
     "iopub.status.idle": "2022-01-07T22:44:16.659397Z",
     "shell.execute_reply": "2022-01-07T22:44:16.658348Z",
     "shell.execute_reply.started": "2022-01-07T22:44:16.228966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "############# CREATE THE ENVIRONMENT #############\n",
      "##################################################\n",
      " device is  cuda:0\n",
      "--------------------------------------------------\n",
      " SparseChem_Backbone  Ver: 1.0 Init() Start \n",
      "-------------------------------------------------- \n",
      "\n",
      " layer config        : [1, 1, 1, 1, 1, 1] \n",
      " skip residual layers: False   skip hidden layers  : False\n",
      " SparseChem_BackBone() Input Layer  - Input: 32000  output: 4000  non-linearity:<class 'torch.nn.modules.activation.ReLU'>\n",
      " Hidden layer 0 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 1 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 2 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 3 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Hidden layer 4 - Input: 4000   output:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Final Hidden layer 4 : Input size: 4000   output size:4000\n",
      "    _make_layer() using block: <class 'models.sparsechem_backbone.SparseChemBlock'>\n",
      "           input_size: 4000 output_sz: 4000  non_linearity: ReLU() dropout: 0.8 bias: True\n",
      "           SparseChemBlock.init(): input_size: 4000 output_sz: 4000   non_linearity: ReLU() dropout: 0.8 bias: True\n",
      " Module List \n",
      "--------------------------------------------------\n",
      " Initialize weights \n",
      "-------------------------------------------------- \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " SparseChem Backbone -- Final configuration(2) : \n",
      "                     self.blocks: <class 'torch.nn.modules.container.ModuleList'>  len:6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "\n",
      " Input_Layer  type:<class 'torch.nn.modules.container.Sequential'>  \n",
      "----------------------------------------------------------------------\n",
      "self.Input_layer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layers/Blocks    : <class 'torch.nn.modules.container.ModuleList'>   len:6 \n",
      "Resdiual layers  : <class 'torch.nn.modules.container.ModuleList'>   len:6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 1  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 1  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 2  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 2  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 3  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 3  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 4  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 4  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 5  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 5  type:<class 'NoneType'> \n",
      " None\n",
      "---------------------------------------------------------------------------\n",
      " Layer #: 6  type:<class 'models.sparsechem_backbone.SparseChemBlock'> \n",
      "--------------------------------------------------------------------------- \n",
      "\n",
      " SparseChemBlock(\n",
      "  (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "  (non_linear): ReLU()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      ") \n",
      "\n",
      " Residual Layer #: 6  type:<class 'NoneType'> \n",
      " None\n",
      "\n",
      "\n",
      "\n",
      " SparseChem_Backbone Init() End \n",
      "----------------------------------------------------\n",
      "* SparseChemEnv environment successfully created\n",
      "---------------------------------------------------- \n",
      "\n",
      " \n",
      "\n",
      "SparseChemEnv  Configuration       \n",
      "---------------------------------------- \n",
      "\n",
      "----------------\n",
      "networks       :\n",
      "----------------\n",
      " {'mtl-net': MTL3(\n",
      "  (backbone): SparseChem_Backbone(\n",
      "    (Input_Layer): Sequential(\n",
      "      (linear): SparseLinear(in_features=32000, out_features=4000, bias=True)\n",
      "      (non_linear): ReLU()\n",
      "      (dropout): Dropout(p=0.8, inplace=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (1): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (2): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (3): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (4): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "      (5): SparseChemBlock(\n",
      "        (linear): Linear(in_features=4000, out_features=4000, bias=True)\n",
      "        (non_linear): ReLU()\n",
      "        (dropout): Dropout(p=0.8, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (residuals): ModuleList(\n",
      "      (0): None\n",
      "      (1): None\n",
      "      (2): None\n",
      "      (3): None\n",
      "      (4): None\n",
      "      (5): None\n",
      "    )\n",
      "  )\n",
      "  (task1_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=472, bias=True)\n",
      "  )\n",
      "  (task2_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=624, bias=True)\n",
      "  )\n",
      "  (task3_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=688, bias=True)\n",
      "  )\n",
      "  (task4_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=192, bias=True)\n",
      "  )\n",
      "  (task5_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=620, bias=True)\n",
      "  )\n",
      "  (task6_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=184, bias=True)\n",
      "  )\n",
      "  (task7_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=224, bias=True)\n",
      "  )\n",
      "  (task8_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=148, bias=True)\n",
      "  )\n",
      "  (task9_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=344, bias=True)\n",
      "  )\n",
      "  (task10_fc1_c0): SparseChem_Classification_Module(\n",
      "    (linear): Linear(in_features=4000, out_features=72, bias=True)\n",
      "  )\n",
      ")}\n",
      "\n",
      "----------------\n",
      "optimizers     :\n",
      "----------------\n",
      " {}\n",
      "\n",
      "----------------\n",
      "schedulers     :\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "environ = init_environment(ns, opt, is_train = True, display_cfg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744b865c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:10.559926Z",
     "start_time": "2022-09-06T20:56:10.520367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparseChemEnv' object has no attribute 'policys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_585233/2837919552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mtl-net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparseChemEnv' object has no attribute 'policys'"
     ]
    }
   ],
   "source": [
    "print(environ.networks['mtl-net'].policys)\n",
    "print(environ.policys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62717fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:32.274521Z",
     "start_time": "2022-09-06T20:56:32.237828Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:16.229028Z",
     "iopub.status.busy": "2022-01-07T22:44:16.227544Z",
     "iopub.status.idle": "2022-01-07T22:44:16.659397Z",
     "shell.execute_reply": "2022-01-07T22:44:16.658348Z",
     "shell.execute_reply.started": "2022-01-07T22:44:16.228966Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model optimizers defined . . . policy_learning: False\n",
      " Model schedulers defined . . . policy_learning: False\n",
      " Metrics CSV file header written . . . \n",
      " Model initializations complete . . . \n"
     ]
    }
   ],
   "source": [
    "model_initializations(ns, opt, environ, policy_learning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9daf16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:33.413179Z",
     "start_time": "2022-09-06T20:56:33.372936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparseChemEnv' object has no attribute 'policys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_585233/2837919552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mtl-net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparseChemEnv' object has no attribute 'policys'"
     ]
    }
   ],
   "source": [
    "print(environ.networks['mtl-net'].policys)\n",
    "print(environ.policys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738062",
   "metadata": {},
   "source": [
    "### Initiate / Resume Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba16a95c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:36.796259Z",
     "start_time": "2022-09-06T20:56:36.759892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt['train']['which_iter'] :  warmup\n",
      "##################################################\n",
      "######## Initiate Training from scratch  #########\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "check_for_resume_training(ns, opt, environ, epoch = 0 , iter = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278d6d1",
   "metadata": {},
   "source": [
    "# Warmup Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa99797",
   "metadata": {},
   "source": [
    "### Warmup Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f8a21db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:40.432895Z",
     "start_time": "2022-09-06T20:56:40.183115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training preparation: - check for CUDA - cuda available as device id: [0]\n",
      " --> sparsechem_env.cuda()\n",
      " cuda()  - environ doesnt have policy1\n",
      "dict_keys(['weights', 'alphas'])\n",
      " opt:  weights  \n",
      " opt:  alphas  \n",
      " training preparation: - set print_freq to                        : 63 \n",
      " training preparation: - set batches per warmup training epoch to : 63\n",
      " training preparation: - set batches per weight training epoch to : 42\n",
      " training preparation: - set batches per policy training epoch to : 21\n",
      " training preparation: - set batches per validation to            : 22\n",
      " training preparation: - warmup_epochs                            : 20\n",
      " training preparation: - weight/policy training epochs            : 250\n",
      " training preparation: - set curriculum speed  to                 : 3\n",
      " training preparation: - set curriculum epochs to                 : 0\n",
      " training preparation: - write checkpoints                        : True\n",
      " training preparation complete . . .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 1000, weight_iterations = 750, policy_iterations = 250, eval_iterations = 250, warmup = True)\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 2, eval_iterations = 2, warmup = True)\n",
    "training_initializations(ns, opt, environ, dldrs, warmup = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f547021d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:50.810533Z",
     "start_time": "2022-09-06T20:56:50.768127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparseChemEnv' object has no attribute 'policys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_585233/2837919552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mtl-net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparseChemEnv' object has no attribute 'policys'"
     ]
    }
   ],
   "source": [
    "print(environ.networks['mtl-net'].policys)\n",
    "print(environ.policys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e7ce6a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:55.749022Z",
     "start_time": "2022-09-06T20:56:55.715398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Num_blocks                : 6                                \n",
      "\n",
      " batch size                : 4096 \n",
      " # batches / Warmup epoch  : 42 \n",
      " # batches / Weight epoch  : 42 \n",
      " # batches / Policy epoch  : 21                                 \n",
      "\n",
      " Print Frequency           : -1 \n",
      " Config Val Frequency      : 500 \n",
      " Config Val Iterations     : 22 \n",
      " Val iterations            : 22 \n",
      " which_iter                : warmup \n",
      " train_resume              : False                                 \n",
      " \n",
      " fix BN parms              : False \n",
      " Task LR                   : 0.001 \n",
      " Backbone LR               : 0.001                                 \n",
      "\n",
      " Sharing  regularization   : 0.05 \n",
      " Sparsity regularization   : 0.005 \n",
      " Task     regularization   : 1.0                                 \n",
      "\n",
      " Current epoch             : 0  \n",
      " Warm-up epochs            : 20 \n",
      " Training epochs           : 250\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    folder: 4000x6_0906_2255_lr0.001_do0.8\n",
      "    layers: 6 [4000, 4000, 4000, 4000, 4000, 4000] \n",
      "    \n",
      "    first dropout          : 0.8\n",
      "    middle dropout         : 0.8\n",
      "    last dropout           : 0.8\n",
      "    diff_sparsity_weights  : False\n",
      "    skip_layer             : 0\n",
      "    is_curriculum          : False\n",
      "    curriculum_speed       : 3\n",
      "    \n",
      "    task_lr                : 0.001\n",
      "    backbone_lr            : 0.001\n",
      "    decay_lr_rate          : 0.5\n",
      "    decay_lr_freq          : 20\n",
      "    \n",
      "    policy_lr              : 0.01\n",
      "    policy_decay_lr_rate   : 0.5\n",
      "    policy_decay_lr_freq   : 20\n",
      "    lambda_sparsity        : 0.005\n",
      "    lambda_sharing         : 0.05\n",
      "    lambda_tasks           : 1.0\n",
      "    \n",
      "    Gumbel init_temp       : 2.5\n",
      "    Gumbel decay_temp      : 0.75\n",
      "    Gumbel decay_temp_freq : 3\n",
      "    Logit init_method      : random\n",
      "    Logit init_neg_logits  : None\n",
      "    Logit hard_sampling    : False\n",
      "    Warm-up epochs         : 20\n",
      "    training epochs        : 250\n",
      "    Data split ratios      : [0.725, 0.225, 0.05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('-'*80)\n",
    "disp_info_1(ns, opt, environ)\n",
    "print('-'*80)\n",
    "print(environ.disp_for_excel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33100539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T19:44:29.523723Z",
     "start_time": "2022-08-26T19:44:29.500199Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# environ.display_trained_logits(ns.current_epoch,out=sys.stdout) \n",
    "# environ.display_trained_policy(ns.current_epoch,out=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73de409d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:56:58.280910Z",
     "start_time": "2022-09-06T20:56:58.245725Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "22\n",
      "63\n",
      "42\n",
      "21\n",
      "------------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  10 - Run epochs 1 to 10\n",
      "------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ns.eval_iters = 250\n",
    "# ns.trn_iters_warmup = 750\n",
    "# ns.eval_iters = 2\n",
    "# ns.trn_iters_warmup = 2\n",
    "ns.warmup_epochs = 10\n",
    "print(ns.warmup_epochs)\n",
    "print(ns.eval_iters )\n",
    "print(ns.trn_iters_warmup)\n",
    "print(ns.trn_iters_weights)\n",
    "print(ns.trn_iters_policy)\n",
    "\n",
    "# ns.check_for_improvment_wait = 0\n",
    "# ns.current_epoch =0 \n",
    "# ns.write_checkpoint = False\n",
    "print_heading(f\" Last Epoch: {ns.current_epoch}   # of warm-up epochs to do:  {ns.warmup_epochs} - Run epochs {ns.current_epoch+1} to {ns.current_epoch + ns.warmup_epochs}\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f7eb2",
   "metadata": {},
   "source": [
    "### Warmup Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bd29e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:57:05.539362Z",
     "start_time": "2022-09-06T20:57:04.705431Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  3 - Run epochs 1 to 3\n",
      "---------------------------------------------------------------------- \n",
      "\n",
      "                                                                                                                                                               \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_585233/3621396657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwarmup_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menviron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdldrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_tqdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/WSL-projs/AdaSparseChem/notebooks/../src/utils/notebook_modules.py\u001b[0m in \u001b[0;36mwarmup_phase\u001b[0;34m(ns, opt, environ, dldrs, disable_tqdm, epochs, verbose)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 environ.optimize(is_policy=False, \n\u001b[0m\u001b[1;32m    500\u001b[0m                                  \u001b[0mnum_train_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                                  \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'update_weights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WSL-projs/AdaSparseChem/notebooks/../src/envs/sparsechem_env.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, is_policy, flag, num_train_layers, policy_sampling_mode, hard_sampling, task_lambdas, verbose)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_loss_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         self.forward(is_policy = is_policy,\n\u001b[0m\u001b[1;32m    475\u001b[0m                      \u001b[0mpolicy_sampling_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_sampling_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                      \u001b[0mnum_train_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_train_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WSL-projs/AdaSparseChem/notebooks/../src/envs/sparsechem_env.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, is_policy, num_train_layers, policy_sampling_mode, hard_sampling, verbose)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'task%d_pred'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'policy%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logit%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "warmup_phase(ns,opt, environ, dldrs, epochs = 3, verbose = False, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41250924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T20:58:36.667082Z",
     "start_time": "2022-09-06T20:58:36.633405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in range(7):\n",
    "    a.append(None)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66b6349d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T12:48:34.216811Z",
     "start_time": "2022-09-06T12:19:10.330108Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      " Last Epoch: 48   # of warm-up epochs to do:  30 - Run epochs 49 to 78\n",
      "-------------------------------------------------------------------------- \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      "  49 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9759   6.006e-05   2.397e-05    0.9760 |  4.167e-06   0.46361   0.67510   0.77184   0.66179   0.70336 |   2.1615   1.418e-05   5.660e-06    2.1615 |  57.7 |\n",
      " Previous best_epoch:    48   best iter:  3024   best_accuracy: 0.67443    best ROC auc: 0.77139\n",
      " New      best_epoch:    49   best iter:  3087   best_accuracy: 0.67510    best ROC auc: 0.77184\n",
      "  50 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9574   6.006e-05   2.397e-05    0.9575 |  4.199e-06   0.46278   0.67343   0.77057   0.65987   0.70201 |   2.1782   1.418e-05   5.660e-06    2.1782 |  57.1 |\n",
      "  51 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9839   6.006e-05   2.397e-05    0.9840 |  4.178e-06   0.46265   0.67438   0.77115   0.66126   0.70247 |   2.1669   1.418e-05   5.660e-06    2.1669 |  57.5 |\n",
      "  52 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9105   6.006e-05   2.397e-05    0.9106 |  4.173e-06   0.46415   0.67394   0.77118   0.66052   0.70196 |   2.1645   1.418e-05   5.660e-06    2.1645 |  57.8 |\n",
      "  53 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.0127   6.006e-05   2.397e-05    1.0128 |  4.190e-06   0.46526   0.67457   0.77161   0.66137   0.70250 |   2.1735   1.418e-05   5.660e-06    2.1735 |  57.3 |\n",
      "  54 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9638   6.006e-05   2.397e-05    0.9639 |  4.231e-06   0.46637   0.67259   0.76960   0.65940   0.70081 |   2.1948   1.418e-05   5.660e-06    2.1948 |  57.2 |\n",
      "  55 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.0625   6.006e-05   2.397e-05    1.0626 |  4.212e-06   0.46600   0.67415   0.77035   0.66120   0.70200 |   2.1848   1.418e-05   5.660e-06    2.1848 |  57.2 |\n",
      "  56 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9275   6.006e-05   2.397e-05    0.9276 |  4.197e-06   0.46669   0.67366   0.76948   0.66061   0.70143 |   2.1769   1.418e-05   5.660e-06    2.1770 |  57.2 |\n",
      "  57 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.0635   6.006e-05   2.397e-05    1.0635 |  4.249e-06   0.46711   0.67228   0.76891   0.65944   0.70079 |   2.2040   1.418e-05   5.660e-06    2.2040 |  57.5 |\n",
      "  58 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9151   6.006e-05   2.397e-05    0.9152 |  4.232e-06   0.46545   0.67444   0.77073   0.66165   0.70210 |   2.1953   1.418e-05   5.660e-06    2.1954 |  56.9 |\n",
      "  59 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9505   6.006e-05   2.397e-05    0.9506 |  4.193e-06   0.46549   0.67261   0.77036   0.65916   0.70133 |   2.1749   1.418e-05   5.660e-06    2.1749 |  58.5 |\n",
      "  60 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9199   6.006e-05   2.397e-05    0.9200 |  4.230e-06   0.46748   0.67345   0.77066   0.66021   0.70133 |   2.1940   1.418e-05   5.660e-06    2.1940 |  57.1 |\n",
      "  61 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9750   6.006e-05   2.397e-05    0.9751 |  4.207e-06   0.46790   0.67291   0.77041   0.66006   0.70141 |   2.1824   1.418e-05   5.660e-06    2.1825 |  57.1 |\n",
      "  62 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9788   6.006e-05   2.397e-05    0.9789 |  4.220e-06   0.46712   0.67246   0.76986   0.65940   0.70122 |   2.1889   1.418e-05   5.660e-06    2.1890 |  57.3 |\n",
      "  63 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.0837   6.006e-05   2.397e-05    1.0838 |  4.228e-06   0.46932   0.67215   0.76951   0.65886   0.70140 |   2.1932   1.418e-05   5.660e-06    2.1933 |  56.7 |\n",
      "  64 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   1.0132   6.006e-05   2.397e-05    1.0133 |  4.299e-06   0.46992   0.67153   0.76891   0.65840   0.70020 |   2.2297   1.418e-05   5.660e-06    2.2297 |  58.1 |\n",
      "  65 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9865   6.006e-05   2.397e-05    0.9866 |  4.226e-06   0.46736   0.67254   0.76876   0.65967   0.70095 |   2.1919   1.418e-05   5.660e-06    2.1919 |  57.3 |\n",
      "  66 | 5.00e-04  5.00e-04  1.00e-02  2.50e+00 |   0.9606   6.006e-05   2.397e-05    0.9607 |  4.258e-06   0.46859   0.67122   0.76983   0.65752   0.70035 |   2.2088   1.418e-05   5.660e-06    2.2088 |  57.6 |\n",
      "Epoch    66: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    66: reducing learning rate of group 1 to 2.5000e-04.\n",
      "  67 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8264   6.006e-05   2.397e-05    0.8265 |  4.300e-06   0.47145   0.67083   0.76903   0.65712   0.69993 |   2.2304   1.418e-05   5.660e-06    2.2304 |  57.0 |\n",
      "  68 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8547   6.006e-05   2.397e-05    0.8548 |  4.294e-06   0.47364   0.67086   0.76898   0.65720   0.70014 |   2.2271   1.418e-05   5.660e-06    2.2272 |  57.4 |\n",
      "  69 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8280   6.006e-05   2.397e-05    0.8281 |  4.297e-06   0.47418   0.67026   0.76837   0.65686   0.69935 |   2.2292   1.418e-05   5.660e-06    2.2292 |  57.4 |\n",
      "  70 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8863   6.006e-05   2.397e-05    0.8863 |  4.297e-06   0.47391   0.67080   0.76889   0.65698   0.69976 |   2.2289   1.418e-05   5.660e-06    2.2289 |  57.3 |\n",
      "  71 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8698   6.006e-05   2.397e-05    0.8699 |  4.322e-06   0.47344   0.67191   0.76978   0.65830   0.70049 |   2.2420   1.418e-05   5.660e-06    2.2420 |  59.1 |\n",
      "  72 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.9054   6.006e-05   2.397e-05    0.9055 |  4.321e-06   0.47468   0.67153   0.76906   0.65834   0.69985 |   2.2412   1.418e-05   5.660e-06    2.2412 |  60.2 |\n",
      "  73 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8671   6.006e-05   2.397e-05    0.8672 |  4.366e-06   0.47775   0.67138   0.76834   0.65815   0.70000 |   2.2645   1.418e-05   5.660e-06    2.2646 |  58.7 |\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      "  74 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8245   6.006e-05   2.397e-05    0.8246 |  4.343e-06   0.47723   0.67176   0.76887   0.65876   0.70049 |   2.2530   1.418e-05   5.660e-06    2.2530 |  59.4 |\n",
      "  75 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.9256   6.006e-05   2.397e-05    0.9257 |  4.310e-06   0.47677   0.67080   0.76829   0.65757   0.69949 |   2.2356   1.418e-05   5.660e-06    2.2356 |  56.9 |\n",
      "  76 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.7958   6.006e-05   2.397e-05    0.7959 |  4.357e-06   0.47674   0.67133   0.76883   0.65795   0.70002 |   2.2599   1.418e-05   5.660e-06    2.2599 |  56.4 |\n",
      "  77 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8499   6.006e-05   2.397e-05    0.8500 |  4.319e-06   0.47846   0.67096   0.76837   0.65747   0.69907 |   2.2405   1.418e-05   5.660e-06    2.2405 |  57.2 |\n",
      "  78 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8406   6.006e-05   2.397e-05    0.8407 |  4.344e-06   0.47885   0.67015   0.76834   0.65659   0.69901 |   2.2534   1.418e-05   5.660e-06    2.2535 |  57.0 |\n",
      " save warmup checkpoint  to :  warmup_final_ep_78\n",
      " save warmup metrics to     :  warmup_final_ep_78\n",
      "[Final] ep:78  it:4914 -  Losses:   \t Task: 2.2534   \t Sparsity: 1.41826e-05    \t Sharing: 5.65983e-06    \t Total: 2.2535 \n",
      "\n",
      " ep:   78   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.0008   -0.0005  1    0.0013    0.0009  1   -0.0001    0.0017  0    0.0017   -0.0002  1    0.0005   -0.0006  1    0.0003   -0.0005  1    0.0010    0.0008  1   -0.0007    0.0006  0    0.0002   -0.0007  1   -0.0003   -0.0013  1\n",
      "  1    0.0004   -0.0003  1   -0.0003    0.0003  0   -0.0009    0.0003  0   -0.0003    0.0010  0    0.0009    0.0000  1   -0.0004   -0.0005  1    0.0012    0.0008  1   -0.0003    0.0020  0    0.0006    0.0001  1   -0.0008    0.0009  0\n",
      "  2    0.0008    0.0018  0   -0.0011   -0.0003  0    0.0016   -0.0010  1   -0.0006   -0.0006  1    0.0016    0.0012  1   -0.0001   -0.0008  1   -0.0016   -0.0004  0   -0.0009   -0.0008  0    0.0006   -0.0002  1   -0.0011    0.0007  0\n",
      "  3   -0.0002    0.0012  0   -0.0003   -0.0002  0    0.0008   -0.0003  1   -0.0006    0.0009  0    0.0011    0.0002  1   -0.0008   -0.0014  1   -0.0004   -0.0001  0    0.0009   -0.0021  1   -0.0006    0.0000  0    0.0001   -0.0002  1\n",
      "  4   -0.0013    0.0009  0    0.0003   -0.0003  1    0.0020    0.0008  1   -0.0016    0.0009  0    0.0006    0.0003  1   -0.0002    0.0004  0   -0.0010    0.0009  0   -0.0015    0.0003  0    0.0011   -0.0021  1   -0.0013   -0.0008  0\n",
      "  5   -0.0001   -0.0014  1   -0.0003    0.0013  0    0.0008   -0.0012  1    0.0001   -0.0004  1   -0.0005   -0.0010  1    0.0007    0.0004  1    0.0000   -0.0005  1   -0.0009    0.0001  0    0.0006   -0.0006  1   -0.0011    0.0001  0\n",
      "\n",
      "\n",
      " ep:   78    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.5003    0.4997  -    0.5001    0.4999  -    0.4996    0.5004  -    0.5005    0.4995  -    0.5003    0.4997  -    0.5002    0.4998  -    0.5000    0.5000  -    0.4997    0.5003  -    0.5002    0.4998  -    0.5003    0.4997  -\n",
      "  1    0.5002    0.4998  -    0.4999    0.5001  -    0.4997    0.5003  -    0.4997    0.5003  -    0.5002    0.4998  -    0.5000    0.5000  -    0.5001    0.4999  -    0.4994    0.5006  -    0.5001    0.4999  -    0.4996    0.5004  -\n",
      "  2    0.4997    0.5003  -    0.4998    0.5002  -    0.5006    0.4994  -    0.5000    0.5000  -    0.5001    0.4999  -    0.5002    0.4998  -    0.4997    0.5003  -    0.5000    0.5000  -    0.5002    0.4998  -    0.4996    0.5004  -\n",
      "  3    0.4996    0.5004  -    0.5000    0.5000  -    0.5003    0.4997  -    0.4996    0.5004  -    0.5002    0.4998  -    0.5002    0.4998  -    0.4999    0.5001  -    0.5008    0.4992  -    0.4998    0.5002  -    0.5001    0.4999  -\n",
      "  4    0.4994    0.5006  -    0.5001    0.4999  -    0.5003    0.4997  -    0.4994    0.5006  -    0.5001    0.4999  -    0.4999    0.5001  -    0.4995    0.5005  -    0.4995    0.5005  -    0.5008    0.4992  -    0.4999    0.5001  -\n",
      "  5    0.5003    0.4997  -    0.4996    0.5004  -    0.5005    0.4995  -    0.5001    0.4999  -    0.5001    0.4999  -    0.5001    0.4999  -    0.5001    0.4999  -    0.4997    0.5003  -    0.5003    0.4997  -    0.4997    0.5003  -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warmup_phase(ns,opt, environ, dldrs, epochs = 30, verbose = False, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a605c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T12:58:03.656045Z",
     "start_time": "2022-09-06T12:58:03.508501Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  78 | 2.50e-04  2.50e-04  1.00e-02  2.50e+00 |   0.8406   6.006e-05   2.397e-05    0.8407 |  4.344e-06   0.47885   0.67015   0.76834   0.65659   0.69901 |   2.2534   1.418e-05   5.660e-06    2.2535 |  -0.0 |\n",
      "\n",
      "[e] Last ep:78  it:4914  -  Losses:   \t Task: 2.2534   \t Sparsity: 1.41826e-05    \t Sharing: 5.65983e-06    \t Total: 2.2535 \n",
      "\n",
      "   best_epoch:    49   best iter:  3087   best_accuracy: 0.67510    best ROC auc: 0.77184\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      \n",
    "# print()\n",
    "# environ.display_trained_logits(ns.current_epoch)\n",
    "# environ.display_trained_policy(ns.current_epoch)\n",
    "# environ.display_current_policy(ns.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc713ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T13:00:26.216091Z",
     "start_time": "2022-09-06T13:00:25.939341Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " key: weights  values: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.00025\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.00025\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "{'state': {0: {'step': 4914, 'exp_avg': tensor([[ 1.0075e-05,  4.2411e-05, -2.5538e-05,  ..., -1.1571e-05,\n",
      "          2.4792e-05, -8.7961e-06],\n",
      "        [ 3.1836e-05, -4.6826e-05, -4.3444e-06,  ..., -9.9101e-06,\n",
      "         -4.1664e-07,  4.0225e-06],\n",
      "        [ 1.0846e-06,  1.2314e-05, -3.0259e-06,  ..., -1.9407e-06,\n",
      "          7.8857e-06,  6.6720e-06],\n",
      "        ...,\n",
      "        [ 1.9086e-07, -2.8435e-07, -1.2597e-06,  ..., -7.2261e-08,\n",
      "         -4.8842e-06, -1.7269e-08],\n",
      "        [-9.3063e-07, -2.6699e-07, -1.3299e-06,  ..., -1.6308e-07,\n",
      "         -9.1830e-06, -8.9973e-07],\n",
      "        [ 1.6819e-06, -1.5741e-07,  9.5266e-08,  ..., -9.7095e-08,\n",
      "          1.2622e-05,  2.8874e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[1.4925e-08, 1.5227e-07, 1.1731e-08,  ..., 4.1012e-08, 2.8871e-08,\n",
      "         4.5133e-09],\n",
      "        [8.6211e-09, 7.8718e-08, 5.5895e-09,  ..., 1.6905e-08, 2.6799e-08,\n",
      "         4.9153e-09],\n",
      "        [8.3295e-09, 1.5895e-08, 1.2000e-09,  ..., 5.0259e-09, 7.7718e-09,\n",
      "         2.2756e-09],\n",
      "        ...,\n",
      "        [2.0757e-09, 6.9532e-10, 4.7946e-10,  ..., 4.6327e-11, 2.6658e-09,\n",
      "         1.6952e-09],\n",
      "        [2.1082e-09, 1.2034e-09, 4.1501e-10,  ..., 1.5696e-10, 2.8607e-09,\n",
      "         1.9435e-09],\n",
      "        [1.0016e-09, 6.6938e-11, 4.2729e-10,  ..., 4.5335e-12, 3.9412e-09,\n",
      "         2.0265e-09]], device='cuda:0')}, 1: {'step': 4914, 'exp_avg': tensor([ 3.6681e-05, -9.1911e-05, -4.0656e-05,  9.5221e-07,  2.7936e-05,\n",
      "         9.3467e-07, -9.1036e-07, -9.0971e-07,  2.3852e-06,  2.9067e-06,\n",
      "        -5.6683e-07, -5.9940e-07,  4.0126e-07,  1.7385e-07, -4.9739e-07,\n",
      "        -4.8189e-07, -6.9481e-07, -7.1556e-07, -7.2450e-07, -7.2468e-07,\n",
      "        -3.2055e-06, -3.7594e-06, -1.1023e-06, -1.1245e-06,  2.0252e-05,\n",
      "         1.4136e-06, -7.5574e-07, -8.6848e-07, -7.5143e-06, -4.9474e-07,\n",
      "        -5.0685e-07, -5.0036e-07,  2.9402e-07,  4.9881e-07, -1.7290e-06,\n",
      "         1.5301e-07,  6.1437e-05, -1.7596e-05,  7.4607e-05,  1.8772e-05,\n",
      "         2.6389e-06, -1.4922e-05,  6.4053e-06,  2.5497e-05, -4.4117e-07,\n",
      "        -4.3352e-07, -4.1945e-07, -4.4194e-07, -3.1533e-05, -1.6148e-05,\n",
      "        -1.9300e-05, -9.1912e-07, -5.9937e-05,  2.6665e-05,  3.8775e-06,\n",
      "        -5.9499e-07, -1.0499e-05, -5.6269e-05, -9.9677e-05,  7.6077e-05,\n",
      "        -5.8856e-06, -1.1706e-05,  2.3679e-07, -8.0099e-06, -6.8859e-05,\n",
      "         5.7215e-05,  8.3241e-06,  3.8649e-05, -6.1156e-06, -8.8946e-05,\n",
      "        -6.3327e-05, -3.5285e-05,  1.0257e-04, -1.3870e-05, -1.1817e-05,\n",
      "         2.8984e-06,  1.0234e-05, -1.6793e-05,  2.0061e-05,  3.2669e-06,\n",
      "        -2.2249e-05, -6.1674e-07, -6.2007e-07, -6.0041e-07, -5.5361e-05,\n",
      "         3.2396e-05,  4.0905e-07,  1.8379e-05, -2.5392e-06, -9.4960e-07,\n",
      "        -1.0676e-06, -1.0010e-06, -4.3742e-05, -7.0432e-07, -6.9300e-07,\n",
      "        -7.0613e-07,  2.4984e-05,  1.5494e-05, -5.8165e-07, -5.9316e-07,\n",
      "         6.2983e-05,  1.6852e-05, -1.9024e-05, -1.2244e-05,  1.5853e-05,\n",
      "        -9.4582e-06,  7.3219e-07,  7.8417e-07, -3.0675e-05,  7.8330e-06,\n",
      "        -8.4303e-08, -1.5023e-07, -2.3794e-05, -1.1788e-05, -2.5400e-06,\n",
      "         1.1091e-05,  3.2744e-05, -2.5252e-06, -6.4563e-06,  1.6549e-07,\n",
      "        -1.3382e-04, -2.0788e-05, -5.2226e-06,  1.9152e-07, -8.0215e-06,\n",
      "         1.8259e-05, -5.7784e-06,  3.0109e-06,  9.4980e-06,  2.8909e-06,\n",
      "        -4.1225e-07, -5.9499e-07, -8.2960e-06,  2.2844e-07,  2.2050e-07,\n",
      "        -1.1803e-06, -2.9526e-05, -1.0739e-04,  1.1873e-04,  4.5882e-05,\n",
      "         1.4383e-05,  1.4369e-06,  1.4126e-06,  1.4440e-06,  1.5427e-05,\n",
      "        -1.6513e-05,  1.2300e-05, -9.0307e-07,  1.3233e-06, -1.6798e-05,\n",
      "         1.3490e-06, -6.9033e-07,  2.3812e-05,  1.3077e-05, -1.3067e-04,\n",
      "         1.5777e-05, -3.4992e-05,  8.5017e-06, -2.1336e-05,  1.3362e-05,\n",
      "         4.0245e-05, -1.7470e-05, -1.3476e-06, -1.3429e-06,  1.1032e-05,\n",
      "         2.7856e-06, -2.3100e-07, -2.1729e-07, -4.2546e-05, -4.0438e-08,\n",
      "        -6.6607e-06,  2.8139e-06,  1.4442e-05, -1.6892e-05, -8.1841e-06,\n",
      "         1.4256e-06,  3.6936e-05, -1.7526e-05,  9.9158e-05,  1.8481e-05,\n",
      "         4.8557e-05,  2.0722e-06,  1.2738e-05, -9.9768e-07,  1.0164e-05,\n",
      "        -9.5087e-07,  2.1077e-05, -2.7067e-06, -1.3985e-05,  1.3238e-05,\n",
      "        -9.8273e-06,  2.6947e-06,  4.0285e-05, -2.5766e-06,  2.1892e-06,\n",
      "        -2.4462e-07, -4.9968e-05,  7.4386e-06,  1.0163e-05,  2.0708e-07,\n",
      "         2.8726e-06, -5.9689e-05,  2.3601e-05, -8.2665e-05,  4.4395e-06,\n",
      "        -2.5758e-06, -7.9624e-07, -8.0679e-07,  3.2300e-05,  1.1583e-06,\n",
      "        -3.8696e-07, -4.7729e-07, -1.2499e-04, -1.0450e-04, -4.7793e-05,\n",
      "        -3.0674e-05, -2.2000e-05, -1.3334e-05, -1.9180e-05, -1.1270e-05,\n",
      "         3.4493e-05, -1.1883e-05, -2.6591e-05, -2.3605e-05, -3.3231e-06,\n",
      "         5.2122e-07,  3.5767e-06,  3.5786e-06, -2.2802e-06,  8.6798e-06,\n",
      "        -2.5560e-06,  1.7836e-06, -2.6055e-05,  1.3767e-06,  7.5688e-07,\n",
      "        -1.2584e-07,  3.9134e-06,  5.6318e-07,  5.7068e-07,  5.7671e-07,\n",
      "        -1.7366e-05, -4.8439e-05, -6.0485e-05, -1.3145e-06,  1.4543e-05,\n",
      "        -4.2013e-06, -1.0840e-05, -1.1486e-05,  4.4749e-06, -1.0807e-06,\n",
      "        -1.0739e-06, -1.0748e-06, -1.0325e-05, -2.1515e-05,  6.5528e-05,\n",
      "         1.3058e-06, -5.4786e-06, -4.2177e-05,  2.5960e-05,  2.0236e-05,\n",
      "        -3.4374e-05, -3.4002e-05, -2.5485e-05, -2.2615e-05, -8.4041e-06,\n",
      "        -4.2905e-05, -1.6252e-06, -1.0483e-05,  2.2152e-05,  1.6369e-05,\n",
      "        -2.9678e-06, -1.0921e-05, -9.0311e-06, -1.8308e-05, -8.6444e-06,\n",
      "        -1.2032e-05,  1.5919e-05, -4.5348e-06, -4.2502e-05, -6.4282e-07,\n",
      "         2.2506e-05, -5.1132e-05, -2.7767e-05, -2.9316e-05, -6.9662e-06,\n",
      "        -1.8870e-05,  2.5976e-05,  2.4703e-05,  9.5701e-06, -2.2956e-07,\n",
      "        -1.2689e-05,  2.1097e-06,  4.8086e-06,  1.4629e-05,  2.7861e-06,\n",
      "        -2.6128e-06, -2.3332e-05, -6.8639e-07, -4.7946e-07, -4.8054e-07,\n",
      "         4.0289e-05, -3.3666e-06, -6.8911e-05,  1.7684e-05,  5.7189e-05,\n",
      "         1.2643e-05, -5.6896e-06,  1.2555e-06, -3.2606e-07, -3.2667e-07,\n",
      "        -3.1773e-07, -3.2549e-07, -1.0293e-06, -1.1598e-06, -1.0270e-06,\n",
      "        -1.0361e-06,  3.8245e-06,  1.1373e-04,  9.2938e-06,  6.6145e-06,\n",
      "         5.2806e-06, -1.0225e-05, -1.8301e-06,  8.9777e-06, -5.3299e-05,\n",
      "         3.0320e-05, -2.0092e-05, -7.8394e-06,  1.2711e-05,  3.4627e-06,\n",
      "         8.5066e-06, -9.6184e-07, -1.5073e-05,  2.8998e-05,  3.5370e-05,\n",
      "         5.3636e-06, -1.7241e-05, -7.0392e-06,  6.9366e-05, -1.3818e-04,\n",
      "        -3.5187e-05,  5.4239e-06, -2.9479e-05, -4.7090e-06, -4.0236e-05,\n",
      "         6.1519e-07,  1.6232e-06,  1.5093e-06, -1.7038e-05, -4.9750e-05,\n",
      "        -1.3757e-05, -2.1035e-05, -8.3553e-06, -3.4313e-06, -8.8123e-07,\n",
      "        -8.7575e-07, -2.2755e-05,  8.7433e-07,  1.0109e-06, -1.5637e-06,\n",
      "        -3.1457e-06,  2.7129e-06, -9.1057e-07, -1.3013e-06,  4.4626e-06,\n",
      "        -1.2550e-05,  1.0284e-07,  8.6789e-08, -2.9021e-05,  7.9186e-06,\n",
      "         5.9618e-07,  5.6891e-07, -3.2853e-07, -1.4717e-05, -7.2704e-06,\n",
      "        -4.9974e-06, -1.1378e-05,  2.4257e-06,  2.4914e-07,  2.5259e-07,\n",
      "        -2.0446e-06, -2.0385e-06, -2.5758e-06, -5.6910e-06,  5.0940e-07,\n",
      "         4.9222e-07,  5.2675e-07, -2.2846e-06, -7.7178e-06, -2.4345e-05,\n",
      "        -1.1032e-05,  1.0884e-06,  1.3123e-05,  2.2736e-05,  1.6575e-05,\n",
      "         3.9369e-07, -2.4095e-06,  4.7374e-05, -5.5944e-07,  3.2796e-05,\n",
      "        -6.0169e-07, -1.7283e-05, -1.3293e-05, -5.1102e-07, -9.4748e-06,\n",
      "         1.4179e-06, -2.1108e-05,  3.4041e-06,  4.6934e-06, -6.7320e-06,\n",
      "         2.4228e-06,  3.7600e-07, -8.4659e-07,  1.7530e-05,  7.0327e-06,\n",
      "         3.7855e-06,  1.7223e-04,  2.2446e-05,  1.3029e-04,  1.7276e-05,\n",
      "         4.6106e-06, -9.5378e-07, -2.2455e-05,  4.1018e-06, -2.7056e-05,\n",
      "        -1.9964e-05,  2.9633e-06,  4.6834e-07, -5.0467e-07, -5.0072e-07,\n",
      "        -4.9194e-07, -6.7389e-06,  7.7966e-05, -4.3320e-05,  6.5113e-05,\n",
      "        -4.3328e-05, -3.0972e-05, -4.7939e-05, -6.7072e-07, -6.6077e-07,\n",
      "         1.3844e-05,  3.0047e-06,  1.0085e-05,  1.7705e-06, -5.7018e-07,\n",
      "        -5.6882e-07, -5.5771e-07, -5.7494e-07,  2.0540e-05,  1.5081e-05,\n",
      "         2.1136e-05, -9.1278e-07, -2.0595e-06, -9.3504e-07, -9.2956e-07,\n",
      "        -9.3109e-07,  2.4756e-05, -4.0350e-06, -2.4192e-05, -2.1352e-07,\n",
      "         7.9959e-06, -8.2182e-06, -7.7831e-06,  1.9253e-06,  5.2842e-07,\n",
      "         7.2605e-06,  5.4350e-07,  3.4776e-06,  2.2188e-06, -1.4959e-07,\n",
      "        -3.1753e-06,  2.1060e-06], device='cuda:0'), 'exp_avg_sq': tensor([8.8568e-08, 6.6861e-08, 2.0602e-08, 3.2377e-09, 7.5001e-09, 1.2370e-09,\n",
      "        1.4019e-11, 1.4201e-11, 5.5223e-09, 8.5865e-10, 1.5242e-11, 1.4001e-11,\n",
      "        1.8613e-09, 6.0401e-10, 1.7354e-11, 1.7537e-11, 1.7792e-11, 2.1868e-11,\n",
      "        1.5511e-11, 1.5288e-11, 5.0454e-09, 1.4018e-09, 2.5362e-11, 1.7257e-11,\n",
      "        7.2574e-09, 3.1427e-09, 2.1671e-11, 2.2525e-11, 3.0686e-09, 2.2846e-11,\n",
      "        2.2796e-11, 2.2843e-11, 1.0544e-08, 1.6221e-09, 1.4032e-09, 2.3375e-11,\n",
      "        7.4539e-08, 9.1184e-08, 7.5958e-08, 6.4542e-08, 6.2294e-08, 8.2759e-08,\n",
      "        1.0023e-07, 3.5280e-08, 1.2185e-11, 1.2071e-11, 1.2236e-11, 1.2113e-11,\n",
      "        8.8108e-09, 2.5272e-09, 8.3961e-10, 1.8994e-11, 1.6986e-08, 1.9522e-08,\n",
      "        2.3569e-09, 1.2227e-11, 2.4692e-08, 8.6875e-08, 1.1000e-07, 1.0500e-07,\n",
      "        1.5296e-08, 1.7376e-08, 1.0596e-08, 1.8545e-09, 7.5282e-08, 1.0403e-07,\n",
      "        9.1765e-08, 3.0003e-08, 9.4622e-08, 1.3306e-07, 1.3302e-07, 6.6928e-08,\n",
      "        6.3410e-08, 6.9988e-08, 5.6433e-08, 2.3518e-08, 5.6318e-08, 2.1296e-08,\n",
      "        8.5807e-09, 1.5296e-09, 4.3069e-09, 1.6368e-11, 1.7400e-11, 1.6155e-11,\n",
      "        1.0853e-07, 1.5589e-08, 6.8937e-09, 3.4294e-09, 4.7474e-09, 1.4924e-11,\n",
      "        3.5886e-11, 1.9622e-11, 9.2530e-09, 1.7993e-11, 1.7960e-11, 1.8079e-11,\n",
      "        2.1894e-08, 3.6238e-09, 1.9005e-11, 1.5865e-11, 2.5926e-08, 2.0609e-08,\n",
      "        5.7111e-09, 7.3110e-09, 1.1767e-08, 1.4269e-09, 1.9666e-11, 1.6664e-11,\n",
      "        2.3374e-08, 6.4147e-09, 2.1983e-11, 1.7351e-11, 2.6937e-08, 6.7170e-08,\n",
      "        1.7441e-08, 2.5313e-09, 1.0087e-08, 1.4237e-09, 9.4841e-10, 1.3684e-11,\n",
      "        1.7264e-08, 1.2700e-08, 9.1033e-09, 2.5371e-11, 5.7813e-09, 8.4607e-09,\n",
      "        5.3822e-09, 1.6066e-09, 2.0207e-09, 5.9672e-10, 2.2454e-11, 2.0986e-11,\n",
      "        7.4675e-09, 4.9307e-10, 4.9342e-10, 2.2203e-11, 1.1040e-08, 1.3374e-08,\n",
      "        1.3089e-08, 8.0488e-09, 2.7762e-09, 1.4002e-11, 1.4102e-11, 1.3484e-11,\n",
      "        6.2479e-09, 9.2572e-09, 4.4844e-09, 1.4459e-11, 7.2081e-09, 5.6569e-09,\n",
      "        7.5868e-10, 1.8112e-11, 6.4278e-08, 9.8536e-08, 7.8879e-08, 1.9699e-08,\n",
      "        7.2123e-08, 1.5810e-07, 1.1686e-07, 2.3732e-08, 7.1806e-08, 3.2120e-08,\n",
      "        1.3797e-11, 1.0874e-11, 3.6722e-09, 7.8506e-10, 1.2922e-11, 1.2320e-11,\n",
      "        5.1786e-08, 1.7952e-08, 7.3701e-09, 1.4873e-09, 6.6151e-09, 1.2416e-08,\n",
      "        5.9051e-09, 6.9161e-10, 4.1708e-08, 4.9768e-08, 3.2581e-08, 9.8400e-09,\n",
      "        3.0488e-08, 2.2656e-08, 3.0618e-09, 1.1703e-11, 5.5880e-09, 4.0531e-09,\n",
      "        2.1752e-09, 1.6875e-09, 9.1853e-09, 5.0244e-09, 4.9644e-09, 5.8065e-10,\n",
      "        1.9708e-08, 2.8408e-09, 9.0739e-10, 1.2065e-11, 1.3970e-08, 1.1647e-08,\n",
      "        3.1177e-09, 1.0369e-11, 6.6502e-08, 5.1204e-08, 7.5017e-08, 1.0542e-07,\n",
      "        6.4219e-09, 5.5511e-09, 1.3797e-11, 1.3499e-11, 2.0191e-08, 2.9170e-09,\n",
      "        9.3221e-12, 1.1262e-11, 7.8470e-08, 1.1150e-07, 1.1300e-07, 5.2404e-08,\n",
      "        7.9300e-09, 1.5316e-08, 7.5174e-09, 4.3330e-09, 1.1953e-08, 8.6443e-09,\n",
      "        1.2157e-08, 9.2342e-09, 1.4271e-08, 1.5050e-09, 1.9275e-11, 1.9247e-11,\n",
      "        5.6391e-09, 5.0201e-09, 8.5425e-10, 2.7626e-11, 2.1941e-08, 5.5941e-09,\n",
      "        2.6427e-10, 1.4787e-11, 9.7703e-10, 1.5030e-11, 1.5004e-11, 1.5068e-11,\n",
      "        4.3032e-08, 4.4364e-08, 2.5996e-08, 1.5657e-08, 2.0835e-09, 7.8178e-09,\n",
      "        4.7694e-09, 3.7926e-09, 2.0547e-09, 1.7397e-11, 1.6699e-11, 1.6846e-11,\n",
      "        1.2204e-08, 1.6691e-08, 1.1807e-08, 8.1694e-10, 1.8534e-08, 2.9576e-08,\n",
      "        3.0110e-08, 1.2369e-08, 1.2423e-08, 1.3948e-08, 4.7415e-09, 2.4084e-09,\n",
      "        3.8286e-08, 3.1048e-08, 4.8486e-08, 1.5678e-08, 2.2139e-08, 1.8235e-08,\n",
      "        1.6892e-08, 2.3945e-09, 2.2372e-08, 5.4567e-09, 2.3035e-09, 1.6880e-09,\n",
      "        1.2338e-08, 7.5273e-09, 3.8645e-09, 1.5329e-11, 1.6528e-08, 2.6652e-08,\n",
      "        2.2887e-08, 1.3706e-08, 6.0466e-09, 2.3139e-08, 5.5172e-08, 3.6970e-08,\n",
      "        6.4501e-09, 1.0207e-08, 7.5651e-09, 2.8980e-09, 1.1709e-09, 8.2462e-09,\n",
      "        2.4814e-09, 3.7679e-10, 5.1868e-09, 1.0395e-09, 1.8469e-11, 1.8429e-11,\n",
      "        3.8672e-08, 6.1772e-08, 5.2270e-08, 1.7530e-08, 1.0479e-08, 8.9844e-09,\n",
      "        3.4488e-09, 1.3515e-11, 1.4951e-11, 1.4806e-11, 1.4968e-11, 1.4728e-11,\n",
      "        1.5448e-09, 8.7978e-10, 4.5496e-11, 4.6892e-11, 2.7550e-08, 6.1958e-08,\n",
      "        2.0583e-08, 4.3962e-09, 7.4017e-09, 6.2533e-09, 6.8772e-09, 2.2539e-09,\n",
      "        6.3257e-09, 4.7086e-09, 4.5392e-09, 2.1053e-09, 1.1614e-08, 4.5416e-09,\n",
      "        7.5045e-09, 1.1406e-11, 1.8721e-08, 1.3051e-08, 8.4277e-09, 3.0429e-09,\n",
      "        3.7369e-08, 6.5679e-08, 9.2549e-08, 7.7441e-08, 9.4401e-09, 9.2020e-09,\n",
      "        7.5823e-09, 2.0589e-09, 1.5721e-08, 6.1121e-09, 8.7525e-10, 8.7617e-10,\n",
      "        1.8906e-08, 3.2483e-08, 3.5641e-08, 1.3894e-08, 1.0085e-08, 1.3165e-09,\n",
      "        2.0258e-11, 1.8783e-11, 1.1372e-08, 1.6026e-09, 4.9716e-10, 7.2178e-10,\n",
      "        8.0335e-09, 1.7170e-09, 1.4727e-09, 2.2578e-11, 2.1351e-08, 1.7846e-08,\n",
      "        1.1200e-11, 1.1155e-11, 6.0184e-09, 2.0657e-09, 1.2536e-11, 1.2269e-11,\n",
      "        7.9957e-09, 9.6970e-09, 6.6675e-09, 6.7290e-10, 5.6553e-09, 8.6653e-10,\n",
      "        1.3136e-11, 1.3066e-11, 2.4209e-10, 2.3988e-10, 2.4964e-09, 1.3316e-08,\n",
      "        1.1248e-11, 1.1170e-11, 1.1337e-11, 1.0536e-09, 1.5259e-08, 1.6722e-08,\n",
      "        1.0657e-09, 1.3688e-11, 4.1664e-08, 1.9603e-08, 4.7405e-09, 1.4736e-11,\n",
      "        1.1756e-09, 1.0328e-08, 1.5700e-08, 1.0954e-08, 2.9460e-09, 1.2576e-08,\n",
      "        3.4564e-09, 1.3110e-11, 1.5123e-09, 5.9861e-09, 1.7360e-08, 8.4841e-09,\n",
      "        4.9249e-09, 4.3066e-09, 1.0520e-09, 8.0810e-10, 3.1641e-09, 9.8599e-09,\n",
      "        1.1052e-08, 8.8662e-10, 1.0846e-08, 3.1552e-08, 3.1544e-08, 1.8160e-08,\n",
      "        5.2455e-09, 6.6072e-09, 1.2484e-08, 6.3757e-09, 8.6547e-09, 4.5263e-09,\n",
      "        2.1360e-09, 1.0448e-11, 7.0429e-12, 8.7822e-12, 8.1282e-12, 3.2232e-09,\n",
      "        1.1403e-07, 1.6207e-07, 9.7176e-08, 1.5298e-08, 1.7494e-08, 6.9162e-08,\n",
      "        2.1525e-11, 2.1725e-11, 4.3991e-09, 1.4172e-09, 2.6972e-09, 2.6542e-09,\n",
      "        1.1200e-11, 1.2355e-11, 1.0769e-11, 1.1150e-11, 5.9995e-08, 1.0461e-07,\n",
      "        7.3618e-08, 2.7037e-09, 1.4384e-08, 1.5890e-11, 1.5722e-11, 1.6345e-11,\n",
      "        2.9317e-09, 8.0971e-09, 2.6536e-09, 1.5546e-11, 1.7583e-09, 2.9377e-09,\n",
      "        6.5306e-10, 1.2716e-11, 4.2753e-09, 6.8737e-09, 1.7584e-09, 7.5175e-10,\n",
      "        2.4007e-08, 2.2030e-09, 2.3390e-09, 2.7913e-09], device='cuda:0')}, 2: {'step': 4914, 'exp_avg': tensor([[-1.3862e-06, -1.6201e-05, -3.1592e-06,  ..., -1.1219e-05,\n",
      "          1.6373e-06,  4.4353e-06],\n",
      "        [-2.4186e-06, -4.9595e-05,  2.6315e-06,  ..., -1.6919e-05,\n",
      "          1.2136e-06,  2.5617e-06],\n",
      "        [-1.1089e-05,  7.3909e-06,  8.1778e-07,  ...,  5.0759e-06,\n",
      "         -3.7502e-07,  9.2811e-07],\n",
      "        ...,\n",
      "        [ 4.4290e-07, -9.6500e-06, -2.6498e-06,  ...,  6.6807e-06,\n",
      "          2.9136e-06, -1.0832e-05],\n",
      "        [ 5.6938e-07,  2.9302e-06,  5.8381e-07,  ..., -4.1535e-08,\n",
      "          2.5429e-06,  1.6188e-07],\n",
      "        [-1.0520e-07, -9.5531e-07, -3.9472e-07,  ..., -4.2691e-08,\n",
      "         -5.6093e-07, -6.9164e-09]], device='cuda:0'), 'exp_avg_sq': tensor([[4.4566e-10, 2.2019e-08, 2.3971e-10,  ..., 1.5286e-08, 6.0800e-10,\n",
      "         2.8789e-10],\n",
      "        [1.0290e-09, 5.2421e-08, 1.2641e-10,  ..., 1.4224e-08, 6.6610e-10,\n",
      "         8.0715e-10],\n",
      "        [8.7381e-10, 2.3911e-08, 8.6466e-11,  ..., 4.8681e-09, 2.0402e-10,\n",
      "         6.3611e-10],\n",
      "        ...,\n",
      "        [4.9299e-09, 1.0191e-07, 5.0539e-09,  ..., 4.6686e-09, 7.0236e-09,\n",
      "         5.9837e-10],\n",
      "        [5.4046e-10, 1.2808e-08, 9.1103e-10,  ..., 1.5150e-10, 2.8719e-09,\n",
      "         4.9186e-11],\n",
      "        [1.2166e-12, 1.5417e-11, 3.3265e-12,  ..., 9.7204e-13, 1.0237e-09,\n",
      "         9.3548e-12]], device='cuda:0')}, 3: {'step': 4914, 'exp_avg': tensor([ 1.7172e-05,  2.1255e-06, -1.3331e-05, -1.7593e-07, -3.1202e-05,\n",
      "        -5.1964e-05, -7.5033e-05,  1.5785e-05,  4.4441e-05, -8.8536e-05,\n",
      "        -8.9561e-05, -1.0735e-05, -2.1112e-05, -3.6684e-06, -1.2628e-05,\n",
      "        -8.0625e-06,  2.2825e-05,  7.3972e-05, -2.9505e-05,  7.3058e-06,\n",
      "         6.4622e-06,  1.2918e-05, -7.8135e-07, -2.2240e-07, -1.6408e-06,\n",
      "         1.8176e-05,  4.9523e-05,  2.6630e-05,  8.8155e-06, -1.9315e-06,\n",
      "         5.0268e-06, -7.4851e-06, -1.4599e-05, -1.0043e-05,  5.6661e-06,\n",
      "         1.8275e-06,  6.8323e-05,  3.1877e-06, -2.8423e-05,  1.8539e-05,\n",
      "         1.9226e-05, -7.1535e-06, -1.4237e-06, -6.5910e-06,  1.1289e-04,\n",
      "         1.7091e-05,  1.0514e-05,  5.8502e-07, -1.3405e-07,  9.8310e-06,\n",
      "         1.0206e-06,  9.6435e-07, -1.4071e-06, -1.2265e-06,  6.9869e-06,\n",
      "         9.5477e-07, -1.0697e-04, -8.9129e-05,  1.4298e-05,  1.2730e-06,\n",
      "         1.1695e-05,  2.4184e-06, -9.5304e-08,  9.9369e-09, -4.1288e-07,\n",
      "        -4.0721e-07, -4.0679e-07, -4.3368e-07, -3.7285e-05, -6.5151e-06,\n",
      "        -1.5064e-07, -9.9891e-08,  5.6964e-06,  2.8495e-06,  1.8975e-06,\n",
      "        -8.7962e-07,  2.1566e-05,  1.3931e-05, -9.1961e-06, -2.9750e-06,\n",
      "         8.2820e-06,  1.9068e-07,  1.9439e-07,  1.9409e-07, -1.9423e-05,\n",
      "        -1.0569e-06, -9.9627e-07, -9.9381e-07, -7.8005e-05,  1.1494e-06,\n",
      "         1.1717e-06,  1.1917e-06, -2.4394e-05, -2.2511e-05,  5.6645e-06,\n",
      "         3.0210e-06, -1.3683e-04, -8.2617e-06, -2.5901e-06,  6.8509e-07,\n",
      "         3.2984e-05, -3.3110e-06,  7.4026e-06,  4.1401e-07,  7.2016e-06,\n",
      "        -1.5453e-05,  3.8706e-07,  3.9241e-07, -2.4952e-05,  1.0808e-05,\n",
      "        -7.2402e-05, -8.7459e-06,  4.3635e-06,  4.2103e-06, -9.4236e-09,\n",
      "         9.5317e-09, -5.0678e-06,  6.5343e-06,  3.3226e-08,  3.4033e-08,\n",
      "         8.0630e-06, -7.5168e-07, -6.0078e-07, -7.0548e-07, -4.5286e-06,\n",
      "         2.2658e-06, -3.0654e-07, -3.3500e-07, -5.8873e-06, -1.3556e-05,\n",
      "        -3.9923e-06,  8.0881e-07, -6.6328e-06,  8.9125e-06,  3.8523e-07,\n",
      "        -1.0016e-06, -5.1558e-06,  4.7979e-06,  5.4643e-07,  5.2939e-07,\n",
      "         3.9409e-06, -1.0808e-06, -1.0617e-06, -1.0216e-06, -2.7601e-05,\n",
      "        -2.1097e-05,  1.4133e-05,  7.5380e-06, -8.8773e-07, -8.8335e-07,\n",
      "        -9.3964e-07, -9.4800e-07, -1.0062e-05,  6.2493e-06,  2.8488e-06,\n",
      "         3.7484e-07, -1.2535e-05, -4.7163e-06, -1.5151e-06, -1.5240e-06,\n",
      "         6.2496e-05,  1.4103e-05, -4.0344e-05, -4.6072e-06,  1.7941e-05,\n",
      "        -3.7973e-06,  1.0549e-05, -7.2720e-08, -2.4288e-06,  1.5601e-05,\n",
      "         2.7603e-05, -1.0757e-07, -1.9880e-05,  1.0934e-05,  1.9953e-05,\n",
      "         1.6460e-06, -6.1732e-06, -8.0913e-05, -2.8117e-05,  2.0758e-05,\n",
      "        -1.1057e-06, -9.2788e-07, -9.3387e-07, -9.7330e-07,  4.6644e-05,\n",
      "        -8.1709e-07, -8.7454e-07, -9.4731e-07,  2.8620e-05,  1.1767e-05,\n",
      "         3.5764e-05,  7.0661e-06, -2.4956e-05, -1.0295e-05, -1.2621e-05,\n",
      "        -1.1374e-06, -4.8384e-06, -5.6840e-05,  1.4933e-05,  6.1409e-07,\n",
      "         3.1901e-06,  6.9594e-07, -9.8486e-07, -9.7729e-07, -8.2437e-05,\n",
      "        -4.6246e-07, -4.3485e-07, -4.0201e-07,  1.7392e-05, -2.4747e-07,\n",
      "        -3.0000e-07, -2.3057e-07,  7.8606e-05, -2.4600e-06,  4.6851e-05,\n",
      "         1.9341e-05,  2.1289e-05,  7.5038e-07, -7.9623e-06, -3.7722e-07,\n",
      "         1.7194e-05, -2.9384e-05, -1.4041e-04, -8.1208e-06, -1.1518e-05,\n",
      "         1.3080e-05,  3.9240e-06,  1.0845e-05,  6.8288e-06, -2.0866e-05,\n",
      "         7.1994e-05,  1.0084e-05, -6.4637e-05, -7.1773e-05, -5.8845e-05,\n",
      "        -2.6065e-05, -8.6383e-05,  1.7286e-05, -3.4032e-06,  5.2920e-06,\n",
      "        -4.7191e-05,  1.5630e-05,  6.6793e-06,  5.9912e-06,  2.7007e-06,\n",
      "         4.1296e-06,  2.6909e-06,  1.8804e-06, -1.0727e-07,  2.8277e-06,\n",
      "         5.0584e-06, -2.7879e-07, -3.8682e-05,  2.1900e-05, -7.3164e-05,\n",
      "        -3.2473e-06,  1.1864e-05,  1.1751e-05, -1.7746e-06, -1.0551e-06,\n",
      "         5.3195e-05,  1.6835e-06,  2.2996e-05, -2.9416e-05,  5.1727e-06,\n",
      "         1.0866e-06, -2.2188e-06, -1.0948e-06, -9.5451e-06,  1.2670e-05,\n",
      "         9.5789e-07,  9.6431e-07,  3.6526e-06,  1.2517e-06,  1.2119e-06,\n",
      "         1.2005e-06, -2.2931e-05, -2.7248e-06,  3.2982e-06, -1.0202e-06,\n",
      "        -6.2966e-05,  2.1091e-05,  7.3457e-05,  7.1331e-06, -1.1168e-04,\n",
      "         1.1112e-05,  1.5214e-06,  1.5480e-06, -2.1909e-05,  2.5930e-05,\n",
      "         2.6211e-05,  1.5718e-05,  2.1275e-05,  7.8098e-06, -6.9976e-07,\n",
      "        -6.9759e-07, -2.8108e-06,  2.7715e-05, -5.6242e-06,  1.7847e-06,\n",
      "         6.4103e-06,  4.3064e-07,  4.2898e-07,  4.2809e-07,  4.0095e-05,\n",
      "        -1.5001e-05, -4.3526e-07, -4.2631e-07, -2.9065e-06,  2.1101e-07,\n",
      "         1.4881e-07,  1.7290e-07,  1.3112e-05, -1.6210e-05, -1.8121e-05,\n",
      "         2.7306e-05, -1.4124e-06, -1.0050e-04,  4.3260e-06,  2.0896e-06,\n",
      "        -3.1750e-05,  1.4906e-05, -2.3629e-05, -5.3090e-05,  5.7158e-05,\n",
      "         2.6061e-06, -3.1273e-07, -3.5014e-07, -2.3240e-05, -1.6081e-06,\n",
      "         1.2111e-06, -5.7337e-07, -1.0075e-05, -9.2575e-07, -9.2359e-07,\n",
      "        -9.2248e-07,  2.4112e-06,  1.0799e-07, -1.3428e-06, -1.6483e-06,\n",
      "         2.4780e-06, -2.5674e-05,  7.2993e-06, -1.2580e-06, -3.3849e-06,\n",
      "        -1.3114e-05,  1.1042e-05, -5.4192e-06,  1.0697e-05, -1.4914e-05,\n",
      "         1.0372e-05,  1.0792e-05,  1.7524e-05,  1.7531e-05, -3.0130e-06,\n",
      "        -6.1139e-06,  6.7086e-05,  5.5747e-05,  7.8584e-06,  2.3945e-05,\n",
      "         9.4190e-05,  3.6904e-05, -1.0517e-05,  3.6589e-06,  1.6210e-05,\n",
      "         1.3337e-05,  2.7024e-06, -1.5118e-06,  3.9141e-06, -2.1758e-07,\n",
      "         5.1308e-07,  5.1648e-07, -7.6109e-06,  4.7690e-05, -1.5626e-06,\n",
      "        -3.0008e-07, -1.7335e-05, -1.2678e-04, -1.5167e-04, -2.1509e-06,\n",
      "        -3.0464e-06,  7.7476e-06,  1.9993e-05, -7.5095e-06,  2.1940e-05,\n",
      "        -1.4646e-05,  9.9720e-06,  8.5275e-07,  2.1860e-05, -4.9453e-05,\n",
      "         2.2196e-05, -1.9478e-05, -1.8805e-05,  2.8906e-05, -6.2004e-07,\n",
      "        -7.1375e-07,  5.7101e-06, -2.8036e-05, -8.9846e-06,  1.1804e-05,\n",
      "         1.8008e-05,  1.9080e-05,  2.5029e-05,  2.2960e-05, -3.0647e-06,\n",
      "         4.0834e-05,  4.8423e-05, -4.8244e-06,  7.8981e-06, -5.3050e-07,\n",
      "        -5.3152e-07, -5.0929e-07,  8.7606e-06,  1.0473e-05, -7.8295e-06,\n",
      "        -2.4928e-05,  4.0080e-05,  6.9730e-05,  2.3919e-05,  3.5331e-05,\n",
      "        -3.0355e-05, -5.4705e-05,  4.0623e-05,  4.5440e-05, -8.9549e-06,\n",
      "        -2.0392e-05, -3.1522e-06, -2.2567e-07, -9.1643e-05,  7.4699e-06,\n",
      "         1.4141e-06,  1.4097e-06,  1.1992e-07,  1.9495e-06,  3.8120e-07,\n",
      "        -5.9019e-08, -2.8084e-07, -8.6015e-06, -8.1584e-06, -7.7010e-06,\n",
      "         2.0897e-07, -1.1760e-05, -3.8110e-09, -2.2692e-05,  4.2334e-05,\n",
      "        -2.2285e-06,  6.6349e-05,  6.2190e-05, -1.0843e-05, -5.2040e-07,\n",
      "        -5.2028e-07, -5.2047e-07, -4.2441e-05, -2.1549e-05, -4.3732e-05,\n",
      "        -1.3945e-04,  4.3760e-05,  7.7338e-07,  8.0370e-07,  8.0802e-07,\n",
      "        -3.1502e-06,  9.4693e-06,  3.6817e-05,  2.1519e-05,  1.6369e-06,\n",
      "         1.6688e-05,  6.6411e-06,  1.8232e-06,  9.3433e-07,  1.7412e-05,\n",
      "         1.0909e-05, -4.8475e-06, -5.9754e-05,  4.4581e-05,  3.1823e-05,\n",
      "         1.1365e-05,  2.3360e-05, -9.0346e-05, -2.6899e-05,  1.7832e-05,\n",
      "        -5.6840e-06, -1.2723e-05, -1.8701e-05,  6.5667e-06, -1.2252e-05,\n",
      "        -4.6265e-05,  2.6824e-06,  3.0174e-06, -2.4976e-06, -1.5187e-04,\n",
      "        -3.2294e-05,  8.2159e-08, -2.0940e-05, -1.0528e-05,  2.2880e-05,\n",
      "        -6.7614e-06,  3.4603e-07, -6.5819e-06, -2.1643e-05,  1.1725e-05,\n",
      "         5.2983e-05,  4.3871e-05, -5.6513e-06, -2.0161e-06, -1.0502e-05,\n",
      "        -3.1813e-06,  1.8653e-06, -1.4091e-06,  8.2105e-05, -2.0681e-07,\n",
      "         1.0844e-05, -4.7985e-07, -2.9653e-05, -4.9865e-06,  6.8984e-05,\n",
      "         1.5264e-05,  6.6934e-07,  6.5868e-07, -4.2039e-45,  4.2039e-45,\n",
      "         2.8026e-45, -5.6052e-45, -4.2039e-45, -5.6052e-45,  1.9809e-06,\n",
      "         1.4758e-06,  1.4948e-06,  1.5014e-06,  2.4331e-06, -5.9773e-06,\n",
      "         1.8971e-05, -1.1279e-05,  9.5247e-06,  4.5522e-05,  4.4704e-07,\n",
      "         4.7927e-07, -6.0458e-06, -6.8001e-06,  2.6587e-06,  3.9831e-06,\n",
      "        -3.8055e-06, -1.0844e-06,  5.1425e-06,  3.1175e-06,  4.6375e-06,\n",
      "         4.3604e-06,  1.6093e-06, -8.9868e-07, -4.6913e-06, -1.5122e-05,\n",
      "        -1.1090e-05,  3.0506e-06, -3.2199e-05,  5.0013e-07,  8.8747e-06,\n",
      "         2.8649e-06,  1.6237e-05,  4.3088e-05, -5.9242e-05, -2.3002e-05,\n",
      "        -3.8656e-06,  6.4159e-05,  7.9520e-05,  2.1483e-05, -3.9278e-05,\n",
      "        -6.7143e-06, -1.4834e-07,  2.0938e-05, -2.2792e-05, -5.8357e-06,\n",
      "         2.9548e-06,  2.9089e-06,  5.7253e-05,  6.2254e-06,  1.3810e-06,\n",
      "         1.3982e-06, -4.5190e-05,  4.6254e-06,  1.6469e-06,  1.6281e-06,\n",
      "        -1.9923e-06, -8.1388e-07, -2.6386e-06, -1.3216e-06,  1.2957e-05,\n",
      "        -1.0827e-05, -9.0299e-06, -2.6572e-06, -7.3748e-06,  4.9717e-06,\n",
      "         3.1594e-06,  3.1504e-06,  3.7989e-05,  5.3412e-08,  1.5425e-05,\n",
      "         7.8368e-06, -9.9493e-06,  3.1310e-05, -1.9005e-05, -3.9174e-06,\n",
      "        -3.5254e-07,  2.3813e-07,  4.2039e-45,  4.2039e-45, -3.6394e-07,\n",
      "         1.5961e-07,  1.4013e-45, -1.4013e-45,  1.0591e-05,  7.7700e-06,\n",
      "        -4.5851e-06, -9.5151e-07,  1.1866e-06,  1.1861e-06,  1.9408e-06,\n",
      "         3.8816e-06,  5.0147e-07, -2.4762e-07, -2.5194e-07, -6.5642e-06,\n",
      "         2.4686e-06, -4.7635e-05, -3.1534e-06, -1.2139e-06], device='cuda:0'), 'exp_avg_sq': tensor([8.1001e-09, 1.2009e-08, 7.7577e-09, 1.8946e-11, 1.4705e-08, 1.8963e-08,\n",
      "        2.5827e-08, 2.9553e-08, 2.1561e-07, 1.8121e-07, 1.4166e-07, 4.9117e-08,\n",
      "        1.4305e-08, 6.1449e-09, 4.8332e-09, 2.3138e-09, 2.4710e-08, 3.2781e-08,\n",
      "        2.5894e-08, 7.5334e-09, 1.3630e-08, 1.0164e-08, 4.6992e-09, 5.8980e-10,\n",
      "        8.5992e-09, 1.9851e-08, 1.7498e-08, 4.3181e-09, 6.5221e-09, 2.7498e-09,\n",
      "        1.8189e-09, 1.2623e-10, 1.9394e-08, 3.6086e-08, 7.6819e-09, 8.8051e-10,\n",
      "        4.9618e-08, 4.6748e-08, 4.7483e-08, 2.3334e-08, 9.9332e-09, 8.9107e-09,\n",
      "        4.6411e-09, 1.6228e-09, 1.4446e-08, 6.5309e-09, 1.8318e-09, 1.6264e-11,\n",
      "        1.1973e-08, 2.4987e-09, 1.7890e-11, 2.0014e-11, 7.4498e-09, 2.5062e-09,\n",
      "        1.3012e-09, 1.9125e-11, 1.2410e-07, 3.7087e-08, 1.2612e-08, 1.1152e-09,\n",
      "        7.8836e-09, 5.5670e-10, 1.5161e-11, 1.6773e-11, 1.2253e-11, 1.2462e-11,\n",
      "        1.2400e-11, 1.2101e-11, 2.1991e-08, 7.8720e-09, 1.5955e-11, 1.9023e-11,\n",
      "        2.1471e-09, 1.0594e-09, 1.3850e-09, 1.7944e-11, 2.4369e-08, 1.6563e-08,\n",
      "        1.7000e-08, 3.0428e-09, 2.1658e-09, 1.7049e-11, 1.6879e-11, 1.7196e-11,\n",
      "        7.9275e-09, 1.4881e-11, 1.3760e-11, 1.4400e-11, 4.1932e-09, 1.3860e-11,\n",
      "        1.5793e-11, 1.3922e-11, 5.4927e-09, 6.4315e-09, 3.8985e-09, 1.5720e-11,\n",
      "        2.3880e-08, 2.2239e-09, 3.6960e-09, 2.9647e-11, 2.2383e-08, 1.5142e-08,\n",
      "        4.8490e-09, 1.0909e-11, 1.3106e-08, 1.0407e-09, 3.0654e-11, 2.6187e-11,\n",
      "        5.9484e-08, 5.0387e-08, 5.3949e-08, 6.0331e-08, 1.6372e-09, 7.9210e-10,\n",
      "        1.4423e-11, 1.4442e-11, 3.8162e-09, 1.0067e-09, 1.4924e-11, 1.4923e-11,\n",
      "        1.5423e-08, 1.3126e-11, 3.5292e-11, 1.6587e-11, 1.1370e-08, 7.6704e-10,\n",
      "        1.3982e-11, 2.6051e-11, 5.0357e-09, 4.7986e-09, 3.2205e-10, 1.9123e-11,\n",
      "        1.5145e-08, 5.3352e-09, 9.9754e-10, 1.3893e-11, 1.1697e-08, 1.9195e-09,\n",
      "        1.4790e-11, 1.4792e-11, 1.3073e-09, 4.4283e-10, 1.7652e-11, 1.9648e-11,\n",
      "        1.1609e-08, 7.7927e-09, 4.5940e-09, 1.7695e-09, 1.9310e-11, 1.9651e-11,\n",
      "        1.9524e-11, 1.9529e-11, 3.1857e-08, 9.9891e-09, 8.5344e-10, 1.4339e-11,\n",
      "        1.6160e-08, 2.3020e-09, 1.8950e-11, 1.5695e-11, 2.8380e-08, 2.2534e-08,\n",
      "        1.3753e-08, 1.6805e-09, 1.1185e-08, 1.0587e-08, 1.0556e-08, 2.9028e-09,\n",
      "        4.6888e-09, 4.6249e-09, 6.8446e-09, 2.8107e-11, 5.4025e-09, 5.8319e-09,\n",
      "        7.4377e-09, 8.0551e-10, 3.5596e-08, 3.1289e-08, 3.4716e-08, 1.4526e-08,\n",
      "        3.1031e-08, 3.9891e-11, 2.8616e-11, 1.8056e-11, 4.9922e-08, 3.9682e-11,\n",
      "        2.7674e-11, 1.4966e-11, 1.6614e-08, 2.6151e-08, 4.8981e-08, 4.1721e-09,\n",
      "        3.8111e-08, 7.1728e-09, 7.8838e-10, 1.5881e-11, 3.2744e-08, 6.5099e-09,\n",
      "        1.1142e-09, 1.5529e-11, 3.6795e-09, 7.9932e-10, 1.5236e-11, 1.4008e-11,\n",
      "        3.6022e-08, 9.0484e-12, 1.1535e-11, 2.5444e-11, 1.0019e-08, 1.7952e-11,\n",
      "        1.7481e-11, 1.5064e-11, 5.8881e-08, 5.9634e-08, 5.9611e-08, 3.5926e-08,\n",
      "        2.4278e-09, 3.9143e-09, 2.2772e-09, 1.3246e-11, 6.2571e-08, 6.2097e-08,\n",
      "        7.8620e-08, 5.7055e-08, 1.3868e-08, 2.1752e-08, 2.9113e-08, 1.3296e-08,\n",
      "        4.2796e-09, 2.2434e-08, 5.6769e-08, 7.6084e-09, 3.1247e-08, 9.8869e-08,\n",
      "        1.8686e-07, 2.0031e-07, 2.2550e-09, 2.6206e-09, 2.2746e-09, 1.6072e-09,\n",
      "        3.2479e-09, 3.8853e-09, 3.1480e-09, 2.7127e-09, 5.7763e-10, 1.0208e-09,\n",
      "        1.4029e-09, 1.2714e-09, 6.7382e-10, 1.9114e-09, 1.6491e-09, 6.0072e-10,\n",
      "        1.2255e-07, 7.7747e-08, 6.6862e-08, 1.6094e-08, 1.2383e-08, 1.3181e-08,\n",
      "        2.7161e-09, 1.6330e-11, 3.4659e-08, 3.7621e-08, 1.7664e-08, 1.0032e-08,\n",
      "        4.7780e-09, 2.1574e-10, 3.4124e-10, 1.9347e-11, 1.7014e-08, 2.0558e-09,\n",
      "        1.9309e-11, 1.9448e-11, 8.9768e-10, 7.7478e-12, 7.3131e-12, 7.3876e-12,\n",
      "        3.0848e-08, 1.2142e-08, 6.6896e-09, 1.6991e-11, 2.9205e-08, 4.6231e-08,\n",
      "        3.4241e-08, 3.3466e-09, 1.8457e-08, 2.3144e-09, 1.4468e-11, 1.4840e-11,\n",
      "        8.3585e-09, 1.9175e-08, 2.3206e-08, 1.2619e-08, 1.1913e-08, 2.6952e-09,\n",
      "        1.4982e-11, 1.5326e-11, 1.6377e-09, 9.6081e-09, 5.2959e-09, 5.7086e-09,\n",
      "        7.2352e-09, 1.3581e-11, 1.3742e-11, 1.3732e-11, 4.1938e-09, 1.1123e-08,\n",
      "        1.2786e-11, 1.2722e-11, 4.2739e-09, 2.3802e-11, 2.1264e-11, 2.1634e-11,\n",
      "        6.5070e-09, 1.2374e-08, 1.3370e-08, 1.3926e-08, 1.7771e-08, 5.8734e-09,\n",
      "        1.5709e-09, 1.8241e-11, 1.1811e-08, 2.5565e-08, 2.4572e-08, 1.3128e-08,\n",
      "        3.1020e-08, 1.1103e-08, 1.2810e-11, 1.4233e-11, 1.5585e-08, 1.9663e-09,\n",
      "        8.4043e-10, 2.1608e-11, 2.3101e-09, 1.2246e-11, 1.2370e-11, 1.2316e-11,\n",
      "        1.7187e-09, 8.9233e-10, 1.8189e-09, 1.3439e-11, 4.2033e-08, 2.7661e-08,\n",
      "        7.5083e-09, 2.7522e-11, 2.9789e-09, 1.1134e-08, 1.3204e-08, 4.2340e-09,\n",
      "        1.2997e-08, 1.1132e-08, 8.3241e-09, 2.1082e-09, 2.0121e-08, 1.1038e-08,\n",
      "        9.4275e-09, 5.3836e-09, 1.0961e-08, 9.8514e-09, 7.8684e-09, 3.5848e-09,\n",
      "        4.8536e-09, 9.9421e-09, 4.5336e-09, 1.6447e-09, 1.2367e-08, 6.0366e-09,\n",
      "        2.5633e-09, 5.0491e-11, 3.5941e-09, 3.5387e-09, 1.6203e-11, 1.6367e-11,\n",
      "        9.0468e-09, 9.6329e-09, 1.0767e-09, 1.3767e-11, 8.2480e-09, 7.5659e-09,\n",
      "        8.3255e-09, 2.9082e-09, 2.1726e-08, 1.9345e-08, 2.3376e-08, 1.2438e-08,\n",
      "        5.0441e-08, 5.1298e-08, 4.5519e-09, 3.5465e-11, 4.5418e-08, 4.0118e-08,\n",
      "        3.4215e-08, 1.1262e-08, 3.7568e-08, 1.0994e-08, 1.2348e-11, 2.1933e-11,\n",
      "        1.2397e-08, 3.7155e-08, 4.4628e-08, 1.1741e-08, 5.4156e-09, 6.9095e-09,\n",
      "        4.1012e-09, 2.8284e-09, 1.4757e-08, 1.6221e-08, 1.5067e-08, 1.9678e-08,\n",
      "        2.4555e-09, 1.0020e-11, 9.9527e-12, 1.0842e-11, 1.4483e-08, 3.0429e-08,\n",
      "        6.0471e-08, 3.5756e-08, 2.3215e-08, 1.5365e-08, 1.1750e-08, 6.6062e-09,\n",
      "        1.6038e-08, 1.7883e-08, 3.0627e-08, 2.9034e-08, 1.1118e-08, 1.1680e-08,\n",
      "        3.2462e-09, 7.6513e-10, 2.4729e-09, 1.0103e-09, 3.6373e-10, 3.6360e-10,\n",
      "        3.5812e-09, 1.3974e-09, 9.3879e-10, 7.0943e-10, 2.4023e-09, 4.9450e-09,\n",
      "        6.9396e-09, 5.5630e-09, 2.3205e-10, 7.4712e-09, 8.9287e-09, 3.5128e-09,\n",
      "        2.1727e-08, 6.7589e-08, 1.5194e-07, 5.4816e-08, 4.7867e-09, 8.5575e-12,\n",
      "        8.6127e-12, 8.5919e-12, 1.4181e-08, 1.0514e-08, 1.1124e-08, 1.4496e-08,\n",
      "        1.2869e-08, 9.5938e-12, 9.6163e-12, 9.8919e-12, 5.1810e-09, 3.9500e-09,\n",
      "        4.8456e-09, 4.0928e-09, 9.6858e-09, 4.6820e-09, 4.3788e-09, 3.9494e-10,\n",
      "        2.2504e-09, 5.6261e-09, 6.6293e-09, 5.0434e-09, 7.2517e-09, 1.3161e-08,\n",
      "        1.2365e-08, 4.3150e-09, 4.0375e-08, 4.5945e-08, 3.7834e-08, 1.6182e-08,\n",
      "        5.4551e-09, 8.7089e-09, 8.8467e-09, 4.6855e-09, 9.5513e-09, 1.0013e-08,\n",
      "        6.7414e-09, 8.6150e-10, 1.9252e-08, 1.7298e-08, 3.3416e-09, 8.8173e-10,\n",
      "        5.7304e-09, 8.7700e-09, 7.3095e-09, 4.2654e-09, 1.4348e-11, 2.0779e-09,\n",
      "        4.9615e-09, 3.7266e-09, 1.2808e-08, 1.7966e-08, 1.1669e-08, 1.6381e-09,\n",
      "        5.7191e-09, 2.7173e-08, 2.1345e-08, 5.4543e-09, 1.9663e-08, 2.1852e-08,\n",
      "        5.1936e-09, 1.6469e-11, 7.9107e-09, 5.5475e-09, 9.3653e-09, 2.8546e-09,\n",
      "        9.8046e-12, 1.1426e-11, 2.5406e-19, 3.4813e-17, 1.6458e-17, 1.3627e-17,\n",
      "        1.5463e-18, 5.9329e-17, 3.0535e-09, 5.6582e-12, 5.6563e-12, 5.4773e-12,\n",
      "        9.2452e-09, 6.0740e-09, 1.4247e-08, 1.5507e-08, 9.5911e-09, 1.9455e-08,\n",
      "        1.0553e-11, 1.0431e-11, 7.3762e-09, 3.2570e-08, 7.1912e-09, 2.7711e-09,\n",
      "        2.5160e-09, 6.8405e-09, 7.6904e-09, 5.6491e-09, 5.1340e-09, 5.3753e-09,\n",
      "        5.5842e-10, 1.6290e-11, 6.9443e-09, 7.2655e-09, 2.7941e-09, 6.0960e-10,\n",
      "        1.4123e-08, 1.6233e-08, 8.7358e-09, 3.6908e-09, 2.3396e-08, 1.8207e-07,\n",
      "        4.1617e-08, 2.8756e-09, 1.7480e-08, 4.4318e-08, 5.1058e-08, 2.2343e-08,\n",
      "        2.9061e-08, 3.0980e-08, 1.9587e-08, 8.5917e-09, 6.1308e-09, 2.4233e-09,\n",
      "        4.1784e-10, 6.9566e-10, 9.4784e-09, 9.5225e-10, 6.1816e-12, 5.7901e-12,\n",
      "        5.2136e-09, 9.4471e-10, 5.6585e-12, 6.4956e-12, 2.1181e-09, 1.1753e-09,\n",
      "        1.2650e-09, 1.6775e-11, 7.9348e-09, 1.0952e-08, 5.4955e-09, 6.0151e-09,\n",
      "        3.9828e-09, 1.7520e-09, 9.1825e-10, 9.1872e-10, 9.3829e-09, 5.5213e-09,\n",
      "        7.1652e-09, 2.4323e-09, 8.4976e-09, 1.4195e-08, 1.7326e-08, 8.7361e-10,\n",
      "        4.5505e-12, 7.9231e-12, 1.4924e-17, 2.8421e-17, 3.9008e-12, 6.1879e-12,\n",
      "        2.3216e-18, 5.6126e-18, 4.4756e-09, 2.1485e-09, 1.8415e-09, 1.7996e-11,\n",
      "        2.3737e-11, 2.3956e-11, 1.4464e-09, 2.4694e-09, 1.2439e-11, 2.2510e-10,\n",
      "        2.2474e-10, 1.7646e-09, 5.2607e-09, 2.2967e-08, 3.4438e-09, 9.7481e-12],\n",
      "       device='cuda:0')}, 4: {'step': 4914, 'exp_avg': tensor([[ 7.3584e-05, -5.2905e-04,  1.8262e-05,  ..., -4.9633e-06,\n",
      "         -2.4464e-05, -1.2676e-05],\n",
      "        [-1.7831e-08,  3.7641e-05,  1.1463e-05,  ..., -1.0282e-05,\n",
      "         -3.6197e-05,  5.7244e-06],\n",
      "        [-1.0654e-05,  6.2043e-05, -3.7130e-06,  ...,  2.9189e-05,\n",
      "         -1.7127e-05, -2.8531e-05],\n",
      "        ...,\n",
      "        [-4.9354e-07,  2.0329e-05, -2.1664e-07,  ...,  1.6675e-05,\n",
      "          1.0990e-05,  3.5653e-06],\n",
      "        [ 1.1913e-09,  4.2170e-06,  2.6122e-06,  ..., -2.4391e-05,\n",
      "          1.2100e-05,  1.3622e-06],\n",
      "        [-1.6888e-07,  2.6184e-06, -3.5277e-08,  ..., -2.9281e-07,\n",
      "         -5.3389e-07, -2.6995e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[9.4351e-09, 3.0900e-07, 6.8720e-09,  ..., 3.7077e-08, 9.1779e-09,\n",
      "         3.6992e-08],\n",
      "        [1.5455e-08, 2.2925e-07, 1.0339e-08,  ..., 2.5537e-08, 1.5677e-08,\n",
      "         2.2712e-08],\n",
      "        [1.9054e-08, 1.4008e-07, 6.7512e-09,  ..., 2.1532e-08, 1.6210e-08,\n",
      "         2.4187e-08],\n",
      "        ...,\n",
      "        [2.5944e-09, 7.2696e-08, 9.7118e-10,  ..., 3.6183e-08, 1.4363e-08,\n",
      "         5.3278e-09],\n",
      "        [8.7378e-10, 5.9994e-09, 9.6416e-10,  ..., 1.0786e-08, 1.2813e-08,\n",
      "         1.7044e-09],\n",
      "        [6.1606e-12, 7.9862e-12, 6.1038e-13,  ..., 3.2259e-12, 3.9861e-12,\n",
      "         1.5067e-12]], device='cuda:0')}, 5: {'step': 4914, 'exp_avg': tensor([-6.0344e-05,  2.7087e-06,  3.4197e-05,  1.1583e-05, -2.0557e-04,\n",
      "         2.6622e-05,  6.2404e-05, -1.2473e-05,  1.6500e-05,  6.3671e-06,\n",
      "         5.1040e-06,  1.7902e-06,  7.1563e-06,  1.4754e-04,  2.7327e-05,\n",
      "        -3.4906e-05,  1.4903e-05,  2.2157e-04,  1.3989e-04, -1.7647e-05,\n",
      "        -1.3133e-04, -1.4660e-05,  1.7734e-04,  5.5887e-05,  4.3908e-07,\n",
      "         8.5876e-07,  1.0585e-05,  8.3700e-06,  3.9503e-06, -3.7662e-07,\n",
      "        -2.7999e-05,  1.3238e-05, -2.5447e-05, -2.2393e-06, -2.9625e-05,\n",
      "         4.7245e-06,  6.7726e-06, -1.7218e-06, -2.7708e-05,  3.3819e-05,\n",
      "         9.0655e-06,  1.0532e-05,  1.3113e-05,  1.9404e-05, -1.4115e-05,\n",
      "         9.0850e-06,  1.3868e-04,  1.4922e-04, -6.1407e-06, -2.1597e-05,\n",
      "        -4.8958e-05, -1.6752e-06, -2.7643e-06, -6.4099e-06, -2.3065e-05,\n",
      "        -4.4817e-07,  1.8794e-05,  9.9839e-06, -7.3046e-05, -9.7764e-06,\n",
      "        -7.3121e-06, -3.6961e-05, -5.5033e-05, -3.5698e-05, -1.3706e-04,\n",
      "        -6.0348e-05,  3.1294e-04,  9.2485e-05,  8.9063e-06, -1.2749e-04,\n",
      "         2.3490e-05, -5.1142e-06, -6.7382e-05,  7.2898e-05, -4.0677e-06,\n",
      "         4.0319e-05, -1.2635e-04,  1.1093e-04, -9.6289e-05, -2.9217e-05,\n",
      "        -5.9098e-07, -2.0590e-05, -2.1402e-05, -3.4879e-05, -6.5327e-05,\n",
      "        -5.4907e-05, -1.9691e-05, -1.2386e-06,  4.0580e-05, -2.3986e-05,\n",
      "         5.3465e-06, -9.1582e-06, -2.0594e-04, -7.0641e-06,  5.5331e-05,\n",
      "         3.4316e-06, -3.3592e-08, -2.4784e-06, -1.5296e-05, -4.4543e-05,\n",
      "         2.3447e-05, -2.1644e-04,  2.0303e-04,  8.0900e-05,  5.3372e-05,\n",
      "         1.4306e-04, -2.0164e-04, -2.2375e-05,  1.1074e-04,  1.7916e-05,\n",
      "        -5.4168e-05,  6.5233e-05, -7.3013e-05, -9.2399e-05,  1.4689e-04,\n",
      "         1.2853e-05, -1.8255e-07, -1.1712e-05, -5.1608e-05, -1.1188e-05,\n",
      "        -2.4428e-05, -2.2930e-04, -8.0230e-05,  5.4162e-05, -6.1974e-05,\n",
      "         1.1685e-04,  5.9729e-05,  6.4522e-05, -9.9054e-05,  3.8048e-06,\n",
      "         8.9400e-05,  3.9132e-05,  6.0026e-06, -3.3048e-05,  7.5651e-05,\n",
      "         2.4340e-05, -5.7246e-05,  1.8453e-04,  1.1366e-04, -1.6910e-05,\n",
      "        -2.7301e-06,  5.5570e-06,  1.7887e-05,  8.0665e-07, -1.4459e-04,\n",
      "         1.2250e-04, -9.0023e-05, -1.7656e-04, -1.4740e-04, -3.5804e-05,\n",
      "        -1.0723e-04,  3.2368e-05, -4.6366e-05,  4.1230e-05, -6.1100e-06,\n",
      "        -1.3236e-05,  1.6338e-06,  1.3645e-06,  1.6997e-05, -4.6688e-05,\n",
      "        -6.0458e-05,  1.7379e-05, -1.8315e-05,  7.5602e-06,  2.1962e-05,\n",
      "         1.1991e-04, -1.3830e-05, -3.5293e-05,  1.6432e-04,  2.4160e-04,\n",
      "         1.9631e-04, -8.5155e-05, -6.7816e-06,  1.7387e-04, -1.1690e-04,\n",
      "         4.7401e-05,  6.3371e-07, -8.3423e-05, -2.1622e-04,  2.9193e-05,\n",
      "        -1.8030e-04, -1.5438e-04,  6.8208e-05, -5.0674e-07, -2.3755e-05,\n",
      "        -5.8933e-05,  3.1467e-05, -4.3294e-06,  3.8853e-05, -5.2058e-05,\n",
      "         1.4503e-05, -1.2875e-05, -8.9247e-06, -1.3650e-05, -3.4510e-05,\n",
      "         1.5805e-05,  6.7351e-05,  1.0288e-04,  2.6960e-05,  8.6521e-06,\n",
      "        -7.4966e-07,  1.2227e-05,  1.2302e-05,  5.0140e-06, -4.7990e-05,\n",
      "         2.4954e-05, -1.0587e-05, -2.1391e-05,  2.0622e-05, -1.1763e-05,\n",
      "         2.0476e-05,  7.9102e-06,  1.9401e-05,  6.8742e-05, -1.6006e-06,\n",
      "        -3.9839e-05, -1.6531e-05, -3.5072e-06, -4.4677e-05, -2.5410e-05,\n",
      "        -6.5223e-06, -5.3830e-05, -1.5416e-05, -5.4829e-07, -8.5654e-06,\n",
      "        -1.2647e-05,  5.0257e-05,  3.1492e-05, -4.5983e-05, -2.2538e-04,\n",
      "        -1.8771e-04, -3.5101e-04, -2.5552e-05, -1.5433e-05, -1.7348e-05,\n",
      "        -1.3503e-05, -3.3675e-05, -1.7453e-05,  2.2656e-05, -7.7924e-06,\n",
      "         1.2793e-04,  3.3280e-05,  5.0920e-06, -1.2834e-06,  2.6753e-05,\n",
      "        -1.0676e-04, -1.2369e-04, -2.1810e-04, -4.4040e-07, -1.2247e-05,\n",
      "        -1.7654e-05, -1.8450e-05, -2.8208e-05, -1.1026e-04,  3.6869e-06,\n",
      "         9.9264e-06, -1.6556e-05,  8.7080e-06, -1.5494e-05,  1.4005e-05,\n",
      "         4.2450e-06,  3.9702e-06,  1.8598e-07, -7.4582e-06,  2.9880e-05,\n",
      "        -1.3002e-05, -4.1163e-05,  3.4155e-05, -2.1475e-06, -4.7796e-05,\n",
      "        -1.7904e-05,  4.3128e-05,  2.8592e-05,  2.9068e-05, -1.9773e-06,\n",
      "         3.5059e-07, -2.0751e-05, -1.2497e-06, -1.0189e-04,  1.7823e-05,\n",
      "         3.0971e-05, -7.9045e-08,  9.6757e-06,  1.1703e-05,  8.4399e-06,\n",
      "         7.1406e-06, -2.7100e-06, -1.6632e-06,  1.1812e-05, -9.4052e-06,\n",
      "        -2.1924e-05, -2.6046e-05,  4.3380e-05,  3.9668e-05,  4.3880e-06,\n",
      "         1.4420e-06,  1.5584e-05, -4.5747e-06, -5.1594e-06,  1.9684e-05,\n",
      "        -1.1826e-05,  1.2705e-04, -1.7657e-05,  5.2223e-06, -1.0921e-04,\n",
      "         1.1310e-05, -2.4163e-05, -5.4968e-05,  3.6404e-05,  4.2475e-05,\n",
      "         6.0374e-06, -2.6385e-05, -7.9944e-06, -4.6396e-06, -7.3662e-05,\n",
      "        -3.3262e-05, -6.4508e-06,  1.2735e-05,  4.5809e-05,  7.2858e-06,\n",
      "         1.1316e-05,  3.2449e-06, -1.4248e-05, -1.8159e-05,  3.7480e-05,\n",
      "        -2.5427e-05, -3.5564e-05, -2.9312e-06,  6.6517e-05, -1.5110e-04,\n",
      "         4.8955e-05,  5.4753e-05,  1.1275e-05,  8.7218e-06,  2.7043e-05,\n",
      "        -5.4060e-06,  2.5604e-06,  1.5633e-07, -2.4169e-05, -2.2894e-05,\n",
      "         2.1824e-05,  3.1131e-05,  2.5130e-05,  1.7053e-05, -3.8455e-05,\n",
      "         1.0993e-05, -4.0918e-05,  6.9868e-06,  4.1833e-05,  1.8802e-05,\n",
      "        -1.1578e-04,  1.8042e-04, -1.6138e-05, -5.0442e-05,  1.3485e-05,\n",
      "         9.6608e-06, -1.6906e-05, -3.4377e-06,  1.0425e-06,  3.7983e-06,\n",
      "        -3.5037e-06, -1.5704e-05,  1.1332e-05,  5.5309e-06, -3.9497e-05,\n",
      "         5.2573e-06,  1.7486e-06, -5.6149e-06, -2.2925e-05,  1.0853e-06,\n",
      "        -1.5769e-05,  5.2600e-05, -1.7477e-05, -9.3971e-06,  1.2000e-05,\n",
      "        -1.5144e-06,  6.9465e-06,  2.3681e-06,  1.8275e-04,  1.0240e-04,\n",
      "        -8.1408e-06, -9.9469e-06,  1.8010e-05, -1.2706e-05, -1.3883e-06,\n",
      "        -1.1037e-05,  9.2732e-06,  2.5319e-06, -2.4602e-05,  8.1773e-05,\n",
      "         3.9452e-05,  6.3746e-06, -5.6157e-07, -7.2332e-06, -8.8496e-06,\n",
      "        -5.3891e-07, -3.5161e-05, -8.0964e-05,  4.8320e-05, -5.0880e-06,\n",
      "         8.6320e-06, -2.8941e-06, -7.8567e-06, -1.2997e-05,  6.9540e-05,\n",
      "        -3.2840e-05, -1.9156e-04, -4.1587e-05, -7.1198e-06,  1.1417e-05,\n",
      "        -2.7785e-07, -2.9584e-07,  2.6572e-06, -2.0624e-05,  3.2242e-05,\n",
      "         3.1124e-05,  1.1800e-06, -9.9942e-06,  9.5494e-06, -4.1965e-06,\n",
      "         2.5875e-05, -5.0101e-05,  3.0226e-05,  4.7932e-05,  3.1989e-05,\n",
      "        -1.7952e-04, -3.2687e-04,  4.8804e-05,  1.0940e-05,  1.9543e-06,\n",
      "         2.0077e-08,  3.1365e-05, -1.9000e-05, -1.4313e-06, -5.5288e-07,\n",
      "        -5.2629e-07, -8.1504e-06, -2.9470e-06, -1.8416e-05,  5.1230e-06,\n",
      "        -1.1028e-06, -1.1012e-06,  1.2768e-06, -1.3533e-06,  5.7486e-06,\n",
      "         1.2479e-04,  3.2380e-05,  9.2443e-06, -9.1267e-06, -4.8764e-05,\n",
      "         1.4550e-05,  1.2016e-05, -1.7310e-05,  3.0810e-05,  9.9594e-05,\n",
      "         3.4842e-05, -1.9111e-05,  1.6417e-05,  1.9848e-05,  5.9394e-07,\n",
      "         6.4665e-06, -8.4943e-06, -1.3299e-06,  1.7924e-06,  1.3110e-05,\n",
      "        -3.0203e-06, -1.8516e-05, -9.1105e-06, -1.2967e-05,  1.2432e-07,\n",
      "         2.9373e-05,  1.7153e-05,  1.1635e-05,  8.7578e-06,  8.6164e-06,\n",
      "         2.9454e-06,  5.4849e-05, -1.5778e-05, -1.5097e-05,  1.6210e-06,\n",
      "        -4.4008e-05, -4.6171e-05, -1.4165e-06, -9.6235e-07, -1.2935e-05,\n",
      "         6.7102e-06, -1.1740e-04,  7.9576e-05,  2.7800e-05,  4.8421e-05,\n",
      "        -3.4341e-06, -2.0039e-05, -2.1572e-05, -2.1468e-05,  2.2520e-05,\n",
      "         1.2789e-05, -5.1202e-05, -7.9394e-05, -3.0847e-05,  9.2801e-05,\n",
      "         2.5097e-05,  2.4769e-06,  8.0607e-06,  7.2519e-06, -1.8191e-06,\n",
      "        -2.2477e-07,  1.7985e-06,  1.7785e-06,  1.2377e-05, -1.9886e-05,\n",
      "        -1.5843e-05, -2.5801e-05,  1.3543e-05, -3.1511e-06, -2.2099e-07,\n",
      "         3.0605e-06, -4.1803e-06, -1.0239e-05, -1.1351e-05,  3.6474e-06,\n",
      "         8.4418e-06,  3.7051e-06, -1.0225e-05,  1.1538e-05,  1.8324e-05,\n",
      "        -5.2379e-06, -1.2558e-04, -1.4051e-05,  1.3617e-06,  2.3895e-05,\n",
      "         1.2085e-05, -7.6787e-06,  1.5775e-05,  1.6185e-05, -2.1402e-06,\n",
      "         2.2269e-08,  1.6632e-04,  1.0252e-04,  6.4753e-05,  2.3258e-06,\n",
      "        -6.0900e-06, -2.2476e-05, -7.9195e-06,  2.6299e-06,  1.9834e-06,\n",
      "        -1.5855e-05,  6.4296e-06, -9.6547e-07,  1.9406e-07, -2.2099e-06,\n",
      "         5.6680e-06, -1.9317e-07, -2.1653e-05, -6.4549e-06,  1.6495e-07,\n",
      "         1.6771e-07,  1.0739e-05,  1.0723e-05,  4.1038e-06,  4.1042e-06,\n",
      "        -1.3525e-05,  1.0811e-05,  3.7838e-05, -1.2825e-06, -5.5081e-06,\n",
      "        -4.4005e-08,  3.3220e-05, -5.0851e-06,  5.7820e-07, -1.2250e-05,\n",
      "        -8.1913e-06,  2.9497e-05, -4.9032e-05,  6.1413e-05, -3.2024e-06,\n",
      "         2.6370e-06,  1.8846e-06,  7.9941e-07, -1.7611e-06, -1.9181e-08,\n",
      "        -2.1144e-05, -2.1158e-06, -1.3845e-07, -2.0716e-07, -2.8876e-06,\n",
      "         1.0545e-05,  1.0215e-05, -5.2642e-06, -1.1763e-05, -2.6380e-05,\n",
      "        -2.2478e-05, -4.2207e-05,  7.3343e-06,  6.5582e-06,  5.6643e-06,\n",
      "         1.7360e-06, -1.4093e-05, -2.6079e-05,  2.5879e-05, -3.0618e-06,\n",
      "         2.7409e-06,  5.9170e-07, -9.6099e-07, -9.6101e-07, -5.8405e-05,\n",
      "         9.2241e-06,  8.6907e-07,  7.0683e-07, -5.6513e-08, -3.2338e-05,\n",
      "         1.8003e-05,  3.3676e-06,  2.3228e-05,  1.1049e-05,  2.5362e-05,\n",
      "         8.1355e-05, -9.7459e-06,  3.5474e-05, -4.7934e-05, -1.1936e-06,\n",
      "         5.7476e-05,  3.4420e-05,  4.8855e-05, -1.5328e-04,  2.3080e-05,\n",
      "        -1.6135e-05,  6.2154e-06,  2.2002e-05,  5.5883e-06, -8.0857e-07,\n",
      "         6.6256e-07,  7.2116e-07,  1.6299e-06,  3.3410e-06,  4.9085e-06,\n",
      "        -8.6857e-06, -1.0903e-05, -2.2439e-05, -1.7687e-08, -5.7100e-05,\n",
      "         5.8105e-05,  3.7514e-05,  3.0782e-05,  2.0325e-05,  4.1920e-05,\n",
      "         3.0234e-05,  7.9827e-06,  8.3661e-07,  2.3148e-06,  4.1212e-05,\n",
      "        -1.2937e-04,  7.9755e-05,  6.9866e-06, -1.8707e-06,  1.4901e-05,\n",
      "         7.0007e-06, -1.1436e-05,  1.1425e-05,  1.5005e-05,  6.0003e-06,\n",
      "        -9.4449e-06,  1.9779e-06, -8.0731e-05,  5.5915e-06,  1.5330e-05,\n",
      "        -1.1058e-07,  7.2449e-06,  1.7091e-06, -3.2265e-06, -1.3647e-05,\n",
      "        -1.3781e-05, -2.9174e-07,  2.6645e-05,  1.5217e-05,  2.9746e-05,\n",
      "         1.8759e-05,  1.1084e-06, -1.3300e-05, -6.0008e-07, -5.8951e-07,\n",
      "        -2.2010e-05, -2.2883e-05, -2.0743e-05, -9.8631e-06, -1.6966e-04,\n",
      "         2.9696e-05, -1.6193e-05, -4.6804e-07], device='cuda:0'), 'exp_avg_sq': tensor([7.2340e-08, 9.9041e-08, 7.7822e-08, 4.1679e-08, 2.5667e-08, 3.8472e-08,\n",
      "        5.9508e-08, 1.9016e-08, 2.4142e-08, 2.5929e-08, 9.1973e-09, 3.9743e-10,\n",
      "        4.3593e-08, 8.8929e-08, 8.1460e-08, 3.8045e-08, 8.6144e-08, 1.2712e-07,\n",
      "        1.1363e-07, 7.3187e-08, 6.5598e-08, 6.7370e-08, 1.0689e-07, 5.6741e-08,\n",
      "        1.9987e-08, 2.5649e-08, 2.5427e-08, 1.0443e-08, 2.2129e-08, 2.5071e-08,\n",
      "        1.8890e-08, 1.0143e-08, 1.0677e-08, 1.3683e-08, 1.3690e-08, 6.0693e-09,\n",
      "        3.3675e-09, 4.2688e-09, 3.6677e-09, 1.7350e-09, 1.2975e-09, 7.2957e-09,\n",
      "        7.1726e-09, 7.1842e-09, 1.2121e-08, 1.9145e-08, 3.6187e-08, 2.7096e-08,\n",
      "        2.0148e-08, 1.4078e-08, 1.1442e-08, 8.3046e-10, 5.7281e-09, 6.8527e-09,\n",
      "        5.8786e-09, 1.5991e-11, 1.0531e-08, 1.6644e-08, 2.9178e-08, 2.8145e-08,\n",
      "        1.8590e-08, 6.8624e-08, 6.5201e-08, 3.7724e-08, 2.3967e-07, 3.9606e-07,\n",
      "        3.8884e-07, 1.7080e-07, 3.5423e-08, 4.3411e-08, 6.4278e-08, 6.7977e-08,\n",
      "        4.3346e-08, 4.7142e-08, 4.5414e-08, 6.9602e-08, 2.6502e-08, 5.6244e-08,\n",
      "        2.5023e-08, 1.4972e-08, 5.7903e-08, 7.1500e-08, 7.8811e-08, 3.1269e-08,\n",
      "        2.7255e-08, 2.8068e-08, 9.5228e-09, 3.5317e-09, 4.2138e-08, 3.2967e-08,\n",
      "        2.2902e-08, 2.0071e-09, 5.9833e-08, 5.3171e-08, 3.5597e-08, 1.7086e-09,\n",
      "        2.7638e-09, 3.5565e-09, 4.4856e-09, 4.6298e-09, 3.5212e-08, 1.8101e-07,\n",
      "        1.8873e-07, 1.3478e-07, 1.1378e-07, 2.1466e-07, 2.1643e-07, 1.0458e-07,\n",
      "        9.1398e-08, 9.9410e-08, 8.3597e-08, 4.1332e-08, 1.2982e-07, 1.6623e-07,\n",
      "        1.4660e-07, 7.8134e-08, 1.2046e-08, 1.4599e-08, 7.6999e-09, 1.3822e-09,\n",
      "        3.8441e-08, 1.0357e-07, 1.5578e-07, 5.4272e-08, 4.4368e-08, 7.2405e-08,\n",
      "        1.2058e-07, 8.9740e-08, 7.4442e-08, 7.7568e-08, 7.2341e-08, 4.4639e-08,\n",
      "        4.9606e-08, 6.3216e-08, 4.7419e-08, 1.3006e-08, 4.1707e-08, 7.5235e-08,\n",
      "        8.1478e-08, 2.0574e-08, 1.7760e-08, 1.1645e-08, 4.0149e-09, 1.6685e-11,\n",
      "        6.1098e-08, 1.0101e-07, 1.7933e-07, 1.4640e-07, 2.0452e-08, 8.7005e-08,\n",
      "        1.8477e-07, 2.2534e-07, 6.4004e-09, 1.2684e-08, 9.5868e-09, 4.7117e-09,\n",
      "        2.6100e-07, 3.3701e-07, 2.6746e-07, 2.0900e-07, 4.2378e-08, 8.6874e-08,\n",
      "        7.6279e-08, 8.1594e-08, 6.4720e-09, 9.7919e-09, 6.7662e-09, 2.5390e-09,\n",
      "        3.8797e-08, 7.4521e-08, 1.1744e-07, 1.5538e-07, 6.9262e-08, 1.5514e-07,\n",
      "        1.2700e-07, 9.0491e-08, 3.8378e-08, 7.7125e-08, 1.5228e-07, 1.8115e-07,\n",
      "        7.2542e-08, 1.9236e-07, 2.7140e-07, 1.4105e-07, 3.9220e-08, 3.8073e-08,\n",
      "        2.6959e-08, 7.8990e-09, 2.2428e-08, 3.5117e-08, 1.6201e-08, 3.2721e-09,\n",
      "        1.3050e-08, 2.9806e-08, 4.2880e-08, 5.5693e-09, 5.6884e-08, 7.9064e-08,\n",
      "        7.2718e-08, 1.6963e-08, 1.4298e-08, 9.3622e-09, 5.9368e-09, 4.9555e-09,\n",
      "        3.6652e-08, 5.0644e-08, 3.1288e-08, 2.5546e-08, 7.1191e-08, 8.0011e-08,\n",
      "        5.2071e-08, 2.2769e-08, 4.1046e-08, 5.7534e-08, 6.5709e-08, 1.9540e-08,\n",
      "        1.5709e-08, 1.3313e-08, 2.3410e-08, 1.1110e-08, 5.7383e-09, 1.8327e-08,\n",
      "        1.4316e-09, 1.4015e-11, 2.1583e-09, 2.6952e-09, 5.0701e-09, 5.0870e-09,\n",
      "        5.3451e-08, 9.7923e-08, 1.1846e-07, 1.3186e-07, 4.4566e-08, 3.0884e-08,\n",
      "        2.9391e-08, 7.3152e-09, 4.1929e-08, 2.5992e-08, 1.7678e-08, 4.2722e-09,\n",
      "        1.9746e-08, 4.9810e-08, 4.7760e-08, 3.9478e-09, 1.3757e-08, 4.7415e-08,\n",
      "        5.9937e-08, 3.2356e-08, 2.8620e-09, 4.4577e-09, 1.9831e-09, 6.4955e-10,\n",
      "        2.0524e-08, 3.6885e-08, 1.5932e-09, 1.4470e-09, 3.3941e-09, 3.4855e-09,\n",
      "        2.0987e-09, 2.3997e-09, 2.3713e-09, 3.2722e-09, 3.2088e-09, 1.9564e-09,\n",
      "        5.8173e-08, 3.5853e-08, 1.6788e-08, 1.3168e-08, 1.0281e-08, 2.7056e-08,\n",
      "        4.4912e-08, 3.9591e-08, 8.3363e-09, 9.4758e-09, 3.8827e-09, 6.0663e-10,\n",
      "        1.5500e-08, 2.0179e-08, 1.0803e-08, 4.5170e-09, 2.0187e-08, 1.9722e-08,\n",
      "        8.9053e-09, 4.6031e-09, 9.5158e-09, 6.8491e-09, 4.3135e-09, 9.0327e-11,\n",
      "        2.5413e-08, 2.8892e-08, 2.9690e-08, 6.9953e-09, 2.4087e-08, 1.8028e-08,\n",
      "        9.1565e-09, 4.8682e-09, 7.4630e-09, 1.7460e-08, 2.2855e-08, 7.0287e-09,\n",
      "        2.7261e-09, 8.7320e-09, 6.9536e-09, 6.5504e-09, 2.6085e-08, 3.5155e-08,\n",
      "        4.1581e-08, 2.3151e-08, 1.0440e-08, 1.1688e-08, 1.5378e-08, 1.0443e-08,\n",
      "        7.2431e-09, 2.1936e-08, 2.3147e-08, 1.2700e-08, 3.5881e-09, 6.3738e-09,\n",
      "        8.1694e-09, 1.9027e-09, 1.1743e-08, 6.0198e-09, 4.9659e-09, 4.1200e-09,\n",
      "        1.1461e-08, 1.7589e-08, 8.4753e-09, 1.1096e-10, 3.6523e-08, 4.2673e-08,\n",
      "        3.9557e-08, 1.2715e-08, 1.2918e-08, 1.2313e-08, 1.1132e-08, 2.4474e-09,\n",
      "        1.7998e-08, 2.6963e-08, 4.4625e-08, 1.1036e-08, 3.3350e-09, 9.8120e-09,\n",
      "        1.0817e-08, 8.1997e-09, 1.4819e-08, 1.3834e-08, 2.3984e-08, 1.1547e-08,\n",
      "        4.8452e-08, 6.1438e-08, 7.3299e-08, 7.6187e-08, 5.8581e-09, 4.0288e-09,\n",
      "        2.1272e-09, 8.5280e-10, 1.5113e-08, 4.2995e-09, 3.4160e-09, 1.1257e-09,\n",
      "        7.9694e-09, 7.3942e-09, 1.3168e-08, 4.4909e-09, 7.4837e-09, 1.6139e-08,\n",
      "        8.8760e-09, 3.6390e-09, 7.3076e-09, 1.7339e-08, 3.5705e-08, 2.7265e-08,\n",
      "        1.3595e-08, 9.9868e-09, 4.0809e-09, 1.7493e-09, 7.8646e-09, 2.3112e-08,\n",
      "        3.7684e-08, 1.7957e-08, 1.1708e-09, 3.7845e-09, 4.7826e-09, 7.4306e-09,\n",
      "        1.9612e-08, 1.0579e-08, 1.8516e-09, 8.5101e-10, 2.9259e-08, 6.3246e-08,\n",
      "        5.1650e-08, 2.5147e-08, 3.7811e-09, 1.0534e-08, 7.0049e-09, 1.5053e-11,\n",
      "        3.6174e-09, 2.8775e-09, 2.2914e-09, 2.1889e-09, 9.2331e-09, 2.8803e-09,\n",
      "        2.3322e-09, 6.7988e-10, 8.8283e-08, 1.3473e-07, 1.0685e-07, 4.1292e-08,\n",
      "        1.3073e-08, 9.8305e-09, 1.2132e-11, 1.2063e-11, 3.3978e-08, 3.5234e-08,\n",
      "        5.1374e-08, 1.5249e-08, 7.3298e-09, 8.4620e-09, 4.1542e-09, 1.2727e-10,\n",
      "        3.2348e-08, 6.6706e-08, 6.7768e-08, 2.6888e-08, 4.4338e-08, 9.4482e-08,\n",
      "        1.3729e-07, 1.0137e-07, 8.5768e-10, 1.0527e-09, 3.0495e-09, 9.0315e-09,\n",
      "        8.6818e-09, 4.2086e-09, 1.8822e-11, 1.8235e-11, 9.0901e-09, 9.5229e-09,\n",
      "        1.2133e-08, 1.2609e-08, 2.7320e-10, 2.7390e-10, 2.8790e-10, 4.9277e-11,\n",
      "        3.5006e-09, 1.0641e-08, 7.9101e-09, 2.0032e-09, 6.3601e-09, 1.0609e-08,\n",
      "        9.8732e-09, 3.7612e-09, 6.9021e-08, 8.4611e-08, 8.5910e-08, 7.6256e-08,\n",
      "        4.9675e-09, 9.5352e-09, 4.5705e-09, 1.4257e-11, 9.6428e-09, 6.7109e-09,\n",
      "        6.7533e-09, 3.6427e-09, 8.7146e-09, 5.1068e-09, 9.6518e-09, 4.4868e-09,\n",
      "        2.5824e-08, 3.1110e-08, 2.0365e-08, 7.5945e-09, 1.1602e-08, 1.2979e-08,\n",
      "        2.4663e-09, 8.4776e-10, 8.7028e-09, 1.4894e-08, 7.5487e-09, 1.3893e-11,\n",
      "        1.6859e-08, 1.0749e-08, 4.2185e-09, 2.1660e-09, 3.1393e-09, 1.4873e-08,\n",
      "        1.8613e-08, 1.8434e-08, 4.7995e-09, 6.7435e-09, 8.0506e-09, 2.5337e-09,\n",
      "        4.8286e-09, 1.1837e-08, 1.4313e-08, 7.0168e-09, 9.1886e-08, 1.3300e-07,\n",
      "        1.7846e-07, 1.8119e-07, 1.7003e-08, 4.2699e-08, 1.8221e-08, 6.9276e-09,\n",
      "        2.5174e-09, 3.1227e-09, 6.6576e-11, 6.6784e-11, 9.3780e-09, 6.4265e-09,\n",
      "        1.0809e-08, 6.6236e-09, 1.3609e-08, 1.8210e-08, 1.3424e-08, 3.2319e-09,\n",
      "        3.0154e-09, 8.4038e-09, 1.0817e-08, 2.8204e-09, 1.5405e-08, 1.0322e-08,\n",
      "        1.0495e-08, 8.9245e-09, 1.3133e-08, 3.8069e-08, 6.3465e-08, 8.3957e-08,\n",
      "        8.6692e-09, 2.9330e-08, 8.0417e-09, 1.3309e-09, 1.0451e-08, 7.3284e-09,\n",
      "        6.3929e-09, 2.1335e-09, 1.7724e-08, 1.9709e-08, 2.3596e-08, 1.7344e-11,\n",
      "        1.3545e-08, 1.6246e-08, 8.0569e-09, 1.6628e-09, 4.2107e-09, 3.0957e-09,\n",
      "        3.9351e-09, 1.8130e-09, 1.0125e-09, 7.1652e-09, 5.1383e-09, 3.3180e-09,\n",
      "        1.7992e-08, 3.5599e-09, 1.0331e-11, 1.0212e-11, 3.5633e-09, 3.5695e-09,\n",
      "        5.1154e-11, 5.1388e-11, 6.9953e-09, 8.4912e-09, 6.6918e-09, 2.7159e-09,\n",
      "        2.4988e-09, 4.7580e-10, 1.8673e-08, 2.1403e-08, 7.7241e-10, 3.4983e-09,\n",
      "        1.1074e-08, 1.5189e-08, 9.9538e-09, 1.6884e-08, 7.3900e-09, 1.5441e-11,\n",
      "        3.3212e-10, 2.3162e-10, 8.0786e-11, 2.0276e-11, 7.6167e-09, 6.4498e-11,\n",
      "        1.3385e-11, 1.5187e-11, 5.3367e-09, 8.0672e-09, 5.4354e-09, 1.3147e-09,\n",
      "        3.8134e-09, 9.4136e-09, 1.7454e-08, 1.3810e-08, 1.8600e-09, 2.2934e-09,\n",
      "        1.6768e-09, 1.1331e-09, 1.0444e-08, 5.4271e-08, 2.5103e-08, 1.9969e-10,\n",
      "        2.4108e-09, 3.3506e-10, 3.2191e-11, 3.2206e-11, 9.8786e-09, 1.4495e-08,\n",
      "        6.8535e-09, 1.6638e-11, 1.9010e-09, 1.0456e-08, 1.0127e-08, 2.4133e-09,\n",
      "        9.2279e-09, 2.0033e-08, 3.9230e-08, 7.4029e-08, 7.5194e-08, 1.0129e-07,\n",
      "        1.6419e-07, 1.6982e-07, 5.2235e-08, 5.9373e-08, 2.9106e-08, 1.5202e-08,\n",
      "        1.3270e-08, 7.0865e-09, 6.2118e-09, 6.4701e-09, 6.4104e-09, 3.0472e-09,\n",
      "        1.6176e-11, 1.6018e-11, 7.1013e-09, 1.1486e-08, 1.4027e-08, 1.1092e-08,\n",
      "        1.6357e-09, 2.4877e-09, 4.4613e-09, 3.8504e-09, 1.1544e-08, 2.2166e-08,\n",
      "        2.8263e-08, 9.1068e-09, 7.0837e-09, 5.2794e-09, 4.9042e-09, 1.9862e-11,\n",
      "        1.7770e-08, 4.5793e-08, 8.2602e-08, 6.1329e-08, 3.3847e-09, 1.0406e-08,\n",
      "        4.1000e-08, 1.4686e-08, 1.6235e-08, 1.2706e-08, 4.7746e-09, 1.2515e-09,\n",
      "        1.4112e-08, 1.3556e-08, 2.0159e-08, 7.0507e-09, 8.4790e-09, 1.2733e-08,\n",
      "        2.8460e-09, 1.5723e-11, 8.2089e-09, 1.3999e-08, 2.0344e-08, 9.8434e-09,\n",
      "        6.3857e-09, 9.9805e-09, 1.1656e-08, 5.7866e-09, 5.5366e-09, 4.2703e-09,\n",
      "        7.9488e-12, 8.4366e-12, 3.1136e-09, 8.9059e-09, 9.3165e-09, 1.2068e-09,\n",
      "        3.1807e-08, 2.5451e-08, 1.7333e-08, 7.5565e-12], device='cuda:0')}, 6: {'step': 4914, 'exp_avg': tensor([[-1.1439e-05,  1.9088e-05, -2.5307e-05,  ..., -3.3178e-05,\n",
      "          4.5094e-05, -1.0463e-05],\n",
      "        [-4.0020e-06, -3.3678e-05, -1.1985e-05,  ...,  1.6268e-05,\n",
      "          1.1399e-05,  3.8775e-06],\n",
      "        [-8.6418e-07, -2.3510e-05, -8.7287e-06,  ..., -2.1113e-06,\n",
      "         -4.9763e-06, -1.5015e-06],\n",
      "        ...,\n",
      "        [-6.0016e-06,  1.8486e-05,  1.0699e-06,  ...,  2.3352e-05,\n",
      "         -9.2049e-07,  3.8644e-06],\n",
      "        [-1.3598e-05, -3.5683e-06,  6.7852e-06,  ...,  1.0643e-05,\n",
      "          1.4279e-05,  4.0645e-06],\n",
      "        [ 1.4833e-07, -1.3121e-07,  1.1872e-07,  ...,  3.7091e-07,\n",
      "         -7.0376e-08,  1.6685e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[2.2659e-08, 3.8092e-07, 2.2621e-08,  ..., 1.2236e-07, 2.9689e-08,\n",
      "         2.1015e-08],\n",
      "        [8.8816e-09, 1.3550e-07, 6.3721e-09,  ..., 2.6965e-08, 1.3086e-08,\n",
      "         8.3091e-09],\n",
      "        [3.7302e-09, 5.1608e-08, 5.4436e-09,  ..., 7.7310e-09, 2.9829e-09,\n",
      "         7.8224e-09],\n",
      "        ...,\n",
      "        [1.3100e-09, 5.3568e-09, 7.6678e-10,  ..., 7.1366e-09, 2.7944e-09,\n",
      "         4.6673e-10],\n",
      "        [5.8268e-09, 1.6280e-08, 1.2113e-09,  ..., 1.7615e-09, 5.5172e-09,\n",
      "         6.9590e-10],\n",
      "        [3.7164e-12, 1.2415e-11, 9.9931e-13,  ..., 1.7289e-12, 3.3543e-12,\n",
      "         1.5516e-12]], device='cuda:0')}, 7: {'step': 4914, 'exp_avg': tensor([-7.0708e-05, -2.6583e-05, -6.8812e-05, -2.2967e-05, -3.9422e-05,\n",
      "        -1.4905e-05,  8.4068e-06,  9.4442e-06, -1.3271e-05, -2.3238e-05,\n",
      "         3.3427e-05, -1.0257e-05,  3.9350e-05, -1.3392e-05, -1.0622e-05,\n",
      "        -1.7552e-05, -1.4218e-04, -1.7359e-05, -2.2233e-05,  1.2290e-05,\n",
      "        -1.4396e-04, -6.3813e-05, -8.7330e-06,  4.7321e-06, -3.5981e-05,\n",
      "        -1.5426e-05,  2.7270e-06,  7.5392e-05,  5.6549e-06,  1.2855e-05,\n",
      "         1.2220e-04,  8.5665e-06, -8.3451e-06,  1.5425e-05,  1.3917e-05,\n",
      "         1.2342e-06,  5.9673e-05, -1.1972e-04, -8.5137e-05,  4.5166e-06,\n",
      "        -4.9038e-05, -8.5834e-05,  2.5013e-04,  7.0237e-05, -1.7168e-05,\n",
      "         8.9768e-05,  5.7157e-05, -1.4308e-04,  1.1673e-05,  5.6660e-07,\n",
      "         5.6323e-07,  5.2344e-07,  1.2068e-05, -3.4691e-05, -6.9097e-06,\n",
      "        -3.5433e-07, -9.1066e-06, -6.6711e-07, -7.1454e-07, -6.8920e-07,\n",
      "         1.6806e-05, -1.0086e-06, -7.1155e-07, -7.1386e-07, -3.9837e-05,\n",
      "         2.8049e-05,  8.3305e-08, -5.3888e-07, -3.9062e-05,  3.8180e-07,\n",
      "         1.8264e-06,  1.7926e-05,  4.2033e-06,  2.1459e-06, -1.2726e-06,\n",
      "        -2.4150e-06,  3.0932e-05, -5.4527e-06, -1.2001e-06, -5.9479e-07,\n",
      "        -1.3680e-06, -8.4113e-06, -9.2384e-07, -5.9423e-07,  1.6902e-05,\n",
      "         2.4947e-05,  3.8493e-06,  5.9032e-07, -5.9517e-06,  1.0811e-05,\n",
      "        -1.8226e-05, -1.0500e-05, -4.1887e-05, -6.2010e-05,  7.1836e-06,\n",
      "         1.1462e-06, -8.6806e-06,  1.4999e-06,  1.5005e-06, -5.6897e-07,\n",
      "        -2.2984e-05,  7.3623e-06, -1.5234e-06, -2.2006e-05,  7.1243e-06,\n",
      "        -9.4519e-06,  8.9155e-06, -1.8646e-07,  6.2079e-05,  1.2797e-05,\n",
      "        -9.4773e-05, -2.5727e-05,  1.6132e-05,  1.1906e-05,  2.2160e-05,\n",
      "        -3.7355e-06,  1.3163e-05,  2.7672e-05, -1.5179e-05,  1.8468e-05,\n",
      "         4.0141e-06,  3.1338e-06,  4.7133e-06, -5.8632e-06,  3.0440e-06,\n",
      "         4.9119e-06,  6.6505e-06, -5.6921e-06, -3.1117e-05,  5.6111e-05,\n",
      "        -2.5884e-05,  4.2957e-07,  3.6839e-05,  2.4422e-05,  2.7064e-05,\n",
      "        -1.5999e-05,  1.4399e-05, -3.3308e-05, -1.7222e-05, -1.6831e-05,\n",
      "        -3.2257e-06, -1.1015e-05,  9.7841e-07,  9.7984e-07, -3.4244e-05,\n",
      "        -2.0440e-04,  2.1118e-05, -6.3825e-07, -1.0477e-05, -2.6804e-05,\n",
      "        -9.0065e-06,  4.5389e-06,  1.3675e-05,  7.3963e-05,  1.0703e-05,\n",
      "         3.1603e-05, -8.6655e-05, -6.0126e-05, -4.1301e-06, -1.1350e-05,\n",
      "         7.9380e-07,  8.8977e-06,  2.8843e-05,  2.0548e-07, -1.2721e-04,\n",
      "        -3.4902e-06, -2.0732e-05, -6.0186e-05,  6.2188e-07,  2.6989e-06,\n",
      "        -6.2151e-06, -9.1057e-07, -1.7974e-06, -3.1022e-07, -3.9841e-05,\n",
      "         1.3108e-05, -1.5546e-07, -6.7511e-07, -7.4607e-07, -7.2107e-07,\n",
      "         1.4132e-05,  2.3411e-06, -5.8732e-06, -1.2505e-05, -3.1531e-05,\n",
      "        -4.2740e-05,  1.6438e-05,  1.3894e-04,  8.6129e-06,  2.2265e-05,\n",
      "         3.1363e-05,  3.6957e-07], device='cuda:0'), 'exp_avg_sq': tensor([1.4546e-07, 6.0848e-08, 2.0831e-08, 8.2711e-09, 1.1178e-07, 4.8611e-08,\n",
      "        9.8418e-09, 3.8933e-09, 2.4257e-08, 3.1674e-08, 2.1108e-08, 2.6869e-09,\n",
      "        6.5586e-08, 6.1411e-08, 3.6431e-08, 8.4142e-09, 2.1080e-07, 1.8809e-07,\n",
      "        1.0934e-07, 3.3446e-08, 3.3823e-08, 2.2683e-08, 1.6738e-08, 3.8962e-09,\n",
      "        1.7513e-08, 2.8004e-08, 1.2137e-08, 1.4056e-08, 8.5663e-09, 1.7749e-08,\n",
      "        4.2472e-08, 1.5689e-08, 1.1331e-08, 1.3343e-08, 7.6738e-09, 2.3299e-09,\n",
      "        8.9159e-08, 1.1557e-07, 4.3641e-08, 2.3117e-09, 2.4562e-07, 1.8072e-07,\n",
      "        1.1618e-07, 3.4698e-08, 4.5378e-08, 1.2341e-07, 1.3944e-07, 8.9657e-08,\n",
      "        2.5153e-09, 3.1568e-11, 3.2112e-11, 3.6549e-11, 1.0150e-08, 1.4805e-08,\n",
      "        1.0682e-08, 2.4342e-11, 2.3911e-09, 1.4250e-11, 1.0776e-11, 1.2325e-11,\n",
      "        1.1038e-08, 3.7572e-09, 1.8229e-11, 1.8173e-11, 8.3421e-09, 9.1627e-09,\n",
      "        2.7495e-09, 1.9321e-11, 5.4120e-09, 2.6923e-09, 3.8545e-09, 1.3517e-08,\n",
      "        8.0875e-10, 4.4364e-10, 9.6006e-10, 1.0358e-09, 1.2900e-08, 6.0874e-09,\n",
      "        7.9176e-10, 1.3956e-11, 3.3608e-09, 1.9767e-09, 7.6936e-10, 1.5555e-11,\n",
      "        1.0158e-08, 5.4739e-09, 1.7629e-09, 1.4248e-11, 1.3086e-08, 1.1236e-08,\n",
      "        4.0876e-09, 2.6953e-09, 1.8259e-08, 1.3615e-08, 2.7847e-09, 1.5323e-11,\n",
      "        2.8295e-09, 4.9592e-10, 4.9501e-10, 2.0470e-11, 8.8363e-09, 1.1806e-08,\n",
      "        8.8674e-09, 1.6530e-09, 5.5253e-09, 4.2405e-09, 2.9087e-09, 1.5824e-11,\n",
      "        4.3557e-08, 3.5457e-08, 3.9531e-08, 9.0854e-09, 7.2434e-09, 1.1119e-08,\n",
      "        1.0120e-08, 6.3557e-09, 4.9779e-08, 4.4905e-08, 2.8317e-08, 1.3435e-08,\n",
      "        1.5298e-08, 1.8190e-08, 1.2668e-08, 1.1019e-08, 1.3522e-09, 3.9233e-09,\n",
      "        9.4823e-09, 8.4353e-09, 1.9368e-08, 3.0980e-08, 1.3711e-08, 9.8069e-12,\n",
      "        1.1302e-08, 2.0155e-08, 1.0977e-08, 3.1827e-09, 1.0727e-08, 1.6932e-08,\n",
      "        1.2866e-08, 1.1158e-08, 1.8078e-09, 1.2499e-09, 1.5718e-11, 1.5706e-11,\n",
      "        6.5170e-09, 3.1609e-08, 6.0018e-09, 1.1147e-11, 5.8585e-09, 6.1638e-09,\n",
      "        2.7843e-09, 2.4306e-09, 4.3580e-08, 2.4010e-08, 2.0609e-08, 1.4685e-08,\n",
      "        3.9515e-08, 2.2019e-08, 3.1758e-09, 5.0041e-09, 9.6613e-09, 1.7099e-08,\n",
      "        1.4128e-08, 3.1570e-11, 1.0223e-07, 1.6147e-07, 8.0337e-08, 1.2785e-08,\n",
      "        4.5246e-09, 2.3737e-09, 1.0967e-09, 1.4095e-11, 8.0787e-10, 4.4350e-09,\n",
      "        4.5006e-09, 9.1501e-10, 2.0528e-09, 1.3685e-11, 1.2584e-11, 1.2665e-11,\n",
      "        4.5755e-09, 7.3780e-09, 1.0327e-08, 1.1127e-08, 2.3052e-08, 5.3739e-08,\n",
      "        6.8809e-08, 1.1806e-07, 3.8636e-09, 1.1074e-08, 1.8349e-08, 1.1765e-11],\n",
      "       device='cuda:0')}, 8: {'step': 4914, 'exp_avg': tensor([[ 4.0406e-07,  9.6923e-05,  1.7498e-06,  ...,  1.0469e-05,\n",
      "          1.9257e-07,  5.9719e-06],\n",
      "        [-7.6729e-06,  4.4618e-05,  3.1635e-06,  ..., -7.1405e-06,\n",
      "          2.9954e-05, -1.0606e-05],\n",
      "        [-3.3574e-06,  1.6323e-05, -8.4375e-08,  ...,  1.4153e-05,\n",
      "          1.2039e-05, -7.5981e-06],\n",
      "        ...,\n",
      "        [-5.1304e-07,  8.0368e-06,  1.9559e-05,  ...,  6.3436e-06,\n",
      "          1.1628e-05,  8.1741e-07],\n",
      "        [-1.1322e-06,  1.6197e-06, -3.4078e-07,  ..., -4.0489e-08,\n",
      "         -6.7956e-07, -1.7958e-05],\n",
      "        [-1.1059e-07, -1.5319e-07,  6.5136e-08,  ...,  3.5695e-08,\n",
      "         -3.4270e-07,  1.8200e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[2.2219e-09, 8.7649e-08, 1.9209e-09,  ..., 1.6981e-08, 1.5024e-09,\n",
      "         3.8907e-09],\n",
      "        [1.2704e-08, 6.6718e-08, 1.2703e-08,  ..., 1.7510e-08, 6.5514e-09,\n",
      "         9.9002e-09],\n",
      "        [3.9292e-09, 3.4405e-08, 2.5462e-09,  ..., 1.3760e-08, 5.3424e-09,\n",
      "         7.5939e-09],\n",
      "        ...,\n",
      "        [1.8361e-09, 5.3286e-08, 5.9000e-09,  ..., 2.5210e-09, 2.8130e-09,\n",
      "         1.2773e-09],\n",
      "        [6.5886e-10, 3.3278e-09, 1.4564e-10,  ..., 4.8424e-10, 1.1701e-09,\n",
      "         5.1943e-10],\n",
      "        [1.8981e-12, 1.1933e-09, 4.4443e-12,  ..., 1.7615e-12, 1.8245e-10,\n",
      "         1.5090e-10]], device='cuda:0')}, 9: {'step': 4914, 'exp_avg': tensor([ 4.9087e-06, -1.5669e-05, -3.0915e-05,  8.9655e-06, -5.6146e-06,\n",
      "        -5.8260e-05,  9.1520e-07,  9.6919e-05,  4.8620e-05,  2.0493e-05,\n",
      "        -2.1228e-05, -4.2664e-05, -4.4901e-05, -3.4097e-06,  3.6947e-06,\n",
      "         8.7776e-05,  2.0912e-05, -2.9553e-05, -1.5624e-05,  2.2984e-05,\n",
      "        -7.7593e-05,  1.2310e-06,  1.1943e-06,  1.1711e-06, -7.0546e-06,\n",
      "         3.3273e-05,  2.2658e-05, -3.7457e-06,  2.0427e-05,  5.9485e-06,\n",
      "        -4.2308e-05, -4.8376e-06,  5.0411e-06, -3.7053e-06, -2.3416e-05,\n",
      "         7.0254e-06,  4.1415e-05,  5.3838e-06,  1.1484e-05,  3.2636e-06,\n",
      "        -1.5215e-05, -8.9051e-07,  5.6121e-06,  3.2318e-06,  1.1003e-05,\n",
      "         3.2907e-06,  1.4691e-06, -3.7697e-08,  1.4948e-06, -1.0526e-06,\n",
      "         1.5935e-05, -2.3706e-06,  4.4532e-06, -2.0100e-05, -7.1952e-06,\n",
      "        -6.1720e-07,  1.7411e-05, -3.7734e-05, -2.4690e-06, -1.8847e-05,\n",
      "        -7.1706e-06, -4.0847e-06,  1.0173e-06, -1.1541e-06, -1.5312e-04,\n",
      "         3.9109e-05,  6.4769e-05,  2.8745e-05,  1.5602e-05, -3.5339e-05,\n",
      "        -2.1249e-05,  6.7682e-06,  8.4407e-06, -2.0201e-05, -8.9825e-06,\n",
      "         5.3896e-06, -4.8607e-05, -1.0519e-04,  8.1777e-05,  2.4136e-05,\n",
      "        -1.0345e-04,  4.4988e-05, -4.2305e-05,  9.0211e-07,  9.3673e-06,\n",
      "        -3.4230e-05, -8.3606e-06,  2.3436e-05,  2.1330e-05,  4.4608e-05,\n",
      "         2.9156e-05,  1.4814e-05,  2.9040e-06, -7.3401e-06, -9.1508e-06,\n",
      "         9.8042e-06,  7.6660e-05, -1.7435e-05, -5.6537e-05,  6.4167e-05,\n",
      "         2.4081e-05, -1.6704e-05,  1.3378e-05, -7.7647e-06,  4.3846e-05,\n",
      "         2.2123e-05,  1.8589e-05, -1.7795e-05,  1.1162e-04,  2.7650e-07,\n",
      "         2.5747e-05,  1.0682e-05, -3.7414e-05,  3.5623e-05,  9.1307e-06,\n",
      "         4.0910e-06, -1.5557e-05, -2.8591e-05,  7.8809e-05, -3.5486e-06,\n",
      "         2.0428e-06, -3.3505e-05,  4.3461e-06, -9.3215e-05, -6.2037e-05,\n",
      "         4.2062e-05, -2.1564e-05,  1.3761e-05,  5.0856e-06, -5.9960e-06,\n",
      "        -7.4334e-06,  3.4072e-07, -3.7156e-06,  2.6483e-05,  3.1095e-05,\n",
      "         4.6170e-05, -2.3047e-06, -7.1195e-05, -5.2253e-05, -1.4063e-05,\n",
      "        -4.0566e-05, -9.4722e-06,  1.8585e-05, -7.9488e-08,  8.2329e-06,\n",
      "         1.1913e-06, -1.0550e-06,  1.4151e-06,  1.5512e-05, -7.1700e-06,\n",
      "        -6.2320e-07, -6.7282e-07,  1.9670e-06, -1.3429e-06,  3.6515e-06,\n",
      "         1.5296e-05, -4.2052e-05,  4.9218e-05,  2.3575e-05,  1.1101e-05,\n",
      "         1.1047e-05, -1.2737e-05, -7.1781e-05,  5.5924e-06,  9.2114e-06,\n",
      "         3.1289e-06, -8.2549e-07, -8.3724e-07, -1.1343e-05, -2.7996e-06,\n",
      "        -5.5030e-05,  2.2698e-05, -4.7775e-06, -3.5978e-06,  8.5437e-06,\n",
      "        -5.2618e-06, -4.4423e-05, -1.9300e-04,  4.8402e-05,  2.4203e-05,\n",
      "        -1.0126e-05, -1.0389e-05, -2.5569e-05, -2.8607e-07, -4.7453e-06,\n",
      "         4.1450e-05, -1.6595e-05,  1.1329e-05,  1.3834e-05, -6.7416e-05,\n",
      "        -1.3988e-05, -2.5446e-05,  2.7295e-06, -2.7994e-07,  8.1590e-06,\n",
      "        -5.5668e-06, -3.5381e-05, -1.5709e-05, -2.1152e-05,  6.6732e-07,\n",
      "        -1.9218e-05,  3.6835e-06,  1.7496e-06,  1.7829e-06,  2.1068e-05,\n",
      "         2.6087e-05,  1.6240e-05, -5.7393e-06,  1.4917e-05,  4.1832e-05,\n",
      "        -1.7261e-05, -4.9270e-05, -3.2851e-07, -3.0982e-07, -3.3917e-07,\n",
      "        -6.8334e-07, -8.3027e-05, -9.2561e-05, -2.1143e-04,  5.4863e-05,\n",
      "         1.0405e-05,  1.9885e-05,  7.6428e-05,  1.2813e-04,  5.1110e-06,\n",
      "         6.8880e-06,  1.0182e-05,  2.8415e-06,  1.3482e-05,  2.3705e-05,\n",
      "         3.5772e-06,  6.2655e-06, -6.2307e-06, -1.5131e-06, -8.0209e-06,\n",
      "         8.8546e-06, -1.0471e-05, -1.4762e-05,  3.9027e-07,  3.6756e-07,\n",
      "        -7.6652e-05, -3.0618e-06, -7.7101e-06, -7.9682e-07, -2.2331e-05,\n",
      "         8.0333e-05,  1.3133e-05,  1.5618e-05, -4.3222e-05,  1.1601e-04,\n",
      "        -6.8106e-05, -3.0694e-06,  5.2415e-05,  9.5938e-06,  5.6361e-07,\n",
      "        -2.4291e-07, -9.8495e-06, -9.9564e-06, -1.9907e-05, -1.3891e-05,\n",
      "        -8.0926e-06, -2.6735e-05, -2.6796e-05, -5.1114e-06,  2.1334e-05,\n",
      "         1.4487e-05,  3.6573e-05,  1.9169e-06,  3.2975e-05,  4.1559e-06,\n",
      "         3.5971e-05,  2.5710e-06, -4.6466e-05,  1.8683e-04,  3.8635e-05,\n",
      "        -9.4753e-05, -3.8518e-06, -2.2810e-06,  4.7422e-06, -5.7963e-07,\n",
      "        -4.4626e-06,  6.8731e-05, -2.4803e-05,  2.9561e-06, -1.1878e-04,\n",
      "         7.8746e-05, -1.0279e-05,  4.9282e-06, -1.0346e-05, -1.2939e-04,\n",
      "         2.9331e-06, -2.0493e-05, -8.4582e-06, -9.7885e-06,  9.3033e-05,\n",
      "         1.6402e-05,  1.4518e-05, -4.5618e-06, -1.0956e-05, -1.1407e-05,\n",
      "         3.0048e-06,  1.9091e-05, -4.1987e-05,  1.5447e-05, -1.2642e-05,\n",
      "         1.5413e-05,  3.4434e-05,  1.7335e-05, -1.6727e-05,  2.5508e-05,\n",
      "         8.4535e-06,  7.7490e-07, -1.5458e-05,  1.0992e-04, -5.3387e-06,\n",
      "        -2.1187e-06,  5.9566e-06,  1.2929e-04,  6.0834e-05,  5.6892e-05,\n",
      "        -4.8734e-06,  7.4513e-05,  7.5068e-05,  5.3196e-05, -1.5951e-08,\n",
      "        -2.3933e-05,  1.2639e-05,  3.8580e-07,  9.0026e-06, -6.9744e-05,\n",
      "        -3.3428e-05, -2.8904e-05,  3.6191e-05, -6.1085e-07,  1.8757e-05,\n",
      "        -5.6373e-06,  5.1845e-05,  2.2157e-05, -4.8861e-05, -2.6388e-06,\n",
      "         5.9665e-05, -4.7236e-05,  2.2277e-05,  6.2225e-06, -1.8317e-05,\n",
      "         3.5729e-06,  5.0822e-05,  3.4592e-05, -8.1131e-07, -2.7998e-05,\n",
      "        -4.5230e-06,  2.7692e-07, -9.1673e-06, -2.2000e-05, -1.6623e-05,\n",
      "        -1.3035e-05,  4.8282e-05, -9.7803e-06, -2.8513e-05, -8.5750e-07,\n",
      "         1.9052e-05, -2.2064e-05,  5.1158e-05, -8.1481e-06,  1.5607e-04,\n",
      "         1.7311e-04,  2.9955e-05,  1.3381e-05, -4.6131e-06,  1.5590e-06,\n",
      "         3.5927e-05, -5.2675e-06,  2.0979e-05,  1.2001e-05,  3.1335e-05,\n",
      "         1.4057e-04,  9.5055e-06,  3.0758e-05,  3.3556e-05,  2.4682e-05,\n",
      "         3.1910e-05,  6.6880e-06, -1.3932e-05, -3.4030e-05, -3.3799e-05,\n",
      "        -1.1469e-04,  2.4698e-05, -2.7519e-05,  2.6355e-05,  8.1207e-06,\n",
      "         1.1967e-06,  1.5742e-06,  1.6899e-05,  1.4043e-04,  5.5925e-05,\n",
      "         2.7101e-05, -5.5640e-06,  7.6956e-05,  1.6743e-05,  3.8264e-05,\n",
      "         1.0431e-05,  8.9604e-05,  4.7015e-05, -1.5784e-05, -1.2118e-05,\n",
      "         9.8975e-06, -5.9907e-06, -1.4609e-06,  6.5031e-05,  5.4432e-05,\n",
      "         7.2745e-05, -3.0558e-05,  1.1566e-04,  1.2549e-04,  5.2922e-05,\n",
      "         9.0117e-06,  1.4165e-05, -1.8381e-05,  4.4411e-05,  1.9643e-06,\n",
      "        -3.6078e-05, -4.0842e-06,  1.1148e-05,  6.6372e-06,  1.9550e-05,\n",
      "        -1.5991e-05, -3.3082e-05, -3.1552e-06,  1.8201e-05, -1.3572e-06,\n",
      "        -1.4090e-06,  1.1080e-05, -4.0335e-05,  1.4134e-04,  9.3083e-05,\n",
      "         2.8967e-05,  2.2494e-05,  3.4473e-05,  4.0696e-05,  1.4118e-05,\n",
      "        -1.1093e-05, -4.9391e-05, -3.1955e-05, -1.1020e-06, -9.3263e-07,\n",
      "         4.6687e-06,  3.9423e-06, -2.5801e-07,  3.0295e-05, -2.6887e-05,\n",
      "        -8.0704e-05,  4.0580e-05,  2.9691e-06, -8.4407e-05,  7.0690e-06,\n",
      "        -7.7387e-06,  2.5197e-05,  1.6953e-05,  1.8337e-04,  2.8117e-04,\n",
      "        -4.2262e-05,  1.3189e-04, -6.3848e-05,  2.8424e-05,  7.1740e-06,\n",
      "         1.2214e-05, -9.5959e-06, -1.0408e-05,  1.0614e-04,  5.9993e-05,\n",
      "         6.1425e-06,  7.1056e-06,  5.5184e-06, -7.8187e-06,  3.6019e-06,\n",
      "        -3.7933e-07,  5.4936e-06,  3.6585e-05,  4.6951e-06, -9.0331e-06,\n",
      "         2.3313e-05, -6.5713e-06,  1.1056e-04,  4.1207e-05, -2.7853e-06,\n",
      "         3.2154e-06, -1.5759e-06,  2.1426e-05, -5.5232e-06,  1.8585e-06,\n",
      "        -1.4961e-05,  8.3447e-07,  1.0902e-05,  1.4000e-06, -1.0168e-05,\n",
      "         3.3582e-06, -2.4125e-05, -1.6942e-05, -5.7034e-06,  3.4540e-07,\n",
      "        -1.5913e-05, -3.9353e-06, -9.4345e-07, -9.6731e-07,  8.1460e-09,\n",
      "         1.2974e-05,  4.5690e-06, -2.3988e-05,  5.4787e-07, -6.8688e-05,\n",
      "         2.9981e-05,  3.0468e-05, -1.4411e-05, -3.8006e-05,  7.1987e-05,\n",
      "         1.1063e-04, -7.9559e-05,  2.3655e-05, -2.2897e-05, -3.6054e-05,\n",
      "        -1.3519e-05,  2.2292e-06, -6.9947e-07, -7.4810e-07, -8.1938e-05,\n",
      "         2.5456e-05,  8.9546e-07, -6.1020e-08, -3.4745e-05, -2.6909e-05,\n",
      "         2.5646e-05,  1.6171e-05,  2.8130e-05,  3.8605e-06,  4.5729e-06,\n",
      "        -1.0693e-06, -2.0799e-05,  8.2511e-05, -3.9997e-05, -5.8723e-06,\n",
      "        -6.9284e-05,  1.3640e-06,  6.8315e-05, -3.8543e-07, -2.1231e-05,\n",
      "        -1.8112e-04, -1.1896e-05,  4.8240e-06, -8.7331e-06, -2.2455e-05,\n",
      "        -9.2514e-08, -1.2716e-05, -2.0932e-05, -5.2766e-05,  7.8657e-06,\n",
      "         1.7695e-05, -2.7951e-07,  1.4530e-05, -2.0867e-05,  3.9695e-07,\n",
      "         2.1826e-05, -3.4732e-06,  1.3187e-07,  8.4895e-06, -2.4529e-06,\n",
      "        -7.3094e-06, -1.1219e-05, -4.7738e-06, -4.5092e-07,  8.9416e-06,\n",
      "        -5.9890e-06,  1.8556e-06, -2.2760e-05,  8.0677e-06,  2.7973e-05,\n",
      "         7.3207e-06, -7.1199e-06,  1.3406e-05, -1.0700e-05, -6.1610e-07,\n",
      "        -1.8070e-05,  3.6986e-06,  2.7577e-06,  1.8435e-07, -6.8517e-05,\n",
      "         5.1233e-06,  2.5103e-05,  3.2881e-05, -4.9735e-07,  1.9799e-05,\n",
      "        -3.9324e-06,  2.1256e-05, -1.4440e-05, -6.0422e-05,  1.1140e-04,\n",
      "        -6.8536e-05,  1.3160e-05,  9.6924e-06,  1.6393e-05,  1.2321e-06,\n",
      "        -3.4615e-06, -7.8165e-05,  1.6651e-05,  1.2955e-05, -5.0434e-06,\n",
      "        -1.4759e-05, -2.7149e-05,  6.1903e-06, -5.6138e-07, -5.6881e-07,\n",
      "        -5.7677e-07, -5.7084e-07, -7.3964e-06, -1.0445e-05, -1.7377e-05,\n",
      "        -6.4651e-06, -6.2271e-06,  1.5040e-05, -1.3779e-05, -6.3662e-07],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([1.1804e-08, 3.3886e-08, 2.5461e-08, 1.3852e-08, 4.1628e-08, 3.7058e-08,\n",
      "        1.9353e-08, 1.0669e-08, 3.9820e-08, 6.1323e-08, 6.6497e-08, 4.7187e-08,\n",
      "        1.1342e-08, 3.0687e-08, 4.1602e-08, 3.6973e-08, 4.1978e-08, 2.0244e-08,\n",
      "        2.4162e-08, 1.4556e-08, 4.2081e-09, 1.6594e-11, 1.3814e-11, 1.5074e-11,\n",
      "        1.6552e-08, 1.7736e-08, 1.1907e-08, 5.6428e-09, 4.8656e-09, 7.4053e-09,\n",
      "        1.4630e-08, 6.0779e-09, 2.9908e-09, 1.0893e-08, 1.0659e-08, 9.9361e-09,\n",
      "        1.7233e-08, 1.7610e-08, 4.3078e-09, 1.9768e-09, 8.3036e-09, 6.8138e-09,\n",
      "        3.2688e-09, 2.7107e-09, 1.1256e-08, 3.4348e-09, 5.8371e-10, 1.8729e-11,\n",
      "        4.8107e-09, 3.3789e-09, 2.9827e-09, 2.5035e-09, 9.7373e-09, 7.0548e-09,\n",
      "        4.3658e-09, 1.2456e-09, 2.5117e-08, 2.4368e-08, 2.1315e-08, 1.1818e-08,\n",
      "        5.9853e-08, 1.6892e-08, 1.0914e-08, 4.8023e-09, 3.8940e-08, 3.3528e-08,\n",
      "        2.3067e-08, 1.1295e-08, 7.8637e-09, 1.5883e-08, 1.0395e-08, 6.6394e-09,\n",
      "        7.8964e-09, 1.5984e-08, 1.3522e-08, 5.0860e-09, 2.2530e-08, 6.3309e-08,\n",
      "        6.4185e-08, 1.8108e-08, 4.3920e-08, 6.7337e-08, 2.9918e-08, 6.0586e-09,\n",
      "        4.9427e-09, 1.4935e-08, 1.7572e-08, 1.4485e-08, 1.0183e-08, 1.9462e-08,\n",
      "        1.7136e-08, 9.9230e-09, 4.7987e-09, 6.0811e-09, 9.1854e-09, 7.8605e-09,\n",
      "        4.9891e-08, 2.8431e-08, 3.7852e-08, 3.1287e-08, 5.8298e-09, 1.6340e-08,\n",
      "        1.6015e-08, 9.0938e-09, 2.3665e-08, 4.7356e-08, 6.3793e-08, 4.9483e-08,\n",
      "        5.8305e-08, 3.6431e-08, 3.0515e-08, 2.6046e-08, 2.4449e-08, 2.0765e-08,\n",
      "        2.2911e-08, 8.5894e-10, 2.7252e-08, 4.0604e-08, 3.4305e-08, 2.6481e-08,\n",
      "        2.3696e-08, 6.4701e-08, 1.0072e-07, 9.4220e-08, 1.5573e-08, 1.9319e-08,\n",
      "        2.6164e-08, 6.8996e-09, 1.3881e-08, 1.6014e-08, 9.1663e-09, 1.9668e-09,\n",
      "        7.6673e-09, 1.2500e-08, 2.3193e-08, 1.2818e-08, 1.7137e-08, 2.8796e-08,\n",
      "        5.0004e-08, 2.0322e-08, 1.7897e-08, 1.8567e-08, 8.9511e-09, 1.2400e-11,\n",
      "        2.2030e-09, 1.0557e-09, 5.8364e-10, 5.8503e-10, 2.3455e-08, 1.9346e-09,\n",
      "        2.1916e-11, 3.7976e-11, 1.0352e-08, 8.7981e-09, 6.6664e-09, 8.1326e-09,\n",
      "        9.2883e-09, 1.7882e-08, 5.4866e-09, 2.1196e-09, 6.8794e-09, 1.4807e-08,\n",
      "        1.9941e-08, 3.4471e-08, 2.7479e-08, 8.2250e-09, 1.5650e-11, 1.5607e-11,\n",
      "        1.8233e-08, 3.4206e-08, 3.7937e-08, 1.6733e-08, 2.8826e-09, 3.1611e-09,\n",
      "        2.4782e-09, 2.5121e-09, 2.8938e-08, 8.3467e-08, 7.9089e-08, 3.7189e-08,\n",
      "        6.5798e-09, 5.7883e-09, 4.6709e-09, 1.6412e-11, 1.7564e-08, 4.4209e-08,\n",
      "        6.3412e-08, 1.6285e-08, 4.3850e-08, 7.9847e-08, 1.2765e-07, 7.9525e-08,\n",
      "        8.5977e-10, 3.5569e-11, 6.4018e-10, 4.2012e-09, 1.1324e-08, 1.0853e-08,\n",
      "        9.2753e-09, 1.3484e-11, 1.4881e-08, 2.9516e-09, 1.3219e-11, 1.3853e-11,\n",
      "        2.3255e-08, 4.3222e-08, 3.8508e-08, 1.5643e-08, 1.6936e-08, 1.8690e-08,\n",
      "        2.1808e-08, 1.6096e-08, 2.1307e-10, 2.0665e-10, 1.9314e-10, 4.0353e-11,\n",
      "        6.6249e-08, 1.0562e-07, 1.2097e-07, 3.4199e-08, 2.5505e-08, 5.0033e-08,\n",
      "        4.3927e-08, 4.7961e-08, 3.5795e-09, 4.9802e-09, 7.9047e-09, 7.7001e-10,\n",
      "        2.1539e-09, 4.1770e-09, 3.8055e-09, 1.6276e-09, 1.0237e-09, 1.0053e-08,\n",
      "        9.2534e-09, 8.7450e-09, 4.9355e-09, 1.6593e-09, 1.8199e-11, 1.4777e-11,\n",
      "        1.5350e-08, 9.4088e-09, 2.6084e-09, 9.7151e-12, 6.1821e-08, 5.3127e-08,\n",
      "        2.9706e-08, 1.0726e-08, 5.3303e-08, 9.1987e-08, 5.5772e-08, 3.4213e-08,\n",
      "        1.0823e-08, 4.1725e-09, 8.2053e-10, 2.2462e-11, 1.5798e-08, 7.8743e-09,\n",
      "        1.6113e-08, 2.3921e-08, 1.3098e-08, 1.7943e-08, 4.4799e-09, 7.9740e-09,\n",
      "        9.3721e-09, 1.4979e-08, 1.2087e-08, 7.9708e-09, 2.4393e-08, 3.5358e-08,\n",
      "        1.5474e-08, 4.8299e-09, 4.6619e-08, 8.2392e-08, 8.3981e-08, 5.3431e-08,\n",
      "        8.3732e-09, 8.2700e-09, 3.8783e-09, 9.6199e-10, 1.1515e-08, 4.0207e-08,\n",
      "        3.7889e-08, 1.5637e-08, 1.1442e-08, 3.0496e-08, 1.8296e-08, 1.1220e-08,\n",
      "        1.5555e-08, 3.0706e-08, 2.3572e-08, 1.3594e-08, 2.6200e-09, 5.0159e-09,\n",
      "        4.9503e-09, 4.2116e-09, 2.0794e-09, 7.7848e-09, 4.7348e-09, 3.8120e-09,\n",
      "        2.6928e-08, 8.2658e-08, 9.4786e-08, 2.5258e-08, 7.8259e-09, 1.4849e-08,\n",
      "        2.3862e-08, 1.1191e-08, 9.7793e-09, 2.2259e-09, 7.3227e-10, 1.6487e-11,\n",
      "        1.6100e-08, 2.7274e-08, 1.2485e-08, 1.0482e-09, 1.2977e-08, 4.8316e-08,\n",
      "        4.2738e-08, 9.8360e-09, 5.0688e-09, 3.6324e-08, 4.9124e-08, 1.8797e-08,\n",
      "        2.3410e-08, 1.5201e-08, 1.3977e-09, 1.1002e-11, 3.2960e-08, 1.0852e-07,\n",
      "        1.5459e-07, 6.4520e-08, 5.2428e-08, 2.2195e-08, 1.3202e-08, 1.5971e-08,\n",
      "        9.3947e-08, 1.3810e-07, 8.0728e-08, 2.2203e-08, 3.7414e-08, 3.0854e-08,\n",
      "        2.1823e-08, 9.0792e-09, 1.1595e-08, 1.5511e-08, 1.3493e-08, 1.7045e-08,\n",
      "        1.6463e-08, 8.3188e-09, 2.9743e-09, 1.2506e-11, 2.0194e-08, 1.5044e-08,\n",
      "        1.9984e-08, 4.9785e-09, 2.4545e-08, 3.6340e-08, 2.1165e-08, 5.2271e-09,\n",
      "        1.5114e-09, 5.7626e-09, 1.1967e-08, 5.5102e-09, 1.9872e-08, 3.1572e-08,\n",
      "        3.6358e-08, 9.0118e-09, 7.0484e-09, 1.2954e-08, 8.2395e-09, 1.1151e-08,\n",
      "        9.4143e-09, 3.1852e-08, 2.4523e-08, 1.6186e-08, 1.6087e-09, 5.3067e-09,\n",
      "        7.2586e-09, 1.1081e-08, 1.4586e-08, 2.0078e-08, 2.1713e-08, 1.3775e-08,\n",
      "        1.1557e-08, 4.1421e-08, 5.9183e-08, 8.4236e-09, 4.6915e-09, 5.8963e-09,\n",
      "        2.9227e-09, 1.1962e-09, 1.3555e-08, 6.1147e-08, 4.5918e-08, 1.4771e-08,\n",
      "        8.3490e-09, 3.5325e-08, 2.0583e-08, 2.5145e-08, 7.9239e-09, 6.4492e-08,\n",
      "        3.2181e-08, 2.1242e-08, 1.8337e-09, 2.5509e-09, 3.9260e-09, 4.5484e-09,\n",
      "        4.2111e-08, 4.1833e-08, 2.6692e-08, 8.4356e-09, 6.1199e-09, 1.0661e-08,\n",
      "        9.5994e-09, 1.2714e-09, 7.8682e-09, 6.0710e-09, 6.5533e-09, 1.7966e-11,\n",
      "        1.3415e-08, 2.1437e-08, 2.5741e-08, 2.1629e-08, 2.0885e-08, 3.9274e-08,\n",
      "        2.0560e-08, 6.1398e-09, 1.2008e-08, 2.1868e-08, 9.5081e-09, 4.9192e-09,\n",
      "        1.4591e-08, 6.2492e-08, 8.3559e-08, 6.6465e-08, 6.0630e-09, 1.5656e-08,\n",
      "        1.6978e-08, 1.0964e-08, 8.9764e-09, 1.2017e-08, 1.3622e-08, 1.2614e-08,\n",
      "        3.9494e-09, 7.9374e-09, 7.8985e-10, 1.5085e-11, 6.0696e-08, 6.0781e-08,\n",
      "        6.3424e-08, 3.8893e-08, 1.4856e-08, 1.8421e-08, 7.7188e-09, 2.6629e-09,\n",
      "        6.1284e-09, 1.8096e-08, 3.0840e-08, 3.1428e-08, 2.5503e-08, 6.8014e-08,\n",
      "        7.4428e-08, 3.3900e-08, 7.5346e-09, 1.4017e-08, 1.1497e-08, 3.0947e-09,\n",
      "        2.3876e-08, 3.0963e-08, 2.3142e-08, 1.1544e-08, 5.0723e-09, 3.9881e-09,\n",
      "        2.1917e-09, 1.8476e-11, 3.5094e-09, 9.9297e-09, 2.8172e-08, 3.8602e-09,\n",
      "        9.2049e-09, 1.3724e-08, 3.4049e-08, 1.2253e-08, 1.0162e-08, 9.4725e-09,\n",
      "        1.4065e-08, 1.2596e-08, 1.5594e-08, 1.0387e-08, 3.5847e-09, 1.3232e-09,\n",
      "        3.0236e-09, 2.3121e-09, 8.6910e-09, 2.2581e-08, 6.2317e-09, 9.8824e-09,\n",
      "        3.2709e-09, 1.4196e-11, 9.9660e-09, 3.1041e-09, 2.0577e-11, 2.4100e-11,\n",
      "        5.2850e-12, 8.6852e-10, 2.4110e-09, 1.2688e-08, 9.5475e-09, 2.4952e-08,\n",
      "        2.5218e-08, 2.7964e-08, 1.6208e-08, 3.5122e-08, 6.8834e-08, 8.2998e-08,\n",
      "        1.8541e-08, 2.8680e-08, 1.3566e-08, 8.8467e-09, 5.8141e-09, 1.8933e-09,\n",
      "        1.7704e-11, 1.8035e-11, 1.7518e-08, 9.1381e-09, 1.2862e-09, 1.2463e-11,\n",
      "        1.0322e-08, 6.8453e-09, 9.1809e-09, 8.0133e-09, 4.0325e-09, 6.2834e-09,\n",
      "        3.8921e-09, 1.7118e-11, 6.8844e-09, 2.5288e-08, 2.0405e-08, 2.8887e-08,\n",
      "        4.3214e-08, 2.7618e-08, 3.3525e-08, 5.7368e-09, 1.9952e-08, 4.0118e-08,\n",
      "        3.5873e-08, 1.7087e-09, 5.3338e-09, 4.5458e-09, 3.1978e-09, 5.5812e-09,\n",
      "        1.3641e-08, 8.8876e-09, 1.4633e-08, 3.6659e-09, 1.7329e-09, 1.5207e-08,\n",
      "        2.0159e-08, 7.8761e-12, 6.8042e-09, 1.3483e-08, 1.3740e-08, 4.0409e-09,\n",
      "        1.4477e-09, 2.9353e-09, 1.1050e-09, 5.1150e-10, 7.1458e-11, 6.4426e-09,\n",
      "        4.1692e-08, 1.1389e-08, 1.6292e-08, 3.3548e-08, 3.0728e-08, 2.4190e-09,\n",
      "        3.1759e-09, 1.1722e-08, 1.4341e-08, 1.4982e-11, 5.8235e-09, 5.0326e-09,\n",
      "        4.8589e-10, 2.0758e-11, 2.1137e-08, 2.3059e-08, 1.7313e-08, 1.1253e-08,\n",
      "        1.3204e-11, 4.4149e-09, 3.3288e-09, 6.6059e-09, 4.4071e-09, 3.4693e-08,\n",
      "        4.8431e-08, 7.1716e-08, 2.1329e-09, 1.2521e-08, 8.0790e-09, 8.9390e-12,\n",
      "        2.7276e-10, 2.9692e-08, 5.4583e-08, 5.7432e-08, 2.6732e-08, 2.1097e-08,\n",
      "        1.8218e-08, 5.0660e-09, 1.0810e-11, 1.2278e-11, 1.1458e-11, 1.1035e-11,\n",
      "        1.6806e-09, 5.1955e-09, 7.6360e-09, 3.9063e-09, 1.3771e-08, 1.3089e-08,\n",
      "        2.7875e-09, 1.2873e-11], device='cuda:0')}, 10: {'step': 4914, 'exp_avg': tensor([[ 9.3808e-06, -6.0145e-05,  2.1128e-06,  ..., -1.5251e-04,\n",
      "         -1.4222e-05,  9.0443e-06],\n",
      "        [ 1.0374e-05, -3.6230e-07, -4.7233e-05,  ..., -5.1429e-05,\n",
      "         -1.3297e-05, -5.2976e-05],\n",
      "        [-1.9440e-05,  1.4626e-04,  5.7788e-05,  ..., -3.7606e-05,\n",
      "         -3.4557e-07,  2.5891e-06],\n",
      "        ...,\n",
      "        [-7.6148e-06, -2.9495e-05, -2.5677e-06,  ...,  3.1232e-06,\n",
      "         -2.9258e-06, -5.9873e-06],\n",
      "        [-7.2017e-07,  2.6985e-06,  4.9061e-06,  ...,  5.0745e-07,\n",
      "          2.0847e-06,  3.3337e-06],\n",
      "        [ 3.4214e-06,  3.7781e-06,  7.5765e-07,  ...,  1.5053e-06,\n",
      "          3.1106e-07,  2.3302e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[1.2079e-08, 1.0961e-07, 1.0692e-08,  ..., 6.7349e-08, 1.2556e-08,\n",
      "         1.3159e-08],\n",
      "        [3.1707e-08, 2.9468e-07, 2.6320e-08,  ..., 8.8004e-08, 1.3653e-08,\n",
      "         4.5298e-08],\n",
      "        [1.5175e-08, 3.1307e-07, 2.2367e-08,  ..., 7.5060e-08, 1.2357e-08,\n",
      "         3.2970e-08],\n",
      "        ...,\n",
      "        [5.5861e-09, 2.3004e-08, 2.5503e-09,  ..., 1.5099e-09, 7.2859e-10,\n",
      "         1.3547e-09],\n",
      "        [4.6036e-09, 1.0075e-08, 2.9570e-09,  ..., 1.2063e-09, 7.2730e-10,\n",
      "         1.0134e-09],\n",
      "        [9.9198e-10, 3.0985e-09, 3.2755e-10,  ..., 5.9256e-10, 4.3007e-11,\n",
      "         6.7351e-11]], device='cuda:0')}, 11: {'step': 4914, 'exp_avg': tensor([-4.9029e-05, -8.7083e-05,  2.3198e-05, -2.7009e-05,  1.0238e-04,\n",
      "         8.7313e-05,  1.2456e-04, -5.7480e-05,  4.5711e-05, -6.4984e-05,\n",
      "         1.7122e-05,  2.2031e-06, -6.3086e-05, -8.1719e-05,  4.1368e-05,\n",
      "        -2.0157e-04, -4.7771e-05,  1.5095e-04,  1.0753e-04,  4.0718e-05,\n",
      "         3.3354e-05, -6.6702e-06,  2.4455e-07,  2.3466e-07,  3.2396e-05,\n",
      "         2.0790e-06,  1.6614e-05,  4.9038e-06, -9.1076e-06, -7.9640e-06,\n",
      "        -1.8282e-05, -8.3760e-06, -2.3896e-07, -3.9142e-06, -9.5755e-07,\n",
      "        -9.5904e-07,  1.1962e-05, -2.2894e-07, -2.9382e-07, -2.9101e-07,\n",
      "         1.9638e-05,  6.0086e-05, -8.2859e-05,  7.0327e-05,  1.5385e-07,\n",
      "        -1.4496e-05,  2.9389e-05, -8.1673e-06, -5.3880e-06, -6.0774e-05,\n",
      "         1.2140e-05, -3.8435e-06, -2.8374e-05,  9.4926e-06, -1.5085e-05,\n",
      "        -2.3829e-05, -1.0975e-05,  1.2201e-05,  5.2013e-05, -5.7207e-06,\n",
      "         1.3975e-05,  5.2559e-05, -1.5820e-05, -5.6764e-07,  7.5122e-07,\n",
      "        -5.4934e-06, -5.3584e-07, -2.9657e-07,  2.0678e-05, -9.4481e-07,\n",
      "        -9.4084e-07, -9.4819e-07,  1.6935e-06, -1.2916e-05, -4.0362e-07,\n",
      "        -3.9594e-07, -1.0362e-05, -3.6423e-05,  5.3065e-06,  3.2649e-06,\n",
      "        -1.2340e-05, -9.2443e-06,  1.7119e-06,  1.6086e-06,  9.8597e-06,\n",
      "         1.7106e-06,  1.7119e-06,  1.7096e-06,  1.7146e-05,  8.4091e-06,\n",
      "         1.7223e-06,  1.7658e-06,  2.5466e-05, -4.7738e-06,  2.8240e-05,\n",
      "         3.5254e-05, -5.6359e-06,  3.4541e-05, -2.8296e-05,  8.8346e-06,\n",
      "        -2.6558e-05, -4.4220e-05,  1.7581e-05,  1.0919e-05,  6.8421e-06,\n",
      "        -1.4592e-06,  1.1849e-05,  5.2502e-06, -2.5739e-05,  1.7690e-05,\n",
      "         1.5973e-05, -2.7124e-05,  5.1778e-05,  8.2205e-06,  4.0608e-06,\n",
      "         3.9796e-06, -5.7522e-06,  1.0850e-04,  1.3439e-06,  7.2195e-07,\n",
      "         2.1603e-06,  3.6579e-06, -2.6585e-05,  2.6634e-05,  4.3121e-05,\n",
      "         2.7509e-05,  4.7810e-05, -5.4282e-05, -2.4935e-05, -5.8390e-06,\n",
      "         5.0307e-06, -1.2200e-05,  3.5760e-06,  1.0998e-06,  1.4643e-06,\n",
      "        -1.7004e-07, -2.3067e-05, -1.7425e-05, -6.2071e-06, -6.7050e-07,\n",
      "        -3.2836e-06, -5.0611e-05, -9.2643e-05, -1.0670e-06, -3.4328e-06,\n",
      "        -1.3840e-05,  1.4224e-05,  3.1254e-06,  2.2169e-06, -3.2296e-07,\n",
      "        -3.6588e-07, -3.3063e-07,  2.7274e-05,  3.6859e-05, -2.9204e-07,\n",
      "         3.0706e-06,  3.9047e-05,  1.0348e-06, -2.1595e-05,  3.4901e-06,\n",
      "        -6.0622e-06,  9.5659e-06, -1.0926e-05, -1.6189e-05, -7.5332e-07,\n",
      "         1.6488e-05,  9.5159e-07,  9.7763e-07,  5.9383e-06,  3.1903e-05,\n",
      "        -2.7645e-06,  1.0713e-06,  1.6166e-05, -1.8040e-05, -2.1745e-05,\n",
      "        -8.6709e-07,  7.6840e-08, -8.1615e-06, -2.0659e-06, -1.1584e-07,\n",
      "        -2.0499e-05, -1.0976e-05,  1.6329e-05,  5.2083e-06], device='cuda:0'), 'exp_avg_sq': tensor([5.6772e-08, 1.2345e-07, 1.3342e-07, 1.0412e-07, 1.0841e-07, 1.0702e-07,\n",
      "        9.2138e-08, 2.0562e-08, 3.6735e-08, 1.2438e-08, 1.4634e-09, 1.3132e-11,\n",
      "        7.7519e-08, 9.5712e-08, 6.2704e-08, 3.1561e-08, 1.4821e-07, 3.0103e-07,\n",
      "        1.8053e-07, 9.6486e-08, 1.1302e-08, 1.8542e-09, 1.2267e-11, 1.2550e-11,\n",
      "        1.2106e-08, 6.3036e-09, 1.0288e-08, 2.2574e-09, 2.7833e-09, 4.4860e-09,\n",
      "        5.5607e-09, 3.3692e-09, 1.5949e-09, 8.5754e-10, 1.8139e-11, 1.8400e-11,\n",
      "        9.4473e-09, 1.4217e-11, 1.2593e-11, 1.3784e-11, 5.3567e-08, 6.2436e-08,\n",
      "        1.0307e-07, 3.2303e-08, 1.0274e-08, 2.1147e-08, 2.1936e-08, 1.0815e-09,\n",
      "        7.6881e-09, 1.9791e-08, 3.4926e-08, 1.1504e-08, 2.3679e-08, 4.5053e-08,\n",
      "        3.4271e-08, 9.6112e-09, 1.6161e-08, 1.9314e-08, 9.5001e-09, 8.7038e-10,\n",
      "        2.5585e-09, 1.5163e-08, 2.1604e-08, 1.6607e-11, 2.0234e-08, 2.3608e-09,\n",
      "        1.5773e-11, 9.4134e-12, 1.2280e-08, 1.8420e-11, 1.8053e-11, 1.8616e-11,\n",
      "        4.5854e-09, 3.7583e-09, 1.5198e-11, 1.3848e-11, 2.6607e-08, 2.6739e-08,\n",
      "        8.0482e-09, 7.3046e-10, 3.7502e-09, 2.5769e-09, 5.6190e-10, 5.6041e-10,\n",
      "        5.5249e-09, 2.6143e-11, 2.6276e-11, 2.6034e-11, 5.0728e-09, 2.4746e-09,\n",
      "        1.9506e-11, 1.8843e-11, 4.6615e-09, 8.1496e-09, 7.2732e-09, 5.6500e-09,\n",
      "        1.0028e-08, 1.3873e-08, 8.9365e-09, 2.2393e-09, 5.8196e-09, 6.6104e-09,\n",
      "        4.5127e-09, 2.6882e-09, 2.4615e-09, 6.8702e-09, 1.1603e-08, 5.0773e-09,\n",
      "        2.1617e-08, 2.2075e-08, 1.3767e-08, 1.0719e-08, 1.4473e-08, 8.4910e-09,\n",
      "        1.6833e-11, 1.5719e-11, 3.1395e-08, 1.5824e-08, 1.4129e-09, 9.0157e-12,\n",
      "        3.1928e-09, 1.0876e-08, 1.2464e-08, 6.9200e-09, 2.1307e-08, 4.8831e-08,\n",
      "        7.5642e-08, 3.5022e-08, 9.2196e-09, 2.5911e-08, 2.2890e-08, 4.0460e-09,\n",
      "        6.6158e-09, 2.2440e-10, 2.3395e-10, 2.3633e-11, 6.6386e-09, 4.9930e-09,\n",
      "        6.0936e-10, 3.0432e-11, 3.6631e-09, 1.3415e-08, 3.2666e-08, 1.9799e-08,\n",
      "        2.4700e-09, 3.5702e-09, 2.0645e-08, 8.9459e-10, 6.2397e-10, 9.1238e-12,\n",
      "        8.2354e-12, 8.9081e-12, 1.1435e-08, 1.7518e-08, 1.6439e-08, 3.5769e-09,\n",
      "        6.9894e-09, 8.8935e-09, 4.1206e-09, 8.8565e-10, 9.6662e-10, 3.7579e-09,\n",
      "        7.8280e-09, 7.5967e-09, 1.7540e-11, 4.2620e-09, 1.0405e-11, 1.0530e-11,\n",
      "        3.3111e-09, 5.0328e-09, 3.5065e-09, 1.6642e-11, 6.4943e-09, 7.2902e-09,\n",
      "        1.4650e-09, 1.4004e-11, 4.5222e-09, 7.1581e-09, 8.0027e-09, 3.2421e-09,\n",
      "        1.6461e-08, 1.0404e-08, 8.2147e-09, 1.2264e-09], device='cuda:0')}, 12: {'step': 4914, 'exp_avg': tensor([[ 4.2325e-05, -4.3016e-05, -2.5688e-05,  ...,  1.1795e-04,\n",
      "          8.1455e-06,  6.4575e-06],\n",
      "        [-6.5256e-06,  6.5735e-05, -2.1493e-05,  ..., -5.8460e-05,\n",
      "          1.1404e-04,  4.2409e-06],\n",
      "        [-2.0690e-05, -2.9080e-05, -4.4028e-05,  ..., -8.5301e-06,\n",
      "         -4.2479e-06, -9.0955e-07],\n",
      "        ...,\n",
      "        [ 9.4412e-06, -8.8447e-06,  1.7823e-05,  ...,  1.0553e-05,\n",
      "         -9.7146e-06, -4.1871e-06],\n",
      "        [-1.4816e-07, -6.1520e-07, -2.6489e-07,  ..., -1.5212e-07,\n",
      "         -1.7830e-07, -2.3694e-07],\n",
      "        [-1.3688e-07, -6.2077e-07, -2.6390e-07,  ..., -1.5525e-07,\n",
      "         -1.7942e-07, -2.3980e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[4.4649e-08, 5.1865e-07, 1.8349e-08,  ..., 1.2130e-07, 3.9748e-08,\n",
      "         1.2861e-08],\n",
      "        [5.0283e-08, 3.2112e-07, 2.7613e-08,  ..., 4.3475e-08, 2.9475e-08,\n",
      "         1.3231e-08],\n",
      "        [4.2727e-08, 1.6833e-07, 1.4827e-08,  ..., 2.6649e-08, 7.7681e-09,\n",
      "         5.0901e-09],\n",
      "        ...,\n",
      "        [3.1802e-09, 4.3444e-09, 3.2034e-09,  ..., 1.5524e-08, 7.1222e-10,\n",
      "         1.6455e-09],\n",
      "        [8.3718e-10, 1.0915e-11, 1.6502e-11,  ..., 4.7881e-12, 2.9454e-12,\n",
      "         3.2214e-12],\n",
      "        [8.9117e-10, 1.1170e-11, 1.7115e-11,  ..., 4.9078e-12, 3.0283e-12,\n",
      "         3.2400e-12]], device='cuda:0')}, 13: {'step': 4914, 'exp_avg': tensor([ 5.6288e-05,  1.1468e-04, -9.3474e-05, -3.1704e-05,  1.1569e-04,\n",
      "         3.6277e-06,  4.5217e-06,  9.1762e-07,  1.1580e-06, -1.4297e-05,\n",
      "         2.2047e-05,  3.3638e-06,  5.7722e-06, -1.8663e-05,  5.4010e-05,\n",
      "        -3.1602e-06, -5.2613e-05,  2.7479e-06,  5.9018e-06,  1.6179e-06,\n",
      "         8.2355e-08,  8.0869e-08,  8.7265e-08,  8.8511e-08, -2.9251e-05,\n",
      "        -6.0208e-06, -1.1618e-06, -1.1940e-06, -1.1196e-05,  1.0392e-05,\n",
      "         4.2448e-06,  1.4505e-06,  4.6134e-07, -3.3505e-07, -3.3102e-07,\n",
      "        -3.2441e-07, -1.2932e-04, -1.7167e-04,  1.0291e-05,  6.9421e-07,\n",
      "         2.3920e-05,  3.4286e-06, -4.2527e-05, -1.4633e-05,  2.1995e-05,\n",
      "         7.0866e-06, -5.0790e-05, -1.6814e-06, -6.3990e-05,  8.5210e-06,\n",
      "         3.1430e-05, -9.6608e-06,  4.0604e-06,  2.8417e-06,  2.4652e-06,\n",
      "         3.0238e-09, -4.3664e-06,  2.0980e-06, -6.1186e-06, -8.5695e-08,\n",
      "         6.6340e-06, -1.6890e-05, -1.7914e-06,  6.6891e-06, -2.2042e-06,\n",
      "        -4.2496e-06,  5.1727e-06,  8.4601e-07, -9.1683e-06, -2.4767e-06,\n",
      "        -1.8963e-07,  3.4243e-06,  9.4600e-06,  2.8456e-06,  3.5293e-06,\n",
      "        -1.4214e-06,  1.3270e-06,  8.6642e-06,  4.6188e-06, -2.2578e-07,\n",
      "        -2.1191e-05, -9.6064e-06,  2.9679e-06, -8.5306e-08,  3.1980e-05,\n",
      "         5.6539e-05,  5.8141e-06,  1.2441e-06, -6.1965e-05,  3.3141e-05,\n",
      "         2.6003e-05, -1.0089e-04,  2.2535e-05,  1.4718e-05,  1.1316e-05,\n",
      "         5.5177e-06,  4.6964e-05, -9.1306e-05, -1.5046e-05, -5.8389e-05,\n",
      "         1.1664e-05, -6.5801e-06,  3.9921e-06,  2.4184e-06, -1.2506e-05,\n",
      "        -8.9160e-05,  1.4972e-05, -1.5283e-05,  4.8299e-05, -4.6797e-05,\n",
      "        -1.1699e-05,  8.5465e-06,  8.8517e-07,  3.0452e-07,  1.4322e-06,\n",
      "         2.3813e-06, -8.2476e-05,  5.0172e-05,  2.5308e-05, -3.9663e-06,\n",
      "         2.0580e-06,  2.4624e-06, -2.0508e-05, -1.8915e-05,  9.0697e-06,\n",
      "        -2.9398e-05, -1.6939e-06,  5.9081e-06, -2.9311e-06, -3.7782e-05,\n",
      "         1.9991e-05, -7.0208e-06, -2.1036e-05, -5.1696e-05,  1.8844e-05,\n",
      "        -3.4881e-06,  5.5423e-05,  6.1149e-06,  5.8939e-06,  6.0442e-06,\n",
      "        -1.4882e-05,  2.1385e-05,  9.0441e-06,  1.0598e-06, -1.0923e-05,\n",
      "        -4.9695e-07, -4.5531e-07, -4.7862e-07, -1.0105e-05, -8.6431e-07,\n",
      "         8.9257e-06, -4.1564e-06, -5.4854e-06, -4.0182e-06, -8.3233e-07,\n",
      "        -8.3373e-07,  1.5919e-06, -4.6655e-06, -5.0520e-07, -4.9416e-07,\n",
      "        -8.9107e-07, -8.9032e-07, -8.8866e-07, -8.9177e-07, -5.6073e-08,\n",
      "        -2.9322e-05,  3.5197e-06, -3.1429e-06,  3.3175e-06, -9.6097e-06,\n",
      "        -1.3209e-06, -1.3106e-06,  7.2739e-06, -2.7035e-06, -7.4718e-06,\n",
      "        -5.2945e-07,  1.8688e-06, -1.4679e-08, -6.6759e-08, -6.6003e-07,\n",
      "        -1.1349e-04, -9.6552e-05,  3.3579e-05,  4.1026e-06,  2.3980e-05,\n",
      "         3.9910e-06, -4.8378e-09, -7.7527e-08, -5.5680e-06, -2.1786e-06,\n",
      "         1.4785e-05, -4.3348e-07, -3.8584e-05, -1.2316e-05,  3.8420e-06,\n",
      "        -1.0343e-06, -5.6661e-05, -2.1775e-05,  1.5617e-05,  7.2020e-07,\n",
      "        -3.7363e-06, -7.5796e-06,  1.1697e-05, -9.1214e-06,  3.1444e-05,\n",
      "         3.2046e-05,  2.7137e-05,  1.8123e-05, -1.2426e-04, -1.2638e-04,\n",
      "        -2.6285e-05, -2.7095e-06, -3.8416e-07, -3.8963e-07, -3.9041e-07,\n",
      "        -3.8310e-07,  7.6315e-06, -1.7137e-05, -1.0925e-06, -1.0987e-06,\n",
      "        -1.1193e-05, -1.4232e-05, -9.5782e-07, -9.5642e-07], device='cuda:0'), 'exp_avg_sq': tensor([1.1240e-07, 1.0098e-07, 6.2160e-08, 1.5902e-08, 5.9214e-08, 2.3921e-08,\n",
      "        1.3042e-08, 7.2967e-10, 5.1634e-09, 7.9618e-09, 4.2480e-09, 1.0546e-09,\n",
      "        8.0440e-09, 1.0182e-08, 6.1607e-09, 1.7790e-09, 1.0947e-08, 1.1850e-09,\n",
      "        1.1483e-09, 1.9252e-11, 1.5719e-11, 1.5873e-11, 1.5621e-11, 1.5637e-11,\n",
      "        2.0763e-08, 2.5125e-09, 1.1547e-11, 1.1161e-11, 3.0269e-08, 1.3798e-08,\n",
      "        1.9292e-09, 8.8717e-10, 4.9504e-10, 2.3548e-11, 2.3530e-11, 2.3339e-11,\n",
      "        1.5798e-08, 1.2617e-08, 1.2935e-09, 1.4225e-11, 2.1803e-08, 1.4419e-08,\n",
      "        1.0343e-08, 1.6937e-09, 1.8514e-08, 1.7308e-08, 9.7823e-09, 8.6528e-10,\n",
      "        2.5667e-08, 3.8430e-08, 2.3584e-08, 1.1247e-08, 3.1930e-09, 1.6385e-09,\n",
      "        9.4095e-10, 1.1380e-11, 8.9116e-09, 3.5125e-09, 1.5470e-09, 1.0796e-11,\n",
      "        2.3401e-08, 1.2339e-08, 5.8480e-09, 4.1446e-09, 8.5638e-09, 3.8655e-09,\n",
      "        1.3615e-09, 1.3726e-11, 7.2242e-09, 8.7252e-09, 4.2469e-09, 1.3555e-09,\n",
      "        7.5863e-09, 2.8796e-09, 1.8102e-09, 1.5876e-11, 4.9366e-09, 6.8519e-09,\n",
      "        3.2774e-09, 1.6884e-11, 8.3508e-09, 2.6415e-09, 5.9885e-10, 1.3337e-11,\n",
      "        1.3784e-08, 5.9263e-09, 1.9959e-09, 4.3500e-10, 9.9449e-08, 1.3617e-07,\n",
      "        1.0462e-07, 8.7166e-08, 1.0208e-08, 7.9619e-09, 5.3126e-09, 1.3395e-09,\n",
      "        1.9121e-07, 2.4388e-07, 1.9259e-07, 6.4098e-08, 7.9685e-09, 6.9519e-09,\n",
      "        5.7513e-09, 1.1683e-09, 4.1593e-08, 6.1297e-08, 4.3147e-08, 1.7735e-08,\n",
      "        6.9458e-08, 7.4443e-08, 4.1093e-08, 9.3004e-09, 9.6650e-09, 7.0289e-09,\n",
      "        5.1293e-09, 2.1016e-09, 8.0550e-08, 7.9279e-08, 2.1498e-08, 3.5273e-09,\n",
      "        1.1453e-08, 9.7051e-09, 7.6169e-09, 4.1764e-09, 2.2089e-08, 3.2879e-08,\n",
      "        2.9237e-08, 1.0208e-08, 1.2823e-08, 1.4400e-08, 1.2414e-08, 2.5790e-09,\n",
      "        4.4631e-09, 4.9175e-09, 2.2420e-09, 1.3811e-09, 2.2050e-09, 2.2150e-10,\n",
      "        2.1908e-10, 2.2106e-10, 1.0294e-08, 9.0725e-09, 4.3186e-09, 2.0016e-11,\n",
      "        3.2885e-09, 2.3251e-11, 2.3885e-11, 2.3587e-11, 3.1958e-08, 2.8904e-08,\n",
      "        1.0501e-08, 5.6910e-09, 1.6620e-09, 8.0359e-10, 4.1701e-11, 4.1439e-11,\n",
      "        1.7950e-09, 1.4252e-09, 1.5319e-11, 1.4445e-11, 3.8570e-11, 3.8114e-11,\n",
      "        3.9490e-11, 3.8042e-11, 1.6075e-08, 1.3981e-08, 1.5443e-09, 8.0954e-10,\n",
      "        4.1284e-09, 2.7770e-09, 1.4582e-11, 1.4501e-11, 1.2074e-08, 1.1624e-09,\n",
      "        6.4963e-10, 1.6931e-11, 6.6606e-10, 1.0516e-10, 1.0030e-10, 1.6950e-11,\n",
      "        1.1247e-08, 8.9824e-09, 1.4519e-08, 6.6333e-10, 7.8585e-09, 1.1962e-09,\n",
      "        1.2239e-11, 1.2043e-11, 3.3068e-09, 6.3161e-09, 4.1686e-09, 9.1395e-12,\n",
      "        9.0630e-09, 9.1984e-09, 4.4182e-09, 1.5070e-11, 2.3258e-08, 2.4273e-08,\n",
      "        9.6676e-09, 7.3280e-10, 3.9640e-09, 9.5389e-09, 1.1112e-08, 9.2705e-10,\n",
      "        6.6561e-10, 6.4906e-10, 4.2392e-08, 4.2344e-09, 1.5747e-08, 2.8057e-08,\n",
      "        2.9744e-08, 1.7060e-08, 1.4019e-11, 1.4721e-11, 1.5031e-11, 1.3960e-11,\n",
      "        5.5645e-09, 2.4813e-09, 1.9976e-11, 2.0738e-11, 2.1214e-09, 6.6721e-09,\n",
      "        1.1796e-11, 1.1840e-11], device='cuda:0')}, 14: {'step': 4914, 'exp_avg': tensor([[-8.1744e-05, -1.3893e-05,  1.0133e-05,  ..., -3.2202e-04,\n",
      "          5.7298e-05, -1.2728e-05],\n",
      "        [ 6.6415e-05,  3.9277e-04, -5.0619e-05,  ...,  3.3856e-05,\n",
      "         -1.5518e-04, -3.0876e-05],\n",
      "        [ 1.2700e-05,  4.3579e-05,  2.8384e-05,  ...,  1.0640e-06,\n",
      "         -3.5287e-05,  5.6917e-06],\n",
      "        ...,\n",
      "        [-1.4879e-07,  6.6678e-06,  2.3007e-06,  ...,  1.7698e-06,\n",
      "         -2.9565e-07,  1.3205e-06],\n",
      "        [ 1.4499e-09, -1.5337e-07, -1.4696e-07,  ...,  6.9978e-08,\n",
      "         -3.6475e-07,  4.8845e-07],\n",
      "        [-2.8329e-07, -1.5104e-07, -4.1713e-08,  ...,  1.5454e-07,\n",
      "         -5.2020e-07,  4.9597e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[1.2134e-07, 1.1535e-06, 8.1179e-08,  ..., 3.0121e-07, 1.0337e-07,\n",
      "         9.3667e-08],\n",
      "        [3.7903e-07, 3.8980e-07, 6.1978e-08,  ..., 7.0452e-08, 1.8350e-07,\n",
      "         1.1381e-07],\n",
      "        [6.9963e-08, 1.0780e-07, 3.8428e-08,  ..., 3.5169e-08, 6.3996e-08,\n",
      "         3.1554e-08],\n",
      "        ...,\n",
      "        [1.8834e-09, 8.1590e-09, 1.2567e-09,  ..., 2.5688e-10, 3.3699e-10,\n",
      "         1.7896e-10],\n",
      "        [3.2933e-12, 1.3820e-11, 2.8022e-11,  ..., 3.2182e-12, 5.1230e-12,\n",
      "         1.0003e-12],\n",
      "        [1.7856e-09, 7.1900e-12, 7.9509e-12,  ..., 9.8019e-12, 1.8602e-10,\n",
      "         1.0899e-12]], device='cuda:0')}, 15: {'step': 4914, 'exp_avg': tensor([-6.6600e-05, -4.7632e-05, -4.2584e-05,  3.9292e-05,  8.7279e-06,\n",
      "        -4.9220e-08, -2.1228e-06, -2.1917e-06,  6.0272e-05,  1.2768e-05,\n",
      "         1.4104e-05,  5.3812e-06,  8.7339e-05,  5.3864e-05, -1.4215e-04,\n",
      "        -3.7037e-05,  6.4549e-06, -1.0128e-05, -7.0780e-06,  2.4486e-07,\n",
      "        -1.0196e-05,  6.8731e-06, -4.3803e-05, -2.7025e-05, -2.0081e-07,\n",
      "         1.7145e-06, -2.6769e-05,  2.6589e-05, -3.4266e-05,  2.5343e-05,\n",
      "        -2.4928e-05, -2.6593e-06,  8.2435e-06,  2.8779e-05, -2.0352e-05,\n",
      "        -2.6104e-05, -1.4892e-05,  3.6310e-06,  2.4147e-05, -2.6793e-05,\n",
      "         2.0320e-05, -5.4150e-05, -8.7292e-08, -9.3814e-08,  3.2248e-05,\n",
      "         5.9942e-05, -8.0176e-06, -2.6263e-05,  7.6063e-08, -9.9067e-07,\n",
      "        -9.8777e-07, -1.0013e-06,  7.7085e-05, -4.2011e-05,  2.2093e-04,\n",
      "         5.6093e-05,  3.4041e-06,  9.8686e-05,  2.7756e-05,  3.0810e-06,\n",
      "        -6.0824e-07,  6.7411e-06,  6.5551e-06, -7.6870e-07, -2.0852e-06,\n",
      "         4.1024e-06,  1.0183e-06, -9.2542e-06,  1.8707e-05,  9.3680e-06,\n",
      "         1.7932e-05, -6.4614e-05, -1.7170e-05, -3.7893e-05,  1.6818e-05,\n",
      "         9.3627e-07, -2.3273e-06, -7.2640e-06, -1.7558e-05, -6.5103e-06,\n",
      "         2.5420e-06, -2.9253e-05, -1.8859e-05, -1.4174e-06, -1.3956e-05,\n",
      "         4.2257e-05,  1.5754e-05, -1.1385e-06, -2.7211e-06, -1.7124e-05,\n",
      "        -9.4536e-06, -5.8167e-05, -3.7037e-06, -1.9932e-05,  1.4931e-05,\n",
      "        -5.0742e-07, -9.0185e-06, -9.3678e-06,  5.3072e-06, -6.6486e-07,\n",
      "         1.0427e-05, -1.1808e-05, -1.9594e-05, -1.3726e-05, -4.2422e-07,\n",
      "        -4.8448e-07, -6.5228e-06, -2.5095e-05, -4.2260e-05, -3.4721e-05,\n",
      "        -1.6940e-05,  6.5324e-06, -1.0589e-05, -9.8505e-06,  1.4511e-05,\n",
      "        -2.9813e-06,  2.3807e-05, -5.1439e-05,  2.5925e-05,  7.9812e-06,\n",
      "         4.1266e-05, -3.3882e-05, -6.8580e-05,  1.9699e-05,  4.0956e-05,\n",
      "         2.4754e-05,  1.3049e-05, -1.2275e-05, -7.8619e-05,  1.8804e-04,\n",
      "         4.3691e-04,  1.0087e-04,  4.0726e-05,  1.4009e-05, -1.0311e-05,\n",
      "        -7.7965e-06,  1.1985e-04,  2.8938e-05, -5.6124e-06,  1.4164e-06,\n",
      "         6.6977e-06,  4.4610e-06, -2.7296e-06,  3.8952e-06, -2.2199e-05,\n",
      "         3.1859e-06, -5.6721e-07, -5.3802e-07], device='cuda:0'), 'exp_avg_sq': tensor([4.8735e-07, 1.6229e-07, 5.0338e-08, 2.8717e-08, 8.6953e-09, 9.8816e-10,\n",
      "        7.3392e-10, 7.3435e-10, 7.3973e-08, 3.2689e-08, 5.7634e-09, 9.4480e-10,\n",
      "        1.0549e-07, 1.9153e-07, 1.6146e-07, 4.2300e-08, 6.0455e-09, 5.8298e-09,\n",
      "        1.6113e-09, 2.0830e-11, 2.7295e-08, 3.1658e-08, 1.2482e-08, 2.1821e-08,\n",
      "        1.5101e-08, 2.2903e-08, 4.1350e-08, 1.2121e-08, 5.6913e-08, 8.0593e-08,\n",
      "        4.3022e-08, 1.0328e-08, 7.9821e-09, 1.5995e-08, 3.2971e-08, 1.7271e-08,\n",
      "        2.0867e-08, 2.3416e-08, 1.4307e-08, 1.3420e-08, 2.7371e-08, 2.2787e-08,\n",
      "        9.1773e-12, 9.2378e-12, 2.3842e-08, 2.6310e-08, 1.5356e-08, 4.2096e-09,\n",
      "        2.9941e-10, 3.0902e-11, 3.1077e-11, 3.1296e-11, 2.4273e-07, 2.8735e-07,\n",
      "        1.8744e-07, 7.0674e-08, 2.8546e-08, 5.4573e-08, 9.1313e-09, 1.8155e-11,\n",
      "        2.6791e-09, 1.8755e-09, 1.6291e-09, 1.6317e-11, 2.0216e-10, 1.2521e-09,\n",
      "        2.1561e-09, 2.0402e-09, 1.0175e-08, 3.7524e-08, 6.3209e-08, 3.7595e-08,\n",
      "        4.2940e-09, 8.5849e-09, 5.6152e-09, 2.6787e-11, 1.0888e-08, 6.0551e-09,\n",
      "        4.0480e-09, 2.0933e-09, 4.6412e-09, 6.7160e-09, 3.8699e-09, 8.2965e-10,\n",
      "        8.5977e-09, 3.7432e-08, 2.1605e-08, 8.2781e-12, 1.0249e-08, 3.7533e-09,\n",
      "        1.0672e-09, 1.7456e-09, 1.2436e-08, 2.1814e-08, 2.0392e-08, 1.7339e-11,\n",
      "        8.9132e-09, 6.2464e-09, 2.0585e-09, 1.4988e-11, 9.2123e-09, 1.1265e-08,\n",
      "        1.3375e-08, 1.2642e-08, 2.6395e-10, 2.8217e-10, 1.0377e-09, 1.9793e-08,\n",
      "        1.5806e-08, 7.7224e-08, 1.1182e-08, 1.9011e-09, 9.1519e-09, 1.9483e-08,\n",
      "        8.6115e-09, 3.8677e-09, 1.0124e-08, 8.2663e-09, 8.5805e-09, 9.8143e-09,\n",
      "        1.5578e-08, 9.5398e-08, 8.9955e-08, 9.6717e-09, 1.4984e-08, 1.6627e-08,\n",
      "        1.2712e-09, 2.9051e-09, 6.4699e-08, 1.8304e-07, 2.2885e-07, 8.5798e-08,\n",
      "        1.8227e-08, 1.9661e-08, 1.3322e-08, 7.3786e-09, 1.0833e-08, 7.2687e-09,\n",
      "        8.8880e-10, 7.8560e-12, 2.1315e-10, 6.2704e-09, 8.1256e-09, 1.4444e-09,\n",
      "        3.1081e-08, 9.1627e-10, 6.8783e-12, 8.1113e-12], device='cuda:0')}, 16: {'step': 4914, 'exp_avg': tensor([[ 2.6237e-05, -3.3100e-05,  4.3632e-06,  ...,  8.9678e-06,\n",
      "          2.1127e-05,  1.7611e-05],\n",
      "        [ 3.9610e-05, -9.4324e-05, -1.5389e-07,  ...,  4.5717e-05,\n",
      "          9.5641e-06,  9.6858e-06],\n",
      "        [ 9.7398e-06,  4.8655e-05, -9.9787e-07,  ...,  6.0936e-05,\n",
      "          1.0695e-05,  1.4651e-05],\n",
      "        ...,\n",
      "        [ 1.5657e-05, -4.3085e-06, -4.9303e-07,  ...,  1.2398e-06,\n",
      "          1.0423e-06,  3.3366e-06],\n",
      "        [ 1.0190e-06, -7.4703e-07,  9.3828e-07,  ...,  1.7379e-07,\n",
      "          2.3173e-06,  2.0715e-06],\n",
      "        [ 2.0284e-07, -6.5206e-07, -7.0075e-08,  ..., -2.1588e-07,\n",
      "          2.6538e-08,  2.9205e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[6.3953e-09, 8.3604e-08, 2.2707e-09,  ..., 4.5620e-08, 3.3972e-09,\n",
      "         6.1129e-09],\n",
      "        [5.4256e-09, 9.8638e-08, 3.2503e-09,  ..., 3.2889e-08, 8.7494e-09,\n",
      "         1.3434e-08],\n",
      "        [8.0350e-09, 1.0113e-07, 4.0794e-09,  ..., 2.5099e-08, 1.2606e-08,\n",
      "         3.2484e-08],\n",
      "        ...,\n",
      "        [6.5694e-10, 2.0315e-09, 1.4051e-09,  ..., 7.3305e-10, 2.9350e-09,\n",
      "         1.7932e-09],\n",
      "        [3.4488e-10, 4.0370e-10, 7.0026e-10,  ..., 1.7866e-10, 6.6928e-10,\n",
      "         7.4256e-10],\n",
      "        [1.9038e-12, 1.3161e-11, 1.8414e-12,  ..., 2.0068e-12, 5.6244e-12,\n",
      "         2.9656e-12]], device='cuda:0')}, 17: {'step': 4914, 'exp_avg': tensor([ 3.3872e-05, -1.3374e-05,  9.7871e-06, -2.8216e-05,  9.3224e-05,\n",
      "         6.4199e-05, -1.2098e-04, -2.4546e-07, -6.3405e-05, -7.2772e-05,\n",
      "        -3.4713e-05,  1.1502e-04, -5.9394e-05, -1.0304e-06, -9.8795e-07,\n",
      "        -9.8486e-07, -2.9509e-05, -1.0229e-06, -9.3896e-07, -1.0153e-06,\n",
      "        -6.4568e-05, -1.1489e-06, -1.1305e-06, -1.1037e-06, -5.9737e-05,\n",
      "         1.0421e-05,  5.4643e-05,  2.7040e-05, -7.0014e-05, -9.9851e-05,\n",
      "        -1.0092e-04,  2.0197e-05, -8.5072e-05, -1.1917e-04, -3.9759e-06,\n",
      "        -7.2450e-05, -3.4358e-05, -1.2374e-04, -5.0566e-05, -4.5871e-05,\n",
      "        -9.2282e-06, -4.9881e-05,  5.7086e-05,  9.7430e-06,  9.3981e-05,\n",
      "         1.0290e-04,  3.3402e-05,  1.2344e-05,  4.6351e-05,  8.4938e-06,\n",
      "        -1.5115e-06,  1.2771e-06,  2.1828e-05, -1.4749e-05, -7.2498e-07,\n",
      "        -6.7095e-07, -5.1608e-06,  1.5996e-05, -1.0985e-04,  6.3027e-05,\n",
      "        -4.2929e-06, -8.7458e-05, -5.4152e-05, -1.0906e-04,  1.4274e-05,\n",
      "        -2.4082e-05,  1.9085e-04,  2.9598e-05,  1.8287e-05, -9.6906e-05,\n",
      "         1.9623e-05,  5.9343e-05, -1.0425e-05, -1.2631e-05,  5.8164e-06,\n",
      "         9.2603e-06,  3.4450e-05, -1.3468e-04,  3.1050e-05, -2.4416e-05,\n",
      "         2.4656e-07,  4.5638e-06, -4.6653e-07, -5.8036e-07,  6.9752e-05,\n",
      "         3.8151e-06, -1.2844e-04, -5.9988e-05, -2.3260e-06, -5.6173e-07,\n",
      "        -5.9645e-07, -6.0267e-07,  5.4916e-05,  4.8776e-05,  3.8101e-06,\n",
      "         1.3329e-06, -7.6680e-06, -9.6855e-05,  4.4094e-05,  9.8162e-06,\n",
      "        -1.2583e-04, -7.7307e-06, -4.3129e-05,  8.9072e-05, -2.3788e-05,\n",
      "        -3.2059e-05,  9.8568e-05,  1.2332e-06, -4.4534e-05, -2.4351e-05,\n",
      "         8.5018e-06, -4.5731e-06,  4.1126e-05, -7.7734e-05, -8.5259e-06,\n",
      "         1.0254e-04, -9.9784e-05, -1.7034e-04, -5.7693e-06, -2.0245e-05,\n",
      "        -1.1812e-05,  8.5010e-05,  3.4767e-05,  1.0656e-05,  1.6272e-05,\n",
      "         3.6410e-05,  1.5543e-05, -3.5029e-05, -4.2626e-05,  3.8671e-05,\n",
      "        -1.2615e-05, -2.4111e-05, -4.5688e-05,  1.6393e-05, -1.5169e-05,\n",
      "        -1.6569e-05, -5.3035e-06, -5.0957e-05, -2.3369e-05,  3.5378e-05,\n",
      "         7.7433e-05,  2.1208e-04,  2.7303e-05,  7.8813e-05,  2.1880e-05,\n",
      "        -2.5962e-05, -1.6135e-05, -7.6107e-07, -6.1006e-07, -3.0185e-08,\n",
      "        -1.0857e-05, -9.5950e-06,  7.5656e-05,  6.5734e-05, -1.1304e-05,\n",
      "        -1.2366e-05,  5.3095e-05, -1.5494e-04, -2.5698e-05, -7.8526e-06,\n",
      "        -6.0319e-06,  3.4008e-05,  1.0533e-05,  5.1283e-06, -2.5048e-05,\n",
      "         1.3576e-05, -2.1717e-05, -3.2876e-06, -2.1135e-05,  2.2949e-05,\n",
      "         4.7913e-05,  1.7614e-05, -5.0257e-06,  1.8102e-04,  5.6276e-05,\n",
      "        -1.3401e-05, -2.8642e-06,  2.7517e-05, -2.4840e-05,  1.4624e-04,\n",
      "        -7.9815e-06,  3.3291e-06,  1.7968e-05,  9.8477e-06, -3.1303e-07,\n",
      "        -2.8116e-05,  6.6773e-05,  1.2037e-05,  5.4747e-05,  2.2660e-05,\n",
      "         2.1064e-05,  4.9415e-06,  3.1675e-05,  4.5777e-05,  1.4757e-05,\n",
      "         9.9328e-06, -7.8815e-06,  5.0105e-05, -1.4279e-05,  3.4026e-08,\n",
      "         1.0041e-05, -1.1511e-05, -5.7931e-06,  5.2221e-06,  6.5202e-05,\n",
      "        -6.2687e-05, -1.4931e-04,  2.0310e-05,  6.2452e-06, -9.3620e-06,\n",
      "         5.5620e-06,  4.8352e-06,  1.9905e-05,  1.2799e-05,  7.7749e-06,\n",
      "         2.7296e-06,  8.5184e-06,  7.4521e-06,  4.4290e-05,  1.4560e-05,\n",
      "         6.6583e-06, -6.6999e-05,  2.4317e-06,  3.0545e-06,  2.2411e-05,\n",
      "        -6.7470e-06,  3.2292e-07, -8.2101e-06,  4.6598e-06, -1.6355e-05,\n",
      "        -8.0190e-06,  5.3036e-06,  3.7166e-05, -2.8129e-06, -1.1172e-05,\n",
      "        -1.3802e-05, -1.1965e-05,  1.3555e-05,  1.3745e-06,  1.3455e-06,\n",
      "        -1.6219e-05, -2.5240e-06,  5.5001e-06,  1.2294e-06, -4.7172e-06,\n",
      "         3.0621e-06,  7.1439e-06, -1.3019e-07,  4.1002e-06,  2.9570e-06,\n",
      "        -2.9004e-05,  2.1628e-05,  9.9233e-07, -2.4089e-06,  3.9168e-08,\n",
      "        -6.8789e-10, -5.8267e-05,  1.4335e-05,  7.8131e-06, -1.9104e-06,\n",
      "        -1.0832e-05, -2.8044e-06, -4.3833e-06,  4.4402e-06, -5.2681e-06,\n",
      "        -3.9789e-05, -9.6697e-05,  1.0288e-05,  1.3933e-04,  7.6548e-05,\n",
      "         3.4191e-05,  1.5441e-05,  7.4975e-05,  4.7905e-06, -1.3410e-04,\n",
      "        -1.0558e-04, -6.4210e-07,  1.3953e-05,  9.6142e-07, -1.3272e-06,\n",
      "        -1.7574e-05, -1.0807e-05, -3.7777e-06, -5.8610e-06,  3.4410e-06,\n",
      "        -1.0407e-05, -1.6462e-06, -3.9407e-05,  2.9133e-06,  1.6282e-05,\n",
      "         1.5529e-05, -4.5867e-07,  1.1432e-04,  5.0706e-06, -1.8438e-05,\n",
      "        -1.5774e-05, -2.7370e-05,  4.4775e-06,  3.4603e-05, -7.6788e-06,\n",
      "         1.7635e-05, -1.7159e-05,  1.9917e-06, -1.6552e-05,  3.9428e-06,\n",
      "         3.6086e-06, -3.2216e-07, -3.3750e-07,  1.3908e-05,  8.9779e-06,\n",
      "         4.7277e-06,  4.7323e-06, -2.5925e-05,  1.9688e-07,  2.3641e-06,\n",
      "        -1.3120e-05, -2.8819e-05,  1.6688e-04,  5.5355e-05,  1.8102e-05,\n",
      "         7.9522e-06,  3.4832e-06,  2.0408e-06,  9.5778e-08, -3.0316e-06,\n",
      "         1.4258e-05,  3.3723e-07,  3.4128e-07, -1.3918e-06, -2.9203e-05,\n",
      "         3.3551e-05,  5.5452e-06, -1.5355e-05, -4.4152e-05, -2.4479e-05,\n",
      "        -1.0909e-06,  7.1157e-06,  8.3909e-06,  1.0291e-05,  1.0607e-06,\n",
      "         1.9220e-06, -2.5614e-06,  2.3113e-06, -3.4433e-07], device='cuda:0'), 'exp_avg_sq': tensor([2.5330e-08, 3.2117e-08, 4.2932e-08, 2.9666e-08, 1.2038e-07, 9.4896e-08,\n",
      "        8.1404e-08, 4.5491e-08, 5.4005e-08, 9.9284e-08, 1.1435e-07, 1.0468e-07,\n",
      "        2.4018e-08, 4.9432e-11, 1.4592e-11, 2.3975e-11, 3.5240e-08, 2.0400e-11,\n",
      "        1.0221e-11, 3.5043e-11, 2.6526e-08, 1.8138e-11, 1.1323e-11, 3.6973e-11,\n",
      "        6.2463e-08, 6.0121e-08, 6.4445e-08, 2.8922e-08, 2.0123e-08, 3.8068e-08,\n",
      "        4.7282e-08, 3.1870e-08, 1.9340e-07, 2.6123e-07, 2.4806e-07, 9.5127e-08,\n",
      "        3.9479e-08, 4.9824e-08, 2.8323e-08, 1.5235e-08, 2.7653e-08, 3.6890e-08,\n",
      "        1.4516e-08, 3.5797e-09, 6.3779e-08, 5.9097e-08, 6.1319e-08, 7.8649e-08,\n",
      "        1.2731e-08, 3.6872e-09, 1.9524e-09, 2.6659e-11, 9.1982e-09, 2.9762e-09,\n",
      "        1.6945e-11, 1.6045e-11, 8.4079e-08, 1.0962e-07, 9.6443e-08, 7.5363e-08,\n",
      "        2.9009e-08, 5.4063e-08, 3.9589e-08, 2.9845e-08, 4.9303e-08, 6.6455e-08,\n",
      "        7.2746e-08, 2.8864e-08, 5.5587e-08, 6.1420e-08, 6.8623e-08, 6.3926e-08,\n",
      "        2.2815e-08, 2.7705e-08, 3.2970e-08, 1.5836e-08, 3.0835e-08, 2.3490e-08,\n",
      "        1.9465e-08, 7.6104e-09, 3.6523e-09, 2.3756e-09, 2.9591e-11, 1.4099e-11,\n",
      "        1.9916e-08, 3.6515e-08, 4.7111e-08, 5.3306e-08, 4.4013e-09, 1.7171e-11,\n",
      "        1.5415e-11, 1.5074e-11, 2.3935e-08, 2.2769e-08, 2.2159e-08, 8.0547e-10,\n",
      "        4.5529e-08, 3.0084e-08, 1.2885e-08, 2.9879e-09, 8.1977e-08, 1.6931e-07,\n",
      "        1.8468e-07, 1.0639e-07, 2.0613e-08, 2.7361e-08, 2.2728e-08, 8.8120e-09,\n",
      "        2.0541e-08, 1.2941e-08, 1.3514e-08, 4.2158e-09, 4.4494e-08, 7.9328e-08,\n",
      "        1.0995e-07, 1.1526e-07, 1.5701e-08, 2.2939e-08, 1.8061e-08, 1.2356e-08,\n",
      "        1.9602e-08, 1.9550e-08, 9.7052e-09, 3.9517e-09, 3.5713e-08, 5.6984e-08,\n",
      "        4.3994e-08, 1.6746e-08, 2.1277e-08, 1.3242e-08, 1.4392e-08, 7.5463e-09,\n",
      "        2.8475e-08, 1.8317e-08, 7.8293e-09, 3.7343e-09, 1.5873e-08, 2.9026e-08,\n",
      "        2.4583e-08, 1.5775e-08, 1.0317e-07, 1.2160e-07, 7.2721e-08, 2.3644e-08,\n",
      "        1.1103e-08, 1.6071e-08, 1.6078e-08, 2.1666e-09, 4.6088e-09, 1.7923e-09,\n",
      "        2.3695e-09, 3.4839e-09, 3.8133e-08, 2.2243e-08, 1.4658e-08, 1.1162e-08,\n",
      "        4.0309e-08, 2.2692e-08, 1.2975e-08, 9.0159e-10, 1.8564e-08, 1.1510e-08,\n",
      "        5.8029e-09, 1.2544e-09, 3.5868e-08, 9.8933e-09, 7.6459e-09, 3.0337e-09,\n",
      "        3.0223e-08, 1.8378e-08, 1.7311e-08, 1.2604e-08, 2.2202e-08, 2.9096e-08,\n",
      "        3.3262e-08, 7.9801e-09, 1.0901e-08, 2.4748e-08, 2.3788e-08, 2.3138e-08,\n",
      "        1.3384e-08, 1.6842e-08, 4.2585e-09, 3.3225e-09, 1.5342e-08, 1.0915e-08,\n",
      "        5.6325e-09, 3.2120e-09, 1.3731e-08, 7.0117e-09, 3.2388e-09, 1.4647e-09,\n",
      "        2.2800e-08, 2.4862e-08, 1.6910e-08, 4.1651e-09, 1.1738e-08, 1.2794e-08,\n",
      "        1.0018e-08, 1.4201e-11, 5.0919e-09, 3.8078e-09, 2.6681e-09, 1.2326e-09,\n",
      "        6.7307e-08, 1.2777e-07, 1.2892e-07, 6.5079e-08, 3.2585e-09, 6.2663e-09,\n",
      "        2.7726e-09, 6.8405e-10, 7.8419e-09, 8.9472e-09, 5.1550e-09, 6.3689e-10,\n",
      "        4.8893e-09, 7.5480e-09, 8.2820e-09, 1.6625e-09, 2.9067e-08, 2.4270e-08,\n",
      "        1.7577e-08, 9.1196e-10, 3.0683e-08, 1.2698e-08, 2.3560e-09, 4.7209e-09,\n",
      "        2.0758e-08, 2.4179e-08, 1.8046e-08, 6.3881e-09, 1.0258e-08, 5.5057e-09,\n",
      "        1.1861e-09, 4.5266e-10, 8.4419e-09, 4.3697e-09, 9.3154e-12, 9.3515e-12,\n",
      "        6.1787e-09, 5.0170e-09, 5.9432e-09, 8.3291e-10, 9.7008e-09, 6.4236e-09,\n",
      "        3.6901e-09, 1.1721e-11, 9.2431e-09, 1.3835e-08, 9.1871e-09, 6.0364e-09,\n",
      "        4.3728e-09, 1.4790e-09, 2.0234e-11, 2.0249e-11, 1.7476e-08, 2.0500e-08,\n",
      "        1.2520e-08, 8.9625e-10, 1.4425e-08, 7.2424e-09, 2.9236e-09, 3.2547e-09,\n",
      "        4.4210e-08, 1.1327e-07, 9.7481e-08, 1.1764e-08, 1.1121e-08, 1.4133e-08,\n",
      "        2.9167e-08, 2.2724e-08, 6.3719e-08, 1.4730e-07, 1.9502e-07, 1.3481e-07,\n",
      "        1.7839e-08, 4.9655e-09, 2.8628e-09, 6.7371e-10, 3.8986e-09, 7.4440e-09,\n",
      "        5.2064e-09, 1.0745e-09, 6.8710e-09, 5.1603e-09, 2.0709e-09, 3.2369e-09,\n",
      "        2.9092e-09, 6.6002e-09, 1.4347e-08, 2.0928e-11, 1.4422e-08, 1.1168e-08,\n",
      "        1.1270e-08, 7.2344e-09, 1.3916e-08, 2.3126e-08, 1.7370e-08, 3.3585e-09,\n",
      "        9.3099e-09, 1.4196e-08, 1.0347e-08, 1.1217e-08, 1.5836e-08, 3.5471e-09,\n",
      "        1.2312e-11, 1.2677e-11, 5.4612e-09, 1.6176e-09, 1.6494e-11, 1.7420e-11,\n",
      "        8.5443e-09, 1.5354e-08, 2.4553e-08, 1.8148e-08, 1.2002e-08, 1.7237e-08,\n",
      "        1.3798e-08, 5.8089e-09, 3.2805e-09, 5.4985e-10, 4.2590e-10, 1.7519e-11,\n",
      "        1.9816e-09, 1.6476e-08, 1.1982e-11, 1.2722e-11, 1.2913e-08, 9.1294e-09,\n",
      "        1.1963e-08, 8.9862e-10, 1.8796e-08, 1.3515e-08, 5.1083e-09, 1.9644e-10,\n",
      "        8.5123e-09, 5.9018e-09, 5.9630e-09, 8.6000e-10, 1.0433e-08, 6.3848e-09,\n",
      "        8.0822e-10, 1.2050e-11], device='cuda:0')}, 18: {'step': 4914, 'exp_avg': tensor([[ 7.8940e-06,  1.4589e-04,  4.0678e-05,  ...,  4.7599e-05,\n",
      "         -9.3521e-06,  2.3302e-05],\n",
      "        [ 8.3903e-06,  1.1414e-04,  3.3188e-05,  ...,  5.9709e-05,\n",
      "         -9.4933e-07,  2.7041e-05],\n",
      "        [ 2.0265e-06,  1.6467e-05, -1.2719e-05,  ...,  1.7186e-05,\n",
      "         -2.1560e-05,  2.7963e-06],\n",
      "        ...,\n",
      "        [ 2.5352e-07, -9.7164e-07, -4.2659e-08,  ..., -3.5615e-08,\n",
      "         -8.0111e-08, -8.1134e-08],\n",
      "        [ 2.5361e-07, -9.5583e-07, -1.9892e-07,  ..., -6.1688e-08,\n",
      "         -8.2973e-08, -6.3949e-08],\n",
      "        [ 2.5511e-07, -9.6956e-07, -1.1856e-07,  ..., -3.4353e-08,\n",
      "         -8.0180e-08, -8.3351e-08]], device='cuda:0'), 'exp_avg_sq': tensor([[1.7562e-08, 2.7134e-07, 1.5153e-08,  ..., 7.5948e-08, 1.2426e-08,\n",
      "         1.1643e-08],\n",
      "        [1.4805e-08, 1.2588e-07, 1.2377e-08,  ..., 4.8367e-08, 2.1668e-08,\n",
      "         9.0009e-09],\n",
      "        [1.0655e-07, 4.9762e-08, 7.6774e-09,  ..., 2.4487e-08, 6.5734e-09,\n",
      "         5.3304e-09],\n",
      "        ...,\n",
      "        [1.4500e-11, 3.3203e-11, 4.7035e-12,  ..., 3.0192e-12, 1.7767e-12,\n",
      "         3.0784e-12],\n",
      "        [1.4284e-11, 3.4942e-11, 3.6170e-10,  ..., 5.4874e-12, 1.7771e-12,\n",
      "         1.6719e-11],\n",
      "        [1.4448e-11, 3.2937e-11, 5.8608e-11,  ..., 3.0141e-12, 1.7780e-12,\n",
      "         2.2704e-12]], device='cuda:0')}, 19: {'step': 4914, 'exp_avg': tensor([ 2.1597e-05,  4.7604e-05, -2.2120e-07,  3.8136e-07,  2.6482e-05,\n",
      "         4.7761e-05,  1.8982e-05,  4.4091e-06,  1.2294e-04,  6.5231e-05,\n",
      "        -1.1299e-05,  5.1595e-06, -2.1348e-06,  5.7032e-05, -7.5840e-06,\n",
      "         1.3002e-05,  2.0472e-05, -8.7587e-05,  5.8424e-06,  2.4598e-05,\n",
      "         8.4366e-06,  1.4049e-05, -8.2518e-05, -1.3188e-05, -1.7365e-05,\n",
      "        -1.4755e-04, -3.4500e-05, -1.1788e-05, -2.9383e-05, -1.0717e-05,\n",
      "        -1.9271e-06, -6.7246e-07,  1.7559e-05, -1.5721e-06, -9.0706e-06,\n",
      "        -9.1418e-06,  1.5877e-05,  1.0240e-06, -9.7985e-07, -9.3754e-07,\n",
      "         4.1152e-05,  3.1889e-05, -1.5532e-06, -4.5885e-06,  1.8772e-05,\n",
      "         4.3407e-05,  2.2646e-05,  8.7329e-06,  4.1967e-06, -7.9816e-06,\n",
      "        -7.2986e-07, -7.5358e-07,  1.1914e-05, -3.8982e-05, -5.7476e-05,\n",
      "        -3.1806e-05, -4.4283e-06, -1.6317e-05,  1.5599e-05,  6.2506e-06,\n",
      "         4.1152e-05, -1.7224e-05, -1.0043e-04,  1.1846e-05,  4.9938e-06,\n",
      "        -7.2634e-06, -5.7323e-06,  8.8051e-07,  1.6602e-05,  7.1693e-07,\n",
      "         6.7476e-07,  7.0594e-07], device='cuda:0'), 'exp_avg_sq': tensor([6.7339e-08, 3.9076e-08, 1.6938e-08, 9.8282e-10, 1.6656e-07, 8.2532e-08,\n",
      "        1.9764e-08, 4.5416e-09, 1.3841e-07, 4.3591e-08, 8.8315e-09, 4.6640e-09,\n",
      "        3.2016e-07, 1.1158e-07, 3.5977e-08, 1.6643e-08, 1.6920e-08, 4.5294e-08,\n",
      "        6.6698e-08, 2.4940e-08, 9.4293e-08, 7.1594e-08, 4.7113e-08, 1.7138e-08,\n",
      "        3.6833e-08, 3.9632e-08, 6.6384e-08, 1.3440e-08, 8.5026e-08, 2.4393e-08,\n",
      "        3.7730e-09, 6.9781e-11, 1.9698e-08, 6.2318e-09, 6.2894e-10, 6.1566e-10,\n",
      "        3.9547e-09, 2.2709e-09, 1.6175e-11, 1.5943e-11, 9.0103e-09, 1.2752e-08,\n",
      "        1.3193e-08, 2.6520e-09, 8.0691e-09, 1.1349e-08, 1.6470e-08, 9.6158e-09,\n",
      "        2.2395e-08, 1.0349e-08, 2.1851e-11, 2.1399e-11, 2.6945e-08, 4.0367e-08,\n",
      "        5.2284e-08, 6.1392e-09, 6.4781e-09, 1.1204e-08, 1.0422e-08, 4.1980e-09,\n",
      "        4.6162e-08, 6.0832e-08, 4.1071e-08, 8.5124e-09, 1.1133e-08, 9.0461e-09,\n",
      "        3.0548e-09, 1.4281e-11, 3.2461e-09, 2.0925e-11, 2.1848e-11, 2.0868e-11],\n",
      "       device='cuda:0')}, 20: {'step': 4914, 'exp_avg': tensor([[-1.0121e-06,  2.6670e-07, -1.3146e-06,  ..., -1.4198e-06,\n",
      "          3.1097e-07, -1.5518e-06],\n",
      "        [-1.5413e-06, -7.7299e-07,  2.5612e-06,  ..., -8.7797e-06,\n",
      "          2.9117e-07,  2.2361e-06],\n",
      "        [-1.8892e-06,  3.5167e-07, -1.9137e-07,  ...,  7.9906e-06,\n",
      "          4.7168e-06, -1.4293e-06],\n",
      "        ...,\n",
      "        [-8.6028e-07,  3.9485e-07,  2.7898e-06,  ..., -1.5621e-06,\n",
      "          5.1289e-07,  4.4715e-08],\n",
      "        [-1.5227e-06,  2.6289e-07,  2.0030e-06,  ...,  4.8516e-07,\n",
      "         -1.2160e-07, -3.1947e-06],\n",
      "        [ 1.8773e-06,  3.2190e-06, -2.4078e-06,  ..., -2.8039e-06,\n",
      "         -5.0984e-07,  2.5697e-06]], device='cuda:0'), 'exp_avg_sq': tensor([[8.1800e-11, 6.9354e-11, 3.9611e-11,  ..., 1.6472e-10, 1.4646e-10,\n",
      "         7.8162e-11],\n",
      "        [1.7028e-10, 2.6119e-10, 3.0701e-10,  ..., 4.0870e-10, 2.1781e-10,\n",
      "         1.5444e-10],\n",
      "        [2.1820e-10, 5.0998e-10, 1.5987e-10,  ..., 3.5188e-10, 1.0947e-10,\n",
      "         1.4918e-10],\n",
      "        ...,\n",
      "        [9.5555e-11, 1.1006e-10, 7.9020e-11,  ..., 7.7868e-11, 1.8022e-10,\n",
      "         1.3344e-10],\n",
      "        [2.3232e-10, 3.7922e-10, 3.6635e-10,  ..., 1.2994e-09, 2.9649e-10,\n",
      "         2.2334e-10],\n",
      "        [9.1665e-11, 2.2560e-10, 6.3753e-11,  ..., 1.1379e-10, 4.9018e-11,\n",
      "         7.2547e-11]], device='cuda:0')}, 21: {'step': 4914, 'exp_avg': tensor([-1.7301e-04, -4.0929e-04,  2.9727e-04,  ...,  5.4341e-05,\n",
      "        -2.3508e-04,  7.3664e-05], device='cuda:0'), 'exp_avg_sq': tensor([3.1028e-07, 4.4444e-07, 6.1658e-07,  ..., 8.0589e-07, 4.2013e-07,\n",
      "        3.4607e-07], device='cuda:0')}, 22: {'step': 4914, 'exp_avg': tensor([[-2.3815e-07, -4.6047e-07,  3.9924e-07,  ..., -2.5104e-07,\n",
      "         -1.7643e-07, -5.5109e-07],\n",
      "        [ 4.1324e-10,  1.6210e-08,  3.2953e-09,  ..., -5.8509e-11,\n",
      "          5.3529e-10, -2.9524e-10],\n",
      "        [-3.6397e-07, -4.2362e-07, -4.5114e-07,  ..., -4.0372e-07,\n",
      "         -1.5535e-07, -1.0690e-06],\n",
      "        ...,\n",
      "        [-4.1064e-05,  3.2553e-06,  3.8709e-05,  ..., -4.9329e-05,\n",
      "          4.6295e-05, -8.4593e-06],\n",
      "        [-1.7098e-07,  3.1541e-08,  4.0511e-07,  ...,  2.7654e-07,\n",
      "          6.1295e-07,  1.7884e-08],\n",
      "        [-8.4908e-07, -2.1036e-07, -2.0885e-07,  ..., -2.4968e-07,\n",
      "         -5.0963e-07,  3.4548e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[1.9511e-10, 1.6245e-10, 1.4436e-10,  ..., 1.1451e-10, 1.2081e-10,\n",
      "         3.5574e-10],\n",
      "        [7.2534e-11, 3.6914e-10, 1.4755e-10,  ..., 3.0923e-11, 3.0746e-10,\n",
      "         6.3819e-11],\n",
      "        [1.0939e-09, 8.5501e-10, 9.5164e-10,  ..., 3.7329e-10, 2.2675e-09,\n",
      "         1.2320e-09],\n",
      "        ...,\n",
      "        [3.6288e-08, 5.5801e-08, 2.9007e-08,  ..., 3.8693e-08, 4.7244e-08,\n",
      "         3.7732e-08],\n",
      "        [2.4211e-10, 2.9859e-10, 2.1672e-10,  ..., 4.0572e-10, 5.8919e-10,\n",
      "         2.0221e-10],\n",
      "        [4.8753e-10, 3.8462e-11, 5.2317e-10,  ..., 1.4758e-10, 6.1573e-10,\n",
      "         2.9753e-10]], device='cuda:0')}, 23: {'step': 4914, 'exp_avg': tensor([ 8.3419e-07,  6.3140e-07,  2.3408e-06,  ..., -6.5607e-05,\n",
      "        -3.1937e-07,  1.3931e-06], device='cuda:0'), 'exp_avg_sq': tensor([2.5776e-09, 1.8116e-09, 9.5412e-09,  ..., 2.3090e-07, 1.5991e-09,\n",
      "        4.7067e-09], device='cuda:0')}, 24: {'step': 4914, 'exp_avg': tensor([[ 1.1264e-07,  7.1522e-09, -3.6658e-08,  ..., -4.2408e-07,\n",
      "         -7.2211e-08, -2.1898e-07],\n",
      "        [ 9.0005e-08,  1.0476e-06,  1.8651e-07,  ..., -5.0997e-07,\n",
      "         -8.3887e-08,  1.9505e-07],\n",
      "        [ 8.0472e-10, -2.1621e-09, -6.0111e-09,  ..., -4.8864e-09,\n",
      "         -1.0063e-08,  7.8207e-09],\n",
      "        ...,\n",
      "        [ 3.4218e-09, -1.1106e-08, -1.3752e-08,  ...,  4.2614e-09,\n",
      "         -1.5713e-07, -8.8320e-09],\n",
      "        [-1.4780e-06, -5.1936e-06, -1.8754e-06,  ..., -4.0653e-06,\n",
      "         -5.7936e-07,  6.7178e-07],\n",
      "        [-3.9308e-07, -9.4802e-08, -1.6749e-07,  ..., -3.2866e-07,\n",
      "         -1.0996e-07,  1.9326e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[5.4653e-11, 2.8821e-11, 3.0582e-11,  ..., 1.2781e-10, 3.5178e-10,\n",
      "         4.1654e-11],\n",
      "        [4.0646e-10, 3.0122e-10, 6.1604e-10,  ..., 1.3617e-09, 9.0791e-10,\n",
      "         2.3046e-10],\n",
      "        [1.1262e-10, 1.2358e-11, 3.8365e-11,  ..., 4.9187e-11, 2.5811e-11,\n",
      "         1.5149e-10],\n",
      "        ...,\n",
      "        [1.1832e-10, 1.6434e-10, 4.5465e-10,  ..., 8.1365e-11, 5.1832e-12,\n",
      "         3.8846e-10],\n",
      "        [1.7970e-09, 1.6984e-09, 1.0839e-09,  ..., 4.5975e-09, 2.8484e-09,\n",
      "         1.0725e-09],\n",
      "        [5.7197e-10, 1.4763e-10, 4.2893e-10,  ..., 5.3805e-10, 1.9986e-10,\n",
      "         4.5857e-10]], device='cuda:0')}, 25: {'step': 4914, 'exp_avg': tensor([-3.6579e-08,  2.2862e-06, -5.4749e-08,  ...,  1.4515e-06,\n",
      "         1.1581e-05, -2.0610e-07], device='cuda:0'), 'exp_avg_sq': tensor([5.7827e-10, 5.5995e-09, 2.4122e-10,  ..., 2.4910e-09, 9.1874e-09,\n",
      "        1.1757e-09], device='cuda:0')}, 26: {'step': 4914, 'exp_avg': tensor([[ 4.1966e-09, -7.1016e-09, -1.1323e-07,  ..., -3.6278e-08,\n",
      "         -1.4961e-09, -1.9359e-08],\n",
      "        [-2.3164e-07,  1.4964e-07, -9.4649e-08,  ..., -5.4453e-07,\n",
      "         -3.1599e-07,  1.0713e-07],\n",
      "        [-6.2654e-07, -3.2095e-07,  1.6363e-06,  ..., -1.8276e-08,\n",
      "          7.7343e-08,  2.4988e-07],\n",
      "        ...,\n",
      "        [-2.7116e-13,  2.4855e-13, -6.6777e-09,  ...,  4.2263e-10,\n",
      "         -9.3739e-10, -1.5977e-09],\n",
      "        [-4.2406e-07,  4.5786e-07, -2.2518e-07,  ..., -2.4618e-07,\n",
      "         -4.5134e-07, -8.3529e-07],\n",
      "        [ 1.2689e-09, -2.7733e-08, -1.9544e-10,  ..., -2.4861e-08,\n",
      "         -4.4461e-09, -1.5317e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[4.0485e-10, 1.6726e-10, 1.0055e-10,  ..., 3.4648e-10, 1.3398e-10,\n",
      "         1.2607e-10],\n",
      "        [2.0652e-10, 1.4059e-10, 1.3374e-10,  ..., 1.1243e-09, 3.0126e-10,\n",
      "         2.2294e-10],\n",
      "        [6.0735e-10, 5.4369e-10, 2.6846e-09,  ..., 1.0257e-09, 9.5174e-10,\n",
      "         5.8671e-10],\n",
      "        ...,\n",
      "        [1.3862e-11, 1.5123e-11, 6.3594e-11,  ..., 9.4354e-11, 1.1716e-10,\n",
      "         6.9488e-11],\n",
      "        [5.5079e-10, 2.3438e-10, 1.2853e-10,  ..., 6.7911e-10, 8.6633e-10,\n",
      "         3.5957e-10],\n",
      "        [3.0779e-11, 1.6534e-10, 4.9375e-12,  ..., 1.0524e-10, 2.4717e-11,\n",
      "         7.3437e-10]], device='cuda:0')}, 27: {'step': 4914, 'exp_avg': tensor([ 5.8861e-08, -4.0963e-07,  4.9601e-07,  ..., -3.1528e-08,\n",
      "        -6.4020e-07, -7.3420e-08], device='cuda:0'), 'exp_avg_sq': tensor([8.7276e-10, 5.1715e-10, 5.2070e-09,  ..., 1.1510e-10, 1.6243e-09,\n",
      "        1.6818e-10], device='cuda:0')}, 28: {'step': 4914, 'exp_avg': tensor([[-1.8316e-07, -2.0465e-05,  3.9337e-08,  ..., -5.7767e-07,\n",
      "         -1.6765e-06,  8.0750e-08],\n",
      "        [-5.6159e-05,  1.0256e-06, -4.5449e-05,  ..., -4.6570e-05,\n",
      "         -2.3150e-04,  1.2170e-05],\n",
      "        [-6.1012e-08, -3.8628e-08,  2.4104e-08,  ..., -5.1561e-08,\n",
      "         -6.1496e-08, -7.5554e-10],\n",
      "        ...,\n",
      "        [ 9.3204e-06, -7.4015e-06, -7.3737e-07,  ..., -3.6765e-06,\n",
      "         -5.2912e-06,  5.7375e-07],\n",
      "        [ 8.4844e-13,  3.5549e-12, -2.9129e-11,  ...,  8.5375e-12,\n",
      "         -1.2703e-12, -2.2166e-09],\n",
      "        [-1.7677e-07,  1.4117e-07, -4.2719e-07,  ..., -6.2591e-07,\n",
      "         -5.2885e-07,  1.9156e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[4.9278e-10, 6.8259e-10, 3.0552e-10,  ..., 2.7081e-09, 4.9650e-10,\n",
      "         1.8488e-10],\n",
      "        [2.3553e-08, 5.6349e-08, 1.9731e-08,  ..., 8.5600e-08, 2.6399e-08,\n",
      "         2.0389e-08],\n",
      "        [2.0983e-10, 1.1466e-10, 2.2981e-10,  ..., 3.9157e-10, 1.6312e-10,\n",
      "         1.5908e-10],\n",
      "        ...,\n",
      "        [2.0759e-09, 1.4044e-09, 1.4747e-09,  ..., 1.9618e-09, 8.8111e-10,\n",
      "         9.1274e-10],\n",
      "        [2.2390e-11, 1.5905e-11, 3.1734e-11,  ..., 4.0785e-11, 5.6497e-12,\n",
      "         6.1783e-11],\n",
      "        [3.4450e-10, 5.1815e-10, 4.7426e-10,  ..., 1.3921e-09, 4.1916e-10,\n",
      "         1.8746e-10]], device='cuda:0')}, 29: {'step': 4914, 'exp_avg': tensor([-3.4183e-05, -5.3862e-05, -5.0996e-07,  ..., -9.5801e-06,\n",
      "        -2.9578e-08, -8.5569e-07], device='cuda:0'), 'exp_avg_sq': tensor([1.4941e-09, 1.2860e-07, 7.0842e-10,  ..., 5.9188e-09, 7.0686e-11,\n",
      "        1.5260e-09], device='cuda:0')}, 30: {'step': 4914, 'exp_avg': tensor([[-5.6414e-09, -1.4085e-07, -1.7995e-09,  ..., -4.8298e-09,\n",
      "         -3.9956e-10, -2.0653e-09],\n",
      "        [-1.1286e-04, -6.9609e-05, -2.4835e-05,  ...,  2.1549e-06,\n",
      "          2.2529e-05, -2.8202e-06],\n",
      "        [-1.6137e-08,  1.2456e-07, -7.0963e-08,  ...,  3.0576e-08,\n",
      "         -3.8674e-08, -3.5817e-08],\n",
      "        ...,\n",
      "        [-3.6521e-07,  2.9731e-05, -6.3474e-07,  ..., -9.3449e-07,\n",
      "          2.1780e-07, -4.5529e-08],\n",
      "        [-2.6707e-08, -6.5976e-10, -2.1060e-08,  ..., -3.7261e-08,\n",
      "         -1.6945e-09,  1.0018e-08],\n",
      "        [ 1.7374e-07,  5.3805e-08, -1.0714e-07,  ..., -2.7208e-07,\n",
      "         -4.1793e-08,  2.5714e-07]], device='cuda:0'), 'exp_avg_sq': tensor([[2.6525e-10, 1.5570e-10, 1.8492e-11,  ..., 2.6535e-10, 5.2240e-11,\n",
      "         2.1068e-10],\n",
      "        [2.5241e-08, 2.4861e-07, 1.7244e-08,  ..., 8.5348e-08, 2.3238e-08,\n",
      "         2.0626e-08],\n",
      "        [1.3060e-10, 4.8749e-10, 1.3202e-10,  ..., 1.2458e-10, 8.8537e-11,\n",
      "         9.7773e-11],\n",
      "        ...,\n",
      "        [4.9481e-10, 4.2305e-09, 4.4520e-10,  ..., 2.3555e-09, 4.2064e-10,\n",
      "         6.2355e-10],\n",
      "        [6.8158e-10, 3.6074e-11, 3.7990e-10,  ..., 3.7853e-10, 3.5502e-11,\n",
      "         1.1687e-10],\n",
      "        [6.9585e-10, 4.0590e-10, 2.5923e-10,  ..., 2.1800e-10, 3.5913e-10,\n",
      "         5.4501e-10]], device='cuda:0')}, 31: {'step': 4914, 'exp_avg': tensor([-3.5034e-08, -1.9009e-05, -3.5647e-07,  ..., -7.5098e-07,\n",
      "        -2.5900e-08, -4.4684e-07], device='cuda:0'), 'exp_avg_sq': tensor([4.1695e-11, 1.1220e-07, 6.6243e-10,  ..., 2.7861e-09, 9.9743e-11,\n",
      "        4.9582e-10], device='cuda:0')}, 32: {'step': 4914, 'exp_avg': tensor([[ 3.6001e-07, -1.6313e-07,  5.1910e-08,  ...,  1.6634e-07,\n",
      "          2.8196e-07, -2.2958e-07],\n",
      "        [-4.6594e-08, -5.1271e-08,  5.6052e-45,  ..., -5.4441e-14,\n",
      "         -7.1982e-15,  1.1698e-11],\n",
      "        [ 2.2948e-06, -2.5553e-06,  2.8106e-06,  ..., -6.2727e-06,\n",
      "         -3.9738e-06, -7.2049e-07],\n",
      "        ...,\n",
      "        [ 7.5202e-08, -5.4803e-07,  3.3602e-08,  ..., -2.1446e-06,\n",
      "         -1.6487e-07, -5.3381e-07],\n",
      "        [ 5.4089e-07, -2.3345e-06,  4.0076e-07,  ...,  1.3638e-06,\n",
      "         -4.4536e-07,  3.1880e-07],\n",
      "        [-3.3968e-07,  6.9643e-07,  7.2450e-08,  ...,  2.2026e-07,\n",
      "         -2.2516e-08, -3.5254e-08]], device='cuda:0'), 'exp_avg_sq': tensor([[5.6011e-10, 1.2170e-09, 1.0928e-10,  ..., 5.3810e-10, 3.9662e-10,\n",
      "         2.5666e-10],\n",
      "        [1.1915e-09, 2.3783e-10, 5.0310e-13,  ..., 1.3590e-11, 1.6575e-11,\n",
      "         3.5774e-11],\n",
      "        [2.1267e-09, 5.6252e-09, 4.2049e-09,  ..., 2.8105e-09, 2.7147e-09,\n",
      "         1.1283e-09],\n",
      "        ...,\n",
      "        [2.9247e-10, 2.3608e-09, 1.4964e-10,  ..., 2.0637e-09, 2.3454e-10,\n",
      "         5.7562e-10],\n",
      "        [1.0316e-09, 2.7480e-09, 1.7166e-10,  ..., 2.6400e-09, 3.8501e-10,\n",
      "         2.4395e-10],\n",
      "        [2.6651e-10, 1.1956e-09, 1.2215e-10,  ..., 4.1119e-10, 9.0766e-11,\n",
      "         2.1203e-10]], device='cuda:0')}, 33: {'step': 4914, 'exp_avg': tensor([-5.0145e-07, -1.5489e-08, -9.1306e-06,  ..., -7.0109e-06,\n",
      "        -8.6318e-07, -6.9866e-07], device='cuda:0'), 'exp_avg_sq': tensor([5.0916e-10, 9.9309e-11, 7.5322e-09,  ..., 1.4996e-09, 2.3538e-09,\n",
      "        7.6118e-10], device='cuda:0')}}, 'param_groups': [{'lr': 0.00025, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}, {'lr': 0.00025, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'params': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]}]}\n",
      " key: alphas  values: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]}\n"
     ]
    }
   ],
   "source": [
    "for k, v in environ.optimizers.items():\n",
    "    print(f' key: {k}  values: {v}')\n",
    "    print(v.state_dict())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b025642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T13:00:30.150214Z",
     "start_time": "2022-09-06T13:00:30.113581Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " key: alphas  values: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a7d609d30>\n",
      "{'factor': 0.5, 'min_lrs': [0], 'patience': 20, 'verbose': True, 'cooldown': 5, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.0001, 'threshold_mode': 'rel', 'best': inf, 'num_bad_epochs': 0, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 0}\n",
      " key: weights  values: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a7d609d00>\n",
      "{'factor': 0.5, 'min_lrs': [0, 0], 'patience': 20, 'verbose': True, 'cooldown': 5, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.0001, 'threshold_mode': 'rel', 'best': 2.1153364644531476, 'num_bad_epochs': 7, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 78, '_last_lr': [0.00025, 0.00025]}\n"
     ]
    }
   ],
   "source": [
    "for k, v in environ.schedulers.items():\n",
    "    print(f' key: {k}  values: {v}')\n",
    "    print(v.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862099d",
   "metadata": {},
   "source": [
    "### End WandB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "952bb1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T13:01:00.780709Z",
     "start_time": "2022-09-06T13:00:56.055480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>â–â–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>avg_prec_score</td><td>â–â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>bceloss</td><td>â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒ</td></tr><tr><td>best_accuracy</td><td>â–â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>best_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ˆâ–ˆ</td></tr><tr><td>best_iter</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ˆâ–ˆ</td></tr><tr><td>best_roc_auc</td><td>â–â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>f1_max</td><td>â–â–„â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>gumbel_temp</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>kappa</td><td>â–â–ƒâ–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>kappa_max</td><td>â–â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>lambda_sharing</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>lambda_sparsity</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>lambda_tasks</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>logloss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒ</td></tr><tr><td>lr_0</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–</td></tr><tr><td>lr_1</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–</td></tr><tr><td>p_f1_max</td><td>â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–</td></tr><tr><td>p_kappa_max</td><td>â–‚â–â–â–ƒâ–ƒâ–ƒâ–„â–„â–‚â–‡â–ƒâ–…â–‚â–…â–…â–…â–…â–„â–†â–…â–ˆâ–…â–‡â–†â–‡â–†â–‡â–…â–…â–…â–…â–‡â–ƒâ–‡â–†â–†â–ˆâ–…â–‡â–…</td></tr><tr><td>policy_lr</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>roc_auc_score</td><td>â–â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>sc_loss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒ</td></tr><tr><td>train_layers</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.65659</td></tr><tr><td>avg_prec_score</td><td>0.67015</td></tr><tr><td>bceloss</td><td>0.47885</td></tr><tr><td>best_roc_auc</td><td>0.77184</td></tr><tr><td>epoch</td><td>78</td></tr><tr><td>f1_max</td><td>0.69901</td></tr><tr><td>gumbel_temp</td><td>2.5</td></tr><tr><td>kappa</td><td>0.28173</td></tr><tr><td>kappa_max</td><td>0.46955</td></tr><tr><td>lambda_sharing</td><td>0.05</td></tr><tr><td>lambda_sparsity</td><td>0.005</td></tr><tr><td>lambda_tasks</td><td>1.0</td></tr><tr><td>logloss</td><td>0.0</td></tr><tr><td>lr_0</td><td>0.00025</td></tr><tr><td>lr_1</td><td>0.00025</td></tr><tr><td>p_f1_max</td><td>0.35493</td></tr><tr><td>p_kappa_max</td><td>0.46961</td></tr><tr><td>policy_lr</td><td>0.01</td></tr><tr><td>roc_auc_score</td><td>0.76834</td></tr><tr><td>sc_loss</td><td>0.10243</td></tr><tr><td>train_layers</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">0906_1312</strong>: <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/3br7gsg1\" target=\"_blank\">https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/3br7gsg1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220906_131259-3br7gsg1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns.wandb_run.finish()\n",
    "# ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c03a5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Some data peeks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a7fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T16:43:04.282818Z",
     "start_time": "2022-08-16T16:43:04.246814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics['sparsity']['total'])\n",
    "pp.pprint(environ.val_metrics['sharing']['total'])\n",
    "pp.pprint(environ.val_metrics['sharing']['total'] +environ.val_metrics['sparsity']['total'])\n",
    "pp.pprint(environ.val_metrics['task'])\n",
    "pp.pprint(environ.val_metrics['total'])\n",
    "pp.pprint(environ.val_metrics['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbb96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T07:56:30.866019Z",
     "start_time": "2022-08-16T07:56:30.793987Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils             import censored_mse_loss, censored_mae_loss, aggregate_results\n",
    "task_key = 'task2'\n",
    "print(environ.val_data[task_key]['yc_aggr_weights'].sum())\n",
    "print(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "print(environ.val_metrics[task_key]['classification'])\n",
    "# print(environ.val_metrics[task_key]['classification'].sum())\n",
    "print(environ.val_metrics[task_key]['classification_agg'])\n",
    "# print(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "# print((environ.batch_data[task_key]['yc_aggr_weights']==environ.val_data[task_key]['yc_aggr_weights']).all())\n",
    "\n",
    "\n",
    "tmp = aggregate_results(environ.val_metrics[task_key][\"classification\"], \n",
    "                      environ.val_data[task_key]['yc_aggr_weights'],\n",
    "                      verbose = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7b40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T07:17:18.629009Z",
     "start_time": "2022-08-16T07:17:18.396695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del all_tgs, all_tgs2\n",
    "del con,con2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52214fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T07:20:29.718039Z",
     "start_time": "2022-08-16T07:20:29.611094Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# del con\n",
    "ttl = 0\n",
    "\n",
    "# con = np.ndarray()\n",
    "appd_df = []\n",
    "for i in range(1,11):\n",
    "    task_key = f\"task{i}\"\n",
    "    print(i, task_key, ' shape: ', environ.val_data[task_key]['yc_aggr_weights'].shape,  'classifiaction:', environ.val_metrics[task_key]['classification'].shape)\n",
    "    tmp_df = environ.val_metrics[task_key]['classification'].where(pd.isnull,1)\n",
    "    print(tmp_df.sum(axis=0))\n",
    "    \n",
    "    if i == 1:\n",
    "        con = np.copy(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "        all_tgs = environ.val_metrics[task_key]['classification'].copy()\n",
    "        print(\"initialize\", con.shape, all_tgs.shape)\n",
    "    else:\n",
    "        con = np.hstack((con, environ.val_data[task_key]['yc_aggr_weights']))\n",
    "        all_tgs = all_tgs.append(environ.val_metrics[task_key]['classification'])\n",
    "        print(\"concatenate: \",task_key, \"    \", con.shape, all_tgs.shape)\n",
    "        \n",
    "    ttl += environ.val_data[task_key]['yc_aggr_weights'].shape[0]\n",
    "    \n",
    "print('ttl : ', ttl,  'con.shape:', con.shape, 'all_tgs.shape', all_tgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22870bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T07:17:56.700Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs2 = pd.concat(environ.val_metrics[f\"task{i}\"]['classification'] for i in range(1,11))\n",
    "\n",
    "all_tgs2.info()\n",
    "all_tgs2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb4137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:27.145197Z",
     "start_time": "2022-08-02T08:53:27.076862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "con2 = np.hstack([ environ.val_data[f\"task{i}\"]['yc_aggr_weights'] for i in range(1,11)])\n",
    "con2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbee89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:51:56.737396Z",
     "start_time": "2022-08-02T08:51:56.667161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_tgs.index = range(all_tgs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8334f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:52:05.606811Z",
     "start_time": "2022-08-02T08:52:05.540830Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(all_tgs2[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56e5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:38.557041Z",
     "start_time": "2022-08-02T08:53:38.479724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs2_mod = all_tgs2.where(pd.isnull, 1) * con2[:,None]\n",
    "all_tgs2_mod.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba25bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:40:54.050290Z",
     "start_time": "2022-08-02T08:40:53.982441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# con3 = pd.concat([environ.val_metrics['task1']['classification'],environ.val_metrics['task2']['classification'] ])\n",
    "# print(con3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cc76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:46.438201Z",
     "start_time": "2022-08-02T08:53:46.318451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp2 = aggregate_results(all_tgs2, con2, verbose = True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:20:37.470235Z",
     "start_time": "2022-08-02T09:20:37.391995Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics['aggregated'])\n",
    "print(environ.val_metrics['aggregated']['sc_loss'] )\n",
    "print(environ.val_metrics['aggregated'][\"logloss\"] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f21fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:02:49.573973Z",
     "start_time": "2022-08-02T09:02:49.497930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(tmp2)\n",
    "pp.pprint(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c58a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0711df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:53:56.393951Z",
     "start_time": "2022-08-02T08:53:56.231370Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tasks_classification_metrics = []\n",
    "all_tasks_aggregation_weights    = [] \n",
    "\n",
    "for i in range(1,11):\n",
    "    task_key = f\"task{i}\"\n",
    "    print(i, task_key, ' shape: ', environ.val_data[task_key]['yc_aggr_weights'].shape,  'classifiaction:', environ.val_metrics[task_key]['classification'].shape)\n",
    "    tmp_df = environ.val_metrics[task_key]['classification'].where(pd.isnull,1)\n",
    "    print(tmp_df.sum(axis=0))\n",
    "    \n",
    "    all_tasks_classification_metrics.append(environ.val_metrics[task_key]['classification'])\n",
    "    all_tasks_aggregation_weights.append(environ.val_data[task_key]['yc_aggr_weights'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9616c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:50:55.310369Z",
     "start_time": "2022-08-02T08:50:55.205109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs3 = pd.concat(all_tasks_classification_metrics)\n",
    "con3 = np.concatenate(all_tasks_aggregation_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdf412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:54:37.322219Z",
     "start_time": "2022-08-02T08:54:37.193928Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_tgs3.info()\n",
    "all_tgs3.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29ee1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T09:02:16.745152Z",
     "start_time": "2022-08-02T09:02:16.608823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp3 = aggregate_results( all_tgs3, con3, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c12369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T08:59:51.408133Z",
     "start_time": "2022-08-02T08:59:51.294153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(all_tgs2[0:1]['roc_auc_score'])\n",
    "print(all_tgs3[0:1]['roc_auc_score'])\n",
    "print((all_tgs2[0:1]['roc_auc_score'] == all_tgs3[0:1]['roc_auc_score']).all())\n",
    "all_tgs2.compare(all_tgs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399eae39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T15:18:30.544168Z",
     "start_time": "2022-08-01T15:18:30.511229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.val_data['task9']['yc_aggr_weights'].shape, con[3152:3496].shape)\n",
    "print((environ.val_data['task9']['yc_aggr_weights'] == con[3152:3496]).all())a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8183a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:53:09.749812Z",
     "start_time": "2022-08-01T14:53:09.749793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76406f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T13:56:23.922805Z",
     "start_time": "2022-07-08T13:56:23.891800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60564cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:50:02.608053Z",
     "start_time": "2022-07-07T13:50:02.553468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a8071",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9c724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:20:10.987625Z",
     "start_time": "2022-07-28T11:20:10.957009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best Epoch :       {ns.best_epoch}\\n\"\n",
    "      f\"Best Iteration :   {ns.best_iter} \\n\"\n",
    "      f\"Best ROC AUC   :   {ns.best_roc_auc:.5f}\\n\"\n",
    "      f\"Best Precision :   {ns.best_accuracy:.5f}\\n\")\n",
    "print()\n",
    "for key in environ.val_metrics['aggregated']:\n",
    "    print(f\"{key:20s}    {environ.val_metrics['aggregated'][key]:0.4f}\")\n",
    "# pp.pprint(environ.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48528a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb49645c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Weight & Policy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aade16",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Weight/Policy Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f68f2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:30:50.804853Z",
     "start_time": "2022-08-30T19:30:50.769079Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Backbone Initial LR            :      0.001000      Current LR : 0.000125 \n",
      " Tasks    Initial LR            :      0.001000      Current LR : 0.000125    \n",
      " Policy   Initial LR            :      0.010000      Current LR : 0.01  \n",
      "\n",
      " Backbone (Group 0) Initial LR  : 0.001000 \n",
      " Tasks    (Group 1) Initial LR  : 0.001000    \n",
      " Params : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.000125\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.000125\n",
      "    weight_decay: 0.0001\n",
      ") \n",
      "\n",
      " Policy   Initial LR            : 0.010000  \n",
      " Params : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")  \n",
      "\n",
      "\n",
      " Backbone Initial LR            : 0.001000      Current LR : 0.000125 \n",
      " Tasks    Initial LR            : 0.001000      Current LR : 0.000125    \n",
      " Policy   Initial LR            : 0.010000      Current LR : 0.01  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\" Backbone Initial LR            :      {environ.opt['train']['backbone_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            :      {environ.opt['train']['task_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[1]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            :      {environ.opt['train']['policy_lr']:4f}      Current LR : {environ.optimizers['alphas'].param_groups[0]['lr']}  \\n\")\n",
    "\n",
    "print( f\" Backbone (Group 0) Initial LR  : {environ.opt['train']['backbone_lr']:4f} \\n\"\n",
    "       f\" Tasks    (Group 1) Initial LR  : {environ.opt['train']['task_lr']:4f}    \\n Params : {environ.optimizers['weights']} \\n\\n\"\n",
    "       f\" Policy   Initial LR            : {environ.opt['train']['policy_lr']:4f}  \\n Params : {environ.optimizers['alphas']}  \\n\\n\")\n",
    "\n",
    "print( f\" Backbone Initial LR            : {environ.opt['train']['backbone_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            : {environ.opt['train']['task_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[1]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            : {environ.opt['train']['policy_lr']:4f}      Current LR : {environ.optimizers['alphas'].param_groups[0]['lr']}  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ccc02c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:31:00.823065Z",
     "start_time": "2022-08-30T19:31:00.782059Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights Scheduler Parameters\n",
      "------------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0, 0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 0\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.0944523516289357\n",
      "    num_bad_epochs           value: 1\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 150\n",
      "    _last_lr                 value: [0.000125, 0.000125]\n",
      "\n",
      "Policy Scheduler Parameters\n",
      "-----------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 0\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: inf\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 0\n"
     ]
    }
   ],
   "source": [
    "print_underline('Weights Scheduler Parameters', verbose = True) \n",
    "for k,i in environ.schedulers['weights'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")\n",
    "\n",
    "print_underline('Policy Scheduler Parameters', verbose = True)\n",
    "for k,i in environ.schedulers['alphas'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b284fe5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:33:47.686028Z",
     "start_time": "2022-08-30T19:33:47.613954Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model optimizers defined . . . policy_learning: True\n",
      " Model schedulers defined . . . policy_learning: True\n",
      " Metrics CSV file header written . . . \n",
      " Model initializations complete . . . \n",
      " training preparation: - check for CUDA - cuda available as device id: [0]\n",
      "sparsechem_env.cuda()\n",
      " policy policy1 is None\n",
      " policy policy2 is None\n",
      " policy policy3 is None\n",
      " policy policy4 is None\n",
      " policy policy5 is None\n",
      " policy policy6 is None\n",
      " policy policy7 is None\n",
      " policy policy8 is None\n",
      " policy policy9 is None\n",
      " policy policy10 is None\n",
      " training preparation: - set print_freq to                                 : 1989 \n",
      " training preparation: - set number of batches per warmup training epoch to: 1989\n",
      " training preparation: - set number of batches per weight training epoch to: 1318\n",
      " training preparation: - set number of batches per policy training epoch to: 671\n",
      " training preparation: - set number of batches per validation to           : 675\n",
      " training preparation complete . . .\n"
     ]
    }
   ],
   "source": [
    "ns.flag = 'update_weights'\n",
    "model_initializations(ns, opt, environ, phase = ns.flag, policy_learning = True)\n",
    "training_initializations(ns, opt, environ, dldrs, warmup = False)\n",
    "\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 200,  weight_iterations = 2, policy_iterations = 2, eval_iterations = 1, warmup = False)\n",
    "# training_initializations(ns, opt, environ, dldrs, warmup_iterations = 1000, weight_iterations = 750, policy_iterations = 250, eval_iterations = 500, warmup = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed1774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-28T07:45:13.453282Z",
     "start_time": "2022-08-28T07:45:13.421740Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90b1ace9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T19:33:51.358513Z",
     "start_time": "2022-08-30T19:33:51.321523Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "** 2022-08-30 21:33:51:354371 \n",
      "** Training epoch: 150 iter: 298350   flag: update_weights \n",
      "** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\n",
      "** Switch from Warm Up training to Alternate training Weights & Policy \n",
      "** Take checkpoint and block gradient flow through Policy net\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_heading( f\"** {timestring()} \\n\"\n",
    "               f\"** Training epoch: {ns.current_epoch} iter: {ns.current_iter}   flag: {ns.flag} \\n\"\n",
    "               f\"** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\\n\"\n",
    "               f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "               f\"** Take checkpoint and block gradient flow through Policy net\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66affd0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:37.495267Z",
     "start_time": "2022-09-02T10:04:37.462324Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "0.05\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(environ.opt['train']['lambda_sparsity'])\n",
    "print(environ.opt['train']['lambda_sharing'])\n",
    "print(environ.opt['train']['decay_temp_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "653fc4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:54.192514Z",
     "start_time": "2022-09-02T10:04:54.158926Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.opt['is_curriculum'] = True\n",
    "# environ.opt['curriculum_speed'] = 4\n",
    "# ns.num_train_layers = None\n",
    "ns.training_epochs = 10\n",
    "\n",
    "environ.opt['train']['lambda_sparsity'] = 0.01\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['decay_temp_freq'] = 6\n",
    "\n",
    "# print(environ.opt['train']['lambda_sparsity'])\n",
    "# print(environ.opt['train']['lambda_sharing'])\n",
    "# print(environ.opt['train']['decay_temp_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef3a11fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:55.839318Z",
     "start_time": "2022-09-02T10:04:55.800877Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ns.flag                        :      update_weights\n",
      " num_train_layers               :      6\n",
      " environ.opt['is_curriculum']   :      False\n",
      " environ.opt['curriculum_speed']:      3\n",
      "\n",
      " Backbone Initial LR            :      0.001000      Current LR : 0.00025 \n",
      " Tasks    Initial LR            :      0.001000      Current LR : 0.00025    \n",
      " Policy   Initial LR            :      0.010000      Current LR : 0.0025  \n",
      "\n",
      " Hard Sampling                  :      False\n",
      "\n",
      " Sparsity regularization        :      0.01\n",
      " Sharing  regularization        :      0.05 \n",
      " Tasks    regularization        :      1.0   \n",
      "\n",
      "\n",
      " Gumbel Temp                    :      0.0004         \n",
      " Gumbel Temp decay              :      3 \n",
      "\n",
      " ns.current_epoch               :      240\n",
      " ns.training_epochs             :      10 \n",
      "\n",
      " ns.current_iters               :      477360\n",
      " Batches in warmup epoch        :      1989\n",
      " Batches in weight epoch        :      1318\n",
      " Batches in policy epoch        :      671\n",
      " Batches in validation          :      675\n",
      " num_train_layers               :      6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\" ns.flag                        :      {ns.flag}\")\n",
    "print( f\" num_train_layers               :      {ns.num_train_layers}\")\n",
    "print( f\" environ.opt['is_curriculum']   :      {environ.opt['is_curriculum']}\")\n",
    "print( f\" environ.opt['curriculum_speed']:      {environ.opt['curriculum_speed']}\\n\")\n",
    "print( f\" Backbone Initial LR            :      {environ.opt['train']['backbone_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[0]['lr']} \\n\"\n",
    "       f\" Tasks    Initial LR            :      {environ.opt['train']['task_lr']:4f}      Current LR : {environ.optimizers['weights'].param_groups[1]['lr']}    \\n\"\n",
    "       f\" Policy   Initial LR            :      {environ.opt['train']['policy_lr']:4f}      Current LR : {environ.optimizers['alphas'].param_groups[0]['lr']}  \\n\")\n",
    "\n",
    "print( f\" Hard Sampling                  :      {environ.opt['train']['hard_sampling']}\\n\")\n",
    "\n",
    "print( f\" Sparsity regularization        :      {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization        :      {environ.opt['train']['lambda_sharing']} \\n\"\n",
    "       f\" Tasks    regularization        :      {environ.opt['train']['lambda_tasks']}   \\n\\n\")\n",
    "\n",
    "print( f\" Gumbel Temp                    :      {environ.gumbel_temperature:.4f}         \\n\" #\n",
    "       f\" Gumbel Temp decay frequency    :      {environ.opt['train']['decay_temp_freq']} \\n\") #\n",
    "\n",
    "print( f\" ns.current_epoch               :      {ns.current_epoch}\")\n",
    "print( f\" ns.training_epochs             :      {ns.training_epochs} \\n\") \n",
    "print( f\" ns.current_iters               :      {ns.current_iter}\")  \n",
    "print( f\" Batches in warmup epoch        :      {ns.trn_iters_warmup}\")\n",
    "print( f\" Batches in weight epoch        :      {ns.trn_iters_weights}\")\n",
    "print( f\" Batches in policy epoch        :      {ns.trn_iters_policy}\")\n",
    "print( f\" Batches in validation          :      {ns.eval_iters}\")\n",
    "print( f\" num_train_layers               :      {ns.num_train_layers} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a037fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:04:13.422760Z",
     "start_time": "2022-09-02T10:04:13.355097Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 240 | 2.50e-04  2.50e-04  2.50e-03  5.95e-04 |   1.0158   5.050e-04   1.769e-03    1.0181 |  4.141e-06   0.45690   0.66556   0.76161   0.65192   0.69522 |   2.1478   5.176e-04   1.816e-03    2.1501 |  -0.0 |\n",
      "\n",
      "[e] Last ep:240  it:477360  -  Losses:   \t Task: 2.1478   \t Sparsity: 5.17625e-04    \t Sharing: 1.81609e-03    \t Total: 2.1501 \n",
      "\n",
      "   best_epoch:   230   best iter: 456799   best_accuracy: 0.66728    best ROC auc: 0.76332\n",
      "\n",
      "\n",
      " ep:  240    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7231    0.2769  1    0.6753    0.3247  1    0.7544    0.2456  1    0.6956    0.3044  1    0.6803    0.3197  1    0.6350    0.3650  1    0.6214    0.3786  1    0.6198    0.3802  1    0.7125    0.2875  1    0.6027    0.3973  1\n",
      "  1    0.7830    0.2170  1    0.8014    0.1986  1    0.7597    0.2403  1    0.7369    0.2631  1    0.7347    0.2653  1    0.7205    0.2795  1    0.6446    0.3554  1    0.6483    0.3517  1    0.8007    0.1993  1    0.6664    0.3336  1\n",
      "  2    0.7687    0.2313  1    0.7744    0.2256  1    0.8217    0.1783  1    0.7234    0.2766  1    0.7369    0.2631  1    0.7301    0.2699  1    0.7209    0.2791  1    0.7198    0.2802  1    0.7593    0.2407  1    0.7238    0.2762  1\n",
      "  3    0.6982    0.3018  1    0.7395    0.2605  1    0.8132    0.1868  1    0.6555    0.3445  1    0.7283    0.2717  1    0.6799    0.3201  1    0.6447    0.3553  1    0.6731    0.3269  1    0.7013    0.2987  1    0.7052    0.2948  1\n",
      "  4    0.6712    0.3288  1    0.6956    0.3044  1    0.6491    0.3509  1    0.7061    0.2939  1    0.6745    0.3255  1    0.6224    0.3776  1    0.6295    0.3705  1    0.5656    0.4344  1    0.6441    0.3559  1    0.5896    0.4104  1\n",
      "  5    0.7430    0.2570  1    0.6916    0.3084  1    0.7363    0.2637  1    0.6437    0.3563  1    0.7140    0.2860  1    0.6199    0.3801  1    0.7134    0.2866  1    0.6610    0.3390  1    0.6756    0.3244  1    0.6279    0.3721  1\n",
      "\n",
      "\n",
      " ep:  240   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.4175   -0.5425  1    0.4175   -0.3146  1    0.4176   -0.7046  1    0.4178   -0.4087  1    0.4175   -0.3378  1    0.4175   -0.1361  1    0.4175   -0.0781  1    0.4175   -0.0713  1    0.4175   -0.4900  1    0.4175    0.0009  1\n",
      "  1    0.6187   -0.6648  1    0.6328   -0.7625  1    0.6186   -0.5324  1    0.6186   -0.4112  1    0.6186   -0.4002  1    0.6187   -0.3285  1    0.6186    0.0233  1    0.6186    0.0071  1    0.6493   -0.7414  1    0.6190   -0.0731  1\n",
      "  2    0.7100   -0.4913  1    0.7100   -0.5231  1    0.7738   -0.7544  1    0.7147   -0.2466  1    0.7099   -0.3199  1    0.7100   -0.2852  1    0.7691   -0.1798  1    0.7101   -0.2332  1    0.7621   -0.3870  1    0.7457   -0.2175  1\n",
      "  3    0.5500   -0.2887  1    0.5500   -0.4931  1    0.5500   -0.9211  1    0.5502   -0.0932  1    0.5501   -0.4359  1    0.5501   -0.2030  1    0.5500   -0.0458  1    0.5500   -0.1723  1    0.5932   -0.2604  1    0.5500   -0.3224  1\n",
      "  4    0.3695   -0.3442  1    0.3695   -0.4568  1    0.3695   -0.2455  1    0.4011   -0.4752  1    0.3695   -0.3589  1    0.3694   -0.1304  1    0.3695   -0.1607  1    0.3453    0.0814  1    0.4378   -0.1553  1    0.3684    0.0060  1\n",
      "  5    0.5264   -0.5352  1    0.4819   -0.3255  1    0.4820   -0.5446  1    0.4100   -0.1815  1    0.4819   -0.4332  1    0.4818   -0.0075  1    0.4831   -0.4289  1    0.4819   -0.1858  1    0.4819   -0.2518  1    0.4819   -0.0413  1\n",
      "\n",
      "\n",
      " ep:  240    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7231    0.2769  1    0.6753    0.3247  0    0.7544    0.2456  1    0.6956    0.3044  1    0.6803    0.3197  1    0.6350    0.3650  0    0.6214    0.3786  1    0.6198    0.3802  1    0.7125    0.2875  1    0.6027    0.3973  1\n",
      "  1    0.7830    0.2170  1    0.8014    0.1986  1    0.7597    0.2403  1    0.7369    0.2631  0    0.7347    0.2653  1    0.7205    0.2795  1    0.6446    0.3554  1    0.6483    0.3517  1    0.8007    0.1993  1    0.6664    0.3336  0\n",
      "  2    0.7687    0.2313  1    0.7744    0.2256  1    0.8217    0.1783  1    0.7234    0.2766  1    0.7369    0.2631  1    0.7301    0.2699  0    0.7209    0.2791  1    0.7198    0.2802  0    0.7593    0.2407  1    0.7238    0.2762  0\n",
      "  3    0.6982    0.3018  1    0.7395    0.2605  1    0.8132    0.1868  1    0.6555    0.3445  0    0.7283    0.2717  1    0.6799    0.3201  1    0.6447    0.3553  1    0.6731    0.3269  0    0.7013    0.2987  0    0.7052    0.2948  1\n",
      "  4    0.6712    0.3288  1    0.6956    0.3044  1    0.6491    0.3509  0    0.7061    0.2939  1    0.6745    0.3255  0    0.6224    0.3776  1    0.6295    0.3705  1    0.5656    0.4344  1    0.6441    0.3559  1    0.5896    0.4104  0\n",
      "  5    0.7430    0.2570  1    0.6916    0.3084  1    0.7363    0.2637  1    0.6437    0.3563  1    0.7140    0.2860  1    0.6199    0.3801  1    0.7134    0.2866  1    0.6610    0.3390  0    0.6756    0.3244  0    0.6279    0.3721  0\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 240       # of epochs to run:  20 -->  epochs 241 to 260\n",
      " policy_learning rate : 0.01 \n",
      " lambda_sparsity      : 0.005\n",
      " lambda_sharing       : 0.05\n",
      " curriculum training  : False     cirriculum speed: 3     num_training_layers : 6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')     \n",
    "print()\n",
    "environ.display_trained_policy(ns.current_epoch)\n",
    "environ.display_trained_logits(ns.current_epoch)\n",
    "environ.display_current_policy(ns.current_epoch)\n",
    "\n",
    "print_heading(f\" Last Epoch Completed : {ns.current_epoch}       # of epochs to run:  {ns.training_epochs} -->  epochs {ns.current_epoch+1} to {ns.training_epochs + ns.current_epoch}\"\n",
    "              f\"\\n policy_learning rate : {environ.opt['train']['policy_lr']} \"\n",
    "              f\"\\n lambda_sparsity      : {environ.opt['train']['lambda_sparsity']}\"\n",
    "              f\"\\n lambda_sharing       : {environ.opt['train']['lambda_sharing']}\"\n",
    "              f\"\\n curriculum training  : {opt['is_curriculum']}     cirriculum speed: {opt['curriculum_speed']}     num_training_layers : {ns.num_train_layers}\", \n",
    "              verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c71af",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Weight/Policy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7f5f2f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T17:55:05.148222Z",
     "start_time": "2022-08-31T17:55:04.758878Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 | 5.00e-04  5.00e-04  5.00e-03  2.51e-02 |   1.2543   5.526e-04   5.472e-04    1.2554 |  4.117e-06   0.45601   0.66590   0.76196   0.65246   0.69572 |   2.1354   5.663e-04   5.754e-04    2.1366 |  -0.0 |\n",
      "\n",
      "[e] Last ep:200  it:397800  -  Losses:   \t Task: 2.1354   \t Sparsity: 5.66264e-04    \t Sharing: 5.75399e-04    \t Total: 2.1366 \n",
      "\n",
      "   best_epoch:   191   best iter: 379899   best_accuracy: 0.66763    best ROC auc: 0.76265\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ad7d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-09-01T09:27:14.595Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 220       # of epochs to run:  20 -->  epochs 221 to 240\n",
      " Backbone Initial LR  : 0.001      Current LR : 0.0005 \n",
      " Heads    Initial LR  : 0.001      Current LR : 0.0005\n",
      " Policy   Initial LR  : 0.01      Current LR : 0.005\n",
      " Regularization tasks : 1.0          Sparsity: 0.005           sharing: 0.05\n",
      " curriculum training  : False      Cirriculum speed: 3     num_training_layers : 6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 221 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   0.9841   8.732e-04   4.989e-03    0.9900 |  4.124e-06   0.45618   0.66685   0.76218   0.65323   0.69657 |   2.1390   5.445e-04   3.111e-03    2.1426 |1307.3 |\n",
      " 221 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   1.0807   5.311e-04   1.895e-03    1.0831 |  4.127e-06   0.45628   0.66579   0.76193   0.65236   0.69549 |   2.1406   5.442e-04   1.950e-03    2.1431 | 247.2 |\n",
      "\n",
      " ep:  221    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7009    0.2991  1    0.7168    0.2832  0    0.8123    0.1877  1    0.7449    0.2551  1    0.8228    0.1772  0    0.6511    0.3489  1    0.7145    0.2855  1    0.7090    0.2910  1    0.6898    0.3102  0    0.6480    0.3520  1\n",
      "  1    0.7047    0.2953  0    0.8110    0.1890  1    0.7521    0.2479  1    0.7679    0.2321  1    0.7527    0.2473  1    0.7574    0.2426  1    0.7024    0.2976  0    0.6696    0.3304  1    0.7728    0.2272  1    0.6133    0.3867  1\n",
      "  2    0.7440    0.2560  0    0.7894    0.2106  0    0.8229    0.1771  1    0.7415    0.2585  1    0.7668    0.2332  1    0.6381    0.3619  1    0.7989    0.2011  0    0.7350    0.2650  1    0.7626    0.2374  1    0.7353    0.2647  1\n",
      "  3    0.7673    0.2327  1    0.7797    0.2203  1    0.8216    0.1784  0    0.6999    0.3001  0    0.7285    0.2715  0    0.6860    0.3140  0    0.6666    0.3334  0    0.7375    0.2625  1    0.6255    0.3745  1    0.7190    0.2810  1\n",
      "  4    0.6417    0.3583  1    0.7763    0.2237  1    0.6803    0.3197  1    0.6761    0.3239  1    0.6282    0.3718  0    0.6713    0.3287  1    0.6016    0.3984  0    0.6119    0.3881  0    0.6333    0.3667  1    0.5807    0.4193  0\n",
      "  5    0.6707    0.3293  1    0.7454    0.2546  1    0.7741    0.2259  1    0.6752    0.3248  1    0.7439    0.2561  1    0.6804    0.3196  1    0.7020    0.2980  1    0.6721    0.3279  1    0.7249    0.2751  0    0.6651    0.3349  1\n",
      "\n",
      " 222 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   1.2788   8.728e-04   3.128e-03    1.2828 |  4.131e-06   0.45669   0.66636   0.76196   0.65314   0.69561 |   2.1426   5.442e-04   1.950e-03    2.1451 |1297.0 |\n",
      " 222 | 5.00e-04  5.00e-04  5.00e-03  3.34e-03 |   0.9801   5.333e-04   1.733e-03    0.9824 |  4.129e-06   0.45632   0.66593   0.76293   0.65220   0.69618 |   2.1420   5.466e-04   1.743e-03    2.1443 | 244.2 |\n",
      "Previous best_epoch:   203   best iter: 403767   best_accuracy: 0.66709    best ROC auc: 0.76280\n",
      "Previous best_epoch:   222   best iter: 441558   best_accuracy: 0.66593    best ROC auc: 0.76293\n",
      " save best metrics to     :  metrics_best.pickle\n",
      " save  best checkpoint to :  model_best\n",
      " decay gumbel temperature to 0.0025084781938833345\n",
      "\n",
      " ep:  222    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7407    0.2593  1    0.7627    0.2373  1    0.8051    0.1949  1    0.7417    0.2583  1    0.8147    0.1853  1    0.6926    0.3074  1    0.7524    0.2476  1    0.6660    0.3340  0    0.7496    0.2504  1    0.6554    0.3446  1\n",
      "  1    0.7044    0.2956  0    0.7919    0.2081  0    0.7725    0.2275  1    0.7749    0.2251  1    0.7310    0.2690  1    0.7472    0.2528  1    0.7145    0.2855  1    0.6686    0.3314  0    0.7967    0.2033  1    0.6567    0.3433  0\n",
      "  2    0.6962    0.3038  1    0.7652    0.2348  0    0.8079    0.1921  1    0.7247    0.2753  0    0.7656    0.2344  1    0.6593    0.3407  1    0.8041    0.1959  0    0.7093    0.2907  0    0.7330    0.2670  1    0.7273    0.2727  1\n",
      "  3    0.7619    0.2381  1    0.8085    0.1915  1    0.8212    0.1788  1    0.6802    0.3198  1    0.7167    0.2833  0    0.7339    0.2661  1    0.6524    0.3476  1    0.7094    0.2906  1    0.6199    0.3801  1    0.7160    0.2840  1\n",
      "  4    0.6413    0.3587  1    0.7559    0.2441  1    0.7173    0.2827  1    0.6811    0.3189  1    0.6544    0.3456  0    0.6436    0.3564  0    0.6188    0.3812  1    0.6472    0.3528  1    0.6194    0.3806  1    0.5473    0.4527  0\n",
      "  5    0.7065    0.2935  1    0.7517    0.2483  1    0.7583    0.2417  1    0.6901    0.3099  0    0.7340    0.2660  1    0.6570    0.3430  1    0.7351    0.2649  1    0.6691    0.3309  0    0.7356    0.2644  1    0.6394    0.3606  0\n",
      "\n",
      " 223 | 5.00e-04  5.00e-04  5.00e-03  2.51e-03 |   1.0176   8.766e-04   2.795e-03    1.0213 |  4.129e-06   0.45639   0.66601   0.76246   0.65256   0.69563 |   2.1415   5.466e-04   1.743e-03    2.1438 |1298.3 |\n",
      "Epoch    73: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    73: reducing learning rate of group 1 to 2.5000e-04.\n",
      " 223 | 2.50e-04  2.50e-04  5.00e-03  2.51e-03 |   0.9768   5.323e-04   1.680e-03    0.9790 |  4.128e-06   0.45671   0.66624   0.76162   0.65288   0.69569 |   2.1411   5.457e-04   1.728e-03    2.1434 | 241.7 |\n",
      "Epoch    73: reducing learning rate of group 0 to 2.5000e-03.\n",
      "\n",
      " ep:  223    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7027    0.2973  0    0.7450    0.2550  1    0.7878    0.2122  1    0.7048    0.2952  1    0.7506    0.2494  1    0.7047    0.2953  1    0.7038    0.2962  1    0.6290    0.3710  0    0.7561    0.2439  1    0.6658    0.3342  1\n",
      "  1    0.7115    0.2885  0    0.7697    0.2303  1    0.7858    0.2142  1    0.8030    0.1970  1    0.7407    0.2593  1    0.7078    0.2922  1    0.7056    0.2944  0    0.6734    0.3266  1    0.7871    0.2129  1    0.6986    0.3014  1\n",
      "  2    0.7420    0.2580  1    0.7860    0.2140  1    0.8293    0.1707  1    0.7564    0.2436  0    0.7739    0.2261  1    0.7005    0.2995  1    0.7917    0.2083  1    0.7435    0.2565  1    0.7801    0.2199  1    0.7106    0.2894  0\n",
      "  3    0.7459    0.2541  1    0.7953    0.2047  1    0.8274    0.1726  1    0.7093    0.2907  1    0.7072    0.2928  1    0.7226    0.2774  0    0.6633    0.3367  1    0.7284    0.2716  0    0.6491    0.3509  1    0.7125    0.2875  0\n",
      "  4    0.6644    0.3356  1    0.7840    0.2160  1    0.7543    0.2457  1    0.6693    0.3307  1    0.6569    0.3431  1    0.6397    0.3603  1    0.6239    0.3761  1    0.6458    0.3542  1    0.6114    0.3886  0    0.5951    0.4049  0\n",
      "  5    0.7108    0.2892  1    0.6978    0.3022  1    0.7250    0.2750  0    0.6826    0.3174  1    0.7200    0.2800  1    0.6426    0.3574  1    0.7195    0.2805  0    0.6827    0.3173  0    0.6949    0.3051  1    0.6214    0.3786  1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 224 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   1.1125   8.751e-04   2.771e-03    1.1162 |  4.123e-06   0.45630   0.66710   0.76259   0.65348   0.69648 |   2.1385   5.457e-04   1.728e-03    2.1407 |1306.5 |\n",
      " 224 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   1.3703   5.354e-04   1.485e-03    1.3723 |  4.122e-06   0.45620   0.66625   0.76183   0.65295   0.69601 |   2.1381   5.488e-04   1.543e-03    2.1402 | 246.4 |\n",
      "\n",
      " ep:  224    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7000    0.3000  1    0.7615    0.2385  1    0.7625    0.2375  0    0.7171    0.2829  1    0.7697    0.2303  1    0.6971    0.3029  1    0.7118    0.2882  0    0.6454    0.3546  1    0.7840    0.2160  1    0.6880    0.3120  1\n",
      "  1    0.7442    0.2558  1    0.7913    0.2087  1    0.7884    0.2116  0    0.8199    0.1801  0    0.7609    0.2391  0    0.7175    0.2825  0    0.7002    0.2998  0    0.6756    0.3244  1    0.8155    0.1845  1    0.7028    0.2972  1\n",
      "  2    0.7445    0.2555  1    0.7703    0.2297  0    0.8113    0.1887  1    0.7602    0.2398  1    0.7750    0.2250  1    0.6996    0.3004  1    0.7856    0.2144  0    0.7546    0.2454  1    0.7739    0.2261  1    0.7164    0.2836  0\n",
      "  3    0.7303    0.2697  1    0.7798    0.2202  0    0.8249    0.1751  1    0.6799    0.3201  0    0.6968    0.3032  1    0.7123    0.2877  1    0.6536    0.3464  1    0.7113    0.2887  1    0.6678    0.3322  0    0.6912    0.3088  1\n",
      "  4    0.6878    0.3122  1    0.7617    0.2383  0    0.7614    0.2386  1    0.6727    0.3273  1    0.6455    0.3545  1    0.6057    0.3943  1    0.6329    0.3671  1    0.6347    0.3653  1    0.6321    0.3679  0    0.5986    0.4014  0\n",
      "  5    0.7100    0.2900  1    0.7230    0.2770  1    0.7416    0.2584  0    0.6777    0.3223  0    0.7221    0.2779  1    0.6396    0.3604  1    0.7330    0.2670  0    0.6717    0.3283  1    0.6918    0.3082  0    0.6308    0.3692  0\n",
      "\n",
      " 225 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   0.9853   8.801e-04   2.475e-03    0.9886 |  4.162e-06   0.45699   0.66588   0.76198   0.65272   0.69538 |   2.1589   5.488e-04   1.543e-03    2.1610 |1307.5 |\n",
      " 225 | 2.50e-04  2.50e-04  2.50e-03  2.51e-03 |   0.9724   5.312e-04   8.965e-04    0.9738 |  4.127e-06   0.45672   0.66584   0.76119   0.65245   0.69542 |   2.1405   5.445e-04   9.087e-04    2.1420 | 248.6 |\n",
      " decay gumbel temperature to 0.0018813586454125009\n",
      "\n",
      "[e] Policy training epoch:225  it:447525 -  Losses:   \t Task: 2.1405   \t Sparsity: 5.44456e-04    \t Sharing: 9.08661e-04    \t Total: 2.1420 \n",
      "\n",
      " ep:  225    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7156    0.2844  0    0.7430    0.2570  1    0.7930    0.2070  1    0.7024    0.2976  1    0.7639    0.2361  1    0.6867    0.3133  1    0.6802    0.3198  1    0.6348    0.3652  1    0.7722    0.2278  1    0.6624    0.3376  1\n",
      "  1    0.7348    0.2652  0    0.7974    0.2026  1    0.7715    0.2285  1    0.8112    0.1888  1    0.7675    0.2325  1    0.7140    0.2860  0    0.6886    0.3114  0    0.6404    0.3596  1    0.8072    0.1928  1    0.7151    0.2849  1\n",
      "  2    0.7406    0.2594  1    0.7762    0.2238  1    0.8138    0.1862  0    0.7519    0.2481  0    0.7804    0.2196  1    0.7066    0.2934  1    0.7818    0.2182  1    0.7802    0.2198  1    0.8030    0.1970  1    0.7145    0.2855  0\n",
      "  3    0.7323    0.2677  1    0.7731    0.2269  1    0.8230    0.1770  0    0.6767    0.3233  1    0.6819    0.3181  1    0.7164    0.2836  1    0.6537    0.3463  1    0.7196    0.2804  0    0.6740    0.3260  0    0.6791    0.3209  0\n",
      "  4    0.6771    0.3229  0    0.7528    0.2472  1    0.7573    0.2427  0    0.6717    0.3283  1    0.6460    0.3540  1    0.5898    0.4102  0    0.6270    0.3730  1    0.6049    0.3951  1    0.6202    0.3798  0    0.5899    0.4101  1\n",
      "  5    0.6892    0.3108  1    0.7391    0.2609  1    0.7463    0.2537  0    0.6883    0.3117  1    0.7111    0.2889  1    0.6364    0.3636  0    0.7288    0.2712  1    0.6731    0.3269  1    0.6868    0.3132  0    0.6382    0.3618  1\n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 226 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.1930   8.732e-04   1.457e-03    1.1953 |  4.121e-06   0.45663   0.66625   0.76220   0.65275   0.69600 |   2.1375   5.445e-04   9.087e-04    2.1390 |1304.7 |\n",
      " 226 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.2375   5.245e-04   1.666e-03    1.2397 |  4.123e-06   0.45594   0.66609   0.76278   0.65263   0.69556 |   2.1384   5.376e-04   1.706e-03    2.1407 | 246.8 |\n",
      "\n",
      " ep:  226    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7114    0.2886  0    0.7319    0.2681  1    0.8046    0.1954  0    0.7026    0.2974  1    0.7554    0.2446  1    0.6941    0.3059  1    0.6752    0.3248  1    0.6147    0.3853  1    0.7549    0.2451  1    0.6457    0.3543  1\n",
      "  1    0.7314    0.2686  1    0.7918    0.2082  1    0.7344    0.2656  1    0.7891    0.2109  1    0.7475    0.2525  0    0.7115    0.2885  0    0.6691    0.3309  0    0.6150    0.3850  1    0.7940    0.2060  0    0.6797    0.3203  1\n",
      "  2    0.7269    0.2731  1    0.7697    0.2303  1    0.8012    0.1988  0    0.7470    0.2530  1    0.7813    0.2187  0    0.6974    0.3026  1    0.7746    0.2254  1    0.7849    0.2151  1    0.7907    0.2093  1    0.6990    0.3010  1\n",
      "  3    0.7518    0.2482  1    0.7994    0.2006  1    0.8272    0.1728  1    0.6811    0.3189  0    0.7083    0.2917  1    0.7147    0.2853  0    0.6750    0.3250  0    0.7213    0.2787  1    0.6913    0.3087  1    0.7055    0.2945  1\n",
      "  4    0.6766    0.3234  0    0.7219    0.2781  1    0.7415    0.2585  1    0.6547    0.3453  1    0.6117    0.3883  1    0.5809    0.4191  1    0.6132    0.3868  0    0.6097    0.3903  0    0.5912    0.4088  1    0.5698    0.4302  1\n",
      "  5    0.6917    0.3083  1    0.7377    0.2623  1    0.7274    0.2726  0    0.6956    0.3044  1    0.7481    0.2519  1    0.6370    0.3630  1    0.7411    0.2589  0    0.6801    0.3199  1    0.7055    0.2945  1    0.6407    0.3593  0\n",
      "\n",
      " 227 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.1329   8.622e-04   2.735e-03    1.1365 |  4.147e-06   0.45704   0.66637   0.76194   0.65285   0.69607 |   2.1509   5.376e-04   1.706e-03    2.1531 |1303.4 |\n",
      " 227 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.1263   5.222e-04   1.186e-03    1.1280 |  4.128e-06   0.45681   0.66641   0.76203   0.65296   0.69631 |   2.1413   5.352e-04   1.218e-03    2.1431 | 250.2 |\n",
      "\n",
      " ep:  227    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7121    0.2879  1    0.7348    0.2652  1    0.7923    0.2077  1    0.7045    0.2955  1    0.7472    0.2528  1    0.6886    0.3114  1    0.6716    0.3284  1    0.6161    0.3839  1    0.7536    0.2464  0    0.6421    0.3579  1\n",
      "  1    0.7410    0.2590  0    0.7853    0.2147  1    0.7548    0.2452  1    0.8004    0.1996  1    0.7472    0.2528  1    0.7126    0.2874  0    0.6686    0.3314  1    0.6160    0.3840  0    0.7964    0.2036  1    0.6912    0.3088  1\n",
      "  2    0.7144    0.2856  1    0.8026    0.1974  1    0.7781    0.2219  0    0.7560    0.2440  1    0.7665    0.2335  1    0.6634    0.3366  1    0.7623    0.2377  0    0.7614    0.2386  1    0.7845    0.2155  1    0.6759    0.3241  1\n",
      "  3    0.7441    0.2559  1    0.7930    0.2070  1    0.8327    0.1673  1    0.6615    0.3385  1    0.7135    0.2865  1    0.7287    0.2713  0    0.6588    0.3412  1    0.7229    0.2771  1    0.7161    0.2839  1    0.7049    0.2951  0\n",
      "  4    0.6727    0.3273  1    0.7365    0.2635  1    0.7381    0.2619  1    0.6758    0.3242  1    0.5935    0.4065  1    0.5805    0.4195  1    0.6227    0.3773  1    0.6173    0.3827  0    0.5925    0.4075  0    0.5583    0.4417  0\n",
      "  5    0.6826    0.3174  1    0.7322    0.2678  1    0.7366    0.2634  1    0.6936    0.3064  1    0.7311    0.2689  0    0.6301    0.3699  0    0.7399    0.2601  1    0.6816    0.3184  1    0.7083    0.2917  1    0.6379    0.3621  1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 228 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.0429   8.584e-04   1.954e-03    1.0457 |  4.133e-06   0.45681   0.66580   0.76230   0.65232   0.69584 |   2.1439   5.352e-04   1.218e-03    2.1456 |1305.6 |\n",
      " 228 | 2.50e-04  2.50e-04  2.50e-03  1.88e-03 |   1.0279   5.245e-04   8.995e-04    1.0294 |  4.154e-06   0.45668   0.66648   0.76262   0.65320   0.69598 |   2.1549   5.377e-04   9.312e-04    2.1564 | 250.8 |\n",
      " decay gumbel temperature to 0.0014110189840593756\n",
      "\n",
      " ep:  228    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7377    0.2623  0    0.7200    0.2800  1    0.8013    0.1987  1    0.7010    0.2990  1    0.7309    0.2691  1    0.6870    0.3130  0    0.6598    0.3402  1    0.6068    0.3932  1    0.7549    0.2451  1    0.6348    0.3652  1\n",
      "  1    0.7649    0.2351  0    0.7841    0.2159  1    0.7600    0.2400  1    0.8171    0.1829  1    0.7382    0.2618  1    0.7379    0.2621  1    0.6746    0.3254  0    0.6475    0.3525  1    0.7945    0.2055  1    0.6957    0.3043  0\n",
      "  2    0.7143    0.2857  1    0.7976    0.2024  1    0.7897    0.2103  1    0.7498    0.2502  1    0.7738    0.2262  1    0.6813    0.3187  1    0.7517    0.2483  0    0.7615    0.2385  0    0.8079    0.1921  0    0.6911    0.3089  1\n",
      "  3    0.7281    0.2719  0    0.7915    0.2085  1    0.8252    0.1748  1    0.6691    0.3309  1    0.7066    0.2934  1    0.7236    0.2764  1    0.6620    0.3380  0    0.7089    0.2911  0    0.7090    0.2910  0    0.7010    0.2990  1\n",
      "  4    0.6784    0.3216  0    0.7260    0.2740  1    0.7340    0.2660  0    0.6781    0.3219  0    0.6019    0.3981  1    0.5810    0.4190  1    0.6008    0.3992  1    0.6289    0.3711  0    0.6005    0.3995  1    0.5639    0.4361  1\n",
      "  5    0.7151    0.2849  0    0.7263    0.2737  1    0.7330    0.2670  1    0.7044    0.2956  0    0.7309    0.2691  0    0.6285    0.3715  1    0.7316    0.2684  1    0.6806    0.3194  0    0.7076    0.2924  0    0.6324    0.3676  0\n",
      "\n",
      "Ep: 229 [weights]:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1290/1318 [19:35<00:25,  1.11it/s, it=454782, Lss=1.0990, Spr=4.9181e-04, Shr=8.5176e-04, lyr=6]"
     ]
    }
   ],
   "source": [
    "weight_policy_training(ns, opt, environ, dldrs, epochs =20, display_policy = True, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9feff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f401632",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f553102f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T17:55:05.148222Z",
     "start_time": "2022-08-31T17:55:04.758878Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 | 5.00e-04  5.00e-04  5.00e-03  2.51e-02 |   1.2543   5.526e-04   5.472e-04    1.2554 |  4.117e-06   0.45601   0.66590   0.76196   0.65246   0.69572 |   2.1354   5.663e-04   5.754e-04    2.1366 |  -0.0 |\n",
      "\n",
      "[e] Last ep:200  it:397800  -  Losses:   \t Task: 2.1354   \t Sparsity: 5.66264e-04    \t Sharing: 5.75399e-04    \t Total: 2.1366 \n",
      "\n",
      "   best_epoch:   191   best iter: 379899   best_accuracy: 0.66763    best ROC auc: 0.76265\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7b3679c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T10:03:26.643974Z",
     "start_time": "2022-09-02T10:03:26.576850Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 240 | 2.50e-04  2.50e-04  2.50e-03  5.95e-04 |   1.0158   5.050e-04   1.769e-03    1.0181 |  4.141e-06   0.45690   0.66556   0.76161   0.65192   0.69522 |   2.1478   5.176e-04   1.816e-03    2.1501 |  -0.0 |\n",
      "\n",
      "[e] Last ep:240  it:477360  -  Losses:   \t Task: 2.1478   \t Sparsity: 5.17625e-04    \t Sharing: 1.81609e-03    \t Total: 2.1501 \n",
      "\n",
      "   best_epoch:   230   best iter: 456799   best_accuracy: 0.66728    best ROC auc: 0.76332\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f3aed31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-31T17:55:29.703004Z",
     "start_time": "2022-08-31T17:55:29.657264Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights Scheduler Parameters\n",
      "------------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0, 0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 2\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1219009263455093\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 50\n",
      "    _last_lr                 value: [0.0005, 0.0005]\n",
      "\n",
      "Policy Scheduler Parameters\n",
      "-----------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 2\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1244821764992645\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 50\n",
      "    _last_lr                 value: [0.005]\n"
     ]
    }
   ],
   "source": [
    "print_underline('Weights Scheduler Parameters', verbose = True) \n",
    "for k,i in environ.schedulers['weights'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")\n",
    "\n",
    "print_underline('Policy Scheduler Parameters', verbose = True)\n",
    "for k,i in environ.schedulers['alphas'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a9d98",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Weight/Policy Training - repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ff502bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:28:01.285918Z",
     "start_time": "2022-09-02T10:05:18.238547Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 240       # of epochs to run:  10 -->  epochs 241 to 250\n",
      " Backbone Initial LR  : 0.001      Current LR : 0.00025 \n",
      " Heads    Initial LR  : 0.001      Current LR : 0.00025\n",
      " Policy   Initial LR  : 0.01      Current LR : 0.0025\n",
      " Regularization tasks : 1.0          Sparsity: 0.01           sharing: 0.05\n",
      " curriculum training  : False      Cirriculum speed: 3     num_training_layers : 6\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 241 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   0.7674   1.660e-03   2.913e-03    0.7719 |  4.130e-06   0.45688   0.66528   0.76121   0.65196   0.69514 |   2.1424   1.035e-03   1.816e-03    2.1453 |1306.5 |\n",
      " 241 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   1.1537   1.011e-03   1.440e-03    1.1562 |  4.127e-06   0.45676   0.66635   0.76127   0.65313   0.69615 |   2.1408   1.037e-03   1.470e-03    2.1433 | 245.9 |\n",
      "\n",
      " ep:  241    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7281    0.2719  1    0.6952    0.3048  1    0.7400    0.2600  1    0.6992    0.3008  1    0.6829    0.3171  1    0.6385    0.3615  1    0.6383    0.3617  0    0.6242    0.3758  1    0.7126    0.2874  1    0.6008    0.3992  1\n",
      "  1    0.7760    0.2240  1    0.7961    0.2039  1    0.7336    0.2664  0    0.7010    0.2990  1    0.7298    0.2702  0    0.7173    0.2827  1    0.6420    0.3580  1    0.6608    0.3392  0    0.7925    0.2075  1    0.6611    0.3389  1\n",
      "  2    0.7988    0.2012  1    0.7765    0.2235  0    0.8204    0.1796  1    0.7287    0.2713  1    0.7424    0.2576  0    0.7490    0.2510  0    0.7158    0.2842  1    0.7449    0.2551  1    0.7533    0.2467  1    0.7201    0.2799  0\n",
      "  3    0.6844    0.3156  1    0.7189    0.2811  1    0.8064    0.1936  1    0.6658    0.3342  1    0.7246    0.2754  1    0.6729    0.3271  1    0.6392    0.3608  0    0.6693    0.3307  1    0.6907    0.3093  1    0.7302    0.2698  1\n",
      "  4    0.6729    0.3271  1    0.6988    0.3012  0    0.6522    0.3478  1    0.7011    0.2989  0    0.6846    0.3154  1    0.6405    0.3595  1    0.6576    0.3424  1    0.5698    0.4302  0    0.6420    0.3580  0    0.5952    0.4048  0\n",
      "  5    0.7297    0.2703  0    0.6858    0.3142  1    0.7503    0.2497  0    0.6506    0.3494  1    0.7095    0.2905  1    0.6334    0.3666  0    0.7049    0.2951  1    0.6562    0.3438  1    0.6579    0.3421  1    0.6527    0.3473  0\n",
      "\n",
      " 242 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   0.8311   1.663e-03   2.358e-03    0.8351 |  4.131e-06   0.45654   0.66684   0.76240   0.65314   0.69612 |   2.1427   1.037e-03   1.470e-03    2.1453 |1318.8 |\n",
      " 242 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   0.8926   1.002e-03   1.041e-03    0.8947 |  4.125e-06   0.45658   0.66560   0.76106   0.65203   0.69532 |   2.1396   1.027e-03   1.060e-03    2.1417 | 251.9 |\n",
      "\n",
      " ep:  242    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7236    0.2764  1    0.6932    0.3068  1    0.7045    0.2955  1    0.6969    0.3031  1    0.6772    0.3228  1    0.6534    0.3466  1    0.6364    0.3636  1    0.6217    0.3783  1    0.7056    0.2944  0    0.6013    0.3987  1\n",
      "  1    0.7632    0.2368  1    0.7905    0.2095  1    0.7318    0.2682  1    0.7062    0.2938  1    0.7334    0.2666  0    0.7125    0.2875  0    0.6510    0.3490  0    0.6556    0.3444  1    0.7885    0.2115  1    0.6552    0.3448  0\n",
      "  2    0.7932    0.2068  0    0.7746    0.2254  1    0.8209    0.1791  1    0.7298    0.2702  1    0.7692    0.2308  1    0.7515    0.2485  1    0.7194    0.2806  1    0.7732    0.2268  1    0.7559    0.2441  1    0.7379    0.2621  0\n",
      "  3    0.7008    0.2992  1    0.7367    0.2633  1    0.8000    0.2000  1    0.6615    0.3385  1    0.7235    0.2765  1    0.6734    0.3266  0    0.6367    0.3633  0    0.6671    0.3329  1    0.6869    0.3131  1    0.6995    0.3005  1\n",
      "  4    0.6676    0.3324  0    0.6940    0.3060  1    0.6399    0.3601  1    0.6965    0.3035  1    0.6655    0.3345  1    0.6450    0.3550  1    0.6495    0.3505  0    0.5696    0.4304  0    0.6381    0.3619  0    0.5940    0.4060  0\n",
      "  5    0.6819    0.3181  1    0.6844    0.3156  1    0.7362    0.2638  1    0.6485    0.3515  1    0.6848    0.3152  1    0.6266    0.3734  0    0.6928    0.3072  1    0.6512    0.3488  0    0.6451    0.3549  1    0.6345    0.3655  1\n",
      "\n",
      " 243 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   1.0960   1.647e-03   1.700e-03    1.0993 |  4.126e-06   0.45669   0.66570   0.76268   0.65217   0.69494 |   2.1402   1.027e-03   1.060e-03    2.1423 |1316.4 |\n",
      " 243 | 2.50e-04  2.50e-04  2.50e-03  4.46e-04 |   1.1727   9.998e-04   1.621e-03    1.1753 |  4.127e-06   0.45657   0.66524   0.76098   0.65175   0.69449 |   2.1408   1.025e-03   1.652e-03    2.1434 | 254.3 |\n",
      " decay gumbel temperature to 0.0003348414190687776\n",
      "\n",
      " ep:  243    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7212    0.2788  0    0.6950    0.3050  1    0.6958    0.3042  1    0.7158    0.2842  0    0.6448    0.3552  0    0.6560    0.3440  1    0.6405    0.3595  1    0.6241    0.3759  1    0.7468    0.2532  1    0.6057    0.3943  1\n",
      "  1    0.7516    0.2484  1    0.7878    0.2122  1    0.7303    0.2697  1    0.7374    0.2626  1    0.7528    0.2472  0    0.7106    0.2894  1    0.6631    0.3369  1    0.6538    0.3462  1    0.7843    0.2157  0    0.6539    0.3461  1\n",
      "  2    0.7899    0.2101  0    0.7813    0.2187  0    0.8266    0.1734  1    0.7273    0.2727  0    0.7551    0.2449  1    0.7538    0.2462  1    0.7164    0.2836  1    0.7680    0.2320  1    0.7528    0.2472  1    0.7231    0.2769  1\n",
      "  3    0.7058    0.2942  1    0.7254    0.2746  0    0.7902    0.2098  0    0.6754    0.3246  1    0.7200    0.2800  0    0.6712    0.3288  0    0.6324    0.3676  0    0.6642    0.3358  0    0.7107    0.2893  1    0.7055    0.2945  1\n",
      "  4    0.6622    0.3378  0    0.6895    0.3105  1    0.6297    0.3703  1    0.7012    0.2988  0    0.6771    0.3229  1    0.6447    0.3553  1    0.6488    0.3512  1    0.5877    0.4123  0    0.6164    0.3836  0    0.5943    0.4057  0\n",
      "  5    0.6920    0.3080  0    0.6740    0.3260  1    0.7277    0.2723  0    0.6404    0.3596  0    0.6821    0.3179  0    0.6050    0.3950  0    0.6770    0.3230  1    0.6407    0.3593  0    0.6177    0.3823  0    0.6248    0.3752  1\n",
      "\n",
      " 244 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.0677   1.643e-03   2.649e-03    1.0720 |  4.131e-06   0.45709   0.66612   0.76234   0.65267   0.69568 |   2.1427   1.025e-03   1.652e-03    2.1453 |1312.0 |\n",
      " 244 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.3442   1.001e-03   1.586e-03    1.3467 |  4.137e-06   0.45695   0.66586   0.76113   0.65263   0.69545 |   2.1460   1.026e-03   1.621e-03    2.1486 | 247.6 |\n",
      "\n",
      " ep:  244    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7093    0.2907  1    0.6983    0.3017  0    0.7193    0.2807  1    0.7179    0.2821  1    0.6873    0.3127  1    0.6614    0.3386  1    0.6679    0.3321  0    0.6311    0.3689  1    0.7406    0.2594  0    0.6127    0.3873  0\n",
      "  1    0.7444    0.2556  1    0.7910    0.2090  1    0.7758    0.2242  1    0.7428    0.2572  1    0.7479    0.2521  1    0.7420    0.2580  1    0.6713    0.3287  0    0.6607    0.3393  1    0.7874    0.2126  1    0.6617    0.3383  0\n",
      "  2    0.7864    0.2136  1    0.7713    0.2287  1    0.8237    0.1763  1    0.7152    0.2848  1    0.7509    0.2491  1    0.7256    0.2744  1    0.7109    0.2891  1    0.7634    0.2366  1    0.7464    0.2536  1    0.7067    0.2933  1\n",
      "  3    0.7014    0.2986  0    0.7221    0.2779  0    0.7732    0.2268  1    0.6730    0.3270  1    0.7175    0.2825  0    0.6654    0.3346  1    0.6300    0.3700  1    0.6905    0.3095  1    0.6959    0.3041  1    0.7038    0.2962  0\n",
      "  4    0.6553    0.3447  0    0.7130    0.2870  1    0.5973    0.4027  1    0.6956    0.3044  0    0.6799    0.3201  1    0.6448    0.3552  0    0.6657    0.3343  0    0.5829    0.4171  0    0.6236    0.3764  1    0.5948    0.4052  1\n",
      "  5    0.6882    0.3118  1    0.6667    0.3333  1    0.7221    0.2779  1    0.6389    0.3611  1    0.6779    0.3221  1    0.6165    0.3835  1    0.6627    0.3373  1    0.6344    0.3656  1    0.6136    0.3864  1    0.6186    0.3814  1\n",
      "\n",
      " 245 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.0938   1.645e-03   2.600e-03    1.0980 |  4.130e-06   0.45727   0.66656   0.76158   0.65329   0.69577 |   2.1423   1.026e-03   1.621e-03    2.1449 |1317.2 |\n",
      " 245 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.1661   1.009e-03   1.191e-03    1.1683 |  4.131e-06   0.45707   0.66685   0.76215   0.65346   0.69532 |   2.1426   1.034e-03   1.217e-03    2.1448 | 250.5 |\n",
      "\n",
      "[e] Policy training epoch:245  it:487305 -  Losses:   \t Task: 2.1426   \t Sparsity: 1.03430e-03    \t Sharing: 1.21653e-03    \t Total: 2.1448 \n",
      "\n",
      " ep:  245    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.7082    0.2918  1    0.7118    0.2882  0    0.7163    0.2837  1    0.7209    0.2791  0    0.6859    0.3141  0    0.6627    0.3373  0    0.6666    0.3334  1    0.6382    0.3618  0    0.7390    0.2610  1    0.6172    0.3828  0\n",
      "  1    0.7649    0.2351  0    0.7969    0.2031  1    0.7675    0.2325  1    0.7453    0.2547  1    0.7536    0.2464  1    0.7405    0.2595  1    0.6798    0.3202  1    0.6929    0.3071  1    0.7891    0.2109  0    0.6794    0.3206  1\n",
      "  2    0.8029    0.1971  1    0.7623    0.2377  1    0.8194    0.1806  1    0.7067    0.2933  1    0.7737    0.2263  0    0.7102    0.2898  1    0.7044    0.2956  1    0.7568    0.2432  1    0.7386    0.2614  1    0.7040    0.2960  1\n",
      "  3    0.7003    0.2997  0    0.7220    0.2780  0    0.7735    0.2265  1    0.6769    0.3231  0    0.7194    0.2806  0    0.6609    0.3391  1    0.6506    0.3494  0    0.6864    0.3136  1    0.7097    0.2903  0    0.7069    0.2931  1\n",
      "  4    0.6174    0.3826  0    0.7032    0.2968  0    0.6138    0.3862  1    0.7325    0.2675  1    0.6768    0.3232  1    0.6521    0.3479  1    0.6648    0.3352  1    0.5837    0.4163  0    0.6235    0.3765  1    0.5947    0.4053  0\n",
      "  5    0.6906    0.3094  1    0.6651    0.3349  1    0.7418    0.2582  0    0.6423    0.3577  1    0.6799    0.3201  1    0.6197    0.3803  0    0.6600    0.3400  1    0.6423    0.3577  1    0.6309    0.3691  0    0.6192    0.3808  0\n",
      "\n",
      " Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |    logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total |  time |\n",
      " 246 | 2.50e-04  2.50e-04  2.50e-03  3.35e-04 |   1.3164   1.659e-03   1.951e-03    1.3200 |  4.148e-06   0.45724   0.66559   0.76008   0.65227   0.69449 |   2.1515   1.034e-03   1.217e-03    2.1537 |1319.0 |\n",
      "Ep:246 [policy] :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 403/671 [01:31<00:57,  4.63it/s, it=489026, Lss=1.3142, Spr=9.3843e-04, Shr=7.9982e-04, lyr=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 247 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   0.9301   1.641e-03   2.323e-03    0.9341 |  4.127e-06   0.45676   0.66581   0.76292   0.65224   0.69544 |   2.1407   1.023e-03   1.448e-03    2.1431 |1325.6 |\n",
      "validation:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 487/675 [01:11<00:25,  7.31it/s, it=488, Lss=1.7573, Spr=9.3013e-04, Shr=1.2727e-03, lyr=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 248 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   1.0390   1.631e-03   2.232e-03    1.0428 |  4.121e-06   0.45608   0.66584   0.76264   0.65271   0.69533 |   2.1376   1.017e-03   1.391e-03    2.1400 |1329.8 |\n",
      " 248 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   1.1531   9.862e-04   1.193e-03    1.1553 |  4.122e-06   0.45629   0.66596   0.76296   0.65243   0.69548 |   2.1384   1.011e-03   1.235e-03    2.1406 | 258.0 |\n",
      "\n",
      " ep:  248    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6878    0.3122  1    0.7411    0.2589  1    0.6969    0.3031  1    0.7301    0.2699  0    0.6863    0.3137  0    0.6284    0.3716  1    0.6614    0.3386  0    0.6744    0.3256  1    0.7533    0.2467  1    0.6251    0.3749  1\n",
      "  1    0.7445    0.2555  1    0.7611    0.2389  1    0.7610    0.2390  1    0.7281    0.2719  1    0.7623    0.2377  1    0.7250    0.2750  0    0.6578    0.3422  1    0.6450    0.3550  1    0.7564    0.2436  1    0.6582    0.3418  0\n",
      "  2    0.7899    0.2101  1    0.7635    0.2365  1    0.8176    0.1824  1    0.7139    0.2861  0    0.7620    0.2380  1    0.7382    0.2618  0    0.6919    0.3081  1    0.7544    0.2456  0    0.7165    0.2835  1    0.6783    0.3217  1\n",
      "  3    0.6892    0.3108  1    0.7060    0.2940  1    0.7304    0.2696  0    0.6562    0.3438  0    0.6908    0.3092  1    0.6097    0.3903  1    0.6387    0.3613  0    0.6863    0.3137  1    0.6862    0.3138  1    0.6967    0.3033  1\n",
      "  4    0.6396    0.3604  0    0.6950    0.3050  1    0.6295    0.3705  1    0.7177    0.2823  1    0.6851    0.3149  1    0.6437    0.3563  0    0.6560    0.3440  0    0.5575    0.4425  1    0.6103    0.3897  1    0.6070    0.3930  0\n",
      "  5    0.6897    0.3103  1    0.6499    0.3501  0    0.7614    0.2386  0    0.6465    0.3535  1    0.6787    0.3213  0    0.6179    0.3821  0    0.6559    0.3441  0    0.6559    0.3441  0    0.6202    0.3798  0    0.6248    0.3752  0\n",
      "\n",
      " 249 | 2.50e-04  2.50e-04  2.50e-03  2.51e-04 |   0.7344   1.621e-03   1.981e-03    0.7380 |  4.133e-06   0.45711   0.66587   0.76145   0.65237   0.69554 |   2.1437   1.011e-03   1.235e-03    2.1460 |1319.1 |\n",
      "Epoch    99: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    99: reducing learning rate of group 1 to 1.2500e-04.\n",
      " 249 | 1.25e-04  1.25e-04  2.50e-03  2.51e-04 |   1.1799   9.773e-04   1.996e-03    1.1829 |  4.128e-06   0.45682   0.66537   0.76152   0.65216   0.69484 |   2.1412   1.002e-03   2.040e-03    2.1442 | 255.2 |\n",
      "Epoch    99: reducing learning rate of group 0 to 1.2500e-03.\n",
      " decay gumbel temperature to 0.0001883482982261874\n",
      "\n",
      " ep:  249    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6785    0.3215  1    0.7336    0.2664  1    0.7004    0.2996  1    0.7308    0.2692  1    0.6883    0.3117  1    0.6448    0.3552  0    0.6542    0.3458  0    0.6757    0.3243  1    0.7506    0.2494  0    0.6237    0.3763  1\n",
      "  1    0.7464    0.2536  1    0.7625    0.2375  1    0.7588    0.2412  1    0.7448    0.2552  0    0.7819    0.2181  1    0.7292    0.2708  1    0.6608    0.3392  1    0.6531    0.3469  1    0.7424    0.2576  1    0.6631    0.3369  0\n",
      "  2    0.7862    0.2138  0    0.7592    0.2408  1    0.8151    0.1849  0    0.6822    0.3178  0    0.7603    0.2397  1    0.7361    0.2639  1    0.6866    0.3134  1    0.7410    0.2590  1    0.7062    0.2938  0    0.6773    0.3227  1\n",
      "  3    0.6401    0.3599  1    0.7256    0.2744  1    0.7234    0.2766  1    0.6729    0.3271  1    0.6814    0.3186  1    0.6085    0.3915  1    0.6389    0.3611  0    0.6858    0.3142  1    0.6987    0.3013  1    0.6956    0.3044  1\n",
      "  4    0.6323    0.3677  1    0.6861    0.3139  1    0.6227    0.3773  0    0.6833    0.3167  1    0.6768    0.3232  1    0.6339    0.3661  0    0.6510    0.3490  1    0.5524    0.4476  1    0.5993    0.4007  1    0.5767    0.4233  0\n",
      "  5    0.6804    0.3196  1    0.6286    0.3714  1    0.7577    0.2423  0    0.6385    0.3615  0    0.7033    0.2967  1    0.6119    0.3881  0    0.6513    0.3487  1    0.6812    0.3188  1    0.5954    0.4046  1    0.6147    0.3853  0\n",
      "\n",
      " 250 | 1.25e-04  1.25e-04  1.25e-03  1.88e-04 |   0.9418   1.607e-03   3.272e-03    0.9467 |  4.131e-06   0.45678   0.66698   0.76186   0.65363   0.69670 |   2.1428   1.002e-03   2.040e-03    2.1458 |1317.1 |\n",
      " 250 | 1.25e-04  1.25e-04  1.25e-03  1.88e-04 |   1.1690   9.724e-04   1.353e-03    1.1713 |  4.130e-06   0.45660   0.66607   0.76266   0.65240   0.69550 |   2.1423   9.967e-04   1.384e-03    2.1447 | 254.7 |\n",
      "\n",
      "[e] Policy training epoch:250  it:497250 -  Losses:   \t Task: 2.1423   \t Sparsity: 9.96697e-04    \t Sharing: 1.38397e-03    \t Total: 2.1447 \n",
      "\n",
      " ep:  250    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6666    0.3334  0    0.7313    0.2687  1    0.6986    0.3014  1    0.7278    0.2722  1    0.6860    0.3140  1    0.6426    0.3574  0    0.6520    0.3480  1    0.6728    0.3272  0    0.7488    0.2512  1    0.6202    0.3798  1\n",
      "  1    0.7586    0.2414  1    0.7597    0.2403  0    0.7576    0.2424  0    0.7451    0.2549  1    0.7766    0.2234  1    0.7319    0.2681  1    0.6608    0.3392  0    0.6551    0.3449  1    0.7355    0.2645  1    0.6645    0.3355  1\n",
      "  2    0.7812    0.2188  1    0.7733    0.2267  0    0.8113    0.1887  1    0.6866    0.3134  1    0.7566    0.2434  1    0.7319    0.2681  1    0.6802    0.3198  1    0.7350    0.2650  0    0.6968    0.3032  1    0.6736    0.3264  1\n",
      "  3    0.6450    0.3550  0    0.7116    0.2884  1    0.7051    0.2949  1    0.6634    0.3366  0    0.6711    0.3289  1    0.6117    0.3883  0    0.6344    0.3656  1    0.6809    0.3191  1    0.6927    0.3073  0    0.6905    0.3095  0\n",
      "  4    0.6454    0.3546  1    0.6851    0.3149  1    0.6031    0.3969  0    0.6912    0.3088  1    0.6743    0.3257  1    0.6302    0.3698  1    0.6466    0.3534  0    0.5520    0.4480  1    0.6047    0.3953  1    0.5761    0.4239  1\n",
      "  5    0.6748    0.3252  1    0.6389    0.3611  0    0.7602    0.2398  1    0.6480    0.3520  1    0.6966    0.3034  1    0.6240    0.3760  0    0.6543    0.3457  1    0.6749    0.3251  1    0.6011    0.3989  0    0.6160    0.3840  1\n",
      "\n",
      " save train checkpoint  to :  model_train_last_ep_250\n",
      " save train metrics to     :  metrics_train_last_ep_250.pickle\n",
      "[Final] ep:250  it:497250 -  Losses:   \t Task: 2.1423   \t Sparsity: 9.96697e-04    \t Sharing: 1.38397e-03    \t Total: 2.1447 \n",
      "\n",
      " ep:  250   logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         logits       s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.4905   -0.2025  1    0.4905   -0.5109  1    0.4905   -0.3503  1    0.4905   -0.4931  1    0.4905   -0.2912  1    0.4905   -0.0962  1    0.4905   -0.1371  1    0.4905   -0.2304  1    0.4905   -0.6016  1    0.4904    0.0001  1\n",
      "  1    0.6562   -0.4889  1    0.6286   -0.5225  1    0.6307   -0.5088  1    0.6285   -0.4442  1    0.6598   -0.5863  1    0.6285   -0.3759  1    0.6285   -0.0384  1    0.6285   -0.0130  1    0.5927   -0.4302  1    0.6285   -0.0548  1\n",
      "  2    0.7313   -0.5413  1    0.8301   -0.3967  1    0.7313   -0.7273  1    0.7045   -0.0796  1    0.7313   -0.4029  1    0.7313   -0.2730  1    0.7313   -0.0233  1    0.7313   -0.2888  1    0.7312   -0.1010  1    0.7313    0.0067  1\n",
      "  3    0.4608   -0.1363  1    0.4982   -0.4049  1    0.4801   -0.3918  1    0.4974   -0.1811  1    0.4972   -0.2160  1    0.4974    0.0430  1    0.4974   -0.0539  1    0.4974   -0.2606  1    0.4974   -0.3153  1    0.4974   -0.3050  1\n",
      "  4    0.3852   -0.2136  1    0.3630   -0.4141  1    0.3249   -0.0935  1    0.3630   -0.4427  1    0.3630   -0.3645  1    0.3631   -0.1701  1    0.3630   -0.2411  1    0.3631    0.1543  1    0.3630   -0.0622  1    0.3630    0.0564  1\n",
      "  5    0.4626   -0.2672  1    0.4626   -0.1080  1    0.4626   -0.6909  1    0.4627   -0.1478  1    0.4839   -0.3473  1    0.4626   -0.0438  1    0.4626   -0.1755  1    0.4775   -0.2531  1    0.4626    0.0528  1    0.4627   -0.0100  1\n",
      "\n",
      "\n",
      " ep:  250    softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s          softmax     s         \n",
      " ----- ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    ----------------- -    \n",
      "  0    0.6666    0.3334  0    0.7313    0.2687  1    0.6986    0.3014  1    0.7278    0.2722  1    0.6860    0.3140  1    0.6426    0.3574  0    0.6520    0.3480  1    0.6728    0.3272  0    0.7488    0.2512  1    0.6202    0.3798  1\n",
      "  1    0.7586    0.2414  1    0.7597    0.2403  0    0.7576    0.2424  0    0.7451    0.2549  1    0.7766    0.2234  1    0.7319    0.2681  1    0.6608    0.3392  0    0.6551    0.3449  1    0.7355    0.2645  1    0.6645    0.3355  1\n",
      "  2    0.7812    0.2188  1    0.7733    0.2267  0    0.8113    0.1887  1    0.6866    0.3134  1    0.7566    0.2434  1    0.7319    0.2681  1    0.6802    0.3198  1    0.7350    0.2650  0    0.6968    0.3032  1    0.6736    0.3264  1\n",
      "  3    0.6450    0.3550  0    0.7116    0.2884  1    0.7051    0.2949  1    0.6634    0.3366  0    0.6711    0.3289  1    0.6117    0.3883  0    0.6344    0.3656  1    0.6809    0.3191  1    0.6927    0.3073  0    0.6905    0.3095  0\n",
      "  4    0.6454    0.3546  1    0.6851    0.3149  1    0.6031    0.3969  0    0.6912    0.3088  1    0.6743    0.3257  1    0.6302    0.3698  1    0.6466    0.3534  0    0.5520    0.4480  1    0.6047    0.3953  1    0.5761    0.4239  1\n",
      "  5    0.6748    0.3252  1    0.6389    0.3611  0    0.7602    0.2398  1    0.6480    0.3520  1    0.6966    0.3034  1    0.6240    0.3760  0    0.6543    0.3457  1    0.6749    0.3251  1    0.6011    0.3989  0    0.6160    0.3840  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight_policy_training(ns, opt, environ, dldrs, epochs = 10, display_policy = True, disable_tqdm = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67fef146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:34:18.944008Z",
     "start_time": "2022-09-02T14:34:18.870734Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 250 | 1.25e-04  1.25e-04  1.25e-03  1.88e-04 |   1.1690   9.724e-04   1.353e-03    1.1713 |  4.130e-06   0.45660   0.66607   0.76266   0.65240   0.69550 |   2.1423   9.967e-04   1.384e-03    2.1447 |  -0.0 |\n",
      "\n",
      "[e] Last ep:250  it:497250  -  Losses:   \t Task: 2.1423   \t Sparsity: 9.96697e-04    \t Sharing: 1.38397e-03    \t Total: 2.1447 \n",
      "\n",
      "   best_epoch:   230   best iter: 456799   best_accuracy: 0.66728    best ROC auc: 0.76332\n"
     ]
    }
   ],
   "source": [
    "print_metrics_cr(ns.current_epoch,  time.time() - time.time() , ns.trn_losses, ns.val_metrics, 1, out=[sys.stdout]) \n",
    "print()\n",
    "print_loss(ns.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter} \")\n",
    "print()\n",
    "print(f'   best_epoch: {ns.best_epoch:5d}   best iter: {ns.best_iter:5d}'\n",
    "      f'   best_accuracy: {ns.best_accuracy:.5f}    best ROC auc: {ns.best_roc_auc:.5f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45f834d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:34:23.755615Z",
     "start_time": "2022-09-02T14:34:23.716842Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights Scheduler Parameters\n",
      "------------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0, 0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 4\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1219009263455093\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 100\n",
      "    _last_lr                 value: [0.000125, 0.000125]\n",
      "\n",
      "Policy Scheduler Parameters\n",
      "-----------------------------\n",
      "    factor                   value: 0.5\n",
      "    min_lrs                  value: [0]\n",
      "    patience                 value: 20\n",
      "    verbose                  value: True\n",
      "    cooldown                 value: 5\n",
      "    cooldown_counter         value: 4\n",
      "    mode                     value: min\n",
      "    threshold                value: 0.0001\n",
      "    threshold_mode           value: rel\n",
      "    best                     value: 2.1244821764992645\n",
      "    num_bad_epochs           value: 0\n",
      "    mode_worse               value: inf\n",
      "    eps                      value: 1e-08\n",
      "    last_epoch               value: 100\n",
      "    _last_lr                 value: [0.00125]\n"
     ]
    }
   ],
   "source": [
    "print_underline('Weights Scheduler Parameters', verbose = True) \n",
    "for k,i in environ.schedulers['weights'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")\n",
    "\n",
    "print_underline('Policy Scheduler Parameters', verbose = True)\n",
    "for k,i in environ.schedulers['alphas'].state_dict().items():\n",
    "    print(f\"    {k:20s}     value: {i}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3ae24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T13:38:02.096634Z",
     "start_time": "2022-08-29T13:38:02.065472Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.schedulers['alphas'].patience = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e42e43",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Close WandB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f6b31da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T14:34:33.774161Z",
     "start_time": "2022-09-02T14:34:28.596477Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>â–â–ƒâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>avg_prec_score</td><td>â–â–ƒâ–„â–„â–„â–„â–„â–„â–„â–ƒâ–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>bceloss</td><td>â–ˆâ–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>best_accuracy</td><td>â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–„â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>best_epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>best_iter</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>best_roc_auc</td><td>â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>f1_max</td><td>â–â–ƒâ–„â–„â–„â–„â–„â–„â–„â–ƒâ–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>gumbel_temp</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>kappa</td><td>â–‚â–‚â–â–â–â–â–â–â–â–â–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>kappa_max</td><td>â–â–ƒâ–„â–„â–„â–„â–„â–„â–„â–ƒâ–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>lambda_sharing</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>lambda_sparsity</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆ</td></tr><tr><td>lambda_tasks</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>logloss</td><td>â–ˆâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>lr_0</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>lr_1</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>p_f1_max</td><td>â–†â–†â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–†â–†â–†â–†â–†â–…â–„â–…â–ƒâ–„â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>p_kappa_max</td><td>â–ˆâ–„â–ƒâ–…â–†â–…â–†â–…â–†â–…â–†â–†â–†â–„â–†â–…â–†â–†â–…â–„â–…â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚</td></tr><tr><td>policy_lr</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–</td></tr><tr><td>roc_auc_score</td><td>â–â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>sc_loss</td><td>â–ˆâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>train_layers</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_pr</td><td>0.6524</td></tr><tr><td>avg_prec_score</td><td>0.66607</td></tr><tr><td>bceloss</td><td>0.4566</td></tr><tr><td>best_roc_auc</td><td>0.76332</td></tr><tr><td>epoch</td><td>250</td></tr><tr><td>f1_max</td><td>0.6955</td></tr><tr><td>gumbel_temp</td><td>0.00019</td></tr><tr><td>kappa</td><td>0.26631</td></tr><tr><td>kappa_max</td><td>0.46433</td></tr><tr><td>lambda_sharing</td><td>0.05</td></tr><tr><td>lambda_sparsity</td><td>0.01</td></tr><tr><td>lambda_tasks</td><td>1.0</td></tr><tr><td>logloss</td><td>0.0</td></tr><tr><td>lr_0</td><td>0.00013</td></tr><tr><td>lr_1</td><td>0.00013</td></tr><tr><td>p_f1_max</td><td>0.33835</td></tr><tr><td>p_kappa_max</td><td>0.43202</td></tr><tr><td>policy_lr</td><td>0.00125</td></tr><tr><td>roc_auc_score</td><td>0.76266</td></tr><tr><td>sc_loss</td><td>0.00317</td></tr><tr><td>train_layers</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">0829_2050</strong>: <a href=\"https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/154b30qo\" target=\"_blank\">https://wandb.ai/kbardool/AdaSparseChem-cb29-10Task/runs/154b30qo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220829_205037-154b30qo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b05db1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Misc Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb846fd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58eeb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:32.253924Z",
     "start_time": "2022-03-28T05:13:32.221329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ns.best_epoch = 0\n",
    "# from utils.notebook_modules import wrapup_phase\n",
    "# wrapup_phase(ns, opt, environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db994f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T05:13:32.307351Z",
     "start_time": "2022-03-28T05:13:32.262822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['policy_lr']       = 0.002\n",
    "# environ.opt['train']['lambda_sparsity'] = 0.05\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['lambda_tasks']    = 1.0\n",
    "# # environ.opt['train']['decay_temp_freq'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc42fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:50.458223Z",
     "start_time": "2022-01-27T00:58:50.430889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(opt['diff_sparsity_weights'])\n",
    "print(opt['is_sharing'])\n",
    "print(opt['diff_sparsity_weights'] and not opt['is_sharing'])\n",
    "print(environ.opt['train']['Lambda_sharing'])\n",
    "print(opt['train']['Lambda_sharing'])\n",
    "print(environ.opt['train']['Lambda_sparsity'])\n",
    "print(opt['train']['Lambda_sparsity'])\n",
    "print(environ.opt['train']['policy_lr'])\n",
    "print(opt['train']['policy_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8a246",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-12T07:35:36.625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print( f\" Backbone Learning Rate      : {environ.opt['train']['backbone_lr']}\\n\"\n",
    "       f\" Tasks    Learning Rate      : {environ.opt['train']['task_lr']}\\n\"\n",
    "       f\" Policy   Learning Rate      : {environ.opt['train']['policy_lr']}\\n\")\n",
    "\n",
    "print( f\" Sparsity regularization     : {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization     : {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization     : {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                 : {environ.gumbel_temperature:.4f}         \\n\" \n",
    "       f\" Gumbel Temp decay           : {environ.opt['train']['decay_temp_freq']}\\n\") \n",
    "\n",
    "print( f\" current_iters               : {ns.current_iter}   \\n\"\n",
    "       f\" current_epochs              : {ns.current_epoch}  \\n\" \n",
    "       f\" train_total_epochs          : {ns.training_epochs}\\n\" \n",
    "       f\" stop_epoch_training         : {ns.stop_epoch_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be2583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:20:45.610411Z",
     "start_time": "2022-08-18T12:20:45.568020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed489b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:24:06.014379Z",
     "start_time": "2022-08-18T12:24:05.974633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name, param in environ.networks['mtl-net'].named_parameters():\n",
    "    print(f\" {name:40s}  {param.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33193377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:25:02.887603Z",
     "start_time": "2022-08-18T12:25:02.846964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name, param in environ.networks['mtl-net'].backbone.named_parameters():\n",
    "        print(f\" {name:40s}  {param.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6d417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T12:34:17.484632Z",
     "start_time": "2022-08-18T12:34:17.320204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for name, param in environ.networks['mtl-net'].named_parameters():\n",
    "    if 'task' in name and 'fc' in name:    \n",
    "        print(f\" {name:40s}  {param.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62568b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:30:31.940280Z",
     "start_time": "2022-01-26T19:30:31.910058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_blocks = 6\n",
    "num_policy_layers = 6\n",
    "gt =  torch.ones((num_blocks)).long()\n",
    "gt0 =  torch.zeros((num_blocks)).long()\n",
    "print(gt)\n",
    "print(gt0)\n",
    "\n",
    "loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61487657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:42:31.300891Z",
     "start_time": "2022-01-26T19:42:31.257774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if environ.opt['diff_sparsity_weights'] and not environ.opt['is_sharing']:\n",
    "    print(' cond 1')\n",
    "    ## Assign higher weights to higher layers \n",
    "    loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "    print(f\"{task_key} sparsity error:  {2 * (loss_weights[-num_blocks:] * environ.cross_entropy2(logits[-num_blocks:], gt)).mean()})\")\n",
    "    print_dbg(f\" loss_weights :  {loss_weights}\", verbose = True)\n",
    "    print_dbg(f\" cross_entropy:  {environ.cross_entropy2(logits[-num_blocks:], gt)}  \", verbose = True)\n",
    "    print_dbg(f\" loss[sparsity][{task_key}]: {self.losses['sparsity'][task_key] } \", verbose = True)\n",
    "\n",
    "else:\n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between \\n Logits   : \\n{logits[-num_blocks:]} \\n and gt: \\n{gt} \\n\", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-num_blocks:], gt)}\")\n",
    "    \n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt[-1:])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt0[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt0[-1:])} \\n\")\n",
    "    \n",
    "    print('\\n cond 3')    \n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt[0:1])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt0[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt0[0:1])} \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12352a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:02:21.600933Z",
     "start_time": "2022-04-28T10:02:21.561452Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\" ns.check_for_improvment_wait:  {ns.check_for_improvment_wait}\")\n",
    "print(\" ns.curriculum_epochs:          {ns.curriculum_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dad034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:14.782725Z",
     "start_time": "2022-04-28T11:09:14.692205Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.val_metrics)\n",
    "df = environ.val_metrics['task1']['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27ebed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:15.186827Z",
     "start_time": "2022-04-28T11:09:15.090906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df[pd.notna(df.roc_auc_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4e2d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:09:44.692326Z",
     "start_time": "2022-04-28T11:09:44.611694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[pd.notna(df.roc_auc_score)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a1a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:22.031664Z",
     "start_time": "2022-06-15T17:39:21.964660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.display_trained_policy(ns.current_epoch,out=[sys.stdout])\n",
    "# environ.num_tasks\n",
    "# print(environ.get_policy_prob().shape)\n",
    "# print(environ.val_data['task1'].keys())\n",
    "# print(environ.val_data['task1']['yc_ind'][0][:40])\n",
    "# print(environ.val_data['task1']['yc_ind'][1][:40])\n",
    "# print(environ.val_data['task1']['yc_data'][:40])\n",
    "# print(environ.val_data['task1']['yc_hat'][:40])\n",
    "# environ.display_trained_policy(ns.current_epoch,out=[sys.stdout])\n",
    "# environ.display_trained_logits(ns.current_epoch,out=[sys.stdout])\n",
    "batch = next(dldrs.warmup_trn_loader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f163b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:39:22.757684Z",
     "start_time": "2022-06-15T17:39:22.679466Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd33b3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf98b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:37:56.954474Z",
     "start_time": "2022-08-29T10:37:56.900806Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\" val_metric keys               : {ns.val_metrics.keys()}\")\n",
    "print(f\" aggreagted keys               : {ns.val_metrics['aggregated'].keys()}\")\n",
    "print(f\" task keys                     : {ns.val_metrics['task'].keys()}\")\n",
    "print(f\" task / task1 keys             : {ns.val_metrics['task']['task1']}\")\n",
    "print(f\" sparsity keys                 : {ns.val_metrics['sparsity'].keys()}\")\n",
    "print(f\" total keys                    : {ns.val_metrics['total'].keys()}\")\n",
    "print(f\" aggregated keys               : {ns.val_metrics['aggregated'].keys()}\")\n",
    "print()\n",
    "print(f\" task1 keys                    : {ns.val_metrics['task1'].keys()}\")\n",
    "print(f\" task1 classification keys     : {ns.val_metrics['task1']['classification'].keys()}\")\n",
    "print(f\" task1 classification_agg keys : {ns.val_metrics['task1']['classification_agg'].keys()}\")\n",
    "\n",
    "print()\n",
    "print(f\" task1 agg sc_loss             : {ns.val_metrics['task1']['classification_agg']['sc_loss']:5f}\")\n",
    "print(f\" task1 agg bce_loss            : {ns.val_metrics['task1']['classification_agg']['bceloss']:5f}\")\n",
    "print(f\" task1 agg bce_loss            : {ns.val_metrics['task1']['classification_agg']['logloss']:5f}\")\n",
    "print(f\" task-task1                    : {ns.val_metrics['task']['task1']:5f}\")\n",
    "print(f\" task-task1                    : \\n  {ns.val_metrics['task1']['classification']}\")\n",
    "print(f\" task-task1                    : \\n  {ns.val_metrics['task1']['classification_agg']}\")\n",
    "\n",
    "print()\n",
    "print(f\" task2                         : {ns.val_metrics['task2']['classification_agg']['sc_loss']:5f}\")\n",
    "print(f\" task3                         : {ns.val_metrics['task3']['classification_agg']['sc_loss']:5f}\")\n",
    "print(f\" loss                          : {ns.val_metrics['loss']['total']:5f}\")\n",
    "print(f\" train_time                    : {ns.val_metrics['train_time']:2f}\")\n",
    "print(f\" epoch                         : {ns.val_metrics['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c09f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:44:38.417573Z",
     "start_time": "2022-08-29T10:44:38.383420Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.batch_data['task1']['yc_trn_weights'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960c48b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:19:43.683550Z",
     "start_time": "2022-01-08T01:19:43.571450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = environ.get_loss_dict()\n",
    "print(tmp.keys())\n",
    "pp.pprint(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90151319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T09:12:32.452187Z",
     "start_time": "2022-04-13T09:12:32.420905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "type(ns.val_metrics['aggregated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c031eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:13:48.522436Z",
     "start_time": "2022-08-29T10:13:48.375346Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(ns.trn_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48149772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:16:26.685693Z",
     "start_time": "2022-08-29T10:16:26.367212Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6b5fa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc10ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:46:04.669904Z",
     "start_time": "2022-08-29T10:46:04.636155Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.val_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df018e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:51:15.390154Z",
     "start_time": "2022-08-29T10:51:15.214311Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.val_loader.dataset.y_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45fcf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:31:55.581510Z",
     "start_time": "2022-04-02T13:31:55.526855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(environ.val_data['task1']['yc_data'][0] == environ.val_data['task1']['yc_data']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02211ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T14:20:55.327255Z",
     "start_time": "2022-04-02T14:20:55.026238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.sparsechem_utils import compute_metrics, aggregate_results\n",
    "import pandas\n",
    "cc = compute_metrics(cols   = environ.val_data['task1']['yc_ind'][1], \n",
    "                     y_true = environ.val_data['task1']['yc_data'], \n",
    "                     y_score= environ.val_data['task1']['yc_hat'] ,\n",
    "                     num_tasks=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a5712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:34:57.196163Z",
     "start_time": "2022-04-02T13:34:57.130013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " df   = pd.DataFrame({\"task\"   : environ.val_data['task1']['yc_ind'][1], \n",
    "                      \"y_true\" : environ.val_data['task1']['yc_data'],  \n",
    "                      \"y_score\": environ.val_data['task1']['yc_hat']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde23676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:44:52.754320Z",
     "start_time": "2022-04-02T13:44:52.611945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for task, frame in df.groupby(\"task\", sort=True):\n",
    "    print(f\" task {task}\")\n",
    "    print(frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887a79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T13:46:29.715440Z",
     "start_time": "2022-04-02T13:46:29.640674Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df\n",
    "df.groupby(\"task\", sort=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862488b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T11:10:20.301689Z",
     "start_time": "2022-04-28T11:10:20.151621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e466147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T14:26:58.189057Z",
     "start_time": "2022-04-02T14:26:58.126134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.batch_data['task1']['yc_aggr_weights'])\n",
    "environ.batch['task1']['aggr_weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007f28d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c2 = aggregate_results(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4570a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T17:11:11.578048Z",
     "start_time": "2022-04-02T17:11:11.535763Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dldrs.trainset0.tasks_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82a453",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Post Warm-up Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f0235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:37:10.158440Z",
     "start_time": "2022-01-28T16:37:09.742327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_all_task_logits\n",
    "    \"p = environ.get_sample_policy(hard_sampling = False)\\n\"print(p)\n",
    "p = environ.get_policy_prob()\n",
    "print(p)\n",
    "p = environ.get_policy_logits()\n",
    "print(p)\n",
    "\n",
    "# p = environ.get_current_policy()\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bddd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:40:37.799917Z",
     "start_time": "2022-01-28T16:40:37.773177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = softmax([0.0, 1])\n",
    "print(a)\n",
    "sampled = np.random.choice((1, 0), p=a)\n",
    "print(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8f376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:13:16.205889Z",
     "start_time": "2022-01-28T16:13:16.179303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.optimizers['weights'])\n",
    "print(environ.schedulers['weights'].get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf9c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:46:50.411465Z",
     "start_time": "2022-02-01T12:46:50.020540Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('losses.keys      : ', environ.losses.keys())\n",
    "print('losses[task]keys : ', environ.losses['task1'].keys())\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T22:57:02.151169Z",
     "start_time": "2022-01-15T22:57:02.056562Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( environ.val_metrics.keys())\n",
    "# pp.pprint(val_metrics)\n",
    "print(type(environ.val_metrics['aggregated']))\n",
    "print()\n",
    "print(type(environ.val_metrics['task1']['classification_agg']))\n",
    "print()\n",
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2d510",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Policy / Logit stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb628497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:35:14.041577Z",
     "start_time": "2022-02-08T20:35:14.018303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.special          import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eed454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:30.103364Z",
     "start_time": "2022-02-08T20:00:30.068021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b5cc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_task_logits(n)` Get logits for task group n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed8b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:56:35.652087Z",
     "start_time": "2022-02-08T20:56:35.327406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task_logits = environ.get_task_logits(1)\n",
    "print(task_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66fa5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_arch_parameters()`: Get last used logits from network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85521e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:31.101960Z",
     "start_time": "2022-02-08T20:00:30.757064Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b0bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:06.054699Z",
     "start_time": "2022-02-09T15:43:05.689327Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871ee38",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_logits()`:  Get Policy Logits - returns same as `get_arch_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb40c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:24.972390Z",
     "start_time": "2022-02-09T15:43:24.636629Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = environ.get_policy_logits()\n",
    "for i in logs:\n",
    "    print(i, '\\n')\n",
    "# probs = softmax(logs, axis= -1)\n",
    "# for i in probs:\n",
    "#     print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080a364",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_prob()` : Gets the softmax of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c75af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:29.733732Z",
     "start_time": "2022-02-09T15:43:29.699600Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "for i in policy_softmaxs:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3160d9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = False)` : Calls test_sample_policy of network with random choices based on softmax of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:21:15.636722Z",
     "start_time": "2022-02-08T22:21:15.165456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "policies,logits = environ.get_sample_policy(hard_sampling = False)\n",
    "\n",
    "for l, p, s in zip(logits, policies, policy_softmaxs) :\n",
    "    for  l_row, p_row, s_row in zip(l, p, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802664ec",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = True)` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea65bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:59:40.790899Z",
     "start_time": "2022-02-08T20:59:40.726657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "hard_policies, logits = environ.get_sample_policy(hard_sampling = True)\n",
    "\n",
    "for p,l,s in zip(hard_policies, logits, policy_softmaxs) :\n",
    "    for  p_row, l_row, s_row in zip(p, l, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c39cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:00:47.452220Z",
     "start_time": "2022-02-08T21:00:47.422902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" Layer    task 1      task 2      task 3\")\n",
    "print(f\" -----    ------      ------      ------\")\n",
    "for idx, (l1, l2, l3) in enumerate(zip(hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1}       {l2}       {l3}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade0ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:39:39.936555Z",
     "start_time": "2022-02-08T22:39:39.911591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_trained_policy(iter):\n",
    "\n",
    "    policy_softmaxs = environ.get_policy_prob()\n",
    "    policy_argmaxs = 1-np.argmax(policy_softmaxs, axis = -1)\n",
    "    print(f\"  Trained polcies at iteration: {iter} \")\n",
    "    print(f\"                   task 1                           task 2                         task 3        \")\n",
    "    print(f\" Layer       softmax        select          softmax        select          softmax        select   \")\n",
    "    print(f\" -----    ---------------   ------       ---------------   ------       ---------------   ------   \")\n",
    "    for idx, (l1,l2,l3,  p1,p2,p3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2], policy_argmaxs[0], policy_argmaxs[1], policy_argmaxs[2]),1):\n",
    "        print(f\"   {idx}      {l1[0]:.4f}   {l1[1]:.4f}   {p1:4d}    {l2[0]:11.4f}   {l2[1]:.4f}   {p2:4d}    {l3[0]:11.4f}   {l3[1]:.4f}   {p3:4d}\")\n",
    "\n",
    "    print()\n",
    "# print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec208dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:42:12.650813Z",
     "start_time": "2022-02-08T22:42:12.330169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_trained_policy(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec517e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:07:30.836214Z",
     "start_time": "2022-02-08T22:07:30.804575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"                        POLICIES (SOFTMAX)                                       task 3          \")\n",
    "print(f\" Layer    task1              task2            task3 softmax         softmax         argmax         softmax         argmax   \")\n",
    "print(f\" -----    -------------     -------------     -------------   ------   \")\n",
    "for idx, (l1,l2,l3, h1,h2,h3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2],hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1[0]:.4f} {l1[1]:.4f}     {l2[0]:.4f} {l2[1]:.4f}     {l3[0]:.4f} {l3[1]:.4f}    {h3}\")\n",
    "    \n",
    "print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbeacb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:04:50.757406Z",
     "start_time": "2022-02-08T22:04:50.731736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(policy_softmaxs[2], np.argmax(1-policy_softmaxs[2], axis = -1))\n",
    "print(policy_softmaxs, np.argmax(policy_softmaxs, axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0240",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_current_logits()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb7240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:19:06.155425Z",
     "start_time": "2022-02-08T21:19:06.118640Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits  = (environ.get_current_logits())\n",
    "for i in logits:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84662",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_current_policy()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cfa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:40:06.543376Z",
     "start_time": "2022-02-08T20:40:06.230711Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pols  = (environ.get_current_policy())\n",
    "\n",
    "for i in pols:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f556a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:27:33.593255Z",
     "start_time": "2022-01-27T18:27:33.553141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792710e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `gumbel_softmax()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265490e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:28:17.107529Z",
     "start_time": "2022-02-08T21:28:17.084910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', floatmode = 'maxprec_equal')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb0087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:35:11.617269Z",
     "start_time": "2022-02-08T21:35:11.569599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.temp)\n",
    "# tau = environ.temp\n",
    "tau = 1\n",
    "for i in range(3): \n",
    "    logits_tensor = torch.tensor(logits[0])\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=tau, hard=False).cpu().numpy() \n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=tau, hard=True).cpu().numpy()\n",
    "    \n",
    "    for l, gs, gh in zip(lgts, gumbel_soft, gumbel_hard):\n",
    "        print(f\"   {l}   \\t {gs}            \\t {gh}\")\n",
    "#     print(lgts)\n",
    "#     print(gumbel_soft)\n",
    "#     print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5ef7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e0e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:21:35.524957Z",
     "start_time": "2022-02-08T21:21:35.488812Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for lgts in logits:\n",
    "    logits_tensor = torch.tensor(lgts)\n",
    "    print(lgts)\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=1, hard=False)\n",
    "    print(gumbel_soft)\n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=1, hard=True)\n",
    "    print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34a06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T20:49:37.643349Z",
     "start_time": "2022-02-03T20:49:37.580786Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "smax = scipy.special.softmax(logs, axis =1)\n",
    "# smax = np.array( \n",
    "# [[0.46973792, 0.530262  ],\n",
    "#  [0.45025694, 0.549743  ],\n",
    "#  [0.4443086 , 0.5556915 ],\n",
    "#  [0.4138397 , 0.58616036],\n",
    "#  [0.4140113 , 0.5859887 ],\n",
    "#  [0.42114905, 0.57885087]])\n",
    "\n",
    "print(smax.shape)\n",
    "print(smax)\n",
    "print(smax[0])\n",
    "print(smax[0].sum())\n",
    "print(np.random.choice((1,0), p =smax[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7de25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:15:19.893888Z",
     "start_time": "2022-02-08T21:15:19.870899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logs = np.array(\n",
    "[[0.33064184, 0.42053092],\n",
    " [0.3532089 , 0.52056104],\n",
    " [0.3888512 , 0.5680909 ],\n",
    " [0.42039296, 0.694217  ],\n",
    " [0.4519742 , 0.73311865],\n",
    " [0.48401102, 0.7522658 ]],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:pyt-gpu]",
   "language": "python",
   "name": "conda-env-pyt-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "399px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "888d4fda4588b3bfc9793c8a97c6f83877963bb7385ca7ca0c08738cf63adc49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
