{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff5690a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:33.721061Z",
     "start_time": "2021-11-04T15:52:33.687864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d70846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:35.641469Z",
     "start_time": "2021-11-04T15:52:33.723485Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataloaders.nyu_v2_dataloader import NYU_v2\n",
    "# from dataloaders.cityscapes_dataloader import CityScapes\n",
    "# from dataloaders.taskonomy_dataloader import Taskonomy\n",
    "from envs.blockdrop_env import BlockDropEnv\n",
    "\n",
    "from utils.util import makedir, print_separator, create_path, print_yaml, should, fix_random_seed, read_yaml_from_input, timestring\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from models.deeplab_resnet import MTL2, MTL_Instance\n",
    "# from models.base import Bottleneck, BasicBlock\n",
    "# from scipy.special import softmax\n",
    "# import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# from models.base import Bottleneck, BasicBlock\n",
    "\n",
    "import torch.nn.functional as F\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cc539a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.312932Z",
     "start_time": "2021-11-04T15:52:35.644665Z"
    }
   },
   "outputs": [],
   "source": [
    "from dev.train_dev import training_validation\n",
    "from dev.MTL2_Dev import MTL2_Dev\n",
    "from dev.blockdrop_env_dev import BlockDropEnv_Dev\n",
    "from dev.deeplab_resnet_dev import Deeplab_ResNet_Backbone_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dae599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T22:05:17.141840Z",
     "start_time": "2021-09-23T22:05:17.085995Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d1a780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.334989Z",
     "start_time": "2021-11-04T15:52:36.315334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--config', 'yamls/adashare/nyu_v2_2task.yml', '--cpu']\n"
     ]
    }
   ],
   "source": [
    "# input_args = \" --config  yamls/adashare/nyu_v2_3task_test.yml --cpu \".split()\n",
    "input_args = \" --config yamls/adashare/nyu_v2_2task.yml --cpu \".split()\n",
    "# pretrained_path = \"/mnt/f/models_adashare/NYU_v2_3Task_pretrain\"\n",
    "print(input_args)\n",
    "# print(pretrained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf604f8",
   "metadata": {},
   "source": [
    "##  Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2211367",
   "metadata": {},
   "source": [
    "### Create folders and print options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c42f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.397951Z",
     "start_time": "2021-11-04T15:52:36.337941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "####################READ YAML#####################\n",
      "##################################################\n",
      "{'config': 'yamls/adashare/nyu_v2_2task.yml', 'exp_ids': [0], 'gpus': [0], 'cpu': True}\n",
      " Create folder ../experiments/logs/nyu_v2_2task\n",
      " Create folder ../experiments/results/nyu_v2_2task\n",
      " Create folder ../experiments/checkpoints/nyu_v2_2task\n",
      "\n",
      "exp_name. : nyu_v2_2task\n",
      "seed. : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "backbone. : ResNet34\n",
      "original_backbone. : ResNet18\n",
      "tasks. : ['seg', 'sn']\n",
      "lambdas. : [1, 20]\n",
      "tasks_num_class. : [40, 3]\n",
      "policy_model. : task-specific\n",
      "paths.\n",
      "paths.log_dir. : ../experiments/logs/\n",
      "paths.result_dir. : ../experiments/results\n",
      "paths.checkpoint_dir. : ../experiments/checkpoints\n",
      "dataload.\n",
      "dataload.dataset. : NYU_v2\n",
      "dataload.dataroot. : /mnt/f/MLDatasets/nyu_v2\n",
      "dataload.orig_dataroot. : /data/datasets/nyu_v2\n",
      "dataload.crop_h. : 321\n",
      "dataload.crop_w. : 321\n",
      "policy. : True\n",
      "init_neg_logits. : None\n",
      "is_sparse. : True\n",
      "is_sharing. : True\n",
      "skip_layer. : 0\n",
      "is_curriculum. : True\n",
      "curriculum_speed. : 3\n",
      "fix_BN. : False\n",
      "diff_sparsity_weights. : True\n",
      "retrain_from_pl. : False\n",
      "train.\n",
      "train.batch_size. : 32\n",
      "train.total_iters. : 20000\n",
      "train.warm_up_iters. : 4000\n",
      "train.lr. : 0.001\n",
      "train.policy_lr. : 0.01\n",
      "train.backbone_lr. : 0.001\n",
      "train.reg_w. : 0.05\n",
      "train.reg_w_hamming. : 0.05\n",
      "train.print_freq. : 100\n",
      "train.val_freq. : 400\n",
      "train.decay_lr_freq. : 4000\n",
      "train.decay_lr_rate. : 0.5\n",
      "train.decay_temp_freq. : 100\n",
      "train.init_temp. : 5\n",
      "train.decay_temp. : 0.965\n",
      "train.resume. : False\n",
      "train.retrain_resume. : False\n",
      "train.policy_iter. : best\n",
      "train.which_iter. : warmup\n",
      "train.init_method. : equal\n",
      "train.hard_sampling. : False\n",
      "test.\n",
      "test.which_iter. : best\n",
      "cpu. : True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbardool/git-local/AdaShare/utils/util.py:258: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  opt = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************\n",
    "# ****************** create folders and print options ****************\n",
    "# ********************************************************************\n",
    "# read the yaml\n",
    "print_separator('READ YAML')\n",
    "opt, gpu_ids, _ = read_yaml_from_input(input_args)\n",
    "fix_random_seed(opt[\"seed\"][0])\n",
    "create_path(opt)\n",
    "# print yaml on the screen\n",
    "lines = print_yaml(opt)\n",
    "for line in lines: print(line)\n",
    "    \n",
    "# print to file\n",
    "with open(os.path.join(opt['paths']['log_dir'], opt['exp_name'], 'opt.txt'), 'w+') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd3ebc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.425691Z",
     "start_time": "2021-11-04T15:52:36.401067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'NYU_v2',\n",
       " 'dataroot': '/mnt/f/MLDatasets/nyu_v2',\n",
       " 'orig_dataroot': '/data/datasets/nyu_v2',\n",
       " 'crop_h': 321,\n",
       " 'crop_w': 321}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt['dataload']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b2930",
   "metadata": {},
   "source": [
    "### Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30085b26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.780939Z",
     "start_time": "2021-11-04T15:52:36.430226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "################CREATE DATALOADERS################\n",
      "##################################################\n",
      "NYU_v2\n",
      "NYU_v2\n",
      "NYU_v2\n",
      "NYU_v2\n"
     ]
    }
   ],
   "source": [
    "    # ********************************************************************\n",
    "    # ******************** Prepare the dataloaders ***********************\n",
    "    # ********************************************************************\n",
    "    # load the dataloader\n",
    "    print_separator('CREATE DATALOADERS')\n",
    "    \n",
    "    if opt['dataload']['dataset'] == 'NYU_v2':\n",
    "        # To warm up\n",
    "        trainset  = NYU_v2(opt['dataload']['dataroot'], 'train', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To update the network parameters\n",
    "        trainset1 = NYU_v2(opt['dataload']['dataroot'], 'train1', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To update the policy weights\n",
    "        trainset2 = NYU_v2(opt['dataload']['dataroot'], 'train2', opt['dataload']['crop_h'], opt['dataload']['crop_w'])\n",
    "        # To validate\n",
    "        valset    = NYU_v2(opt['dataload']['dataroot'], 'test')\n",
    " \n",
    "    else:\n",
    "        raise NotImplementedError('Dataset %s is not implemented' % opt['dataload']['dataset'])\n",
    "## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8d4f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.808210Z",
     "start_time": "2021-11-04T15:52:36.784557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valset.__dict__['triples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494aeadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.836485Z",
     "start_time": "2021-11-04T15:52:36.812533Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n",
    "# print(IMG_MEAN.shape, IMG_MEAN)\n",
    "# print(IMG_MEAN[np.newaxis, np.newaxis, :].shape, IMG_MEAN[np.newaxis, np.newaxis, :])\n",
    "# IMG_MEAN = np.tile(IMG_MEAN[np.newaxis, np.newaxis, :], (480, 640,1))\n",
    "# print(IMG_MEAN.shape) \n",
    "# print(IMG_MEAN[:5,:5,0])\n",
    "# print(IMG_MEAN[:5,:5,1])\n",
    "# print(IMG_MEAN[:5,:5,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d547124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.866155Z",
     "start_time": "2021-11-04T15:52:36.840224Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(trainset , batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "train1_loader = DataLoader(trainset1, batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "train2_loader = DataLoader(trainset2, batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=True)\n",
    "val_loader    = DataLoader(valset   , batch_size=opt['train']['batch_size'], drop_last=True, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5874bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.893281Z",
     "start_time": "2021-11-04T15:52:36.870172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 4\n"
     ]
    }
   ],
   "source": [
    "opt['train']['weight_iter_alternate'] = opt['train'].get('weight_iter_alternate', len(train1_loader))\n",
    "opt['train']['alpha_iter_alternate'] = opt['train'].get('alpha_iter_alternate'  , len(train2_loader))\n",
    "\n",
    "print(opt['train']['weight_iter_alternate'], opt['train']['alpha_iter_alternate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a891e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:49:33.553156Z",
     "start_time": "2021-09-22T22:49:33.532706Z"
    }
   },
   "source": [
    "When Batch Size == 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c92345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.932824Z",
     "start_time": "2021-11-04T15:52:36.896247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set 0 (warm up)       :  795\n",
      "size of training set 1 (network parms) :  636\n",
      "size of training set 2 (policy weights):  159\n",
      "size of validation set                 :  654\n",
      "                               Total   :  2244\n",
      " batch size       : 32\n",
      " len train_loader : 24\n",
      " len train1_loader: 19\n",
      " len train2_loader: 4\n",
      " len val_loader   : 20\n"
     ]
    }
   ],
   "source": [
    "print('size of training set 0 (warm up)       : ', len(trainset))\n",
    "print('size of training set 1 (network parms) : ', len(trainset1))\n",
    "print('size of training set 2 (policy weights): ', len(trainset2))\n",
    "print('size of validation set                 : ', len(valset))\n",
    "print('                               Total   : ', len(trainset)+len(trainset1)+len(trainset2)+len(valset))\n",
    "\n",
    "print(f\" batch size       : {opt['train']['batch_size']}\")\n",
    "print(f' len train_loader : {len(train_loader)}')\n",
    "print(f' len train1_loader: {len(train1_loader)}')\n",
    "print(f' len train2_loader: {len(train2_loader)}')\n",
    "print(f' len val_loader   : {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a03100",
   "metadata": {},
   "source": [
    "###  Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8f298f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:36.971536Z",
     "start_time": "2021-11-04T15:52:36.937224Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " backbone                : ResNet34 \n",
      " paths.log_dir           : ../experiments/logs/ \n",
      " paths.checkpoint_dir    : ../experiments/checkpoints \n",
      " experiment name         : nyu_v2_2task \n",
      " tasks_num_class         : ([40, 3],) \n",
      " init_neg_logits         : (None,) \n",
      " device id               : 0 \n",
      " init temp               : (5,) \n",
      " decay temp              : 0.965 \n",
      " fix BN parms            : False \n",
      " skip_layer              : 0 \n",
      " train.init_method       : equal\n",
      " Total iterations        : 30\n",
      " Warm-up iterations      : 4\n",
      " Print Frequency         : 1\n",
      " Validation Frequency    : 2\n",
      " Weight iter alternate   : 19\n",
      " Alpha  iter alternate   : 4\n"
     ]
    }
   ],
   "source": [
    "opt['train']['total_iters']   = 30\n",
    "opt['train']['warm_up_iters'] = 4\n",
    "opt['train']['print_freq']    = 1\n",
    "opt['train']['val_freq']      = 2\n",
    "\n",
    "print( \n",
    "    f\"\\n backbone                : {opt['backbone']}\",\n",
    "    f\"\\n paths.log_dir           : {opt['paths']['log_dir']}\", \n",
    "    f\"\\n paths.checkpoint_dir    : {opt['paths']['checkpoint_dir']}\", \n",
    "    f\"\\n experiment name         : {opt['exp_name']}\",\n",
    "    f\"\\n tasks_num_class         : {opt['tasks_num_class'],}\",\n",
    "    f\"\\n init_neg_logits         : {opt['init_neg_logits'],}\",\n",
    "    f\"\\n device id               : {gpu_ids[0]}\",\n",
    "    f\"\\n init temp               : {opt['train']['init_temp'],}\",\n",
    "    f\"\\n decay temp              : {opt['train']['decay_temp']}\",\n",
    "    f\"\\n fix BN parms            : {opt['fix_BN']}\",\n",
    "    f\"\\n skip_layer              : {opt['skip_layer']}\",\n",
    "    f\"\\n train.init_method       : {opt['train']['init_method']}\")\n",
    "print(f\" Total iterations        : {opt['train']['total_iters']}\")\n",
    "print(f\" Warm-up iterations      : {opt['train']['warm_up_iters']}\")\n",
    "print(f\" Print Frequency         : {opt['train']['print_freq']}\")\n",
    "print(f\" Validation Frequency    : {opt['train']['val_freq']}\")\n",
    "print(f\" Weight iter alternate   : {opt['train']['weight_iter_alternate'] }\")\n",
    "print(f\" Alpha  iter alternate   : {opt['train']['alpha_iter_alternate'] }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c567bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:37.002397Z",
     "start_time": "2021-11-04T15:52:36.974282Z"
    }
   },
   "outputs": [],
   "source": [
    "from dev.MTL2_Dev import MTL2_Dev\n",
    "from dev.blockdrop_env_dev import BlockDropEnv_Dev\n",
    "from dev.sparsechem_env_dev import SparseChemEnv_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c439a3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:37.025101Z",
     "start_time": "2021-11-04T15:52:37.005158Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# del environ\n",
    "# del BlockDropEnv_Dev\n",
    "# del MTL2_Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a7794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b635215f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:37.048679Z",
     "start_time": "2021-11-04T15:52:37.028239Z"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "# environ = SparseChemEnv_Dev(opt['paths']['log_dir'], \n",
    "#                        opt['paths']['checkpoint_dir'], \n",
    "#                        opt['exp_name'],\n",
    "#                        opt['tasks_num_class'], \n",
    "#                        opt['init_neg_logits'], \n",
    "#                        gpu_ids[0],\n",
    "#                        opt['train']['init_temp'], \n",
    "#                        opt['train']['decay_temp'], \n",
    "#                        is_train=True, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba66cc00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:37.860667Z",
     "start_time": "2021-11-04T15:52:37.051660Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "##############CREATE THE ENVIRONMENT##############\n",
      "##################################################\n",
      "*  BlockDropEnvDev Initializtion\n",
      "\n",
      " log_dir        :  ../experiments/logs/ \n",
      " checkpoint_dir :  ../experiments/checkpoints \n",
      " exp_name       :  nyu_v2_2task \n",
      " tasks_num_class:  [40, 3] \n",
      " init_neg_logits:  None \n",
      " gpu device     :  0 \n",
      " init temp      :  5 \n",
      " decay temp     :  0.965 \n",
      " is_train       :  True \n",
      "\n",
      "===============================================================================\n",
      "===============================================================================\n",
      "===============================================================================\n",
      "===============================================================================\n",
      "BlockDropEnvDev .super()\n",
      "\n",
      " log_dir        :  ../experiments/logs/nyu_v2_2task \n",
      " checkpoint_dir :  ../experiments/checkpoints/nyu_v2_2task \n",
      " exp_name       :  nyu_v2_2task \n",
      " tasks_num_class:  [40, 3] \n",
      " device         :  cpu \n",
      " device id      :  0 \n",
      " dataset        :  NYU_v2 \n",
      " tasks          :  ['seg', 'sn'] \n",
      "\n",
      "Create MTL2 with \n",
      " block: <class 'dev.models_base_dev.BasicBlock'> \n",
      " layers: [3, 4, 6, 3] \n",
      " tasks_num_class: [40, 3] \n",
      " init_method: equal\n",
      "\n",
      " block            :  <class 'dev.models_base_dev.BasicBlock'> \n",
      " layers           :  [3, 4, 6, 3] \n",
      " num_classes_tasks:  [40, 3] \n",
      " init_method      :  equal \n",
      " init_neg_logits  :  None \n",
      " skip_layer       :  0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Deeplab_ResNet_Backbone_Dev : making layer 0  filter size: 64  num-blocks in layer:3  strides: 1 dilation: 1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " - BasicBlock Inplanes:64  planes:64   stride:1   dilation:1 \n",
      " - BasicBlock Inplanes:64  planes:64   stride:1   dilation:1 \n",
      " - BasicBlock Inplanes:64  planes:64   stride:1   dilation:1 \n",
      " blocks: <class 'list'>   len:1 | <class 'torch.nn.modules.container.ModuleList'> \n",
      " ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " downsamples : <class 'list'>    len:1 | <class 'NoneType'>     \n",
      " None\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Deeplab_ResNet_Backbone_Dev : making layer 1  filter size: 128  num-blocks in layer:4  strides: 2 dilation: 1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " - BasicBlock Inplanes:64  planes:128   stride:2   dilation:1 \n",
      " - BasicBlock Inplanes:128  planes:128   stride:1   dilation:1 \n",
      " - BasicBlock Inplanes:128  planes:128   stride:1   dilation:1 \n",
      " - BasicBlock Inplanes:128  planes:128   stride:1   dilation:1 \n",
      " blocks: <class 'list'>   len:2 | <class 'torch.nn.modules.container.ModuleList'> \n",
      " ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " downsamples : <class 'list'>    len:2 | <class 'torch.nn.modules.container.Sequential'>     \n",
      " Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Deeplab_ResNet_Backbone_Dev : making layer 2  filter size: 256  num-blocks in layer:6  strides: 1 dilation: 2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " - BasicBlock Inplanes:128  planes:256   stride:1   dilation:2 \n",
      " - BasicBlock Inplanes:256  planes:256   stride:1   dilation:2 \n",
      " - BasicBlock Inplanes:256  planes:256   stride:1   dilation:2 \n",
      " - BasicBlock Inplanes:256  planes:256   stride:1   dilation:2 \n",
      " - BasicBlock Inplanes:256  planes:256   stride:1   dilation:2 \n",
      " - BasicBlock Inplanes:256  planes:256   stride:1   dilation:2 \n",
      " blocks: <class 'list'>   len:3 | <class 'torch.nn.modules.container.ModuleList'> \n",
      " ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (5): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " downsamples : <class 'list'>    len:3 | <class 'torch.nn.modules.container.Sequential'>     \n",
      " Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Deeplab_ResNet_Backbone_Dev : making layer 3  filter size: 512  num-blocks in layer:3  strides: 1 dilation: 4\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " - BasicBlock Inplanes:256  planes:512   stride:1   dilation:4 \n",
      " - BasicBlock Inplanes:512  planes:512   stride:1   dilation:4 \n",
      " - BasicBlock Inplanes:512  planes:512   stride:1   dilation:4 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blocks: <class 'list'>   len:4 | <class 'torch.nn.modules.container.ModuleList'> \n",
      " ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " downsamples : <class 'list'>    len:4 | <class 'torch.nn.modules.container.Sequential'>     \n",
      " Sequential(\n",
      "  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Deeplab Resent Backbone Final :\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " self.blocks: <class 'list'>  len:4 \n",
      "\n",
      " <class 'torch.nn.modules.container.ModuleList'>:  ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      " <class 'torch.nn.modules.container.ModuleList'>:  ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      " <class 'torch.nn.modules.container.ModuleList'>:  ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (4): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (5): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      " <class 'torch.nn.modules.container.ModuleList'>:  ModuleList(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "self.downsamples    : <class 'list'>   len:4 \n",
      "\n",
      " <class 'NoneType'>:  None \n",
      "\n",
      " <class 'torch.nn.modules.container.Sequential'>:  Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ") \n",
      "\n",
      " <class 'torch.nn.modules.container.Sequential'>:  Sequential(\n",
      "  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ") \n",
      "\n",
      " <class 'torch.nn.modules.container.Sequential'>:  Sequential(\n",
      "  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ") \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Task Specific Heads :\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Task 1 :\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "task1_fc1_c0:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "task1_fc1_c1:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "task1_fc1_c2:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "task1_fc1_c3:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      " Task 2 :\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "task2_fc1_c0:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "task2_fc1_c1:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "task2_fc1_c2:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "task2_fc1_c3:  Classification_Module(\n",
      "  (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
      "  (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "\n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      " Reset Logits - num layers : 16\n",
      "------------------------------------------------------------------------------------------------------------------------ -\n",
      " Task: 0  Parm name: task1_logits  shape: torch.Size([16, 2])\n",
      " Task: 0  Parm name: task1_logits  Init Method: equal   task_logits: torch.Size([16, 2])\n",
      " Task: 1  Parm name: task2_logits  shape: torch.Size([16, 2])\n",
      " Task: 1  Parm name: task2_logits  Init Method: equal   task_logits: torch.Size([16, 2])\n",
      "\n",
      " Append policies : \n",
      "\n",
      "-----------------------\n",
      " Define Optimizers    :\n",
      "-----------------------\n",
      "\n",
      "define the weights optimizer - learning mode: non-policy\n",
      " optimizers for weights : \n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "define the logits optimizer (init_method: equal)\n",
      " optimizers for alphas : \n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "\n",
      "-----------------------\n",
      " Define Scheduler     :\n",
      "-----------------------\n",
      "learning mode: non-policy\n",
      "<torch.optim.lr_scheduler.StepLR object at 0x7f283d2520a0>\n",
      "\n",
      "\n",
      " networks       : \n",
      " ----------------\n",
      " {'mtl-net': MTL2_Dev(\n",
      "  (backbone): Deeplab_ResNet_Backbone_Dev(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=True)\n",
      "    (blocks): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ds): ModuleList(\n",
      "      (0): None\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (task1_fc1_c0): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task1_fc1_c1): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task1_fc1_c2): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task1_fc1_c3): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task2_fc1_c0): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task2_fc1_c1): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task2_fc1_c2): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (task2_fc1_c3): Classification_Module(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")}\n",
      "\n",
      "\n",
      " losses         : \n",
      " ----------------\n",
      " {}\n",
      "\n",
      "\n",
      " optimizers     : \n",
      " ----------------\n",
      " {'weights': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      "), 'alphas': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")}\n",
      "\n",
      "\n",
      " schedulers     : \n",
      " ----------------\n",
      " {'weights': <torch.optim.lr_scheduler.StepLR object at 0x7f283d2520a0>}\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************\n",
    "# ********************Create the environment *************************\n",
    "# ********************************************************************\n",
    "# create the model and the pretrain model\n",
    "print_separator('CREATE THE ENVIRONMENT')\n",
    "environ = BlockDropEnv_Dev(opt['paths']['log_dir'], \n",
    "                       opt['paths']['checkpoint_dir'], \n",
    "                       opt['exp_name'],\n",
    "                       opt['tasks_num_class'], \n",
    "                       opt['init_neg_logits'], \n",
    "                       gpu_ids[0],\n",
    "                       opt['train']['init_temp'], \n",
    "                       opt['train']['decay_temp'], \n",
    "                       is_train=True, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a06dc450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:37.889115Z",
     "start_time": "2021-11-04T15:52:37.863352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment successfully created\n"
     ]
    }
   ],
   "source": [
    "print(\"environment successfully created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11e2ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:37.916388Z",
     "start_time": "2021-11-04T15:52:37.892438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "False\n",
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "print(environ.num_tasks)\n",
    "# hasattr(environ, 'policy1')\n",
    "print(hasattr(environ, 'policy1'))\n",
    "# print(environ.networks['mtl-net'])\n",
    "print(environ.networks['mtl-net'].policys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dde1bb",
   "metadata": {},
   "source": [
    "### Training Loop Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56d4d8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:52:48.558522Z",
     "start_time": "2021-11-04T15:52:48.381961Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which_iter   : warmup\n",
      "train_resume :  False\n",
      "[3, 4, 6, 3]\n",
      " Initiate Training \n",
      "cuda not available\n",
      "base_env.cpu()\n",
      " Network item mtl-net moved to cpu\n",
      "blockdrop_env.cpu()\n",
      "\n",
      "\n",
      "-----------------------\n",
      " Define Optimizers    :\n",
      "-----------------------\n",
      "\n",
      "define the weights optimizer - learning mode: non-policy\n",
      " optimizers for weights : \n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "define the logits optimizer (init_method: equal)\n",
      " optimizers for alphas : \n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "\n",
      "-----------------------\n",
      " Define Scheduler     :\n",
      "-----------------------\n",
      "learning mode: non-policy\n",
      "<torch.optim.lr_scheduler.StepLR object at 0x7f282cb4bac0>\n",
      "ResNet34 num_blocks:  18\n",
      "{   'seg': {'Pixel Acc': 0.691, 'mIoU': 0.413},\n",
      "    'sn': {   'Angle 11.25': 49.2,\n",
      "              'Angle 22.5': 76.7,\n",
      "              'Angle 30': 86.8,\n",
      "              'Angle Mean': 15,\n",
      "              'Angle Median': 11.5}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"which_iter   : {opt['train']['which_iter']}\\n\"\n",
    "      f\"train_resume :  {opt['train']['resume']}\")\n",
    "\n",
    "print(environ.networks['mtl-net'].layers)\n",
    "\n",
    "current_iter   = 0\n",
    "current_iter_w = 0 \n",
    "current_iter_a = 0\n",
    "\n",
    "if opt['train']['resume']:\n",
    "    print(' Resume training')\n",
    "    current_iter = environ.load(opt['train']['which_iter'])\n",
    "    environ.networks['mtl-net'].reset_logits()\n",
    "else:\n",
    "    print(' Initiate Training ')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    environ.cuda(gpu_ids)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    environ.cpu()\n",
    "print('\\n')\n",
    "\n",
    "environ.define_optimizer(policy_learning=False)\n",
    "environ.define_scheduler(policy_learning=False)\n",
    "# Fix Alpha - \n",
    "environ.fix_alpha()\n",
    "environ.free_w(opt['fix_BN'])\n",
    "\n",
    "####  continue training preparation\n",
    "\n",
    "batch_enumerator  = enumerate(train_loader)\n",
    "batch_enumerator1 = enumerate(train1_loader)\n",
    "batch_enumerator2 = enumerate(train2_loader)\n",
    "\n",
    "\n",
    "\n",
    "if opt['dataload']['dataset'] == 'NYU_v2':\n",
    "    if len(opt['tasks_num_class']) == 2:\n",
    "        refer_metrics = {'seg': {'mIoU'     : 0.413, \n",
    "                                 'Pixel Acc': 0.691},\n",
    "                         'sn' : {'Angle Mean'  : 15, \n",
    "                                 'Angle Median': 11.5, \n",
    "                                 'Angle 11.25' : 49.2, \n",
    "                                 'Angle 22.5'  : 76.7,\n",
    "                                 'Angle 30'    : 86.8}}\n",
    "    elif len(opt['tasks_num_class']) == 3:\n",
    "        refer_metrics = {'seg': {'mIoU'     : 0.275, \n",
    "                                 'Pixel Acc': 0.589},\n",
    "                         'sn' : {'Angle Mean'  : 17.5, \n",
    "                                 'Angle Median': 14.2, \n",
    "                                 'Angle 11.25' : 34.9, \n",
    "                                 'Angle 22.5'  : 73.3,\n",
    "                                 'Angle 30'    : 85.7},\n",
    "                         'depth':{'abs_err'     : 0.62, \n",
    "                                  'rel_err'     : 0.25, \n",
    "                                  'sigma_1.25'  : 57.9,\n",
    "                                  'sigma_1.25^2': 85.8, \n",
    "                                  'sigma_1.25^3': 95.7}}\n",
    "    else:\n",
    "        raise ValueError('num_class = %d is invalid' % len(opt['tasks_num_class']))\n",
    "else:\n",
    "    raise NotImplementedError('Dataset %s is not implemented' % opt['dataload']['dataset'])\n",
    "\n",
    "flag         = 'update_w'\n",
    "\n",
    "\n",
    "best_value   = 0 \n",
    "best_iter    = 0\n",
    "p_epoch      = 0\n",
    "best_metrics = None\n",
    "flag_warmup  = True\n",
    "\n",
    "if opt['backbone'] == 'ResNet18':\n",
    "    num_blocks = 8\n",
    "elif opt['backbone'] in ['ResNet34', 'ResNet50']:\n",
    "    num_blocks = 18\n",
    "elif opt['backbone'] == 'ResNet101':\n",
    "    num_blocks = 33\n",
    "elif opt['backbone'] == 'WRN':\n",
    "    num_blocks = 15\n",
    "else:\n",
    "    raise ValueError('Backbone %s is invalid' % opt['backbone'])\n",
    "\n",
    "print(opt['backbone'], 'num_blocks: ', num_blocks)\n",
    "\n",
    "\n",
    "pp.pprint(refer_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e369e9",
   "metadata": {},
   "source": [
    "###  Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b33c44d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T16:11:55.007232Z",
     "start_time": "2021-11-04T15:52:50.852181Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "** 2021-11-04 16:52:50:917748 - Training iteration 1 WARMUP iteration: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "** BlockDrop optimize() pass start:  2021-11-04 16:53:10:307529\n",
      "   flag:  update_w   num_train_layers:  None is_policy:  False  hard_sampling: False lambdas:  [1, 20]\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 321, 321])\n",
      "      BlockDrop forward() start:  2021-11-04 16:53:10:313422\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 16:53:10:315605\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 41, 41])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 16:53:59:523335\n",
      "      BlockDrop forward()  end:  2021-11-04 16:53:59:523856\n",
      "   ** BlockDrop get_losses start:  2021-11-04 16:53:59:524038\n",
      "       get_seg_loss:  self.seg_pred:torch.Size([32, 40, 41, 41])   prediction: torch.Size([53792, 40])\n",
      "       seg:torch.Size([32, 1, 41, 41])   seg_resize:torch.Size([32, 1, 41, 41])    gt: torch.Size([53792])\n",
      "       get_seg_loss:  self.sn_pred:torch.Size([32, 3, 41, 41])   prediction: torch.Size([53792, 3])\n",
      "       normal:torch.Size([32, 3, 321, 321])   seg_resize:torch.Size([32, 3, 41, 41])    gt: torch.Size([53792, 3])\n",
      "   ** BlockDrop get_losses end  :  2021-11-04 16:53:59:576783\n",
      "   ** BlockDrop backward_network start:  2021-11-04 16:53:59:576967\n",
      "      backward_network - task seg  loss is :', 1 * 3.7307 = 3.7307\n",
      "      backward_network - task sn  loss is :', 20 * 0.9326 = 18.6526\n",
      "      backward_network - Total loss is    : 22.3833\n",
      "   ** BlockDrop backward_network end  :  2021-11-04 16:55:32:096086\n",
      "** BlockDrop optimize() end  :  2021-11-04 16:55:32:096238\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Iteration  1 Loss : {'seg': {'total': tensor(3.7307)}, 'sn': {'total': tensor(0.9326)}, 'total': {'total': tensor(22.3833)}}\n",
      "-------------------------------------------------------------\n",
      "\n",
      "resize results  torch.Size([32, 40, 41, 41]) to torch.Size([321, 321])\n",
      "resize results  torch.Size([32, 3, 41, 41]) to torch.Size([321, 321])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "** 2021-11-04 16:55:32:822410 - Training iteration 2 WARMUP iteration: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "** BlockDrop optimize() pass start:  2021-11-04 16:55:32:837871\n",
      "   flag:  update_w   num_train_layers:  None is_policy:  False  hard_sampling: False lambdas:  [1, 20]\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 321, 321])\n",
      "      BlockDrop forward() start:  2021-11-04 16:55:32:843808\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 16:55:32:844011\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 41, 41])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 16:56:20:151267\n",
      "      BlockDrop forward()  end:  2021-11-04 16:56:20:152321\n",
      "   ** BlockDrop get_losses start:  2021-11-04 16:56:20:152493\n",
      "       get_seg_loss:  self.seg_pred:torch.Size([32, 40, 41, 41])   prediction: torch.Size([53792, 40])\n",
      "       seg:torch.Size([32, 1, 41, 41])   seg_resize:torch.Size([32, 1, 41, 41])    gt: torch.Size([53792])\n",
      "       get_seg_loss:  self.sn_pred:torch.Size([32, 3, 41, 41])   prediction: torch.Size([53792, 3])\n",
      "       normal:torch.Size([32, 3, 321, 321])   seg_resize:torch.Size([32, 3, 41, 41])    gt: torch.Size([53792, 3])\n",
      "   ** BlockDrop get_losses end  :  2021-11-04 16:56:20:182553\n",
      "   ** BlockDrop backward_network start:  2021-11-04 16:56:20:182729\n",
      "      backward_network - task seg  loss is :', 1 * 35.2072 = 35.2072\n",
      "      backward_network - task sn  loss is :', 20 * 0.0705 = 1.4091\n",
      "      backward_network - Total loss is    : 36.6163\n",
      "   ** BlockDrop backward_network end  :  2021-11-04 16:58:24:278072\n",
      "** BlockDrop optimize() end  :  2021-11-04 16:58:24:278998\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Iteration  2 Loss : {'seg': {'total': tensor(35.2072)}, 'sn': {'total': tensor(0.0705)}, 'total': {'total': tensor(36.6163)}}\n",
      "-------------------------------------------------------------\n",
      "\n",
      "resize results  torch.Size([32, 40, 41, 41]) to torch.Size([321, 321])\n",
      "resize results  torch.Size([32, 3, 41, 41]) to torch.Size([321, 321])\n",
      "**  2021-11-04 16:58:24:973317  START VALIDATION iteration: 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 2021-11-04 16:58:37:410596  VALIDATION iteration (0)\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 480, 640])\n",
      "      BlockDrop forward() start:  2021-11-04 16:58:37:411842\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 16:58:37:412949\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 61, 81])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 17:00:52:309272\n",
      "      BlockDrop forward()  end:  2021-11-04 17:00:52:310908\n",
      "resize results  torch.Size([32, 40, 61, 81]) to torch.Size([480, 640])\n",
      "resize results  torch.Size([32, 3, 61, 81]) to torch.Size([480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbardool/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "  5%|▌         | 1/20 [02:40<50:58, 160.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 2021-11-04 17:01:05:940550  VALIDATION iteration (1)\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 480, 640])\n",
      "      BlockDrop forward() start:  2021-11-04 17:01:05:964257\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 17:01:05:964357\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 61, 81])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 17:03:27:997904\n",
      "      BlockDrop forward()  end:  2021-11-04 17:03:27:999058\n",
      "resize results  torch.Size([32, 40, 61, 81]) to torch.Size([480, 640])\n",
      "resize results  torch.Size([32, 3, 61, 81]) to torch.Size([480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbardool/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      " 10%|█         | 2/20 [05:17<47:27, 158.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 2021-11-04 17:03:42:219159  VALIDATION iteration (2)\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 480, 640])\n",
      "      BlockDrop forward() start:  2021-11-04 17:03:42:245084\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 17:03:42:245352\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 61, 81])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 17:06:05:870225\n",
      "      BlockDrop forward()  end:  2021-11-04 17:06:05:871402\n",
      "resize results  torch.Size([32, 40, 61, 81]) to torch.Size([480, 640])\n",
      "resize results  torch.Size([32, 3, 61, 81]) to torch.Size([480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbardool/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      " 15%|█▌        | 3/20 [07:54<44:45, 157.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 2021-11-04 17:06:19:943691  VALIDATION iteration (3)\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 480, 640])\n",
      "      BlockDrop forward() start:  2021-11-04 17:06:19:976642\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 17:06:19:978587\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 61, 81])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 17:08:50:549434\n",
      "      BlockDrop forward()  end:  2021-11-04 17:08:50:550718\n",
      "resize results  torch.Size([32, 40, 61, 81]) to torch.Size([480, 640])\n",
      "resize results  torch.Size([32, 3, 61, 81]) to torch.Size([480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbardool/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      " 20%|██        | 4/20 [10:38<42:35, 159.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2021-11-04 17:09:03:945449   VALIDATION LOOP COMPLETE (4)\n",
      "\n",
      "-------------------------------------------------------------\n",
      "validation  2 Loss : {'seg': {'mIoU': 0.1770162832376019, 'Pixel Acc': 0.17520231381058693, 'err': 56.26309013366699}, 'sn': {'cosine_similarity': 0.9274355, 'Angle Mean': 20.058374, 'Angle Median': 18.152489, 'Angle RMSE': 22.098604, 'Angle 11.25': 10.847000175961071, 'Angle 22.5': 70.34749672516895, 'Angle 30': 87.15338475282078, 'Angle 45': 97.74630814418758}}\n",
      "-------------------------------------------------------------\n",
      "\n",
      "** 2021-11-04 17:09:06:315712 - END VALIDATION iteration:  2 \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "** 2021-11-04 17:09:06:317786 - Training iteration 3 WARMUP iteration: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "** BlockDrop optimize() pass start:  2021-11-04 17:09:06:362604\n",
      "   flag:  update_w   num_train_layers:  None is_policy:  False  hard_sampling: False lambdas:  [1, 20]\n",
      "   ** BlockDrop forward() num_train_layers: None  is_policy:  False hard_smapling:  False input img:  torch.Size([32, 3, 321, 321])\n",
      "      BlockDrop forward() start:  2021-11-04 17:09:06:363227\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 17:09:06:363335\n",
      "         MTL2_Dev    num_train_layers: None    hard_sampling:False  mode:train  temperature:5  is_policy:False  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 16\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  16 \n",
      "         MTL2_forward() feature set shape: torch.Size([32, 512, 41, 41])\n",
      "         MTL2_Dev forward  pass end:  2021-11-04 17:10:01:894730\n",
      "      BlockDrop forward()  end:  2021-11-04 17:10:01:895027\n",
      "   ** BlockDrop get_losses start:  2021-11-04 17:10:01:895210\n",
      "       get_seg_loss:  self.seg_pred:torch.Size([32, 40, 41, 41])   prediction: torch.Size([53792, 40])\n",
      "       seg:torch.Size([32, 1, 41, 41])   seg_resize:torch.Size([32, 1, 41, 41])    gt: torch.Size([53792])\n",
      "       get_seg_loss:  self.sn_pred:torch.Size([32, 3, 41, 41])   prediction: torch.Size([53792, 3])\n",
      "       normal:torch.Size([32, 3, 321, 321])   seg_resize:torch.Size([32, 3, 41, 41])    gt: torch.Size([53792, 3])\n",
      "   ** BlockDrop get_losses end  :  2021-11-04 17:10:01:956666\n",
      "   ** BlockDrop backward_network start:  2021-11-04 17:10:01:957341\n",
      "      backward_network - task seg  loss is :', 1 * 3.8150 = 3.8150\n",
      "      backward_network - task sn  loss is :', 20 * 0.0652 = 1.3032\n",
      "      backward_network - Total loss is    : 5.1182\n",
      "   ** BlockDrop backward_network end  :  2021-11-04 17:11:53:860373\n",
      "** BlockDrop optimize() end  :  2021-11-04 17:11:53:860641\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Iteration  3 Loss : {'seg': {'total': tensor(3.8150)}, 'sn': {'total': tensor(0.0652)}, 'total': {'total': tensor(5.1182)}}\n",
      "-------------------------------------------------------------\n",
      "\n",
      "resize results  torch.Size([32, 40, 41, 41]) to torch.Size([321, 321])\n",
      "resize results  torch.Size([32, 3, 41, 41]) to torch.Size([321, 321])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "** 2021-11-04 17:11:54:454807 - Training iteration 4  flag: update_w \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "-----------------------\n",
      " Define Optimizers    :\n",
      "-----------------------\n",
      "\n",
      "define the weights optimizer - learning mode: policy\n",
      " optimizers for weights : \n",
      " SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      "\n",
      "Parameter Group 1\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "define the logits optimizer (init_method: equal)\n",
      " optimizers for alphas : \n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "\n",
      "-----------------------\n",
      " Define Scheduler     :\n",
      "-----------------------\n",
      "learning mode: policy\n",
      "<torch.optim.lr_scheduler.StepLR object at 0x7f282cb4bac0>\n",
      "++ CALL ENVIRON.OPTIMIZE() flag: update_w is_policy: True p_epoch: 0  num_train_layers: 1\n",
      "\n",
      "** BlockDrop optimize() pass start:  2021-11-04 17:11:54:828946\n",
      "   flag:  update_w   num_train_layers:  1 is_policy:  True  hard_sampling: False lambdas:  [1, 20]\n",
      "   ** BlockDrop forward() num_train_layers: 1  is_policy:  True hard_smapling:  False input img:  torch.Size([32, 3, 321, 321])\n",
      "      BlockDrop forward() start:  2021-11-04 17:11:54:835392\n",
      "      ** MTL2_Dev forward  pass start: 2021-11-04 17:11:54:835527\n",
      "         MTL2_Dev    num_train_layers: 1    hard_sampling:False  mode:train  temperature:5  is_policy:True  self.skip_layer:0   sum(layers):16\n",
      "         MTL2_Dev self.layers: 16   self.skip_layer: 0    num_train_layers: 1\n",
      "         MTL2_Dev num_training_layers  = min(sum(self.layers) - self.skip_layer, num_train_layers) =  1 \n",
      "        MTL2 TRAIN SAMPLE POLICY - temp: 5  hard_sampling: False\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'policy' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-88ef12560f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"++ CALL ENVIRON.OPTIMIZE() flag: {flag} is_policy: {opt['policy']} p_epoch: {p_epoch}  num_train_layers: {num_train_layers}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             environ.optimize(opt['lambdas'], \n\u001b[0m\u001b[1;32m     81\u001b[0m                              \u001b[0mis_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                              \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-local/AdaShare/dev/blockdrop_env_dev.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, lambdas, is_policy, flag, num_train_layers, hard_sampling)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   flag: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'  num_train_layers: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_policy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' hard_sampling:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambdas: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_train_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard_sampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhard_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   ** BlockDrop get_losses start: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-local/AdaShare/dev/blockdrop_env_dev.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, is_policy, num_train_layers, hard_sampling)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'      BlockDrop forward() start: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mtl-net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard_sampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhard_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;31m# import pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyt-cpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-local/AdaShare/dev/MTL2_Dev.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, temperature, is_policy, num_train_layers, hard_sampling, mode)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_policy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sample_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_sample_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhard_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-local/AdaShare/dev/MTL2_Dev.py\u001b[0m in \u001b[0;36mtrain_sample_policy\u001b[0;34m(self, temperature, hard_sampling)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mpolicys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"        t_id{t_id+1}_logits: \\n{getattr(self, 'task%d_logits' % (t_id + 1))}  \\n gumbel_softmax: \\n{policy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgumbel_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'task%d_logits'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhard_sampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'policy' referenced before assignment"
     ]
    }
   ],
   "source": [
    "while current_iter < opt['train']['total_iters']:\n",
    "    start_time = time.time()\n",
    "\n",
    "    environ.train()\n",
    "    current_iter += 1\n",
    "\n",
    "##---------------------------------------------------------------     \n",
    "## part one: warm up\n",
    "##--------------------------------------------------------------- \n",
    "\n",
    "    if current_iter < opt['train']['warm_up_iters']:\n",
    "        print(f\"\\n{'-'*100}\")        \n",
    "        print(f\"** {timestring()} - Training iteration {current_iter} WARMUP iteration: {current_iter}\")    \n",
    "        print(f\"{'-'*100}\\n\")\n",
    "        \n",
    "        batch_idx, batch = next(batch_enumerator)\n",
    "        environ.set_inputs(batch)\n",
    "        \n",
    "        environ.optimize(opt['lambdas'], is_policy=False, flag='update_w')\n",
    "        \n",
    "        ## If we've exhausted the dataload, reset to beginning\n",
    "        if batch_idx == len(train_loader) - 1:\n",
    "            batch_enumerator = enumerate(train_loader)\n",
    "\n",
    "        if should(current_iter, opt['train']['print_freq']):\n",
    "            environ.print_loss(current_iter, start_time)\n",
    "            environ.resize_results()\n",
    "\n",
    "        # validation\n",
    "        if should(current_iter, opt['train']['val_freq']):\n",
    "            print(f\"**  {timestring()}  START VALIDATION iteration: {current_iter} \")    \n",
    "            \n",
    "            environ.eval()     # set to evaluation mode (train = False)\n",
    "            num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "            val_metrics = training_validation(environ, val_loader, opt['tasks'], policy=False, num_train_layers=None, num_seg_cls=num_seg_class)\n",
    "            \n",
    "            environ.print_loss(current_iter, start_time, val_metrics, title='validation')\n",
    "            environ.save_checkpoint('latest', current_iter)\n",
    "            \n",
    "            print(f\"** {timestring()} - END VALIDATION iteration:  {current_iter} \")                \n",
    "            environ.train()    # set to training mode (train = True)\n",
    "\n",
    "##---------------------------------------------------------------            \n",
    "## part two  `current_iter >= opt['train']['warm_up_iters']`\n",
    "##---------------------------------------------------------------\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n{'-'*100}\")\n",
    "        print(f\"** {timestring()} - Training iteration {current_iter}  flag: {flag} \")    \n",
    "        print(f\"{'-'*100}\\n\")\n",
    "\n",
    "        if flag_warmup:\n",
    "            environ.define_optimizer(policy_learning=True)\n",
    "            environ.define_scheduler(policy_learning=True)\n",
    "            flag_warmup = False\n",
    "\n",
    "        if current_iter == opt['train']['warm_up_iters']:\n",
    "            environ.save_checkpoint('warmup', current_iter)\n",
    "            environ.fix_alpha()\n",
    "\n",
    "        #-----------------------------------------\n",
    "        # Train & Update the network weights\n",
    "        #-----------------------------------------\n",
    "        if flag == 'update_w':\n",
    "            \n",
    "            current_iter_w += 1\n",
    "            batch_idx_w, batch = next(batch_enumerator1,1)\n",
    "            environ.set_inputs(batch)\n",
    "\n",
    "            ## Set number of layers to train based on cirriculum_speed \n",
    "            ## and p_epoch \n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "                \n",
    "            print(f\"++ CALL ENVIRON.OPTIMIZE() flag: {flag} is_policy: {opt['policy']} p_epoch: {p_epoch}  num_train_layers: {num_train_layers}\\n\") \n",
    "            \n",
    "            environ.optimize(opt['lambdas'], \n",
    "                             is_policy=opt['policy'], \n",
    "                             flag=flag, \n",
    "                             num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'])\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss(current_iter, start_time)\n",
    "                environ.resize_results()\n",
    "\n",
    "            ## when current_iter_w == len(train1_loader) switch to policy training\n",
    "\n",
    "            print(f\"++ current_iter_w: {current_iter_w}  batch_idx_w:{batch_idx_w} current_iter_a: {current_iter_a} ['weight_iter_alternate']:\"\\\n",
    "                  f\" {opt['train']['weight_iter_alternate']}\")            \n",
    "            \n",
    "            if should(current_iter_w, opt['train']['weight_iter_alternate']):\n",
    "                print(f\"++ Switch to update_alpha\")\n",
    "                flag = 'update_alpha'\n",
    "                environ.fix_w()\n",
    "                environ.free_alpha()\n",
    "                \n",
    "                # do the validation on the test set\n",
    "                environ.eval()\n",
    "                print('++ Weight Training Validation ...')\n",
    "                \n",
    "                num_seg_class = opt['tasks_num_class'][opt['tasks'].index('seg')] if 'seg' in opt['tasks'] else -1\n",
    "                \n",
    "                val_metrics = training_validation(environ, val_loader, opt['tasks'], policy=opt['policy'],\n",
    "                                   num_train_layers=num_train_layers, \n",
    "                                   hard_sampling=opt['train']['hard_sampling'],\n",
    "                                   num_seg_cls=num_seg_class)\n",
    "                \n",
    "                environ.print_loss(current_iter, start_time, val_metrics)\n",
    "                environ.save_checkpoint('latest', current_iter)\n",
    "                \n",
    "                print(f\"++  current iter: {current_iter}       warm_up_iters: {opt['train']['warm_up_iters']} \\n\"\n",
    "                      f\" num_blocks  : {num_blocks}         cirriculum_speed: {opt['curriculum_speed']}    \\n\"\n",
    "                      f\" opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'] :\"\n",
    "                      f\" {opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']}    \\n\"\n",
    "                      f\" {current_iter - opt['train']['warm_up_iters']}  ??\"\n",
    "                      f\"{num_blocks * opt['curriculum_speed'] * (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate'])}\")\n",
    "                                             \n",
    "                ## if the training iteration is larger than a certain amount \n",
    "                ## (num_blocks (8 for ResNet))* (curriculum_speed(3)) * (weight training iterations + policy training iterations (19+4))\n",
    "                ## check metrics for improvement, and issue a checkpoint if necessary\n",
    "                \n",
    "                if current_iter - opt['train']['warm_up_iters'] >= num_blocks * opt['curriculum_speed'] * \\\n",
    "                        (opt['train']['weight_iter_alternate'] + opt['train']['alpha_iter_alternate']):\n",
    "                    new_value = 0\n",
    "\n",
    "                    for k in refer_metrics.keys():\n",
    "                        if k in val_metrics.keys():\n",
    "                            for kk in val_metrics[k].keys():\n",
    "                                if not kk in refer_metrics[k].keys():\n",
    "                                    continue\n",
    "                                if (k == 'sn' and kk in ['Angle Mean', 'Angle Median']) or (\n",
    "                                        k == 'depth' and not kk.startswith('sigma')) or (kk == 'err'):\n",
    "                                    value = refer_metrics[k][kk] / val_metrics[k][kk]\n",
    "                                else:\n",
    "                                    value = val_metrics[k][kk] / refer_metrics[k][kk]\n",
    "\n",
    "                                value = value / len(list(set(val_metrics[k].keys()) & set(refer_metrics[k].keys())))\n",
    "                                new_value += value\n",
    "                    \n",
    "                    print('Best Value %.4f  New value: %.4f' % new_value)\n",
    "\n",
    "                    if new_value > best_value:\n",
    "                        print('Previous best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)\n",
    "                        best_value = new_value\n",
    "                        best_metrics = val_metrics\n",
    "                        best_iter = current_iter\n",
    "                        environ.save_checkpoint('best', current_iter)\n",
    "                        print('New      best iter: %d, best_value: %.4f' % (best_iter, best_value), best_metrics)                    \n",
    "\n",
    "                environ.train()\n",
    "\n",
    "            print(f\"++ Current loader (train1) index: {batch_idx_w}   len(train1_loader) = {len(train1_loader)} \")\n",
    "            if batch_idx_w == len(train1_loader):\n",
    "                print(f\"++ REENUMERATE train1_loader\")\n",
    "                batch_enumerator1 = enumerate(train1_loader,1)\n",
    "                \n",
    "        #-----------------------------------------\n",
    "        # Train & update the policy network\n",
    "        #-----------------------------------------\n",
    "        elif flag == 'update_alpha':\n",
    "            current_iter_a += 1\n",
    "            batch_idx_a, batch = next(batch_enumerator2, 1)\n",
    "            environ.set_inputs(batch)\n",
    "            \n",
    "            if opt['is_curriculum']:\n",
    "                num_train_layers = p_epoch // opt['curriculum_speed'] + 1\n",
    "            else:\n",
    "                num_train_layers = None\n",
    "                \n",
    "            print(f\"++ CALL ENVIRON.OPTIMIZE() flag: {flag} is_policy: {opt['policy']} p_epoch: {p_epoch}  num_train_layers: {num_train_layers}\\n\") \n",
    "            \n",
    "            environ.optimize(opt['lambdas'], \n",
    "                             is_policy=opt['policy'], \n",
    "                             flag=flag, \n",
    "                             num_train_layers=num_train_layers,\n",
    "                             hard_sampling=opt['train']['hard_sampling'])\n",
    "\n",
    "            if should(current_iter, opt['train']['print_freq']):\n",
    "                environ.print_loss(current_iter, start_time)\n",
    "                environ.resize_results()\n",
    "                # environ.visual_policy(current_iter)\n",
    "            \n",
    "            ## when current_iter_a == len(train2_loader) switch to policy training\n",
    "            print(f\"** current_iter_a: {current_iter_a} ['alpha_iter_alternate']: {opt['train']['alpha_iter_alternate']}\",\n",
    "                  f\"   batch_idx_a {batch_ix_a}  Current iter w: {current_iter_w} \")            \n",
    "           \n",
    "            if should(current_iter_a, opt['train']['alpha_iter_alternate']):\n",
    "                print(f\"** Switch training to update_weight\")                \n",
    " \n",
    "                flag = 'update_w'\n",
    "  \n",
    "                environ.fix_alpha()\n",
    "                environ.free_w(opt['fix_BN'])\n",
    "                environ.decay_temperature()\n",
    "                \n",
    "                # print the distribution\n",
    "                dists = environ.get_policy_prob()\n",
    "                print(np.concatenate(dists, axis=-1))\n",
    "                p_epoch += 1\n",
    "                print(f\"** p_epoch incremented: {p_epoch}\")\n",
    "\n",
    "            \n",
    "            print(f\"++ Current loader (train2 index:: {batch_idx_a} len(train2_loader) = {len(train2_loader)}\")                \n",
    "            if batch_idx_a == len(train2_loader):\n",
    "                print(f\"++ REENUMERATE train2loader\")\n",
    "                batch_enumerator2 = enumerate(train2_loader,1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('flag %s is not recognized' % flag)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5319dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c1492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ef5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0133a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3496d09",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Display parameters of the network. \n",
    "\n",
    "`taskn_logits` are architecture parms\n",
    "others are network parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc55455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T19:31:59.143549Z",
     "start_time": "2021-09-23T19:31:59.040784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(environ.networks['mtl-net'].layers)\n",
    "print(environ.networks['mtl-net'].backbone.layer_config)\n",
    "for m in environ.networks['mtl-net'].backbone.modules():\n",
    "#     print(type(m))\n",
    "    print('==> ',type(m),'\\n',m)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.kernel_size, m.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1122604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T19:39:05.357994Z",
     "start_time": "2021-09-22T19:39:05.318080Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planes = 128\n",
    "keep_channels = (planes * np.cumsum([0, 0.25, 0.25, 0.5])).astype(np.int32)\n",
    "print(keep_channels)\n",
    "keep_masks = []\n",
    "for kc in keep_channels:\n",
    "    mask = np.zeros([1, planes, 1, 1])\n",
    "    print(f' keep channels: {kc:2d}  mask: {mask.shape} - set mask[:, :{kc:2d}] =1')\n",
    "    mask[:, :kc] = 1\n",
    "    keep_masks.append(mask)\n",
    "    \n",
    "for i in keep_masks:\n",
    "    print(type(i), i.shape, i.sum())\n",
    "    \n",
    "keep_masks_py = torch.from_numpy(np.concatenate(keep_masks)).float()\n",
    "\n",
    "print(keep_masks_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139ec6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T20:54:26.471166Z",
     "start_time": "2021-09-21T20:54:26.156423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dilation = 1 \n",
    "kernel_size = np.asarray((3, 3))\n",
    "upsampled_kernel_size = (kernel_size - 1) * (dilation - 1) + kernel_size\n",
    "print(upsampled_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ea815",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a6fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T19:29:35.681008Z",
     "start_time": "2021-09-21T19:29:35.598776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(environ.get_arch_parameters())\n",
    "for i,j in environ.networks['mtl-net'].named_parameters():\n",
    "    print(f\"name: {i}    shape: {j.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7a11e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22057a15",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Call MTL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a01a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T18:59:25.718262Z",
     "start_time": "2021-09-21T18:59:25.701695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "block = BasicBlock\n",
    "layers = [2, 2, 2, 2]\n",
    "num_classes_tasks =  opt['tasks_num_class']\n",
    "init_method =  opt['train']['init_method']\n",
    "init_neg_logits = opt['init_neg_logits']\n",
    "skip_layer = opt['skip_layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e15f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T18:59:00.365777Z",
     "start_time": "2021-09-21T18:59:00.328214Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtl2 = MTL2_Dev(block, layers, num_classes_tasks, init_method, init_neg_logits, skip_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3679d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a152e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3b2f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4982a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-cpu",
   "language": "python",
   "name": "pyt-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
