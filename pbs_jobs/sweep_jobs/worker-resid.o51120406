time: 21600
nodes: 1
procs: 9
account string: lp_symbiosys
queue: q24h
========================================================================
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:09:20 CEST 2022
Conda Initialization Start
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.2  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.2  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.15  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.15  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.25  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.25  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.3  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.3  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.1  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.1  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.0  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.0  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.35  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.35  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:09:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.05  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.05  Task LR: 0.001 

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  10iwaiei
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.0
 middle_dropout...........  0.0
 last_dropout.............  0.0
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
10iwaiei 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  23rcco5n
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.3
 middle_dropout...........  0.3
 last_dropout.............  0.3
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
23rcco5n 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  1p08kybe
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.1
 middle_dropout...........  0.1
 last_dropout.............  0.1
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
1p08kybe 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  1ww49ont
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.15
 middle_dropout...........  0.15
 last_dropout.............  0.15
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
1ww49ont 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  3l19mge3
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.05
 middle_dropout...........  0.05
 last_dropout.............  0.05
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
3l19mge3 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  11yudqda
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.2
 middle_dropout...........  0.2
 last_dropout.............  0.2
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
11yudqda 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  733bjh4l
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.35
 middle_dropout...........  0.35
 last_dropout.............  0.35
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
733bjh4l 0420_1109 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  c4umf8gu
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.25
 middle_dropout...........  0.25
 last_dropout.............  0.25
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
c4umf8gu 0420_1109 AdaSparseChem-Mini
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 10iwaiei 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 10iwaiei 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 10iwaiei 
 folder_name           : 700x5_0420_1109_lr0.001_do0.0 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 10iwaiei
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.0
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.0
      middle_dropout : 0.0
        last_dropout : 0.0
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.0

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 23rcco5n 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 23rcco5n 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 23rcco5n 
 folder_name           : 700x5_0420_1109_lr0.001_do0.3 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 23rcco5n
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.3
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.3
      middle_dropout : 0.3
        last_dropout : 0.3
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.3

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : c4umf8gu 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : c4umf8gu 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : c4umf8gu 
 folder_name           : 700x5_0420_1109_lr0.001_do0.25 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : c4umf8gu
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.25
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.25
      middle_dropout : 0.25
        last_dropout : 0.25
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.25

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386


SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 3l19mge3 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 3l19mge3 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 3l19mge3 
 folder_name           : 700x5_0420_1109_lr0.001_do0.05 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 3l19mge3
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.05
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.05
      middle_dropout : 0.05
        last_dropout : 0.05
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.05

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1ww49ont 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1ww49ont 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 1ww49ont 
 folder_name           : 700x5_0420_1109_lr0.001_do0.15 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 1ww49ont
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.15
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.15
      middle_dropout : 0.15
        last_dropout : 0.15
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.15

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 733bjh4l 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 733bjh4l 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 733bjh4l 
 folder_name           : 700x5_0420_1109_lr0.001_do0.35 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 733bjh4l
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.35
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.35
      middle_dropout : 0.35
        last_dropout : 0.35
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.35

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 11yudqda 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 11yudqda 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 11yudqda 
 folder_name           : 700x5_0420_1109_lr0.001_do0.2 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 11yudqda
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.2
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.2
      middle_dropout : 0.2
        last_dropout : 0.2
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.2

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.35, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.35, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.35, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.35, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.35, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.35, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.05, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.15, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.15, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.15, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.15, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.15, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.15, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1p08kybe 
 RUN NAME    : 0420_1109
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1p08kybe 
 RUN NAME    : 0420_1109


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1109 
 experiment id         : 1p08kybe 
 folder_name           : 700x5_0420_1109_lr0.001_do0.1 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 1p08kybe
            exp_name : 0420_1109
          exp_folder : 700x5_0420_1109_lr0.001_do0.1
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.1
      middle_dropout : 0.1
        last_dropout : 0.1
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1109_lr0.001_do0.1

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1109
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]





Job 51120406 finished : Wed Apr 20 11:09:55 CEST 2022
Job 51120406 finished : Wed Apr 20 11:09:55 CEST 2022
Job 51120406 finished : Wed Apr 20 11:09:55 CEST 2022
Job 51120406 finished : Wed Apr 20 11:09:55 CEST 2022
Job 51120406 finished : Wed Apr 20 11:09:55 CEST 2022

Job 51120406 finished : Wed Apr 20 11:09:56 CEST 2022

Job 51120406 finished : Wed Apr 20 11:09:59 CEST 2022

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.25, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.25, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.25, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.25, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.25, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.25, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]
 training preparation: - set print_freq to length of train loader: 115
 training preparation: - set eval_iters to length of val loader : 30
 training preparation: - set number of batches per weight training epoch to: 115
 training preparation: - set number of batches per policy training epoch to: 115
--------------------------------------------------------------------------------

    folder: 700x5_0420_1109_lr0.001_do0.25
    layers: 5 [700, 700, 700, 700, 700] 
    
    first dropout          : 0.25
    middle dropout         : 0.25
    last dropout           : 0.25
    diff_sparsity_weights  : False
    skip_layer             : 0
    is_curriculum          : False
    curriculum_speed       : 3
    
    task_lr                : 0.001
    backbone_lr            : 0.001
    decay_lr_rate          : 0.3
    decay_lr_freq          : 10.0
    
    policy_lr              : 0.001
    policy_decay_lr_rate   : 0.75
    policy_decay_lr_freq   : 50
    lambda_sparsity        : 0.02
    lambda_sharing         : 0.01
    lambda_tasks           : 1
    
    Gumbel init_temp       : 4
    Gumbel decay_temp      : 0.965
    Gumbel decay_temp_freq : 16
    Logit init_method      : random
    Logit init_neg_logits  : None
    Logit hard_sampling    : False
    Warm-up epochs         : 100
    training epochs        : 250
    Data split ratios      : [0.725, 0.225, 0.05]

--------------------------------------------------------------------------
 Last Epoch: 0   # of warm-up epochs to do:  100 - Run epochs 1 to 100
-------------------------------------------------------------------------- 

--------------------------------------------------------------------------
 Last Epoch: 0   # of warm-up epochs to do:  100 - Run epochs 1 to 100
-------------------------------------------------------------------------- 

 Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |   logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total | time |
   1 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.3645   3.382e-04   0.000e+00    0.3648 |   0.00002   0.34455   0.92645   0.88012   0.92017   0.93513 |   0.0975   1.155e-04   0.000e+00    0.0976 | 12.6 |
Previous best_epoch:     0   best iter:     0,   best_value: 0.00000
New      best_epoch:     1   best iter:   115,   best_value: 0.92645
   2 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0140   3.382e-04   0.000e+00    0.0143 |   0.00002   0.36944   0.93157   0.88512   0.92379   0.94017 |   0.1047   1.155e-04   0.000e+00    0.1048 |  9.8 |
Previous best_epoch:     1   best iter:   115,   best_value: 0.92645
New      best_epoch:     2   best iter:   230,   best_value: 0.93157
Job 51120406 start : Wed Apr 20 11:10:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:10:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:10:20 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:10:20 CEST 2022
Conda Initialization Start
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Job 51120406 start : Wed Apr 20 11:10:21 CEST 2022
Conda Initialization Start
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Job 51120406 start : Wed Apr 20 11:10:21 CEST 2022
Conda Initialization Start
Python 3.9.5



program excution start: Wed Apr 20 11:10:21 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.4  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.4  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:10:21 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.45  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.45  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:10:21 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.5  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.5  Task LR: 0.001 
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Job 51120406 start : Wed Apr 20 11:10:22 CEST 2022
Conda Initialization Start
Python 3.9.5



program excution start: Wed Apr 20 11:10:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.6  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.6  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:10:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.55  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.55  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:10:22 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.65  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.65  Task LR: 0.001 
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Python 3.9.5



program excution start: Wed Apr 20 11:10:23 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.7  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.7  Task LR: 0.001 
   3 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0091   3.382e-04   0.000e+00    0.0094 |   0.00002   0.44494   0.91514   0.87054   0.90296   0.92353 |   0.1054   1.155e-04   0.000e+00    0.1055 | 12.4 |

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  3k8x35wq
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.5
 middle_dropout...........  0.5
 last_dropout.............  0.5
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
3k8x35wq 0420_1110 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  2yj13ru3
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.4
 middle_dropout...........  0.4
 last_dropout.............  0.4
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
2yj13ru3 0420_1110 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  17qp420c
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.6
 middle_dropout...........  0.6
 last_dropout.............  0.6
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
17qp420c 0420_1110 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  2wjmjpzb
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.45
 middle_dropout...........  0.45
 last_dropout.............  0.45
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
2wjmjpzb 0420_1110 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  j6bks0iq
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.65
 middle_dropout...........  0.65
 last_dropout.............  0.65
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
j6bks0iq 0420_1110 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  1segrhtj
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.55
 middle_dropout...........  0.55
 last_dropout.............  0.55
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
1segrhtj 0420_1110 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  8jqdbd1o
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.7
 middle_dropout...........  0.7
 last_dropout.............  0.7
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
8jqdbd1o 0420_1110 AdaSparseChem-Mini
   4 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0543   3.382e-04   0.000e+00    0.0546 |   0.00003   0.57983   0.91948   0.84566   0.91017   0.92135 |   0.1400   1.155e-04   0.000e+00    0.1402 | 11.9 |
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 2wjmjpzb 
 RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 2wjmjpzb 
 RUN NAME    : 0420_1110


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45

 PROJECT NAME: AdaSparseChem-Mini
------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1110 
 RUN ID      : 2yj13ru3 
 experiment id         : 2wjmjpzb 
 folder_name           : 700x5_0420_1110_lr0.001_do0.45  RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 experiment description: 51120406 - Run with residiual layers

 RUN ID      : 2yj13ru3 
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88  RUN NAME    : 0420_1110



 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45
 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45
 Gpu ids               : [0]
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4

 Seed index            : 0
 policy_iter           : best
------------------------------------------------------------------------------------------------------------------------

 Data Split ratios     : [0.725, 0.225, 0.05]
 experiment name       : 0420_1110 
 experiment id         : 2yj13ru3 ------------------------------------------------------------------------------------------------------------------------ 


 folder_name           : 700x5_0420_1110_lr0.001_do0.4 
 experiment description: 51120406 - Run with residiual layers        project_name : AdaSparseChem-Mini
              exp_id : 2wjmjpzb
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]

            exp_name : 0420_1110
          exp_folder : 700x5_0420_1110_lr0.001_do0.45 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4

     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4
 Gpu ids               : [0]
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]

 Seed index            : 0
 policy_iter           : best
                 cpu : False
             gpu_ids : [0] Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

            backbone : SparseChem
               tasks : ['class']
        project_name : AdaSparseChem-Mini

     tasks_num_class : [100]
              exp_id : 2yj13ru3
            exp_name : 0420_1110
             lambdas : [1]
             verbose : False
          exp_folder : 700x5_0420_1110_lr0.001_do0.4
     exp_description : 51120406 - Run with residiual layers
     input_size_freq : None
          input_size : 32000
          folder_sfx : None
         random_seed : 88
 first_non_linearity : relu
middle_non_linearity : relu
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
  last_non_linearity : relu
       first_dropout : 0.45
      middle_dropout : 0.45            backbone : SparseChem
               tasks : ['class']

        last_dropout : 0.45
     tasks_num_class : [100]
             lambdas : [1]   class_output_size : None
    regr_output_size : None
             verbose : False

              policy : True
     input_size_freq : None
          input_size : 32000        policy_model : task-specific
         is_residual : True
 first_non_linearity : relu

           is_sparse : True
middle_non_linearity : relu
  last_non_linearity : relu          is_sharing : True
diff_sparsity_weights : False

       first_dropout : 0.4          skip_layer : 0
       is_curriculum : False
      middle_dropout : 0.4
        last_dropout : 0.4
    curriculum_speed : 3

   class_output_size : None
              fix_BN : False
     retrain_from_pl : False    regr_output_size : None
              policy : True

train
        policy_model : task-specific

-----
         is_residual : True
           is_sparse : True          batch_size : 128
       warmup_epochs : 100
          is_sharing : True

     training_epochs : 250
diff_sparsity_weights : False
          skip_layer : 0         total_iters : 25000
       warm_up_iters : None

       is_curriculum : False
             task_lr : 0.001
         backbone_lr : 0.001    curriculum_speed : 3
              fix_BN : False
    weight_optimizer : adam

     retrain_from_pl : False

    policy_optimizer : adam
       decay_lr_rate : 0.3train
-----
       decay_lr_freq : 10.0

          batch_size : 128
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
       warmup_epochs : 100
     training_epochs : 250
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0         total_iters : 25000
       warm_up_iters : None
           policy_lr : 0.001

             task_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01         backbone_lr : 0.001
    weight_optimizer : adam
        lambda_tasks : 1

    policy_optimizer : adam
         init_method : random
           init_temp : 4       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
          decay_temp : 0.965

   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75     decay_temp_freq : 16
     init_neg_logits : None

policy_decay_lr_freq : 50
       hard_sampling : False
            val_freq : 500policy_decay_lr_cooldown : 0
           policy_lr : 0.001
          print_freq : -1

     lambda_sparsity : 0.02
           val_iters : -1
              resume : False      lambda_sharing : 0.01
        lambda_tasks : 1
      retrain_resume : False

         init_method : random
         policy_iter : best
          which_iter : warmup           init_temp : 4
          decay_temp : 0.965

paths
     decay_temp_freq : 16
     init_neg_logits : None
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45
       hard_sampling : False

          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45
            val_freq : 500
          print_freq : -1      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.45


           val_iters : -1
dataload
--------              resume : False
      retrain_resume : False
             dataset : Chembl23_mini

         policy_iter : best
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy          which_iter : warmup


      x_split_ratios : [0.725, 0.225, 0.05]
paths
-----             y_tasks : ['chembl_23mini_y.npy']

             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4            y_censor : None
             folding : chembl_23mini_folds.npy
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4

         fold_inputs : 32000
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.4

     input_transform : None
       weights_class : Nonedataload
--------
   min_samples_class : 2
             dataset : Chembl23_mini

             fold_va : 0
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy             fold_te : None


      x_split_ratios : [0.725, 0.225, 0.05]
SC
--             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
      normalize_loss : None

             folding : chembl_23mini_folds.npy
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110         fold_inputs : 32000
     input_transform : None
        hidden_sizes : [700, 700, 700, 700, 700]
       weights_class : None

    tail_hidden_size : [700]
   min_samples_class : 2
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387             fold_va : 0

Validation dataset shape: (3755,)   last index #: 18386
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 17qp420c 
 RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 17qp420c 
 RUN NAME    : 0420_1110


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1110 
 experiment id         : 17qp420c 
 folder_name           : 700x5_0420_1110_lr0.001_do0.6 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 17qp420c
            exp_name : 0420_1110
          exp_folder : 700x5_0420_1110_lr0.001_do0.6
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.6
      middle_dropout : 0.6
        last_dropout : 0.6
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.6

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 3k8x35wq 
 RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 3k8x35wq 
 RUN NAME    : 0420_1110


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1110 
 experiment id         : 3k8x35wq 
 folder_name           : 700x5_0420_1110_lr0.001_do0.5 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 3k8x35wq
            exp_name : 0420_1110
          exp_folder : 700x5_0420_1110_lr0.001_do0.5
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.5
      middle_dropout : 0.5
        last_dropout : 0.5
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.5

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : j6bks0iq 
 RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : j6bks0iq 
 RUN NAME    : 0420_1110


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1110 
 experiment id         : j6bks0iq 
 folder_name           : 700x5_0420_1110_lr0.001_do0.65 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : j6bks0iq
            exp_name : 0420_1110
          exp_folder : 700x5_0420_1110_lr0.001_do0.65
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.65
      middle_dropout : 0.65
        last_dropout : 0.65
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.65

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1segrhtj 
 RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1segrhtj 
 RUN NAME    : 0420_1110


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1110 
 experiment id         : 1segrhtj 
 folder_name           : 700x5_0420_1110_lr0.001_do0.55 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 1segrhtj
            exp_name : 0420_1110
          exp_folder : 700x5_0420_1110_lr0.001_do0.55
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.55
      middle_dropout : 0.55
        last_dropout : 0.55
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.55

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 8jqdbd1o 
 RUN NAME    : 0420_1110
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 8jqdbd1o 
 RUN NAME    : 0420_1110


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1110 
 experiment id         : 8jqdbd1o 
 folder_name           : 700x5_0420_1110_lr0.001_do0.7 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 8jqdbd1o
            exp_name : 0420_1110
          exp_folder : 700x5_0420_1110_lr0.001_do0.7
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.7
      middle_dropout : 0.7
        last_dropout : 0.7
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1110_lr0.001_do0.7

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1110
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.45, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.45, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.45, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.45, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.45, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.45, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.4, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.4, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.4, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.4, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.4, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.4, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.6, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.6, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.6, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.6, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.6, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.6, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.5, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.5, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.65, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.65, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.65, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.65, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.65, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.65, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.55, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.55, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.55, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.55, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.55, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.55, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.7, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.7, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.7, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.7, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]
   5 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0580   3.382e-04   0.000e+00    0.0583 |   0.00003   0.58262   0.91851   0.86894   0.90478   0.92890 |   0.1489   1.155e-04   0.000e+00    0.1490 | 13.1 |

Job 51120406 finished : Wed Apr 20 11:10:51 CEST 2022


Job 51120406 finished : Wed Apr 20 11:10:52 CEST 2022

Job 51120406 finished : Wed Apr 20 11:10:52 CEST 2022



Job 51120406 finished : Wed Apr 20 11:10:52 CEST 2022
Job 51120406 finished : Wed Apr 20 11:10:52 CEST 2022
Job 51120406 finished : Wed Apr 20 11:10:52 CEST 2022
Job 51120406 finished : Wed Apr 20 11:10:52 CEST 2022
   6 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0014   3.382e-04   0.000e+00    0.0017 |   0.00003   0.48513   0.92237   0.85813   0.91278   0.92240 |   0.1246   1.155e-04   0.000e+00    0.1247 | 12.8 |
Job 51120406 start : Wed Apr 20 11:11:08 CEST 2022
Conda Initialization Start
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Python 3.9.5



program excution start: Wed Apr 20 11:11:09 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.75  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.75  Task LR: 0.001 
Job 51120406 start : Wed Apr 20 11:11:11 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:11:11 CEST 2022
Conda Initialization Start
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Job 51120406 start : Wed Apr 20 11:11:12 CEST 2022
Conda Initialization Start
Job 51120406 start : Wed Apr 20 11:11:12 CEST 2022
Conda Initialization Start
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Python 3.9.5



program excution start: Wed Apr 20 11:11:12 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.8  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.8  Task LR: 0.001 
   7 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.1612   3.382e-04   0.000e+00    0.1615 |   0.00003   0.43727   0.95071   0.86809   0.94641   0.94105 |   0.1157   1.155e-04   0.000e+00    0.1158 |  9.6 |
Previous best_epoch:     2   best iter:   230,   best_value: 0.93157
New      best_epoch:     7   best iter:   805,   best_value: 0.95071
Python 3.9.5



program excution start: Wed Apr 20 11:11:12 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.85  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.85  Task LR: 0.001 
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Conda Initialization End
/data/leuven/326/vsc32647/miniconda3/bin:/data/leuven/326/vsc32647/miniconda3/condabin:/apps/leuven/bin:/usr/local/bin:/usr/lpp/mmfs/bin:.:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/bin:/usr/local/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin
PBS VERSION is TORQUE-6.1.3
switch to pyt-gpu
Python 3.9.5



program excution start: Wed Apr 20 11:11:13 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.9  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.9  Task LR: 0.001 
Python 3.9.5



program excution start: Wed Apr 20 11:11:13 CEST 2022
datadir: ../../MLDatasets/chembl23_mini outdir: ../../experiments/mini-AdaSparseChem confg: ../yamls/chembl_mini_train.yaml
Number Layers: 5   Layer size: 700   Dropout: 0.95  Task LR: 0.001 
Number Layers: 5   Layer size:  700  700  700  700  700    Dropout: 0.95  Task LR: 0.001 

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  1jwadk8s
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.75
 middle_dropout...........  0.75
 last_dropout.............  0.75
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
1jwadk8s 0420_1111 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  23b66ch5
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.85
 middle_dropout...........  0.85
 last_dropout.............  0.85
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
23b66ch5 0420_1111 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  36j3ziff
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.8
 middle_dropout...........  0.8
 last_dropout.............  0.8
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
36j3ziff 0420_1111 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  2iwdtgiu
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.9
 middle_dropout...........  0.9
 last_dropout.............  0.9
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
2iwdtgiu 0420_1111 AdaSparseChem-Mini

  command line parms : 
------------------------
 config...................  ../yamls/chembl_mini_train.yaml
 exp_id...................  28v7vask
 exp_name.................  None
 folder_sfx...............  None
 exp_desc.................  51120406 - Run with residiual layers
 hidden_sizes.............  [700, 700, 700, 700, 700]
 tail_hidden_size.........  [700]
 warmup_epochs............  100
 training_epochs..........  None
 seed_idx.................  0
 batch_size...............  128
 first_dropout............  0.95
 middle_dropout...........  0.95
 last_dropout.............  0.95
 backbone_lr..............  0.001
 task_lr..................  0.001
 policy_lr................  None
 decay_lr_rate............  0.3
 decay_lr_freq............  10.0
 lambda_sparsity..........  None
 lambda_sharing...........  None
 gpu_ids..................  [0]
 no_residual..............  False
 resume...................  False
 cpu......................  False
 min_samples_class........  2



##################################################
################### READ YAML ####################
##################################################
28v7vask 0420_1111 AdaSparseChem-Mini
   8 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0025   3.382e-04   0.000e+00    0.0028 |   0.00003   0.39231   0.94405   0.87161   0.93917   0.93180 |   0.1078   1.155e-04   0.000e+00    0.1079 | 12.4 |
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1jwadk8s 
 RUN NAME    : 0420_1111
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 1jwadk8s 
 RUN NAME    : 0420_1111


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1111 
 experiment id         : 1jwadk8s 
 folder_name           : 700x5_0420_1111_lr0.001_do0.75 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 1jwadk8s
            exp_name : 0420_1111
          exp_folder : 700x5_0420_1111_lr0.001_do0.75
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.75
      middle_dropout : 0.75
        last_dropout : 0.75
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.75

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1111
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.75, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.75, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.75, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.75, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.75, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 36j3ziff 
 RUN NAME    : 0420_1111
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 36j3ziff 
 RUN NAME    : 0420_1111


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1111 
 experiment id         : 36j3ziff 
 folder_name           : 700x5_0420_1111_lr0.001_do0.8 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 36j3ziff
            exp_name : 0420_1111
          exp_folder : 700x5_0420_1111_lr0.001_do0.8
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.8
      middle_dropout : 0.8
        last_dropout : 0.8
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.8

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1111
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 23b66ch5 
 RUN NAME    : 0420_1111
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 23b66ch5 
 RUN NAME    : 0420_1111


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1111 
 experiment id         : 23b66ch5 
 folder_name           : 700x5_0420_1111_lr0.001_do0.85 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 23b66ch5
            exp_name : 0420_1111
          exp_folder : 700x5_0420_1111_lr0.001_do0.85
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.85
      middle_dropout : 0.85
        last_dropout : 0.85
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.85

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1111
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 2iwdtgiu 
 RUN NAME    : 0420_1111
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 2iwdtgiu 
 RUN NAME    : 0420_1111


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1111 
 experiment id         : 2iwdtgiu 
 folder_name           : 700x5_0420_1111_lr0.001_do0.9 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 2iwdtgiu
            exp_name : 0420_1111
          exp_folder : 700x5_0420_1111_lr0.001_do0.9
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.9
      middle_dropout : 0.9
        last_dropout : 0.9
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.9

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1111
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.8, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.8, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.8, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.8, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.8, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.8, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.85, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.85, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.85, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.85, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.85, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.85, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 28v7vask 
 RUN NAME    : 0420_1111
 PROJECT NAME: AdaSparseChem-Mini
 RUN ID      : 28v7vask 
 RUN NAME    : 0420_1111


 log_dir              create folder:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95
 result_dir           folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95
 checkpoint_dir       folder exists:  ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95

------------------------------------------------------------------------------------------------------------------------
 experiment name       : 0420_1111 
 experiment id         : 28v7vask 
 folder_name           : 700x5_0420_1111_lr0.001_do0.95 
 experiment description: 51120406 - Run with residiual layers
 Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
 Random  seed used     : 88 
 log folder            : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95
 checkpoint folder     : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95
 Gpu ids               : [0]
 Seed index            : 0
 policy_iter           : best
 Data Split ratios     : [0.725, 0.225, 0.05]
------------------------------------------------------------------------------------------------------------------------ 

        project_name : AdaSparseChem-Mini
              exp_id : 28v7vask
            exp_name : 0420_1111
          exp_folder : 700x5_0420_1111_lr0.001_do0.95
     exp_description : 51120406 - Run with residiual layers
          folder_sfx : None
         random_seed : 88
           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]
                 cpu : False
             gpu_ids : [0]
            backbone : SparseChem
               tasks : ['class']
     tasks_num_class : [100]
             lambdas : [1]
             verbose : False
     input_size_freq : None
          input_size : 32000
 first_non_linearity : relu
middle_non_linearity : relu
  last_non_linearity : relu
       first_dropout : 0.95
      middle_dropout : 0.95
        last_dropout : 0.95
   class_output_size : None
    regr_output_size : None
              policy : True
        policy_model : task-specific
         is_residual : True
           is_sparse : True
          is_sharing : True
diff_sparsity_weights : False
          skip_layer : 0
       is_curriculum : False
    curriculum_speed : 3
              fix_BN : False
     retrain_from_pl : False

train
-----
          batch_size : 128
       warmup_epochs : 100
     training_epochs : 250
         total_iters : 25000
       warm_up_iters : None
             task_lr : 0.001
         backbone_lr : 0.001
    weight_optimizer : adam
    policy_optimizer : adam
       decay_lr_rate : 0.3
       decay_lr_freq : 10.0
   decay_lr_cooldown : 0
policy_decay_lr_rate : 0.75
policy_decay_lr_freq : 50
policy_decay_lr_cooldown : 0
           policy_lr : 0.001
     lambda_sparsity : 0.02
      lambda_sharing : 0.01
        lambda_tasks : 1
         init_method : random
           init_temp : 4
          decay_temp : 0.965
     decay_temp_freq : 16
     init_neg_logits : None
       hard_sampling : False
            val_freq : 500
          print_freq : -1
           val_iters : -1
              resume : False
      retrain_resume : False
         policy_iter : best
          which_iter : warmup

paths
-----
             log_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95
          result_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95
      checkpoint_dir : ../../experiments/mini-AdaSparseChem/700x5_0420_1111_lr0.001_do0.95

dataload
--------
             dataset : Chembl23_mini
            dataroot : ../../MLDatasets/chembl23_mini
                   x : chembl_23mini_x.npy
      x_split_ratios : [0.725, 0.225, 0.05]
             y_tasks : ['chembl_23mini_y.npy']
            y_censor : None
             folding : chembl_23mini_folds.npy
         fold_inputs : 32000
     input_transform : None
       weights_class : None
   min_samples_class : 2
             fold_va : 0
             fold_te : None

SC
--
      normalize_loss : None
              config : ../yamls/chembl_mini_train.yaml
        exp_name_pfx : 0420_1111
        hidden_sizes : [700, 700, 700, 700, 700]
    tail_hidden_size : [700]
(18388, 85277) (18388,)
Training dataset shape  : (14633,)   last index #: 18387
Validation dataset shape: (3755,)   last index #: 18386

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.9, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.9, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.9, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.9, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.9, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

 trainset.y_class                                   :  [(14633, 100)] 
 trainset1.y_class                                  :  [(14633, 100)] 
 trainset2.y_class                                  :  [(14633, 100)] 
 valset.y_class                                     :  [(3755, 100)]  
                                 
 size of training set 0 (warm up)                   :  14633 
 size of training set 1 (network parms)             :  14633 
 size of training set 2 (policy weights)            :  14633 
 size of validation set                             :  3755 
                               Total                :  47654 
                                 
 Number of batches in training 0 (warm up)          :  115 
 Number of batches in training 1 (network parms)    :  115 
 Number of batches in training 2 (policy weights)   :  115 
 Number of batches in validation dataset            :  30 
                                
##################################################
############# CREATE THE ENVIRONMENT #############
##################################################
 device is  cuda:0
 SparseChem_BackBone() Input Layer  - Input: 32000  output: 700  non-linearity:<class 'torch.nn.modules.activation.ReLU'>
--------------------------------------------------
 Initialize weights 
-------------------------------------------------- 

----------------------------------------------------
* SparseChemEnv environment successfully created
---------------------------------------------------- 

 

SparseChemEnv  Configuration       
---------------------------------------- 

----------------
networks       :
----------------
 {'mtl-net': MTL3(
  (backbone): SparseChem_Backbone(
    (Input_Layer): Sequential(
      (linear): SparseLinear(in_features=32000, out_features=700, bias=True)
      (non_linear): ReLU()
      (dropout): Dropout(p=0.95, inplace=False)
    )
    (blocks): ModuleList(
      (0): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.95, inplace=False)
      )
      (1): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.95, inplace=False)
      )
      (2): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.95, inplace=False)
      )
      (3): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.95, inplace=False)
      )
      (4): SparseChemBlock(
        (linear): Linear(in_features=700, out_features=700, bias=True)
        (non_linear): ReLU()
        (dropout): Dropout(p=0.95, inplace=False)
      )
    )
    (residuals): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
      (4): None
    )
  )
  (task1_fc1_c0): SparseChem_Classification_Module(
    (linear): Linear(in_features=700, out_features=100, bias=True)
  )
)}

----------------
optimizers     :
----------------
 {'weights': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
), 'alphas': Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
)}

----------------
schedulers     :
----------------
factor                        : 0.75 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0005
) 
min_lrs                       : [0] 
patience                      : 50 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 
factor                        : 0.3 
optimizer                     : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0.0001
) 
min_lrs                       : [0, 0] 
patience                      : 10.0 
verbose                       : True 
cooldown                      : 0 
cooldown_counter              : 0 
mode                          : min 
threshold                     : 0.0001 
threshold_mode                : rel 
best                          : inf 
num_bad_epochs                : 0 
mode_worse                    : inf 
eps                           : 1e-08 
last_epoch                    : 0 

 training preparation: - check for CUDA - cuda available as device id: [0]

Job 51120406 finished : Wed Apr 20 11:11:34 CEST 2022
   9 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0230   3.382e-04   0.000e+00    0.0234 |   0.00003   0.42611   0.94283   0.88372   0.93786   0.93292 |   0.1173   1.155e-04   0.000e+00    0.1174 |  9.7 |

Job 51120406 finished : Wed Apr 20 11:11:35 CEST 2022

Job 51120406 finished : Wed Apr 20 11:11:36 CEST 2022


Job 51120406 finished : Wed Apr 20 11:11:39 CEST 2022
Job 51120406 finished : Wed Apr 20 11:11:39 CEST 2022
  10 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0079   3.382e-04   0.000e+00    0.0082 |   0.00003   0.53870   0.93627   0.85253   0.93133   0.92785 |   0.1249   1.155e-04   0.000e+00    0.1250 | 12.3 |
  11 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0076   3.382e-04   0.000e+00    0.0080 |   0.00004   0.74698   0.93421   0.86894   0.92711   0.92140 |   0.1819   1.155e-04   0.000e+00    0.1820 |  9.3 |
  12 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0228   3.382e-04   0.000e+00    0.0231 |   0.00004   0.70562   0.94053   0.89053   0.93593   0.93033 |   0.1645   1.155e-04   0.000e+00    0.1646 | 14.7 |
  13 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0101   3.382e-04   0.000e+00    0.0105 |   0.00003   0.58534   0.92902   0.87147   0.92204   0.92211 |   0.1289   1.155e-04   0.000e+00    0.1290 | 11.3 |
  14 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0022   3.382e-04   0.000e+00    0.0026 |   0.00003   0.53528   0.93863   0.88108   0.93255   0.92944 |   0.1323   1.155e-04   0.000e+00    0.1324 |  9.7 |
  15 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0068   3.382e-04   0.000e+00    0.0071 |   0.00003   0.61642   0.92557   0.85612   0.91756   0.92502 |   0.1269   1.155e-04   0.000e+00    0.1270 | 12.2 |
  16 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0008 |   0.00003   0.52466   0.94561   0.89668   0.94050   0.93943 |   0.1086   1.155e-04   0.000e+00    0.1087 |  9.9 |
  17 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0295   3.382e-04   0.000e+00    0.0299 |   0.00003   0.50865   0.94132   0.89603   0.93585   0.93116 |   0.1172   1.155e-04   0.000e+00    0.1173 | 11.9 |
  18 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0504   3.382e-04   0.000e+00    0.0508 |   0.00002   0.45106   0.94442   0.89694   0.93951   0.93558 |   0.1024   1.155e-04   0.000e+00    0.1026 |  9.8 |
  19 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0188   3.382e-04   0.000e+00    0.0192 |   0.00003   0.63039   0.93590   0.88376   0.93049   0.92416 |   0.1287   1.155e-04   0.000e+00    0.1288 | 11.8 |
  20 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0440   3.382e-04   0.000e+00    0.0443 |   0.00003   0.57727   0.92622   0.87476   0.91765   0.92491 |   0.1343   1.155e-04   0.000e+00    0.1344 |  9.8 |
  21 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0313   3.382e-04   0.000e+00    0.0316 |   0.00003   0.55665   0.92995   0.89166   0.92204   0.92819 |   0.1232   1.155e-04   0.000e+00    0.1233 | 11.5 |
  22 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0005   3.382e-04   0.000e+00    0.0009 |   0.00003   0.54319   0.95183   0.89162   0.94856   0.94250 |   0.1292   1.155e-04   0.000e+00    0.1293 |  9.8 |
Previous best_epoch:     7   best iter:   805,   best_value: 0.95071
New      best_epoch:    22   best iter:  2530,   best_value: 0.95183
  23 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0020   3.382e-04   0.000e+00    0.0023 |   0.00003   0.60571   0.93443   0.88139   0.92920   0.92803 |   0.1229   1.155e-04   0.000e+00    0.1230 | 14.5 |
  24 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0310   3.382e-04   0.000e+00    0.0314 |   0.00003   0.56706   0.93864   0.87116   0.93250   0.92523 |   0.1281   1.155e-04   0.000e+00    0.1283 | 13.4 |
  25 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0054   3.382e-04   0.000e+00    0.0057 |   0.00004   0.57830   0.93531   0.86162   0.92837   0.92674 |   0.1630   1.155e-04   0.000e+00    0.1631 | 10.2 |
 Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |   logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total | time |
  26 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0108   3.382e-04   0.000e+00    0.0112 |   0.00003   0.59198   0.92928   0.85364   0.92181   0.92541 |   0.1322   1.155e-04   0.000e+00    0.1323 | 12.6 |
  27 | 1.00e-03  1.00e-03  1.00e-03  4.00e+00 |   0.0015   3.382e-04   0.000e+00    0.0018 |   0.00004   0.81579   0.91472   0.84845   0.90446   0.91908 |   0.1588   1.155e-04   0.000e+00    0.1589 | 13.7 |
Epoch    27: reducing learning rate of group 0 to 3.0000e-04.
Epoch    27: reducing learning rate of group 1 to 3.0000e-04.
  28 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0007   3.382e-04   0.000e+00    0.0010 |   0.00003   0.71821   0.93063   0.85850   0.92423   0.92098 |   0.1497   1.155e-04   0.000e+00    0.1499 |  9.8 |
  29 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0050   3.382e-04   0.000e+00    0.0053 |   0.00004   0.77245   0.92645   0.85326   0.91959   0.92052 |   0.1809   1.155e-04   0.000e+00    0.1811 | 11.8 |
  30 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0007 |   0.00004   0.74955   0.92763   0.87048   0.92087   0.92128 |   0.1527   1.155e-04   0.000e+00    0.1528 |  9.8 |
  31 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0010   3.382e-04   0.000e+00    0.0013 |   0.00004   0.77167   0.93077   0.86006   0.92368   0.92491 |   0.1672   1.155e-04   0.000e+00    0.1673 | 13.8 |
  32 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0006 |   0.00004   0.81124   0.92629   0.85265   0.91951   0.92213 |   0.1760   1.155e-04   0.000e+00    0.1761 |  9.8 |
  33 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0008   3.382e-04   0.000e+00    0.0011 |   0.00004   0.74339   0.92964   0.85618   0.92372   0.92437 |   0.1577   1.155e-04   0.000e+00    0.1578 |  9.9 |
  34 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0007 |   0.00004   0.78739   0.92615   0.86551   0.91840   0.92369 |   0.1590   1.155e-04   0.000e+00    0.1591 |  9.8 |
  35 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0008   3.382e-04   0.000e+00    0.0011 |   0.00004   0.75450   0.92438   0.86126   0.91627   0.92575 |   0.1694   1.155e-04   0.000e+00    0.1696 | 10.8 |
  36 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.78568   0.92761   0.86629   0.92062   0.92578 |   0.1681   1.155e-04   0.000e+00    0.1683 | 14.3 |
  37 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0052   3.382e-04   0.000e+00    0.0055 |   0.00004   0.70034   0.92521   0.86077   0.91803   0.92406 |   0.1607   1.155e-04   0.000e+00    0.1609 |  9.4 |
  38 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0003   3.382e-04   0.000e+00    0.0006 |   0.00004   0.67021   0.91598   0.85278   0.90841   0.92162 |   0.1599   1.155e-04   0.000e+00    0.1600 | 11.6 |
  39 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0316   3.382e-04   0.000e+00    0.0320 |   0.00004   0.72976   0.92222   0.85422   0.91412   0.92254 |   0.1536   1.155e-04   0.000e+00    0.1537 |  9.4 |
  40 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.70704   0.92353   0.85878   0.91518   0.92235 |   0.1656   1.155e-04   0.000e+00    0.1658 | 12.3 |
  41 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.67522   0.92011   0.84986   0.91168   0.92234 |   0.1620   1.155e-04   0.000e+00    0.1621 |  9.4 |
  42 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0039   3.382e-04   0.000e+00    0.0042 |   0.00004   0.71498   0.91951   0.85452   0.91213   0.92154 |   0.1637   1.155e-04   0.000e+00    0.1638 | 12.7 |
  43 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.75950   0.91485   0.85693   0.90727   0.92521 |   0.1772   1.155e-04   0.000e+00    0.1773 |  9.4 |
  44 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0029   3.382e-04   0.000e+00    0.0032 |   0.00004   0.80395   0.92762   0.86754   0.92004   0.92609 |   0.1668   1.155e-04   0.000e+00    0.1669 | 13.5 |
  45 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0012   3.382e-04   0.000e+00    0.0015 |   0.00004   0.83391   0.93027   0.86436   0.92380   0.92922 |   0.1732   1.155e-04   0.000e+00    0.1734 | 11.8 |
  46 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0003   3.382e-04   0.000e+00    0.0007 |   0.00004   0.65261   0.93453   0.86386   0.92805   0.92882 |   0.1549   1.155e-04   0.000e+00    0.1550 |  9.8 |
  47 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0007 |   0.00004   0.71984   0.93288   0.86715   0.92670   0.92322 |   0.1671   1.155e-04   0.000e+00    0.1672 | 12.3 |
  48 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0007   3.382e-04   0.000e+00    0.0010 |   0.00004   0.71134   0.92762   0.85889   0.92115   0.92344 |   0.1749   1.155e-04   0.000e+00    0.1751 | 12.2 |
  49 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0016   3.382e-04   0.000e+00    0.0019 |   0.00004   0.72380   0.93395   0.88035   0.92751   0.92356 |   0.1582   1.155e-04   0.000e+00    0.1583 | 11.6 |
  50 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0006   3.382e-04   0.000e+00    0.0009 |   0.00004   0.73580   0.93487   0.86329   0.92876   0.92802 |   0.1744   1.155e-04   0.000e+00    0.1745 |  9.9 |
 Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |   logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total | time |
  51 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0008   3.382e-04   0.000e+00    0.0011 |   0.00003   0.70224   0.93214   0.86479   0.92440   0.92005 |   0.1451   1.155e-04   0.000e+00    0.1452 | 10.2 |
  52 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.76121   0.93891   0.87344   0.93311   0.93293 |   0.1814   1.155e-04   0.000e+00    0.1815 |  9.9 |
  53 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0017   3.382e-04   0.000e+00    0.0021 |   0.00004   0.61790   0.92823   0.86090   0.92294   0.92975 |   0.1534   1.155e-04   0.000e+00    0.1536 | 12.4 |
  54 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0006 |   0.00004   0.69975   0.92450   0.85662   0.91809   0.92077 |   0.1617   1.155e-04   0.000e+00    0.1618 | 12.8 |
  55 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.71324   0.91804   0.85249   0.91095   0.91743 |   0.1611   1.155e-04   0.000e+00    0.1612 |  9.4 |
  56 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0005   3.382e-04   0.000e+00    0.0008 |   0.00003   0.63046   0.92228   0.85550   0.91554   0.92358 |   0.1494   1.155e-04   0.000e+00    0.1495 | 12.7 |
  57 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0005   3.382e-04   0.000e+00    0.0008 |   0.00004   0.64687   0.92250   0.86439   0.91509   0.91939 |   0.1547   1.155e-04   0.000e+00    0.1548 | 11.5 |
  58 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0003   3.382e-04   0.000e+00    0.0006 |   0.00004   0.67115   0.92808   0.87238   0.92057   0.92245 |   0.1604   1.155e-04   0.000e+00    0.1605 | 11.9 |
  59 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00003   0.66011   0.92150   0.86532   0.91379   0.91986 |   0.1476   1.155e-04   0.000e+00    0.1477 |  9.4 |
  60 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0017   3.382e-04   0.000e+00    0.0021 |   0.00004   0.62155   0.92005   0.86754   0.90932   0.92057 |   0.1508   1.155e-04   0.000e+00    0.1509 | 10.3 |
  61 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0005   3.382e-04   0.000e+00    0.0008 |   0.00003   0.66087   0.92359   0.86365   0.91598   0.91692 |   0.1462   1.155e-04   0.000e+00    0.1463 |  9.6 |
  62 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0003   3.382e-04   0.000e+00    0.0007 |   0.00004   0.76049   0.92514   0.85088   0.91861   0.91904 |   0.1651   1.155e-04   0.000e+00    0.1652 | 10.5 |
  63 | 3.00e-04  3.00e-04  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.70729   0.93146   0.86223   0.92593   0.92613 |   0.1580   1.155e-04   0.000e+00    0.1581 |  9.4 |
Epoch    63: reducing learning rate of group 0 to 9.0000e-05.
Epoch    63: reducing learning rate of group 1 to 9.0000e-05.
  64 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.73937   0.93073   0.86220   0.92511   0.92583 |   0.1639   1.155e-04   0.000e+00    0.1640 | 10.6 |
  65 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.72470   0.92846   0.85625   0.92289   0.92438 |   0.1731   1.155e-04   0.000e+00    0.1733 |  9.5 |
  66 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.76737   0.92345   0.85041   0.91555   0.92443 |   0.1660   1.155e-04   0.000e+00    0.1661 | 12.5 |
  67 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.72889   0.92749   0.85693   0.92159   0.92574 |   0.1668   1.155e-04   0.000e+00    0.1669 | 11.8 |
  68 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.73087   0.92310   0.85336   0.91727   0.92345 |   0.1684   1.155e-04   0.000e+00    0.1685 |  9.8 |
  69 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.75615   0.92288   0.85349   0.91704   0.92369 |   0.1720   1.155e-04   0.000e+00    0.1721 | 12.4 |
  70 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.71366   0.92320   0.85368   0.91728   0.92669 |   0.1637   1.155e-04   0.000e+00    0.1638 | 11.8 |
  71 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0000   3.382e-04   0.000e+00    0.0004 |   0.00004   0.74145   0.92703   0.85756   0.92090   0.92669 |   0.1703   1.155e-04   0.000e+00    0.1705 | 11.4 |
  72 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.74348   0.92496   0.85702   0.91914   0.92669 |   0.1746   1.155e-04   0.000e+00    0.1747 |  9.9 |
  73 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.74150   0.91997   0.85295   0.91158   0.92333 |   0.1770   1.155e-04   0.000e+00    0.1771 | 10.2 |
  74 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0003   3.382e-04   0.000e+00    0.0006 |   0.00004   0.74655   0.92006   0.85305   0.91199   0.92662 |   0.1727   1.155e-04   0.000e+00    0.1728 |  9.9 |
  75 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0020   3.382e-04   0.000e+00    0.0024 |   0.00004   0.76690   0.92515   0.85537   0.91946   0.92558 |   0.1850   1.155e-04   0.000e+00    0.1851 | 10.1 |
 Ep  | Trunk LR  Heads LR  Polcy LR  Gmbl Tmp |  trn tsk    trn spar    trn shar   trn ttl |   logloss   bceloss  avg prec    aucroc     aucpr    f1_max |  val tsk    val spar    val shar     total | time |
  76 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0003   3.382e-04   0.000e+00    0.0006 |   0.00004   0.79906   0.92495   0.85515   0.91925   0.92587 |   0.1747   1.155e-04   0.000e+00    0.1748 |  9.9 |
  77 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0000   3.382e-04   0.000e+00    0.0004 |   0.00004   0.71896   0.91980   0.85036   0.91172   0.92598 |   0.1661   1.155e-04   0.000e+00    0.1662 |  9.5 |
  78 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.74951   0.92431   0.85461   0.91850   0.92592 |   0.1657   1.155e-04   0.000e+00    0.1659 |  9.9 |
  79 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0005   3.382e-04   0.000e+00    0.0008 |   0.00004   0.66452   0.91960   0.85054   0.91048   0.92668 |   0.1563   1.155e-04   0.000e+00    0.1565 | 12.7 |
  80 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.72427   0.91905   0.84936   0.91096   0.92586 |   0.1673   1.155e-04   0.000e+00    0.1674 | 11.8 |
  81 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0007 |   0.00004   0.72339   0.92095   0.85500   0.91283   0.92654 |   0.1658   1.155e-04   0.000e+00    0.1659 |  9.4 |
  82 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.70573   0.92110   0.85611   0.91280   0.92668 |   0.1709   1.155e-04   0.000e+00    0.1710 | 12.4 |
  83 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0006   3.382e-04   0.000e+00    0.0009 |   0.00004   0.72989   0.91491   0.84903   0.90664   0.92668 |   0.1697   1.155e-04   0.000e+00    0.1698 |  9.4 |
  84 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.71481   0.91858   0.84861   0.91028   0.92668 |   0.1858   1.155e-04   0.000e+00    0.1859 | 13.2 |
  85 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.69464   0.91894   0.85394   0.91077   0.92667 |   0.1590   1.155e-04   0.000e+00    0.1591 |  9.3 |
  86 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.73779   0.92037   0.85182   0.91194   0.92572 |   0.1684   1.155e-04   0.000e+00    0.1685 | 12.7 |
  87 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0005 |   0.00004   0.71819   0.92091   0.85311   0.91264   0.92658 |   0.1896   1.155e-04   0.000e+00    0.1898 |  9.5 |
  88 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0000   3.382e-04   0.000e+00    0.0004 |   0.00004   0.70792   0.92680   0.85822   0.92087   0.92572 |   0.1762   1.155e-04   0.000e+00    0.1763 | 12.1 |
  89 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0007 |   0.00004   0.73614   0.91778   0.84897   0.90914   0.92543 |   0.1784   1.155e-04   0.000e+00    0.1786 | 11.6 |
  90 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.71677   0.92311   0.85427   0.91714   0.92698 |   0.1743   1.155e-04   0.000e+00    0.1744 |  9.8 |
  91 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0046   3.382e-04   0.000e+00    0.0049 |   0.00004   0.65120   0.92841   0.86183   0.92269   0.92687 |   0.1551   1.155e-04   0.000e+00    0.1552 | 12.1 |
  92 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0018   3.382e-04   0.000e+00    0.0022 |   0.00004   0.68438   0.92248   0.85938   0.91668   0.92672 |   0.1696   1.155e-04   0.000e+00    0.1697 | 11.6 |
  93 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0005 |   0.00004   0.71756   0.91996   0.85855   0.91351   0.92184 |   0.1625   1.155e-04   0.000e+00    0.1626 | 11.4 |
  94 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.73633   0.92563   0.85976   0.91947   0.92577 |   0.1698   1.155e-04   0.000e+00    0.1700 |  9.9 |
  95 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0000   3.382e-04   0.000e+00    0.0004 |   0.00004   0.71338   0.92228   0.85533   0.91657   0.92677 |   0.1662   1.155e-04   0.000e+00    0.1663 | 11.1 |
  96 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0004   3.382e-04   0.000e+00    0.0007 |   0.00004   0.72256   0.92202   0.85453   0.91572   0.92343 |   0.1735   1.155e-04   0.000e+00    0.1736 |  9.9 |
  97 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0000   3.382e-04   0.000e+00    0.0004 |   0.00004   0.72455   0.92241   0.85506   0.91662   0.92642 |   0.1731   1.155e-04   0.000e+00    0.1732 | 10.0 |
  98 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0001   3.382e-04   0.000e+00    0.0004 |   0.00004   0.76067   0.92563   0.85888   0.92012   0.92671 |   0.1849   1.155e-04   0.000e+00    0.1850 |  9.9 |
  99 | 9.00e-05  9.00e-05  1.00e-03  4.00e+00 |   0.0002   3.382e-04   0.000e+00    0.0006 |   0.00004   0.70806   0.92730   0.85863   0.92137   0.92642 |   0.1695   1.155e-04   0.000e+00    0.1696 |  9.9 |
Epoch    99: reducing learning rate of group 0 to 2.7000e-05.
Epoch    99: reducing learning rate of group 1 to 2.7000e-05.
 100 | 2.70e-05  2.70e-05  1.00e-03  4.00e+00 |   0.0006   3.382e-04   0.000e+00    0.0009 |   0.00004   0.69700   0.92643   0.85846   0.92085   0.92672 |   0.1647   1.155e-04   0.000e+00    0.1648 |  9.9 |
 save warmup val_metrics to :  metrics_warmup_ep_100.pickle
[Final] ep:100  it:11500 -  Total Loss: 0.1648     
Task: 0.1647   Sparsity: 1.15483e-04    Sharing: 0.00000e+00 

 ep:  100   softmax      s        
 ----- ----------------- -    
  0    0.5000    0.5000  1
  1    0.4996    0.5004  0
  2    0.5005    0.4995  1
  3    0.4999    0.5001  0
  4    0.5005    0.4995  1


 ep:  100   logits       s         
 ----- ----------------- -    
  0   -0.0000   -0.0000  1
  1    0.0013    0.0027  0
  2    0.0007   -0.0013  1
  3    0.0004    0.0006  0
  4    0.0005   -0.0017  1

Best Epoch :       22
Best Iteration :   2530 
Best Precision :   0.95183


{   'auc_pr': 0.9208468765397508,
    'avg_prec_score': 0.9264342040437786,
    'bceloss': 0.6970037489778338,
    'f1_max': 0.9267199568348344,
    'kappa': 0.5062113901717917,
    'kappa_max': 0.7736314439959184,
    'logloss': 3.82171851507384e-05,
    'p_f1_max': 0.681644048598375,
    'p_kappa_max': 0.8325684892910498,
    'roc_auc_score': 0.8584605890115621,
    'sc_loss': 0.005489261693817725}

Job 51120406 finished : Wed Apr 20 11:28:27 CEST 2022
========================================================================
Epilogue args:
Date: Wed Apr 20 11:28:29 CEST 2022
Allocated nodes:
r23g37
r23g37
r23g37
r23g37
r23g37
r23g37
r23g37
r23g37
r23g37
Job ID: 51120406.tier2-p-moab-2.tier2.hpc.kuleuven.be
User ID: vsc32647
Group ID: vsc32647
Job Name: AS-resid
Session ID: 10211
Resource List: nodes=1:ppn=9:gpus=1,walltime=06:00:00,neednodes=1:ppn=9:gpus=1,pmem=5gb
Resources Used: cput=02:37:40,vmem=518275492kb,walltime=00:19:50,mem=5041668kb,energy_used=0
Queue Name: q24h
Account String: lp_symbiosys
-------------------------------------------------------------------------
time: 1190
nodes: 1
procs: 9
account: lp_symbiosys
