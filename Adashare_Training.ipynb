{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408f1261",
   "metadata": {},
   "source": [
    "## Initialization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d574cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:53:53.445651Z",
     "start_time": "2022-03-25T13:53:51.629808Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:08.233990Z",
     "iopub.status.busy": "2022-01-07T22:44:08.233053Z",
     "iopub.status.idle": "2022-01-07T22:44:08.273284Z",
     "shell.execute_reply": "2022-01-07T22:44:08.271908Z",
     "shell.execute_reply.started": "2022-01-07T22:44:08.233943Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "import types\n",
    "import copy, pprint\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import numpy  as np\n",
    "import torch  \n",
    "import wandb\n",
    "import pandas as pd\n",
    "from utils.notebook_modules import initialize, init_dataloaders, init_environment, init_wandb, \\\n",
    "                                   training_prep, disp_dataloader_info,disp_info_1, \\\n",
    "                                   warmup_phase, weight_policy_training, disp_gpu_info\n",
    "\n",
    "from utils.util import (print_separator, print_heading, timestring, print_loss, load_from_pickle) #, print_underline, \n",
    "#                       print_dbg, get_command_line_args ) \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions(precision=6, linewidth=132)\n",
    "pd.options.display.width = 132\n",
    "# torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)\n",
    "# sys.path.insert(0, '/home/kbardool/kusanagi/AdaSparseChem/src')\n",
    "# print(sys.path)\n",
    "# disp_gpu_info() \n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Adashare_Training.ipynb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33ac6e",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee652c57",
   "metadata": {},
   "source": [
    "### Parse Input Args  - Read YAML config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bb98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T09:44:48.366583Z",
     "start_time": "2022-03-25T09:44:48.329620Z"
    }
   },
   "outputs": [],
   "source": [
    "# RESUME_MODEL_CKPT = 'model_train_ep_25_seed_0088'\n",
    "\n",
    "## For RESTARTING\n",
    "##\n",
    "# input_args = \" --config yamls/chembl_3task_train.yaml \" \\\n",
    "#              \" --resume \" \\\n",
    "#              \" --exp_id      330i85cg\" \\\n",
    "#              \" --exp_name    0308_1204\" \\\n",
    "#              \" --exp_desc    Train with dropout 0.5\" \\\n",
    "#              \" --seed_idx    0 \"\\\n",
    "#              \" --batch_size  128\" \\\n",
    "#              \" --lambda_sparsity  0.01\"\\\n",
    "#              \" --lambda_sharing   0.01\" \n",
    "## get command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3a7f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:12.015814Z",
     "start_time": "2022-03-25T13:54:11.989447Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    }
   },
   "outputs": [],
   "source": [
    "##  For Initiating \n",
    "##\n",
    "input_args = \" --config yamls/chembl_3task_train.yaml \" \\\n",
    "             \" --exp_desc    6 lyrs,dropout 0.5, weight 105 bch/ep policy 105 bch/ep \" \\\n",
    "             \" --hidden_size   50 50 50 50 50 50   \" \\\n",
    "             \" --tail_hidden_size   50\" \\\n",
    "             \" --seed_idx            0\" \\\n",
    "             \" --batch_size        128\" \\\n",
    "             \" --task_lr          0.001\" \\\n",
    "             \" --backbone_lr      0.001\" \\\n",
    "             \" --policy_lr        0.01\" \\\n",
    "             \" --lambda_sparsity  0.02\" \\\n",
    "             \" --lambda_sharing   0.01\" \n",
    "\n",
    "#              \" --hidden_size   100 100 100 100 100 100\" \\\n",
    "#              \" --tail_hidden_size  100 \" \\\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc14177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:13.684693Z",
     "start_time": "2022-03-25T13:54:12.739939Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  command line parms : \n",
      "------------------------\n",
      " config...................  yamls/chembl_3task_train.yaml\n",
      " exp_id...................  None\n",
      " exp_name.................  None\n",
      " folder_sfx...............  None\n",
      " exp_desc.................  6 lyrs,dropout 0.5, weight 105 bch/ep policy 105 bch/ep\n",
      " hidden_sizes.............  [50, 50, 50, 50, 50, 50]\n",
      " tail_hidden_size.........  50\n",
      " seed_idx.................  0\n",
      " batch_size...............  128\n",
      " backbone_lr..............  0.001\n",
      " task_lr..................  0.001\n",
      " policy_lr................  0.01\n",
      " decay_lr_rate............  None\n",
      " decay_lr_freq............  None\n",
      " lambda_sparsity..........  0.02\n",
      " lambda_sharing...........  0.01\n",
      " gpu_ids..................  [0]\n",
      " resume...................  False\n",
      " cpu......................  False\n",
      "\n",
      "\n",
      "\n",
      "##################################################\n",
      "################### READ YAML ####################\n",
      "##################################################\n",
      "\n",
      "\n",
      " log_dir              create folder:  ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      " result_dir           folder exists:  ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      " checkpoint_dir       folder exists:  ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " experiment name       : 0325_1454 \n",
      " experiment id         : 12ys3f1b \n",
      " folder_name           : 50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001 \n",
      " experiment description: 6 lyrs,dropout 0.5, weight 105 bch/ep policy 105 bch/ep\n",
      " Random seeds          : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      " Random  seed used     : 88 \n",
      " log folder            : ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      " checkpoint folder     : ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      " Gpu ids               : [0]\n",
      " Seed index            : 0\n",
      " policy_iter           : best\n",
      " Data Split ratios     : [0.725, 0.225, 0.05]\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "        project_name : AdaSparseChem\n",
      "              exp_id : 12ys3f1b\n",
      "            exp_name : 0325_1454\n",
      "          exp_folder : 50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      "     exp_description : 6 lyrs,dropout 0.5, weight 105 bch/ep policy 105 bch/ep\n",
      "          folder_sfx : None\n",
      "         random_seed : 88\n",
      "           seed_list : [88, 45, 50, 100, 44, 48, 2048, 2222, 9999]\n",
      "                 cpu : False\n",
      "             gpu_ids : [0]\n",
      "            backbone : SparseChem\n",
      "               tasks : ['class', 'class', 'class']\n",
      "     tasks_num_class : [5, 5, 5]\n",
      "             lambdas : [1, 1, 1]\n",
      "        policy_model : task-specific\n",
      "             verbose : False\n",
      "       backbone_orig : ResNet18\n",
      "          tasks_orig : ['seg', 'sn']\n",
      "     input_size_freq : None\n",
      "          input_size : 32000\n",
      " first_non_linearity : relu\n",
      "middle_non_linearity : relu\n",
      "  last_non_linearity : relu\n",
      "      middle_dropout : 0.5\n",
      "        last_dropout : 0.5\n",
      "   class_output_size : None\n",
      "    regr_output_size : None\n",
      "              policy : True\n",
      "           is_sparse : True\n",
      "          is_sharing : True\n",
      "diff_sparsity_weights : False\n",
      "          skip_layer : 0\n",
      "       is_curriculum : False\n",
      "    curriculum_speed : 3\n",
      "              fix_BN : False\n",
      "     retrain_from_pl : False\n",
      "\n",
      "train\n",
      "-----\n",
      "          batch_size : 128\n",
      "       warmup_epochs : 75\n",
      "     training_epochs : 125\n",
      "         total_iters : 25000\n",
      "       warm_up_iters : None\n",
      "             task_lr : 0.001\n",
      "         backbone_lr : 0.001\n",
      "       decay_lr_rate : 0.75\n",
      "       decay_lr_freq : 40\n",
      "policy_decay_lr_rate : 0.75\n",
      "policy_decay_lr_freq : 50\n",
      "           policy_lr : 0.01\n",
      "     lambda_sparsity : 0.02\n",
      "      lambda_sharing : 0.01\n",
      "        lambda_tasks : 1\n",
      "         init_method : random\n",
      "           init_temp : 4\n",
      "          decay_temp : 0.965\n",
      "     decay_temp_freq : 16\n",
      "     init_neg_logits : None\n",
      "       hard_sampling : False\n",
      "            val_freq : 500\n",
      "          print_freq : -1\n",
      "           val_iters : -1\n",
      "              resume : False\n",
      "      retrain_resume : False\n",
      "         policy_iter : best\n",
      "          which_iter : warmup\n",
      "\n",
      "paths\n",
      "-----\n",
      "             log_dir : ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      "          result_dir : ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      "      checkpoint_dir : ../experiments/AdaSparseChem/50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      "\n",
      "dataload\n",
      "--------\n",
      "             dataset : Chembl_23_mini\n",
      "            dataroot : /home/kbardool/kusanagi/MLDatasets/chembl_23mini_synthetic\n",
      "                   x : chembl_23mini_x.npy\n",
      "      x_split_ratios : [0.725, 0.225, 0.05]\n",
      "             folding : chembl_23mini_folds.npy\n",
      "         fold_inputs : 32000\n",
      "     input_transform : None\n",
      "             y_tasks : ['chembl_23mini_adashare_y1_bin_sparse.npy', 'chembl_23mini_adashare_y2_bin_sparse.npy', 'chembl_23mini_adashare_y3_bin_sparse.npy']\n",
      "            y_censor : None\n",
      "       weights_class : None\n",
      "              crop_h : 321\n",
      "              crop_w : 321\n",
      "   min_samples_class : 5\n",
      "             fold_va : 0\n",
      "             fold_te : None\n",
      "\n",
      "SC\n",
      "--\n",
      "      normalize_loss : None\n",
      "        hidden_sizes : [50, 50, 50, 50, 50, 50]\n",
      "    tail_hidden_size : 50\n"
     ]
    }
   ],
   "source": [
    "opt, ns = initialize(input_args, build_folders = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bfa8d",
   "metadata": {},
   "source": [
    "### Setup Dataloader and Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c631eb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:21.809472Z",
     "start_time": "2022-03-25T13:54:21.114568Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:16.229028Z",
     "iopub.status.busy": "2022-01-07T22:44:16.227544Z",
     "iopub.status.idle": "2022-01-07T22:44:16.659397Z",
     "shell.execute_reply": "2022-01-07T22:44:16.658348Z",
     "shell.execute_reply.started": "2022-01-07T22:44:16.228966Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "############### CREATE DATALOADERS ###############\n",
      "##################################################\n",
      "\n",
      " trainset.y_class                                   :  [(13331, 5), (13331, 5), (13331, 5)] \n",
      " trainset1.y_class                                  :  [(13331, 5), (13331, 5), (13331, 5)] \n",
      " trainset2.y_class                                  :  [(13331, 5), (13331, 5), (13331, 5)] \n",
      " valset.y_class                                     :  [(4137, 5), (4137, 5), (4137, 5)]  \n",
      " testset.y_class                                    :  [(920, 5), (920, 5), (920, 5)]  \n",
      "                                 \n",
      " size of training set 0 (warm up)                   :  13331 \n",
      " size of training set 1 (network parms)             :  13331 \n",
      " size of training set 2 (policy weights)            :  13331 \n",
      " size of validation set                             :  4137 \n",
      " size of test set                                   :  920 \n",
      "                               Total                :  45050 \n",
      "                                 \n",
      " lenght (# batches) in training 0 (warm up)         :  105 \n",
      " lenght (# batches) in training 1 (network parms)   :  105 \n",
      " lenght (# batches) in training 2 (policy weights)  :  105 \n",
      " lenght (# batches) in validation dataset           :  33 \n",
      " lenght (# batches) in test dataset                 :  29 \n",
      "                                \n",
      "##################################################\n",
      "############# CREATE THE ENVIRONMENT #############\n",
      "##################################################\n",
      " device is  cuda:0\n",
      "--------------------------------------------------------\n",
      "* SparseChemEnv_Dev environment successfully created\n",
      "-------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dldrs = init_dataloaders(opt)\n",
    "\n",
    "disp_dataloader_info(dldrs)\n",
    "\n",
    "environ = init_environment(ns, opt, is_train = True, policy_learning = False, display_cfg = False)\n",
    "\n",
    "# ********************************************************************\n",
    "# **************** define optimizer and schedulers *******************\n",
    "# ********************************************************************                                \n",
    "environ.define_optimizer(policy_learning=False)\n",
    "environ.define_scheduler(policy_learning=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ab8c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:21.931412Z",
     "start_time": "2022-03-25T13:54:21.824179Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Current LR: 0.01\n",
      " Current LR: 0.001\n",
      " Current LR: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(f\" Current LR: {environ.optimizers['alphas'].param_groups[0]['lr'] }\")\n",
    "print(f\" Current LR: {environ.optimizers['weights'].param_groups[0]['lr']}\")\n",
    "print(f\" Current LR: {environ.optimizers['weights'].param_groups[1]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677fa3f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:26.842030Z",
     "start_time": "2022-03-25T13:54:26.814962Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.optimizers['weights'].param_groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fdeee",
   "metadata": {},
   "source": [
    "###  Weights and Biases Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c2469c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:32.188644Z",
     "start_time": "2022-03-25T13:54:27.998671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkbardool\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12ys3f1b 0325_1454 AdaSparseChem\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kbardool/kusanagi/AdaSparseChem/wandb/run-20220325_145428-12ys3f1b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/kbardool/AdaSparseChem/runs/12ys3f1b\" target=\"_blank\">0325_1454</a></strong> to <a href=\"http://localhost:8080/kbardool/AdaSparseChem\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROJECT NAME: AdaSparseChem\n",
      " RUN ID      : 12ys3f1b \n",
      " RUN NAME    : 0325_1454\n",
      " PROJECT NAME: AdaSparseChem\n",
      " RUN ID      : 12ys3f1b \n",
      " RUN NAME    : 0325_1454\n"
     ]
    }
   ],
   "source": [
    "init_wandb(ns, opt, environment = environ)\n",
    "\n",
    "print(f\" PROJECT NAME: {ns.wandb_run.project}\\n\"\n",
    "      f\" RUN ID      : {ns.wandb_run.id} \\n\"\n",
    "      f\" RUN NAME    : {ns.wandb_run.name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d949180d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:33.016151Z",
     "start_time": "2022-03-25T13:54:32.961643Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d738062",
   "metadata": {},
   "source": [
    "### Initiate / Resume Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd2a36e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:34.121511Z",
     "start_time": "2022-03-25T13:54:34.066217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "############### Initiate Training  ###############\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "if opt['train']['resume']:\n",
    "    RESUME_MODEL_CKPT = \"\"\n",
    "    RESUME_METRICS_CKPT = \"\"    \n",
    "    print(opt['train']['which_iter'])\n",
    "    print(opt['paths']['checkpoint_dir'])\n",
    "    print(RESUME_MODEL_CKPT)\n",
    "    # opt['train']['resume'] = True\n",
    "    # opt['train']['which_iter'] = 'warmup_ep_40_seed_0088'\n",
    "    print_separator('Resume training')\n",
    "    loaded_iter, loaded_epoch = environ.load_checkpoint(RESUME_MODEL_CKPT, path = opt['paths']['checkpoint_dir'], verbose = True)\n",
    "    print(loaded_iter, loaded_epoch)    \n",
    "#     current_iter = environ.load_checkpoint(opt['train']['which_iter'])\n",
    "    environ.networks['mtl-net'].reset_logits()\n",
    "    val_metrics = load_from_pickle(opt['paths']['checkpoint_dir'], RESUME_METRICS_CKPT)\n",
    "    # training_prep(ns, opt, environ, dldrs, epoch = loaded_epoch, iter = loaded_iter )\n",
    "\n",
    "else:\n",
    "    print_separator('Initiate Training ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7774f",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b6afdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:39.845858Z",
     "start_time": "2022-03-25T13:54:39.502524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cuda available [0]\n",
      " set print_freq to length of train loader: 105\n",
      " set eval_iters to length of val loader  : 33\n"
     ]
    }
   ],
   "source": [
    "training_prep(ns, opt, environ, dldrs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea212ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:39.947663Z",
     "start_time": "2022-03-25T13:54:39.872051Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:18.146907Z",
     "iopub.status.busy": "2022-01-07T22:44:18.145721Z",
     "iopub.status.idle": "2022-01-07T22:44:18.191126Z",
     "shell.execute_reply": "2022-01-07T22:44:18.189994Z",
     "shell.execute_reply.started": "2022-01-07T22:44:18.146867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Num_blocks                : 6                                \n",
      "\n",
      " batch size                : 128 \n",
      " batches/ Weight trn epoch : 105 \n",
      " batches/ Policy trn epoch : 105                                 \n",
      "\n",
      " Print Frequency           : -1 \n",
      " Config Val Frequency      : 500 \n",
      " Config Val Iterations     : -1 \n",
      " Val iterations            : 33 \n",
      " which_iter                : warmup \n",
      " train_resume              : False                                 \n",
      " \n",
      " fix BN parms              : False \n",
      " Task LR                   : 0.001 \n",
      " Backbone LR               : 0.001                                 \n",
      "\n",
      " Sharing  regularization   : 0.01 \n",
      " Sparsity regularization   : 0.02 \n",
      " Task     regularization   : 1                                 \n",
      "\n",
      " Current epoch             : 0  \n",
      " Warm-up epochs            : 75 \n",
      " Training epochs           : 125\n"
     ]
    }
   ],
   "source": [
    "disp_info_1(ns, opt, environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bc6107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:40.253295Z",
     "start_time": "2022-03-25T13:54:40.175592Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:18.146907Z",
     "iopub.status.busy": "2022-01-07T22:44:18.145721Z",
     "iopub.status.idle": "2022-01-07T22:44:18.191126Z",
     "shell.execute_reply": "2022-01-07T22:44:18.189994Z",
     "shell.execute_reply.started": "2022-01-07T22:44:18.146867Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    folder: 50x6_0325_1454_plr0.01_sp0.02_sh0.01_lr0.001\n",
      "    layers: 6 [50, 50, 50, 50, 50, 50] \n",
      "    \n",
      "    middle dropout         : 0.5\n",
      "    last dropout           : 0.5\n",
      "    diff_sparsity_weights  : False\n",
      "    skip_layer             : 0\n",
      "    is_curriculum          : False\n",
      "    curriculum_speed       : 3\n",
      "    \n",
      "    task_lr                : 0.001\n",
      "    backbone_lr            : 0.001\n",
      "    decay_lr_rate          : 0.75\n",
      "    decay_lr_freq          : 40\n",
      "    \n",
      "    policy_lr              : 0.01\n",
      "    policy_decay_lr_rate   : 0.75\n",
      "    policy_decay_lr_freq   : 50\n",
      "    lambda_sparsity        : 0.02\n",
      "    lambda_sharing         : 0.01\n",
      "    lambda_tasks           : 1\n",
      "    \n",
      "    Gumbel init_temp       : 4\n",
      "    Gumbel decay_temp      : 0.965\n",
      "    Gumbel decay_temp_freq : 16\n",
      "    Logit init_method      : random\n",
      "    Logit init_neg_logits  : None\n",
      "    Logit hard_sampling    : False\n",
      "    Warm-up epochs         : 75\n",
      "    training epochs        : 125\n",
      "    Data split ratios      : [0.725, 0.225, 0.05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(environ.disp_for_excel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278d6d1",
   "metadata": {},
   "source": [
    "## Warmup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92380a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:54:43.631729Z",
     "start_time": "2022-03-25T13:54:43.563634Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 0\n",
      "------------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  75 - Run epochs 1 to 75\n",
      "------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# environ.display_trained_policy(ns.current_epoch,out=sys.stdout)\n",
    "# ns.stop_epoch_warmup = 10\n",
    "# ns.warmup_epochs = 10\n",
    "print(ns.warmup_epochs, ns.current_epoch)\n",
    "print_heading(f\" Last Epoch: {ns.current_epoch}   # of warm-up epochs to do:  {ns.warmup_epochs} - Run epochs {ns.current_epoch+1} to {ns.current_epoch + ns.warmup_epochs}\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be9d65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:15.856440Z",
     "start_time": "2022-03-25T13:54:44.502661Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      " Last Epoch: 0   # of warm-up epochs to do:  75 - Run epochs 1 to 75\n",
      "------------------------------------------------------------------------ \n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "    1 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |   10.5782   4.1595e-02   2.2045e-04   10.6200 |   0.69343   0.52137   0.51910   0.52092 |   10.4007   4.1595e-02   2.2045e-04    10.4426 |   6.2 |\n",
      "    2 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |   10.3769   4.1595e-02   2.2045e-04   10.4187 |   0.69258   0.54596   0.54898   0.54551 |   10.3883   4.1595e-02   2.2045e-04    10.4301 |   6.2 |\n",
      "    3 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |   10.3866   4.1595e-02   2.2045e-04   10.4285 |   0.68992   0.56623   0.57273   0.56581 |   10.3482   4.1595e-02   2.2045e-04    10.3900 |   7.4 |\n",
      "    4 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |   10.2932   4.1595e-02   2.2045e-04   10.3350 |   0.68581   0.58173   0.58970   0.58131 |   10.2871   4.1595e-02   2.2045e-04    10.3289 |   7.4 |\n",
      "    5 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |   10.2478   4.1595e-02   2.2045e-04   10.2897 |   0.68205   0.58857   0.59766   0.58815 |   10.2318   4.1595e-02   2.2045e-04    10.2736 |   7.5 |\n",
      "    6 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.7503   4.1595e-02   2.2045e-04    9.7921 |   0.67855   0.60059   0.60871   0.60023 |   10.1789   4.1595e-02   2.2045e-04    10.2207 |   8.6 |\n",
      "    7 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.9578   4.1595e-02   2.2045e-04    9.9996 |   0.67398   0.61035   0.61777   0.60998 |   10.1076   4.1595e-02   2.2045e-04    10.1494 |   6.9 |\n",
      "    8 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.9530   4.1595e-02   2.2045e-04    9.9948 |   0.67104   0.61847   0.62707   0.61811 |   10.0641   4.1595e-02   2.2045e-04    10.1059 |   7.4 |\n",
      "    9 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.9227   4.1595e-02   2.2045e-04    9.9645 |   0.66954   0.62293   0.62957   0.62255 |   10.0477   4.1595e-02   2.2045e-04    10.0895 |   6.7 |\n",
      "   10 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.2356   4.1595e-02   2.2045e-04    9.2775 |   0.66460   0.63171   0.63732   0.63136 |    9.9655   4.1595e-02   2.2045e-04    10.0073 |   6.8 |\n",
      "   11 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.7055   4.1595e-02   2.2045e-04    9.7474 |   0.66313   0.63744   0.64354   0.63714 |    9.9499   4.1595e-02   2.2045e-04     9.9917 |   6.6 |\n",
      "   12 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.6064   4.1595e-02   2.2045e-04    9.6482 |   0.65968   0.64284   0.64999   0.64252 |    9.8988   4.1595e-02   2.2045e-04     9.9406 |   6.9 |\n",
      "   13 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.4817   4.1595e-02   2.2045e-04    9.5235 |   0.65863   0.64778   0.65319   0.64747 |    9.8756   4.1595e-02   2.2045e-04     9.9174 |   6.7 |\n",
      "   14 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.1272   4.1595e-02   2.2045e-04    9.1691 |   0.65399   0.65431   0.65981   0.65402 |    9.8098   4.1595e-02   2.2045e-04     9.8516 |   7.4 |\n",
      "   15 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.9899   4.1595e-02   2.2045e-04    9.0317 |   0.65092   0.66120   0.66665   0.66090 |    9.7678   4.1595e-02   2.2045e-04     9.8096 |   9.0 |\n",
      "   16 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.1859   4.1595e-02   2.2045e-04    9.2277 |   0.64818   0.66863   0.67229   0.66835 |    9.7202   4.1595e-02   2.2045e-04     9.7620 |   7.2 |\n",
      "   17 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.8383   4.1595e-02   2.2045e-04    8.8801 |   0.64380   0.67383   0.67710   0.67357 |    9.6587   4.1595e-02   2.2045e-04     9.7005 |   6.7 |\n",
      "   18 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.5553   4.1595e-02   2.2045e-04    9.5972 |   0.64403   0.67754   0.68057   0.67728 |    9.6582   4.1595e-02   2.2045e-04     9.7000 |   6.8 |\n",
      "   19 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.5104   4.1595e-02   2.2045e-04    8.5523 |   0.64004   0.68499   0.68703   0.68475 |    9.6002   4.1595e-02   2.2045e-04     9.6420 |   6.8 |\n",
      "   20 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.6047   4.1595e-02   2.2045e-04    8.6465 |   0.63740   0.68990   0.69177   0.68968 |    9.5589   4.1595e-02   2.2045e-04     9.6008 |   6.6 |\n",
      "   21 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.9241   4.1595e-02   2.2045e-04    8.9659 |   0.63394   0.69431   0.69583   0.69410 |    9.5076   4.1595e-02   2.2045e-04     9.5495 |   6.5 |\n",
      "   22 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.1271   4.1595e-02   2.2045e-04    8.1689 |   0.63094   0.70058   0.70096   0.70039 |    9.4701   4.1595e-02   2.2045e-04     9.5119 |   6.4 |\n",
      "   23 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    9.3353   4.1595e-02   2.2045e-04    9.3772 |   0.62715   0.70431   0.70525   0.70412 |    9.4096   4.1595e-02   2.2045e-04     9.4515 |   9.1 |\n",
      "   24 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.6962   4.1595e-02   2.2045e-04    8.7380 |   0.62474   0.70854   0.70974   0.70835 |    9.3761   4.1595e-02   2.2045e-04     9.4179 |   7.7 |\n",
      "   25 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.2675   4.1595e-02   2.2045e-04    8.3093 |   0.62133   0.71354   0.71343   0.71335 |    9.3176   4.1595e-02   2.2045e-04     9.3594 |   7.3 |\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   26 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.7318   4.1595e-02   2.2045e-04    7.7737 |   0.61924   0.71670   0.71683   0.71651 |    9.2822   4.1595e-02   2.2045e-04     9.3240 |   7.1 |\n",
      "   27 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.1098   4.1595e-02   2.2045e-04    8.1516 |   0.61531   0.72067   0.72124   0.72050 |    9.2382   4.1595e-02   2.2045e-04     9.2800 |   9.8 |\n",
      "   28 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.4690   4.1595e-02   2.2045e-04    8.5108 |   0.61453   0.72197   0.72216   0.72180 |    9.2187   4.1595e-02   2.2045e-04     9.2605 |   8.5 |\n",
      "   29 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.7484   4.1595e-02   2.2045e-04    8.7902 |   0.61095   0.72598   0.72593   0.72582 |    9.1742   4.1595e-02   2.2045e-04     9.2160 |   7.6 |\n",
      "   30 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.5296   4.1595e-02   2.2045e-04    7.5714 |   0.60729   0.72934   0.72926   0.72919 |    9.1077   4.1595e-02   2.2045e-04     9.1495 |   8.0 |\n",
      "   31 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.0659   4.1595e-02   2.2045e-04    8.1077 |   0.60453   0.73112   0.73085   0.73097 |    9.0704   4.1595e-02   2.2045e-04     9.1122 |   7.7 |\n",
      "   32 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.3097   4.1595e-02   2.2045e-04    7.3515 |   0.60230   0.73476   0.73513   0.73462 |    9.0425   4.1595e-02   2.2045e-04     9.0844 |   7.5 |\n",
      "   33 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    8.1205   4.1595e-02   2.2045e-04    8.1623 |   0.59917   0.73727   0.73727   0.73713 |    8.9902   4.1595e-02   2.2045e-04     9.0321 |   9.8 |\n",
      "   34 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.6737   4.1595e-02   2.2045e-04    7.7155 |   0.59765   0.73942   0.73948   0.73928 |    8.9609   4.1595e-02   2.2045e-04     9.0027 |   7.1 |\n",
      "   35 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.4105   4.1595e-02   2.2045e-04    7.4523 |   0.59657   0.74000   0.74077   0.73986 |    8.9469   4.1595e-02   2.2045e-04     8.9887 |  14.2 |\n",
      "   36 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.5776   4.1595e-02   2.2045e-04    7.6194 |   0.59532   0.74252   0.74302   0.74238 |    8.9382   4.1595e-02   2.2045e-04     8.9800 |   9.6 |\n",
      "   37 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.1251   4.1595e-02   2.2045e-04    7.1669 |   0.59413   0.74432   0.74461   0.74419 |    8.9114   4.1595e-02   2.2045e-04     8.9533 |   6.7 |\n",
      "   38 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.5150   4.1595e-02   2.2045e-04    7.5568 |   0.59232   0.74576   0.74700   0.74562 |    8.8909   4.1595e-02   2.2045e-04     8.9327 |   7.2 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.3170   4.1595e-02   2.2045e-04    7.3588 |   0.59234   0.74613   0.74761   0.74599 |    8.8803   4.1595e-02   2.2045e-04     8.9221 |   8.5 |\n",
      "   40 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.7006   4.1595e-02   2.2045e-04    6.7424 |   0.58873   0.74851   0.75013   0.74838 |    8.8317   4.1595e-02   2.2045e-04     8.8735 |   7.0 |\n",
      "   41 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.1674   4.1595e-02   2.2045e-04    7.2092 |   0.58744   0.74976   0.75154   0.74964 |    8.7992   4.1595e-02   2.2045e-04     8.8411 |   9.1 |\n",
      "   42 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.8035   4.1595e-02   2.2045e-04    6.8453 |   0.58645   0.75102   0.75287   0.75090 |    8.8012   4.1595e-02   2.2045e-04     8.8431 |   7.5 |\n",
      "   43 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.6833   4.1595e-02   2.2045e-04    6.7251 |   0.58623   0.75268   0.75433   0.75256 |    8.7920   4.1595e-02   2.2045e-04     8.8338 |   7.3 |\n",
      "   44 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.5065   4.1595e-02   2.2045e-04    6.5483 |   0.58582   0.75414   0.75531   0.75402 |    8.7896   4.1595e-02   2.2045e-04     8.8314 |   6.7 |\n",
      "   45 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    7.1669   4.1595e-02   2.2045e-04    7.2087 |   0.58404   0.75453   0.75661   0.75440 |    8.7505   4.1595e-02   2.2045e-04     8.7923 |   6.6 |\n",
      "   46 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.6411   4.1595e-02   2.2045e-04    6.6830 |   0.58295   0.75558   0.75767   0.75546 |    8.7446   4.1595e-02   2.2045e-04     8.7864 |   6.5 |\n",
      "   47 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.2503   4.1595e-02   2.2045e-04    6.2921 |   0.58097   0.75700   0.75914   0.75688 |    8.7141   4.1595e-02   2.2045e-04     8.7559 |   8.1 |\n",
      "   48 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.0865   4.1595e-02   2.2045e-04    6.1283 |   0.58216   0.75824   0.75962   0.75812 |    8.7349   4.1595e-02   2.2045e-04     8.7767 |   7.6 |\n",
      "   49 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.0844   4.1595e-02   2.2045e-04    6.1262 |   0.58259   0.75908   0.76083   0.75897 |    8.7452   4.1595e-02   2.2045e-04     8.7870 |   7.4 |\n",
      "   50 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.7470   4.1595e-02   2.2045e-04    6.7889 |   0.58187   0.75986   0.76191   0.75974 |    8.7348   4.1595e-02   2.2045e-04     8.7766 |   7.4 |\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   51 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.8890   4.1595e-02   2.2045e-04    5.9308 |   0.58030   0.76066   0.76260   0.76054 |    8.6934   4.1595e-02   2.2045e-04     8.7352 |   6.8 |\n",
      "   52 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.0365   4.1595e-02   2.2045e-04    6.0784 |   0.57904   0.76163   0.76419   0.76151 |    8.6852   4.1595e-02   2.2045e-04     8.7270 |   7.1 |\n",
      "   53 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    6.2862   4.1595e-02   2.2045e-04    6.3280 |   0.57909   0.76372   0.76555   0.76360 |    8.6911   4.1595e-02   2.2045e-04     8.7330 |   6.8 |\n",
      "   54 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.8401   4.1595e-02   2.2045e-04    5.8819 |   0.57805   0.76367   0.76573   0.76355 |    8.6727   4.1595e-02   2.2045e-04     8.7145 |   6.6 |\n",
      "   55 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.9589   4.1595e-02   2.2045e-04    6.0007 |   0.57746   0.76521   0.76714   0.76509 |    8.6621   4.1595e-02   2.2045e-04     8.7039 |   6.9 |\n",
      "   56 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.8854   4.1595e-02   2.2045e-04    5.9272 |   0.57740   0.76617   0.76853   0.76605 |    8.6461   4.1595e-02   2.2045e-04     8.6879 |   6.6 |\n",
      "   57 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.8477   4.1595e-02   2.2045e-04    5.8896 |   0.57651   0.76874   0.77093   0.76862 |    8.6405   4.1595e-02   2.2045e-04     8.6823 |   6.6 |\n",
      "   58 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.6142   4.1595e-02   2.2045e-04    5.6560 |   0.57573   0.76926   0.77160   0.76914 |    8.6382   4.1595e-02   2.2045e-04     8.6800 |   8.2 |\n",
      "   59 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.9507   4.1595e-02   2.2045e-04    5.9925 |   0.57370   0.77042   0.77256   0.77031 |    8.6030   4.1595e-02   2.2045e-04     8.6448 |   6.6 |\n",
      "   60 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.2042   4.1595e-02   2.2045e-04    5.2460 |   0.57305   0.77102   0.77329   0.77090 |    8.5790   4.1595e-02   2.2045e-04     8.6208 |   6.9 |\n",
      "   61 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.1204   4.1595e-02   2.2045e-04    5.1622 |   0.57449   0.77246   0.77408   0.77235 |    8.6113   4.1595e-02   2.2045e-04     8.6532 |   6.5 |\n",
      "   62 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.1595   4.1595e-02   2.2045e-04    5.2014 |   0.57283   0.77330   0.77551   0.77319 |    8.5923   4.1595e-02   2.2045e-04     8.6341 |   7.1 |\n",
      "   63 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.6230   4.1595e-02   2.2045e-04    5.6649 |   0.57375   0.77457   0.77650   0.77446 |    8.6063   4.1595e-02   2.2045e-04     8.6481 |   6.6 |\n",
      "   64 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.6891   4.1595e-02   2.2045e-04    5.7309 |   0.57321   0.77513   0.77688   0.77502 |    8.5956   4.1595e-02   2.2045e-04     8.6374 |   6.7 |\n",
      "   65 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.1772   4.1595e-02   2.2045e-04    5.2190 |   0.57492   0.77610   0.77793   0.77598 |    8.6277   4.1595e-02   2.2045e-04     8.6695 |   6.7 |\n",
      "   66 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.7862   4.1595e-02   2.2045e-04    4.8280 |   0.57052   0.77716   0.77893   0.77705 |    8.5560   4.1595e-02   2.2045e-04     8.5978 |   6.9 |\n",
      "   67 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.5231   4.1595e-02   2.2045e-04    5.5650 |   0.57222   0.77812   0.78015   0.77801 |    8.5952   4.1595e-02   2.2045e-04     8.6370 |   6.8 |\n",
      "   68 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.9253   4.1595e-02   2.2045e-04    5.9671 |   0.57064   0.77847   0.78040   0.77835 |    8.5458   4.1595e-02   2.2045e-04     8.5877 |   7.1 |\n",
      "   69 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8633   4.1595e-02   2.2045e-04    4.9051 |   0.57169   0.77882   0.78086   0.77871 |    8.5849   4.1595e-02   2.2045e-04     8.6268 |   7.1 |\n",
      "   70 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8942   4.1595e-02   2.2045e-04    4.9360 |   0.57121   0.77902   0.78098   0.77891 |    8.5630   4.1595e-02   2.2045e-04     8.6048 |   6.8 |\n",
      "   71 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.5890   4.1595e-02   2.2045e-04    5.6308 |   0.57125   0.78053   0.78257   0.78042 |    8.5794   4.1595e-02   2.2045e-04     8.6212 |   6.6 |\n",
      "   72 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.4773   4.1595e-02   2.2045e-04    5.5191 |   0.57046   0.78069   0.78277   0.78058 |    8.5720   4.1595e-02   2.2045e-04     8.6139 |   7.0 |\n",
      "   73 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.6500   4.1595e-02   2.2045e-04    5.6918 |   0.56967   0.78187   0.78397   0.78176 |    8.5520   4.1595e-02   2.2045e-04     8.5938 |   7.0 |\n",
      "   74 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.0896   4.1595e-02   2.2045e-04    5.1314 |   0.56999   0.78249   0.78453   0.78238 |    8.5348   4.1595e-02   2.2045e-04     8.5766 |   7.1 |\n",
      "   75 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.0727   4.1595e-02   2.2045e-04    5.1145 |   0.56707   0.78374   0.78551   0.78364 |    8.5109   4.1595e-02   2.2045e-04     8.5527 |   7.6 |\n",
      "[Final] ep:75  it:7875 -  Total Loss: 8.5527     \n",
      "Task: 8.5109   Sparsity: 4.15950e-02    Sharing: 2.20447e-04 \n",
      "\n",
      " epch:  75   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.4998    0.5002  0    0.5006    0.4994  1    0.4996    0.5004  0\n",
      "   2    0.4999    0.5001  0    0.4998    0.5002  0    0.4996    0.5004  0\n",
      "   3    0.4998    0.5002  0    0.4993    0.5007  0    0.5006    0.4994  1\n",
      "   4    0.5001    0.4999  1    0.5000    0.5000  0    0.4998    0.5002  0\n",
      "   5    0.5009    0.4991  1    0.5001    0.4999  1    0.5007    0.4993  1\n",
      "   6    0.5002    0.4998  1    0.5000    0.5000  0    0.5002    0.4998  1\n",
      "\n",
      "\n",
      "\n",
      " epch:  75   logits       s          logits      s         logits       s\n",
      " -----  ----------------- -    ----------------  -    ----------------  - \n",
      "   1   -0.0017   -0.0008  0    0.0015   -0.0010  1   -0.0014    0.0003  0\n",
      "   2   -0.0007   -0.0002  0   -0.0009    0.0001  0   -0.0006    0.0008  0\n",
      "   3   -0.0009   -0.0002  0   -0.0019    0.0009  0    0.0015   -0.0008  1\n",
      "   4    0.0012    0.0007  1    0.0008    0.0009  0   -0.0005    0.0001  0\n",
      "   5    0.0012   -0.0024  1    0.0003   -0.0001  1    0.0020   -0.0007  1\n",
      "   6    0.0004   -0.0005  1    0.0003    0.0005  0    0.0011    0.0002  1\n",
      "\n",
      "\n",
      " save warmup val_metrics to :  model_warmup_ep_75\n",
      " save warmup checkpoint  to :  model_warmup_ep_75\n"
     ]
    }
   ],
   "source": [
    "# warmup_phase(ns,opt, environ, dldrs, epochs = 25)\n",
    "warmup_phase(ns,opt, environ, dldrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05b99542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:15.991118Z",
     "start_time": "2022-03-25T14:04:15.868106Z"
    }
   },
   "outputs": [],
   "source": [
    "# warmup_phase(ns,opt, environ, dldrs, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dc43cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:16.079538Z",
     "start_time": "2022-03-25T14:04:15.995605Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06410fec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:16.154632Z",
     "start_time": "2022-03-25T14:04:16.085469Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d68d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:16.235720Z",
     "start_time": "2022-03-25T14:04:16.158754Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# environ.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46fadf1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:16.310883Z",
     "start_time": "2022-03-25T14:04:16.239271Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# environ.val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea84c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d238e8",
   "metadata": {},
   "source": [
    "#### display parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22e75ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:16.393536Z",
     "start_time": "2022-03-25T14:04:16.314733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Backbone Learning Rate      : 0.001\n",
      " Tasks    Learning Rate      : 0.001\n",
      " Policy   Learning Rate      : 0.01\n",
      "\n",
      " Sparsity regularization     : 0.02\n",
      " Sharing  regularization     : 0.01 \n",
      "\n",
      " Tasks    regularization     : 1   \n",
      " Gumbel Temp                 : 4.0000         \n",
      " Gumbel Temp decay           : 16\n",
      " current lr:  0.01\n",
      " current lr:  0.001\n",
      " current lr:  0.001\n"
     ]
    }
   ],
   "source": [
    "print( f\" Backbone Learning Rate      : {environ.opt['train']['backbone_lr']}\\n\"\n",
    "       f\" Tasks    Learning Rate      : {environ.opt['train']['task_lr']}\\n\"\n",
    "       f\" Policy   Learning Rate      : {environ.opt['train']['policy_lr']}\\n\")\n",
    "print( f\" Sparsity regularization     : {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization     : {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization     : {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                 : {environ.gumbel_temperature:.4f}         \\n\" #\n",
    "       f\" Gumbel Temp decay           : {environ.opt['train']['decay_temp_freq']}\") #\n",
    "print(' current lr: ', environ.optimizers['alphas'].param_groups[0]['lr'],)\n",
    "print(' current lr: ', environ.optimizers['weights'].param_groups[0]['lr'])\n",
    "print(' current lr: ', environ.optimizers['weights'].param_groups[1]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2db34fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:04:16.477940Z",
     "start_time": "2022-03-25T14:04:16.398719Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['policy_lr'] = 0.01\n",
    "# opt['train']['policy_lr']         = 0.01\n",
    "# environ.opt['train']['lambda_sparsity'] = 0.1\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['lambda_tasks']    = 1.0\n",
    "# environ.opt['train']['decay_temp_freq'] = 2\n",
    "# print(environ.optimizers['alphas'].param_groups)\n",
    "# print(environ.optimizers['weights'].param_groups)\n",
    "# print('initial lr: ', environ.optimizers['alphas'].param_groups[0]['initial_lr'] , 'current lr: ', environ.optimizers['alphas'].param_groups[0]['lr'],)\n",
    "# print('current lr: ', environ.optimizers['weights'].param_groups[0]['lr'])\n",
    "# print('current lr: ', environ.optimizers['weights'].param_groups[1]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49645c",
   "metadata": {},
   "source": [
    "## Weight & Policy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aade16",
   "metadata": {},
   "source": [
    "### Weight/Policy Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe24a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T21:44:35.165637Z",
     "start_time": "2022-03-24T21:44:35.112114Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.flag_warmup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "365996be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:13:08.363875Z",
     "start_time": "2022-03-25T14:13:08.287458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "** 2022-03-25 15:13:08:312655 \n",
      "** Training epoch: 75 iter: 7875   flag: update_weights \n",
      "** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\n",
      "** Switch from Warm Up training to Alternate training Weights & Policy \n",
      "** Take checkpoint and block gradient flow through Policy net\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if ns.flag_warmup:\n",
    "    print_heading( f\"** {timestring()} \\n\"\n",
    "                   f\"** Training epoch: {ns.current_epoch} iter: {ns.current_iter}   flag: {ns.flag} \\n\"\n",
    "                   f\"** Set optimizer and scheduler to policy_learning = True (Switch weight optimizer from ADAM to SGD)\\n\"\n",
    "                   f\"** Switch from Warm Up training to Alternate training Weights & Policy \\n\"\n",
    "                   f\"** Take checkpoint and block gradient flow through Policy net\", verbose=True)\n",
    "#     environ.define_optimizer(policy_learning=True)\n",
    "#     environ.define_scheduler(policy_learning=True)\n",
    "    ns.flag_warmup = False\n",
    "    ns.flag = 'update_weights'\n",
    "    environ.fix_alpha()\n",
    "    environ.free_weights(opt['fix_BN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8593384c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:13:08.870156Z",
     "start_time": "2022-03-25T14:13:08.814786Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_train_layers = None \n",
    "# environ.opt['is_curriculum'] = True\n",
    "# environ.opt['curriculum_speed'] = 4\n",
    "# ns.num_train_layers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "753d84a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:13:09.715329Z",
     "start_time": "2022-03-25T14:13:09.653409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 125\n",
      "ns.current_epoch           : 75\n",
      "ns.current_iters           : 7875 \n",
      "\n",
      "ns.training_epochs         : 125\n",
      "Batches in weight epoch    : 105\n",
      "Batches in policy epoch    : 105\n",
      "num_train_layers           : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ns.current_epoch,ns.training_epochs)\n",
    "print(f\"ns.current_epoch           : {ns.current_epoch}\") \n",
    "print(f\"ns.current_iters           : {ns.current_iter} \\n\")  \n",
    "print(f\"ns.training_epochs         : {ns.training_epochs}\") \n",
    "# print(f\"ns.stop_epoch_training     : {ns.stop_epoch_training}\")\n",
    "print(f\"Batches in weight epoch    : {ns.stop_iter_w}\")\n",
    "print(f\"Batches in policy epoch    : {ns.stop_iter_a}\")\n",
    "print(f\"num_train_layers           : {ns.num_train_layers}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25e913c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:13:10.012755Z",
     "start_time": "2022-03-25T14:13:09.948841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e] Last ep:75  it:7875 -  Total Loss: 8.5527     \n",
      "Task: 8.5109   Sparsity: 4.15950e-02    Sharing: 2.20447e-04 \n"
     ]
    }
   ],
   "source": [
    "ns.training_epochs = 250\n",
    "print_loss(environ.val_metrics, title = f\"[e] Last ep:{ns.current_epoch}  it:{ns.current_iter}\")\n",
    "# environ.display_trained_policy(ns.current_epoch)\n",
    "# environ.display_trained_logits(ns.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a1c72de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:13:11.096097Z",
     "start_time": "2022-03-25T14:13:11.034309Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.current_epoch = 200\n",
    "# ns.flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fcd6751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T14:13:11.837813Z",
     "start_time": "2022-03-25T14:13:11.777574Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 75       # of epochs to run:  250 -->  epochs 76 to 325\n",
      " policy_learning rate : 0.01 \n",
      " lambda_sparsity      : 0.02\n",
      " lambda_sharing       : 0.01\n",
      " curriculum training  : False     cirriculum speed: 3     num_training_layers : None\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_heading(f\" Last Epoch Completed : {ns.current_epoch}       # of epochs to run:  {ns.training_epochs} -->  epochs {ns.current_epoch+1} to {ns.training_epochs + ns.current_epoch}\"\n",
    "              f\"\\n policy_learning rate : {environ.opt['train']['policy_lr']} \"\n",
    "              f\"\\n lambda_sparsity      : {environ.opt['train']['lambda_sparsity']}\"\n",
    "              f\"\\n lambda_sharing       : {environ.opt['train']['lambda_sharing']}\"\n",
    "              f\"\\n curriculum training  : {opt['is_curriculum']}     cirriculum speed: {opt['curriculum_speed']}     num_training_layers : {ns.num_train_layers}\", \n",
    "              verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c71af",
   "metadata": {},
   "source": [
    "### Weight/Policy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d10ad7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T15:42:03.349066Z",
     "start_time": "2022-03-25T14:13:17.261286Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      " Last Epoch Completed : 75   # of epochs to run:  250 -->  epochs 76 to 325    \n",
      " policy_learning rate : 0.01      \n",
      " lambda_sparsity      : 0.02\n",
      " lambda_sharing       : 0.01 \n",
      " curriculum training  : False     cirriculum speed: 3     num_training_layers : None\n",
      "------------------------------------------------------------------------------------------------------------------------ \n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   76 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.9859   4.1595e-02   2.2045e-04    5.0277 |   0.64633   0.74361   0.74800   0.74347 |    9.6836   4.1595e-02   2.2045e-04     9.7254 |  11.5 |\n",
      "Previous best_epoch:     0   best iter:     0,   best_value: 0.00000\n",
      "New      best_epoch:    76   best iter:  7980,   best_value: 0.74361\n",
      "   76 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.4851   4.6118e-02   3.3701e-04    4.5315 |   0.63222   0.74835   0.75328   0.74820 |    9.4673   4.6138e-02   2.7476e-04     9.5137 |   9.5 |\n",
      "Previous best_epoch:    76   best iter:  7980,   best_value: 0.74361\n",
      "New      best_epoch:    76   best iter:  8085,   best_value: 0.74835\n",
      "   77 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.9600   4.6138e-02   2.7476e-04    5.0064 |   0.62561   0.75449   0.75952   0.75436 |    9.3526   4.6138e-02   2.7476e-04     9.3990 |  11.5 |\n",
      "Previous best_epoch:    76   best iter:  8085,   best_value: 0.74835\n",
      "New      best_epoch:    77   best iter:  8190,   best_value: 0.75449\n",
      "   77 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.7136   4.6022e-02   2.3810e-04    4.7599 |   0.60700   0.76428   0.76728   0.76416 |    9.1298   4.6091e-02   3.4945e-04     9.1763 |  11.2 |\n",
      "Previous best_epoch:    77   best iter:  8190,   best_value: 0.75449\n",
      "New      best_epoch:    77   best iter:  8295,   best_value: 0.76428\n",
      "   78 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.0210   4.6091e-02   3.4945e-04    5.0674 |   0.60964   0.76457   0.76613   0.76445 |    9.1308   4.6091e-02   3.4945e-04     9.1772 |  10.6 |\n",
      "Previous best_epoch:    77   best iter:  8295,   best_value: 0.76428\n",
      "New      best_epoch:    78   best iter:  8400,   best_value: 0.76457\n",
      "   78 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.1195   4.5442e-02   4.0125e-04    5.1654 |   0.61231   0.76771   0.76927   0.76756 |    9.1623   4.5425e-02   1.9094e-04     9.2080 |  15.0 |\n",
      "Previous best_epoch:    78   best iter:  8400,   best_value: 0.76457\n",
      "New      best_epoch:    78   best iter:  8505,   best_value: 0.76771\n",
      "   79 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8583   4.5425e-02   1.9094e-04    4.9039 |   0.61311   0.76257   0.76573   0.76242 |    9.2217   4.5425e-02   1.9094e-04     9.2673 |  10.8 |\n",
      "   79 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8801   4.4995e-02   3.5362e-04    4.9254 |   0.61281   0.76515   0.76745   0.76500 |    9.1854   4.4953e-02   3.1404e-04     9.2307 |  12.5 |\n",
      "   80 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.2567   4.4953e-02   3.1404e-04    4.3020 |   0.61596   0.76594   0.76752   0.76582 |    9.2321   4.4953e-02   3.1404e-04     9.2774 |  11.1 |\n",
      "   80 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.0010   4.1427e-02   3.7950e-04    5.0428 |   0.60976   0.76543   0.76895   0.76531 |    9.1316   4.1409e-02   2.8067e-04     9.1733 |  11.0 |\n",
      "\n",
      "[e] Policy training epoch:80  it:8925 -  Total Loss: 9.1733     \n",
      "Task: 9.1316   Sparsity: 4.14088e-02    Sharing: 2.80665e-04 \n",
      "\n",
      " epch:  80   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.4392    0.5608  0    0.3450    0.6550  0    0.5622    0.4378  1\n",
      "   2    0.4945    0.5055  0    0.4385    0.5615  0    0.6096    0.3904  1\n",
      "   3    0.5654    0.4346  1    0.5236    0.4764  1    0.6731    0.3269  1\n",
      "   4    0.5584    0.4416  1    0.5598    0.4402  1    0.5108    0.4892  1\n",
      "   5    0.3159    0.6841  0    0.3798    0.6202  0    0.3595    0.6405  0\n",
      "   6    0.5810    0.4190  1    0.4170    0.5830  0    0.4712    0.5288  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   81 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.5429   4.1409e-02   2.8067e-04    5.5845 |   0.61369   0.76640   0.76805   0.76626 |    9.2064   4.1409e-02   2.8067e-04     9.2481 |  10.8 |\n",
      "   81 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.6796   4.2444e-02   2.3557e-04    4.7223 |   0.60929   0.76808   0.76907   0.76796 |    9.1431   4.2400e-02   2.8709e-04     9.1858 |  10.5 |\n",
      "Previous best_epoch:    78   best iter:  8505,   best_value: 0.76771\n",
      "New      best_epoch:    81   best iter:  9135,   best_value: 0.76808\n",
      "   82 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.7972   4.2400e-02   2.8709e-04    4.8399 |   0.60931   0.76866   0.77093   0.76854 |    9.1061   4.2400e-02   2.8709e-04     9.1487 |  10.8 |\n",
      "Previous best_epoch:    81   best iter:  9135,   best_value: 0.76808\n",
      "New      best_epoch:    82   best iter:  9240,   best_value: 0.76866\n",
      "   82 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.6137   3.8799e-02   2.9313e-04    4.6528 |   0.61643   0.76610   0.76733   0.76599 |    9.2474   3.8781e-02   2.5986e-04     9.2864 |   9.8 |\n",
      "   83 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8117   3.8781e-02   2.5986e-04    4.8508 |   0.60810   0.77144   0.77261   0.77132 |    9.1344   3.8781e-02   2.5986e-04     9.1734 |   9.8 |\n",
      "Previous best_epoch:    82   best iter:  9240,   best_value: 0.76866\n",
      "New      best_epoch:    83   best iter:  9450,   best_value: 0.77144\n",
      "   83 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.6226   3.7222e-02   5.5522e-04    4.6604 |   0.60061   0.77185   0.77397   0.77172 |    9.0187   3.7195e-02   5.5874e-04     9.0565 |  11.0 |\n",
      "Previous best_epoch:    83   best iter:  9450,   best_value: 0.77144\n",
      "New      best_epoch:    83   best iter:  9555,   best_value: 0.77185\n",
      "   84 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.2979   3.7195e-02   5.5874e-04    4.3357 |   0.62291   0.76658   0.76710   0.76645 |    9.3235   3.7195e-02   5.5874e-04     9.3613 |  10.7 |\n",
      "   84 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.0620   3.5834e-02   4.9268e-04    4.0983 |   0.61663   0.76769   0.76937   0.76756 |    9.2664   3.5862e-02   3.8983e-04     9.3027 |  10.1 |\n",
      "   85 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.5404   3.5862e-02   3.8983e-04    4.5767 |   0.61996   0.76692   0.76765   0.76681 |    9.3020   3.5862e-02   3.8983e-04     9.3382 |  10.1 |\n",
      "   85 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.2546   3.7042e-02   3.1120e-04    4.2919 |   0.61787   0.77186   0.77159   0.77173 |    9.3019   3.7046e-02   3.4351e-04     9.3393 |  10.4 |\n",
      "Previous best_epoch:    83   best iter:  9555,   best_value: 0.77185\n",
      "New      best_epoch:    85   best iter:  9975,   best_value: 0.77186\n",
      "\n",
      "[e] Policy training epoch:85  it:9975 -  Total Loss: 9.3393     \n",
      "Task: 9.3019   Sparsity: 3.70462e-02    Sharing: 3.43515e-04 \n",
      "\n",
      " epch:  85   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.4350    0.5650  0    0.2633    0.7367  0    0.5008    0.4992  1\n",
      "   2    0.5505    0.4495  1    0.4813    0.5187  0    0.5655    0.4345  1\n",
      "   3    0.6154    0.3846  1    0.4697    0.5303  0    0.6027    0.3973  1\n",
      "   4    0.5296    0.4704  1    0.4414    0.5586  0    0.3711    0.6289  0\n",
      "   5    0.2641    0.7359  0    0.3571    0.6429  0    0.2937    0.7063  0\n",
      "   6    0.5556    0.4444  1    0.4518    0.5482  0    0.3504    0.6496  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   86 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.4410   3.7046e-02   3.4351e-04    4.4784 |   0.59131   0.78103   0.78207   0.78092 |    8.8687   3.7046e-02   3.4351e-04     8.9061 |   9.9 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous best_epoch:    85   best iter:  9975,   best_value: 0.77186\n",
      "New      best_epoch:    86   best iter: 10080,   best_value: 0.78103\n",
      "   86 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.2160   3.2840e-02   5.2308e-04    4.2494 |   0.63077   0.76261   0.76377   0.76247 |    9.4710   3.2865e-02   4.6720e-04     9.5043 |   9.7 |\n",
      "   87 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.3695   3.2865e-02   4.6720e-04    4.4028 |   0.59963   0.77493   0.77724   0.77480 |    8.9900   3.2865e-02   4.6720e-04     9.0234 |  10.1 |\n",
      "   87 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.6279   3.0570e-02   3.3570e-04    4.6588 |   0.61407   0.77079   0.77138   0.77069 |    9.2039   3.0551e-02   3.3904e-04     9.2348 |   9.8 |\n",
      "   88 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.0554   3.0551e-02   3.3904e-04    5.0863 |   0.60927   0.77290   0.77398   0.77279 |    9.1172   3.0551e-02   3.3904e-04     9.1481 |   9.7 |\n",
      "   88 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8383   3.1478e-02   5.0999e-04    4.8703 |   0.61279   0.77142   0.77219   0.77131 |    9.1683   3.1488e-02   4.4416e-04     9.2002 |  10.1 |\n",
      "   89 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    3.8385   3.1488e-02   4.4416e-04    3.8704 |   0.61498   0.77190   0.77129   0.77178 |    9.2125   3.1488e-02   4.4416e-04     9.2444 |   9.8 |\n",
      "   89 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.2840   3.0964e-02   2.8924e-04    4.3152 |   0.61264   0.77070   0.77101   0.77059 |    9.1778   3.0941e-02   3.4032e-04     9.2091 |   9.7 |\n",
      "   90 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.3315   3.0941e-02   3.4032e-04    4.3628 |   0.61158   0.77381   0.77432   0.77369 |    9.1653   3.0941e-02   3.4032e-04     9.1966 |   9.8 |\n",
      "   90 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.4154   2.9345e-02   2.1488e-04    4.4450 |   0.62896   0.76979   0.76954   0.76966 |    9.4221   2.9342e-02   3.5865e-04     9.4518 |   9.6 |\n",
      "\n",
      "[e] Policy training epoch:90  it:11025 -  Total Loss: 9.4518     \n",
      "Task: 9.4221   Sparsity: 2.93416e-02    Sharing: 3.58652e-04 \n",
      "\n",
      " epch:  90   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.3832    0.6168  0    0.2010    0.7990  0    0.3294    0.6706  0\n",
      "   2    0.5167    0.4833  1    0.3583    0.6417  0    0.4294    0.5706  0\n",
      "   3    0.5532    0.4468  1    0.3963    0.6037  0    0.5200    0.4800  1\n",
      "   4    0.4641    0.5359  0    0.4096    0.5904  0    0.3564    0.6436  0\n",
      "   5    0.2415    0.7585  0    0.2621    0.7379  0    0.2498    0.7502  0\n",
      "   6    0.4758    0.5242  0    0.3426    0.6574  0    0.3262    0.6738  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   91 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    4.8319   2.9342e-02   3.5865e-04    4.8616 |   0.61666   0.77419   0.77413   0.77408 |    9.2319   2.9342e-02   3.5865e-04     9.2616 |   9.7 |\n",
      "   91 |   1.00e-03   1.00e-03   1.00e-02  4.000e+00 |    5.4337   2.9259e-02   2.6907e-04    5.4633 |   0.61778   0.77208   0.77320   0.77195 |    9.2681   2.9236e-02   3.6026e-04     9.2977 |   9.8 |\n",
      " decay gumbel softmax to 3.86\n",
      "   92 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.8920   2.9236e-02   3.6026e-04    3.9216 |   0.59862   0.77773   0.78017   0.77762 |    8.9596   2.9236e-02   3.6026e-04     8.9892 |   9.8 |\n",
      "   92 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.5691   3.0009e-02   2.1474e-04    4.5993 |   0.60663   0.77646   0.77844   0.77634 |    9.1201   3.0038e-02   2.9518e-04     9.1505 |  10.1 |\n",
      "   93 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.6574   3.0038e-02   2.9518e-04    4.6877 |   0.60747   0.78052   0.78042   0.78041 |    9.1087   3.0038e-02   2.9518e-04     9.1391 |  10.5 |\n",
      "   93 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.4824   3.1289e-02   2.7115e-04    3.5140 |   0.59948   0.78185   0.78243   0.78174 |    8.9898   3.1305e-02   3.2555e-04     9.0215 |  10.1 |\n",
      "Previous best_epoch:    86   best iter: 10080,   best_value: 0.78103\n",
      "New      best_epoch:    93   best iter: 11655,   best_value: 0.78185\n",
      "   94 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.0656   3.1305e-02   3.2555e-04    4.0972 |   0.61889   0.77703   0.77655   0.77692 |    9.2645   3.1305e-02   3.2555e-04     9.2961 |  10.0 |\n",
      "   94 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.9043   2.8805e-02   3.1010e-04    3.9334 |   0.61881   0.77553   0.77594   0.77542 |    9.2717   2.8802e-02   4.8173e-04     9.3010 |  10.8 |\n",
      "   95 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.1307   2.8802e-02   4.8173e-04    4.1600 |   0.60523   0.77997   0.78017   0.77986 |    9.0784   2.8802e-02   4.8173e-04     9.1076 |   9.8 |\n",
      "   95 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.3954   2.9056e-02   2.3148e-04    4.4247 |   0.60689   0.78173   0.78265   0.78163 |    9.1231   2.9089e-02   3.9173e-04     9.1526 |   9.7 |\n",
      "\n",
      "[e] Policy training epoch:95  it:12075 -  Total Loss: 9.1526     \n",
      "Task: 9.1231   Sparsity: 2.90889e-02    Sharing: 3.91733e-04 \n",
      "\n",
      " epch:  95   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.3211    0.6789  0    0.1542    0.8458  0    0.3109    0.6891  0\n",
      "   2    0.4988    0.5012  0    0.3182    0.6818  0    0.4341    0.5659  0\n",
      "   3    0.5826    0.4174  1    0.4294    0.5706  0    0.5605    0.4395  1\n",
      "   4    0.5122    0.4878  1    0.4175    0.5825  0    0.3759    0.6241  0\n",
      "   5    0.1976    0.8024  0    0.2466    0.7534  0    0.2633    0.7367  0\n",
      "   6    0.4227    0.5773  0    0.3459    0.6541  0    0.3209    0.6791  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "   96 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.4532   2.9089e-02   3.9173e-04    4.4826 |   0.61363   0.77829   0.77939   0.77818 |    9.1892   2.9089e-02   3.9173e-04     9.2187 |   9.7 |\n",
      "   96 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.5964   3.0540e-02   3.2019e-04    3.6272 |   0.60849   0.78115   0.78246   0.78104 |    9.1265   3.0509e-02   2.8814e-04     9.1573 |   9.7 |\n",
      "   97 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.5496   3.0509e-02   2.8814e-04    4.5804 |   0.60809   0.78152   0.78160   0.78142 |    9.1580   3.0509e-02   2.8814e-04     9.1888 |  10.0 |\n",
      "   97 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.4827   3.1340e-02   3.4722e-04    3.5143 |   0.60146   0.78529   0.78552   0.78519 |    9.0193   3.1313e-02   3.5275e-04     9.0509 |  10.3 |\n",
      "Previous best_epoch:    93   best iter: 11655,   best_value: 0.78185\n",
      "New      best_epoch:    97   best iter: 12495,   best_value: 0.78529\n",
      "   98 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.4716   3.1313e-02   3.5275e-04    4.5032 |   0.60411   0.78324   0.78420   0.78313 |    9.0739   3.1313e-02   3.5275e-04     9.1055 |   9.8 |\n",
      "   98 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.4471   3.0397e-02   3.2748e-04    4.4778 |   0.60863   0.78268   0.78370   0.78257 |    9.1170   3.0422e-02   3.1795e-04     9.1478 |   9.7 |\n",
      "   99 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.1199   3.0422e-02   3.1795e-04    4.1507 |   0.60748   0.78376   0.78381   0.78366 |    9.1005   3.0422e-02   3.1795e-04     9.1312 |   9.7 |\n",
      "   99 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.5850   2.8939e-02   3.7562e-04    3.6143 |   0.59174   0.78701   0.78869   0.78690 |    8.8591   2.8906e-02   3.7578e-04     8.8884 |  10.0 |\n",
      "Previous best_epoch:    97   best iter: 12495,   best_value: 0.78529\n",
      "New      best_epoch:    99   best iter: 12915,   best_value: 0.78701\n",
      "  100 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.7677   2.8906e-02   3.7578e-04    3.7970 |   0.60043   0.78671   0.78757   0.78660 |    9.0149   2.8906e-02   3.7578e-04     9.0442 |   9.8 |\n",
      "  100 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.7615   2.7061e-02   3.4538e-04    3.7889 |   0.59535   0.78742   0.78828   0.78733 |    8.9217   2.7067e-02   3.2760e-04     8.9491 |   9.8 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous best_epoch:    99   best iter: 12915,   best_value: 0.78701\n",
      "New      best_epoch:   100   best iter: 13125,   best_value: 0.78742\n",
      "\n",
      "[e] Policy training epoch:100  it:13125 -  Total Loss: 8.9491     \n",
      "Task: 8.9217   Sparsity: 2.70670e-02    Sharing: 3.27597e-04 \n",
      "\n",
      " epch: 100   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.2392    0.7608  0    0.1471    0.8529  0    0.3036    0.6964  0\n",
      "   2    0.4690    0.5310  0    0.2959    0.7041  0    0.4746    0.5254  0\n",
      "   3    0.5637    0.4363  1    0.4218    0.5782  0    0.5655    0.4345  1\n",
      "   4    0.4572    0.5428  0    0.3825    0.6175  0    0.3477    0.6523  0\n",
      "   5    0.2017    0.7983  0    0.2342    0.7658  0    0.2216    0.7784  0\n",
      "   6    0.4137    0.5863  0    0.3384    0.6616  0    0.2424    0.7576  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  101 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.5912   2.7067e-02   3.2760e-04    3.6186 |   0.60264   0.78510   0.78588   0.78500 |    9.0563   2.7067e-02   3.2760e-04     9.0837 |   9.6 |\n",
      "  101 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.1627   2.4309e-02   3.4057e-04    4.1874 |   0.60969   0.78321   0.78434   0.78310 |    9.1197   2.4328e-02   3.3224e-04     9.1444 |   9.7 |\n",
      "  102 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.2750   2.4328e-02   3.3224e-04    4.2997 |   0.60356   0.78410   0.78606   0.78399 |    9.0305   2.4328e-02   3.3224e-04     9.0552 |  10.0 |\n",
      "  102 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.8390   2.3334e-02   4.1951e-04    3.8628 |   0.61000   0.78503   0.78607   0.78492 |    9.1865   2.3349e-02   4.3064e-04     9.2103 |   9.8 |\n",
      "  103 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.8356   2.3349e-02   4.3064e-04    3.8594 |   0.60358   0.78623   0.78706   0.78613 |    9.0539   2.3349e-02   4.3064e-04     9.0777 |   9.9 |\n",
      "  103 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.7473   2.3079e-02   4.6223e-04    3.7709 |   0.61750   0.78186   0.78283   0.78173 |    9.2814   2.3088e-02   4.2610e-04     9.3049 |  10.0 |\n",
      "  104 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.8548   2.3088e-02   4.2610e-04    3.8784 |   0.61010   0.78513   0.78576   0.78502 |    9.1676   2.3088e-02   4.2610e-04     9.1911 |   9.9 |\n",
      "  104 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    4.4118   2.3413e-02   2.6788e-04    4.4355 |   0.60164   0.78806   0.78889   0.78794 |    9.0276   2.3430e-02   4.3819e-04     9.0515 |   9.6 |\n",
      "Previous best_epoch:   100   best iter: 13125,   best_value: 0.78742\n",
      "New      best_epoch:   104   best iter: 13965,   best_value: 0.78806\n",
      "  105 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.9353   2.3430e-02   4.3819e-04    3.9592 |   0.60612   0.79077   0.79027   0.79067 |    9.0703   2.3430e-02   4.3819e-04     9.0942 |   9.6 |\n",
      "Previous best_epoch:   104   best iter: 13965,   best_value: 0.78806\n",
      "New      best_epoch:   105   best iter: 14070,   best_value: 0.79077\n",
      "  105 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.5994   2.3339e-02   4.0087e-04    3.6231 |   0.60581   0.78695   0.78812   0.78684 |    9.1552   2.3349e-02   4.6226e-04     9.1790 |   9.7 |\n",
      "\n",
      "[e] Policy training epoch:105  it:14175 -  Total Loss: 9.1790     \n",
      "Task: 9.1552   Sparsity: 2.33486e-02    Sharing: 4.62257e-04 \n",
      "\n",
      " epch: 105   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1783    0.8217  0    0.1011    0.8989  0    0.3000    0.7000  0\n",
      "   2    0.4676    0.5324  0    0.2171    0.7829  0    0.4664    0.5336  0\n",
      "   3    0.4629    0.5371  0    0.3101    0.6899  0    0.5267    0.4733  1\n",
      "   4    0.3410    0.6590  0    0.2958    0.7042  0    0.3164    0.6836  0\n",
      "   5    0.2267    0.7733  0    0.1628    0.8372  0    0.2673    0.7327  0\n",
      "   6    0.4138    0.5862  0    0.2733    0.7267  0    0.2980    0.7020  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  106 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.2814   2.3349e-02   4.6226e-04    3.3052 |   0.60658   0.78805   0.78882   0.78794 |    9.0883   2.3349e-02   4.6226e-04     9.1121 |   9.7 |\n",
      "  106 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.3964   2.3751e-02   3.4209e-04    3.4205 |   0.59989   0.78809   0.78904   0.78799 |    9.0018   2.3731e-02   3.2684e-04     9.0258 |   9.8 |\n",
      "  107 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.7990   2.3731e-02   3.2684e-04    3.8231 |   0.61189   0.78686   0.78922   0.78674 |    9.1644   2.3731e-02   3.2684e-04     9.1884 |   9.7 |\n",
      "  107 |   1.00e-03   1.00e-03   1.00e-02  3.860e+00 |    3.2241   2.3036e-02   3.6812e-04    3.2475 |   0.59776   0.79143   0.79272   0.79133 |    8.9699   2.3043e-02   2.4318e-04     8.9932 |   9.7 |\n",
      "Previous best_epoch:   105   best iter: 14070,   best_value: 0.79077\n",
      "New      best_epoch:   107   best iter: 14595,   best_value: 0.79143\n",
      " decay gumbel softmax to 3.7249\n",
      "  108 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.4360   2.3043e-02   2.4318e-04    3.4592 |   0.60069   0.79105   0.79306   0.79095 |    9.0089   2.3043e-02   2.4318e-04     9.0322 |  10.4 |\n",
      "  108 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.0260   2.3912e-02   2.1503e-04    3.0501 |   0.60834   0.78864   0.79058   0.78850 |    9.1196   2.3913e-02   1.7531e-04     9.1436 |  10.1 |\n",
      "  109 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.5186   2.3913e-02   1.7531e-04    3.5427 |   0.60186   0.79148   0.79384   0.79136 |    9.0468   2.3913e-02   1.7531e-04     9.0709 |  10.6 |\n",
      "Previous best_epoch:   107   best iter: 14595,   best_value: 0.79143\n",
      "New      best_epoch:   109   best iter: 14910,   best_value: 0.79148\n",
      "  109 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.5465   2.4470e-02   3.1458e-04    3.5713 |   0.60178   0.79182   0.79318   0.79172 |    9.0276   2.4497e-02   4.0841e-04     9.0525 |  10.7 |\n",
      "Previous best_epoch:   109   best iter: 14910,   best_value: 0.79148\n",
      "New      best_epoch:   109   best iter: 15015,   best_value: 0.79182\n",
      "  110 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    4.0210   2.4497e-02   4.0841e-04    4.0459 |   0.60358   0.79295   0.79375   0.79285 |    9.0252   2.4497e-02   4.0841e-04     9.0501 |  10.0 |\n",
      "Previous best_epoch:   109   best iter: 15015,   best_value: 0.79182\n",
      "New      best_epoch:   110   best iter: 15120,   best_value: 0.79295\n",
      "  110 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.3194   2.4529e-02   3.4160e-04    3.3443 |   0.60299   0.79240   0.79369   0.79230 |    9.0410   2.4527e-02   4.3647e-04     9.0660 |   9.7 |\n",
      "\n",
      "[e] Policy training epoch:110  it:15225 -  Total Loss: 9.0660     \n",
      "Task: 9.0410   Sparsity: 2.45266e-02    Sharing: 4.36466e-04 \n",
      "\n",
      " epch: 110   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1651    0.8349  0    0.1041    0.8959  0    0.2439    0.7561  0\n",
      "   2    0.5660    0.4340  1    0.2648    0.7352  0    0.4717    0.5283  0\n",
      "   3    0.5004    0.4996  1    0.3826    0.6174  0    0.5003    0.4997  1\n",
      "   4    0.2783    0.7217  0    0.3840    0.6160  0    0.3038    0.6962  0\n",
      "   5    0.2208    0.7792  0    0.2246    0.7754  0    0.2680    0.7320  0\n",
      "   6    0.3281    0.6719  0    0.3383    0.6617  0    0.2872    0.7128  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  111 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.1994   2.4527e-02   4.3647e-04    3.2244 |   0.60262   0.79288   0.79359   0.79278 |    9.0508   2.4527e-02   4.3647e-04     9.0758 |   9.5 |\n",
      "  111 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.5443   2.3264e-02   2.3404e-04    3.5678 |   0.60244   0.79233   0.79420   0.79220 |    9.0259   2.3238e-02   3.5201e-04     9.0495 |   9.8 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  112 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.4824   2.3238e-02   3.5201e-04    3.5060 |   0.60120   0.79297   0.79432   0.79288 |    9.0244   2.3238e-02   3.5201e-04     9.0480 |   9.7 |\n",
      "Previous best_epoch:   110   best iter: 15120,   best_value: 0.79295\n",
      "New      best_epoch:   112   best iter: 15540,   best_value: 0.79297\n",
      "  112 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.7427   2.3146e-02   2.8960e-04    3.7661 |   0.61109   0.79112   0.79278   0.79100 |    9.1692   2.3203e-02   3.0908e-04     9.1927 |   9.9 |\n",
      "  113 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    2.9822   2.3203e-02   3.0908e-04    3.0057 |   0.60355   0.79461   0.79569   0.79451 |    9.0594   2.3203e-02   3.0908e-04     9.0829 |   9.7 |\n",
      "Previous best_epoch:   112   best iter: 15540,   best_value: 0.79297\n",
      "New      best_epoch:   113   best iter: 15750,   best_value: 0.79461\n",
      "  113 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.4840   2.3226e-02   1.6251e-04    3.5074 |   0.60253   0.79512   0.79692   0.79503 |    9.0438   2.3173e-02   3.2425e-04     9.0673 |   9.4 |\n",
      "Previous best_epoch:   113   best iter: 15750,   best_value: 0.79461\n",
      "New      best_epoch:   113   best iter: 15855,   best_value: 0.79512\n",
      "  114 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.0392   2.3173e-02   3.2425e-04    3.0627 |   0.61698   0.79020   0.79200   0.79010 |    9.2296   2.3173e-02   3.2425e-04     9.2531 |   9.7 |\n",
      "  114 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.5679   2.2926e-02   3.1264e-04    3.5911 |   0.60371   0.79497   0.79589   0.79486 |    9.0692   2.2904e-02   2.6385e-04     9.0924 |   9.7 |\n",
      "  115 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.4497   2.2904e-02   2.6385e-04    3.4728 |   0.59754   0.79747   0.79942   0.79735 |    8.9369   2.2904e-02   2.6385e-04     8.9601 |   9.8 |\n",
      "Previous best_epoch:   113   best iter: 15855,   best_value: 0.79512\n",
      "New      best_epoch:   115   best iter: 16170,   best_value: 0.79747\n",
      "  115 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.1181   2.2389e-02   3.3231e-04    3.1408 |   0.59929   0.79692   0.79835   0.79682 |    8.9889   2.2413e-02   4.0959e-04     9.0117 |  10.0 |\n",
      "\n",
      "[e] Policy training epoch:115  it:16275 -  Total Loss: 9.0117     \n",
      "Task: 8.9889   Sparsity: 2.24126e-02    Sharing: 4.09587e-04 \n",
      "\n",
      " epch: 115   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1425    0.8575  0    0.0864    0.9136  0    0.2031    0.7969  0\n",
      "   2    0.4431    0.5569  0    0.2370    0.7630  0    0.3630    0.6370  0\n",
      "   3    0.4966    0.5034  0    0.3368    0.6632  0    0.4370    0.5630  0\n",
      "   4    0.3655    0.6345  0    0.3482    0.6518  0    0.2755    0.7245  0\n",
      "   5    0.1947    0.8053  0    0.2100    0.7900  0    0.2340    0.7660  0\n",
      "   6    0.4582    0.5418  0    0.3166    0.6834  0    0.2999    0.7001  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  116 |   1.00e-03   1.00e-03   1.00e-02  3.725e+00 |    3.4800   2.2413e-02   4.0959e-04    3.5028 |   0.61344   0.79307   0.79523   0.79297 |    9.2098   2.2413e-02   4.0959e-04     9.2326 |   9.9 |\n",
      "Epoch   116: reducing learning rate of group 0 to 7.5000e-04.\n",
      "Epoch   116: reducing learning rate of group 1 to 7.5000e-04.\n",
      "  116 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.9233   2.4069e-02   3.9187e-04    2.9478 |   0.61254   0.79392   0.79504   0.79381 |    9.1925   2.4086e-02   2.1024e-04     9.2168 |   9.9 |\n",
      "  117 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.8762   2.4086e-02   2.1024e-04    2.9005 |   0.60501   0.79637   0.79770   0.79627 |    9.0712   2.4086e-02   2.1024e-04     9.0955 |  10.0 |\n",
      "  117 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    3.6695   2.3888e-02   3.5538e-04    3.6938 |   0.60737   0.79641   0.79742   0.79631 |    9.1230   2.3871e-02   3.7315e-04     9.1473 |  10.1 |\n",
      "  118 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.9486   2.3871e-02   3.7315e-04    2.9728 |   0.61213   0.79703   0.79720   0.79693 |    9.1732   2.3871e-02   3.7315e-04     9.1975 |  10.0 |\n",
      "  118 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    3.5130   2.2154e-02   3.6207e-04    3.5355 |   0.60958   0.79563   0.79682   0.79553 |    9.1314   2.2127e-02   2.2941e-04     9.1537 |  10.0 |\n",
      "  119 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.9900   2.2127e-02   2.2941e-04    3.0124 |   0.60945   0.79539   0.79669   0.79529 |    9.1240   2.2127e-02   2.2941e-04     9.1464 |  10.0 |\n",
      "  119 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.6807   2.1875e-02   3.3461e-04    2.7029 |   0.60739   0.79778   0.79958   0.79768 |    9.1056   2.1899e-02   2.9691e-04     9.1278 |  10.1 |\n",
      "Previous best_epoch:   115   best iter: 16170,   best_value: 0.79747\n",
      "New      best_epoch:   119   best iter: 17115,   best_value: 0.79778\n",
      "  120 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    3.1328   2.1899e-02   2.9691e-04    3.1550 |   0.60468   0.79975   0.80038   0.79965 |    9.0892   2.1899e-02   2.9691e-04     9.1114 |   9.9 |\n",
      "Previous best_epoch:   119   best iter: 17115,   best_value: 0.79778\n",
      "New      best_epoch:   120   best iter: 17220,   best_value: 0.79975\n",
      "  120 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    3.2526   2.1776e-02   2.8809e-04    3.2746 |   0.60209   0.79862   0.79862   0.79853 |    9.0394   2.1757e-02   3.4476e-04     9.0615 |  10.0 |\n",
      "\n",
      "[e] Policy training epoch:120  it:17325 -  Total Loss: 9.0615     \n",
      "Task: 9.0394   Sparsity: 2.17565e-02    Sharing: 3.44756e-04 \n",
      "\n",
      " epch: 120   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1186    0.8814  0    0.0886    0.9114  0    0.1945    0.8055  0\n",
      "   2    0.4427    0.5573  0    0.2197    0.7803  0    0.3706    0.6294  0\n",
      "   3    0.4823    0.5177  0    0.3441    0.6559  0    0.4945    0.5055  0\n",
      "   4    0.3111    0.6889  0    0.3298    0.6702  0    0.3074    0.6926  0\n",
      "   5    0.2048    0.7952  0    0.1987    0.8013  0    0.2560    0.7440  0\n",
      "   6    0.3625    0.6375  0    0.2732    0.7268  0    0.3146    0.6854  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  121 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.8753   2.1757e-02   3.4476e-04    2.8974 |   0.61465   0.79609   0.79659   0.79599 |    9.2081   2.1757e-02   3.4476e-04     9.2302 |   9.9 |\n",
      "  121 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    3.7656   2.0319e-02   2.4781e-04    3.7862 |   0.60716   0.79807   0.79947   0.79796 |    9.1037   2.0323e-02   3.6883e-04     9.1244 |  10.1 |\n",
      "  122 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.9231   2.0323e-02   3.6883e-04    2.9438 |   0.60594   0.79818   0.79793   0.79809 |    9.0699   2.0323e-02   3.6883e-04     9.0906 |   9.9 |\n",
      "  122 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.9006   2.0992e-02   3.4148e-04    2.9220 |   0.60691   0.79872   0.79941   0.79861 |    9.0976   2.0999e-02   2.8209e-04     9.1189 |   9.9 |\n",
      "  123 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    3.6030   2.0999e-02   2.8209e-04    3.6243 |   0.61047   0.79697   0.79882   0.79686 |    9.1871   2.0999e-02   2.8209e-04     9.2084 |  10.4 |\n",
      "  123 |   7.50e-04   7.50e-04   1.00e-02  3.725e+00 |    2.9633   2.0542e-02   2.3394e-04    2.9841 |   0.59592   0.80082   0.80179   0.80073 |    8.9266   2.0554e-02   2.3170e-04     8.9474 |  12.2 |\n",
      "Previous best_epoch:   120   best iter: 17220,   best_value: 0.79975\n",
      "New      best_epoch:   123   best iter: 17955,   best_value: 0.80082\n",
      " decay gumbel softmax to 3.5945284999999996\n",
      "  124 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.5513   2.0554e-02   2.3170e-04    3.5721 |   0.59700   0.80090   0.80248   0.80079 |    8.9460   2.0554e-02   2.3170e-04     8.9667 |  11.6 |\n",
      "Previous best_epoch:   123   best iter: 17955,   best_value: 0.80082\n",
      "New      best_epoch:   124   best iter: 18060,   best_value: 0.80090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  124 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9742   2.1400e-02   2.1432e-04    2.9958 |   0.60938   0.79771   0.79910   0.79761 |    9.1079   2.1440e-02   3.0042e-04     9.1296 |   9.9 |\n",
      "  125 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.2408   2.1440e-02   3.0042e-04    3.2625 |   0.60317   0.80051   0.80202   0.80042 |    9.0563   2.1440e-02   3.0042e-04     9.0780 |   9.8 |\n",
      "  125 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9202   2.1208e-02   4.2453e-04    2.9419 |   0.59943   0.80223   0.80264   0.80214 |    8.9962   2.1240e-02   3.3437e-04     9.0178 |   9.9 |\n",
      "Previous best_epoch:   124   best iter: 18060,   best_value: 0.80090\n",
      "New      best_epoch:   125   best iter: 18375,   best_value: 0.80223\n",
      "\n",
      "[e] Policy training epoch:125  it:18375 -  Total Loss: 9.0178     \n",
      "Task: 8.9962   Sparsity: 2.12404e-02    Sharing: 3.34368e-04 \n",
      "\n",
      " epch: 125   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1385    0.8615  0    0.1059    0.8941  0    0.2121    0.7879  0\n",
      "   2    0.5314    0.4686  1    0.1849    0.8151  0    0.4010    0.5990  0\n",
      "   3    0.4573    0.5427  0    0.2534    0.7466  0    0.4833    0.5167  0\n",
      "   4    0.3504    0.6496  0    0.2876    0.7124  0    0.3012    0.6988  0\n",
      "   5    0.2028    0.7972  0    0.1337    0.8663  0    0.2128    0.7872  0\n",
      "   6    0.3987    0.6013  0    0.2156    0.7844  0    0.2912    0.7088  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  126 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.2100   2.1240e-02   3.3437e-04    3.2316 |   0.60645   0.80050   0.80263   0.80040 |    9.0896   2.1240e-02   3.3437e-04     9.1111 |   9.7 |\n",
      "  126 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.2426   2.2081e-02   3.2709e-04    3.2650 |   0.60148   0.80313   0.80451   0.80302 |    9.0242   2.2075e-02   2.9101e-04     9.0465 |   9.7 |\n",
      "Previous best_epoch:   125   best iter: 18375,   best_value: 0.80223\n",
      "New      best_epoch:   126   best iter: 18585,   best_value: 0.80313\n",
      "  127 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.2582   2.2075e-02   2.9101e-04    3.2805 |   0.60565   0.80346   0.80392   0.80338 |    9.0850   2.2075e-02   2.9101e-04     9.1074 |   9.7 |\n",
      "Previous best_epoch:   126   best iter: 18585,   best_value: 0.80313\n",
      "New      best_epoch:   127   best iter: 18690,   best_value: 0.80346\n",
      "  127 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.8088   2.0899e-02   2.8129e-04    2.8300 |   0.60745   0.80123   0.80281   0.80114 |    9.1345   2.0891e-02   2.8552e-04     9.1557 |  10.2 |\n",
      "  128 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.8290   2.0891e-02   2.8552e-04    2.8502 |   0.60555   0.79961   0.80241   0.79952 |    9.0712   2.0891e-02   2.8552e-04     9.0924 |  10.0 |\n",
      "  128 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.1073   2.0811e-02   4.7262e-04    3.1286 |   0.60713   0.80000   0.80115   0.79989 |    9.0955   2.0817e-02   3.7210e-04     9.1167 |   9.5 |\n",
      "  129 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.6539   2.0817e-02   3.7210e-04    3.6751 |   0.61250   0.80085   0.80231   0.80075 |    9.2016   2.0817e-02   3.7210e-04     9.2228 |   9.7 |\n",
      "  129 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9468   2.1261e-02   3.9793e-04    2.9685 |   0.61588   0.79955   0.80094   0.79944 |    9.2458   2.1244e-02   3.3152e-04     9.2674 |   9.7 |\n",
      "  130 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.2227   2.1244e-02   3.3152e-04    3.2443 |   0.61068   0.80125   0.80177   0.80117 |    9.1637   2.1244e-02   3.3152e-04     9.1853 |   9.9 |\n",
      "  130 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.8322   2.1855e-02   2.7485e-04    2.8544 |   0.60330   0.80234   0.80433   0.80224 |    9.0723   2.1851e-02   2.0197e-04     9.0944 |   9.7 |\n",
      "\n",
      "[e] Policy training epoch:130  it:19425 -  Total Loss: 9.0944     \n",
      "Task: 9.0723   Sparsity: 2.18513e-02    Sharing: 2.01970e-04 \n",
      "\n",
      " epch: 130   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1387    0.8613  0    0.0864    0.9136  0    0.1753    0.8247  0\n",
      "   2    0.4305    0.5695  0    0.1837    0.8163  0    0.3482    0.6518  0\n",
      "   3    0.5160    0.4840  1    0.3220    0.6780  0    0.4705    0.5295  0\n",
      "   4    0.3666    0.6334  0    0.3009    0.6991  0    0.3131    0.6869  0\n",
      "   5    0.2015    0.7985  0    0.1830    0.8170  0    0.2573    0.7427  0\n",
      "   6    0.4023    0.5977  0    0.3320    0.6680  0    0.2923    0.7077  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  131 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.7940   2.1851e-02   2.0197e-04    2.8160 |   0.60769   0.80329   0.80400   0.80320 |    9.1110   2.1851e-02   2.0197e-04     9.1330 |  10.1 |\n",
      "  131 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.3196   2.2019e-02   3.6995e-04    3.3420 |   0.60445   0.80228   0.80369   0.80218 |    9.0720   2.1999e-02   3.7346e-04     9.0944 |   9.7 |\n",
      "  132 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.8916   2.1999e-02   3.7346e-04    2.9140 |   0.60520   0.80181   0.80406   0.80168 |    9.0630   2.1999e-02   3.7346e-04     9.0854 |   9.8 |\n",
      "  132 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.7938   2.1479e-02   2.8238e-04    2.8156 |   0.60392   0.80385   0.80490   0.80375 |    9.0600   2.1498e-02   2.4423e-04     9.0817 |   9.8 |\n",
      "Previous best_epoch:   127   best iter: 18690,   best_value: 0.80346\n",
      "New      best_epoch:   132   best iter: 19845,   best_value: 0.80385\n",
      "  133 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.1424   2.1498e-02   2.4423e-04    3.1642 |   0.60312   0.80466   0.80508   0.80456 |    9.0493   2.1498e-02   2.4423e-04     9.0711 |   9.8 |\n",
      "Previous best_epoch:   132   best iter: 19845,   best_value: 0.80385\n",
      "New      best_epoch:   133   best iter: 19950,   best_value: 0.80466\n",
      "  133 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.7771   2.1319e-02   2.6915e-04    2.7987 |   0.61524   0.80152   0.80305   0.80143 |    9.2385   2.1307e-02   3.0830e-04     9.2602 |   9.6 |\n",
      "  134 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.3888   2.1307e-02   3.0830e-04    3.4104 |   0.60995   0.80212   0.80464   0.80201 |    9.1497   2.1307e-02   3.0830e-04     9.1713 |  10.0 |\n",
      "  134 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9609   1.9507e-02   3.3041e-04    2.9807 |   0.60680   0.80353   0.80504   0.80344 |    9.1120   1.9502e-02   2.6378e-04     9.1317 |   9.6 |\n",
      "  135 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.0142   1.9502e-02   2.6378e-04    3.0339 |   0.60789   0.80176   0.80334   0.80167 |    9.1055   1.9502e-02   2.6378e-04     9.1253 |   9.9 |\n",
      "  135 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.2047   2.1100e-02   5.4812e-04    3.2264 |   0.60716   0.80563   0.80675   0.80554 |    9.1203   2.1150e-02   4.3298e-04     9.1419 |   9.7 |\n",
      "Previous best_epoch:   133   best iter: 19950,   best_value: 0.80466\n",
      "New      best_epoch:   135   best iter: 20475,   best_value: 0.80563\n",
      "\n",
      "[e] Policy training epoch:135  it:20475 -  Total Loss: 9.1419     \n",
      "Task: 9.1203   Sparsity: 2.11500e-02    Sharing: 4.32980e-04 \n",
      "\n",
      " epch: 135   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0974    0.9026  0    0.0723    0.9277  0    0.1664    0.8336  0\n",
      "   2    0.4456    0.5544  0    0.1659    0.8341  0    0.3981    0.6019  0\n",
      "   3    0.4631    0.5369  0    0.2829    0.7171  0    0.4976    0.5024  0\n",
      "   4    0.3785    0.6215  0    0.3157    0.6843  0    0.3443    0.6557  0\n",
      "   5    0.1782    0.8218  0    0.1540    0.8460  0    0.2387    0.7613  0\n",
      "   6    0.3987    0.6013  0    0.2187    0.7813  0    0.3264    0.6736  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  136 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9960   2.1150e-02   4.3298e-04    3.0176 |   0.60430   0.80518   0.80669   0.80508 |    9.0644   2.1150e-02   4.3298e-04     9.0860 |  11.0 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  136 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9432   2.1555e-02   2.8763e-04    2.9651 |   0.60690   0.80600   0.80677   0.80591 |    9.1173   2.1566e-02   2.7076e-04     9.1391 |  11.6 |\n",
      "Previous best_epoch:   135   best iter: 20475,   best_value: 0.80563\n",
      "New      best_epoch:   136   best iter: 20685,   best_value: 0.80600\n",
      "  137 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.8861   2.1566e-02   2.7076e-04    2.9079 |   0.60601   0.80492   0.80605   0.80483 |    9.0804   2.1566e-02   2.7076e-04     9.1023 |  13.2 |\n",
      "  137 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.4642   2.0914e-02   3.6125e-04    2.4855 |   0.60866   0.80358   0.80525   0.80346 |    9.1185   2.0901e-02   3.6323e-04     9.1398 |  13.2 |\n",
      "  138 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.9188   2.0901e-02   3.6323e-04    2.9401 |   0.61553   0.80187   0.80366   0.80178 |    9.2423   2.0901e-02   3.6323e-04     9.2635 |  12.6 |\n",
      "  138 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    3.0448   2.1342e-02   4.2512e-04    3.0666 |   0.61790   0.80372   0.80447   0.80362 |    9.2552   2.1339e-02   4.9399e-04     9.2770 |  10.7 |\n",
      "  139 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.5213   2.1339e-02   4.9399e-04    2.5431 |   0.61183   0.80449   0.80623   0.80440 |    9.1660   2.1339e-02   4.9399e-04     9.1878 |  10.7 |\n",
      "  139 |   7.50e-04   7.50e-04   1.00e-02  3.595e+00 |    2.2603   1.9581e-02   3.6908e-04    2.2802 |   0.60942   0.80610   0.80633   0.80601 |    9.1603   1.9580e-02   3.9429e-04     9.1803 |  10.3 |\n",
      "Previous best_epoch:   136   best iter: 20685,   best_value: 0.80600\n",
      "New      best_epoch:   139   best iter: 21315,   best_value: 0.80610\n",
      " decay gumbel softmax to 3.4687200024999996\n",
      "  140 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.4705   1.9580e-02   3.9429e-04    2.4904 |   0.60830   0.80474   0.80613   0.80465 |    9.1296   1.9580e-02   3.9429e-04     9.1495 |  10.7 |\n",
      "  140 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.4937   2.0133e-02   2.8098e-04    2.5142 |   0.60399   0.80808   0.80867   0.80797 |    9.0488   2.0133e-02   2.7779e-04     9.0692 |  11.1 |\n",
      "Previous best_epoch:   139   best iter: 21315,   best_value: 0.80610\n",
      "New      best_epoch:   140   best iter: 21525,   best_value: 0.80808\n",
      "\n",
      "[e] Policy training epoch:140  it:21525 -  Total Loss: 9.0692     \n",
      "Task: 9.0488   Sparsity: 2.01335e-02    Sharing: 2.77787e-04 \n",
      "\n",
      " epch: 140   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0956    0.9044  0    0.1001    0.8999  0    0.1740    0.8260  0\n",
      "   2    0.5370    0.4630  1    0.2111    0.7889  0    0.3329    0.6671  0\n",
      "   3    0.4594    0.5406  0    0.2769    0.7231  0    0.4429    0.5571  0\n",
      "   4    0.2958    0.7042  0    0.2766    0.7234  0    0.2557    0.7443  0\n",
      "   5    0.1902    0.8098  0    0.1500    0.8500  0    0.2166    0.7834  0\n",
      "   6    0.3819    0.6181  0    0.2486    0.7514  0    0.2969    0.7031  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  141 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.5874   2.0133e-02   2.7779e-04    2.6078 |   0.60940   0.80563   0.80758   0.80554 |    9.1444   2.0133e-02   2.7779e-04     9.1649 |  10.4 |\n",
      "  141 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.8164   2.1387e-02   3.3590e-04    2.8382 |   0.61174   0.80704   0.80812   0.80695 |    9.1881   2.1387e-02   3.1325e-04     9.2098 |  12.2 |\n",
      "  142 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.7381   2.1387e-02   3.1325e-04    2.7598 |   0.61581   0.80470   0.80578   0.80461 |    9.2521   2.1387e-02   3.1325e-04     9.2738 |  16.8 |\n",
      "  142 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    3.1198   2.1457e-02   4.5051e-04    3.1417 |   0.61720   0.80533   0.80645   0.80523 |    9.2803   2.1477e-02   4.1786e-04     9.3022 |  10.1 |\n",
      "  143 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.8637   2.1477e-02   4.1786e-04    2.8856 |   0.61096   0.80581   0.80729   0.80571 |    9.1695   2.1477e-02   4.1786e-04     9.1914 |  11.4 |\n",
      "  143 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.6887   2.2238e-02   2.7087e-04    2.7112 |   0.60553   0.80775   0.80823   0.80766 |    9.1005   2.2209e-02   2.3026e-04     9.1229 |  11.1 |\n",
      "  144 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.7534   2.2209e-02   2.3026e-04    2.7758 |   0.62038   0.80590   0.80740   0.80581 |    9.3399   2.2209e-02   2.3026e-04     9.3623 |  10.3 |\n",
      "  144 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.3501   2.2202e-02   4.6017e-04    2.3728 |   0.61612   0.80663   0.80808   0.80654 |    9.2184   2.2165e-02   3.5921e-04     9.2409 |  11.5 |\n",
      "  145 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.5307   2.2165e-02   3.5921e-04    2.5532 |   0.61190   0.80777   0.80840   0.80769 |    9.1483   2.2165e-02   3.5921e-04     9.1709 |  10.2 |\n",
      "  145 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.5055   2.1374e-02   2.6892e-04    2.5272 |   0.61268   0.80722   0.80810   0.80712 |    9.1976   2.1392e-02   3.1477e-04     9.2193 |  11.2 |\n",
      "\n",
      "[e] Policy training epoch:145  it:22575 -  Total Loss: 9.2193     \n",
      "Task: 9.1976   Sparsity: 2.13917e-02    Sharing: 3.14773e-04 \n",
      "\n",
      " epch: 145   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0995    0.9005  0    0.0850    0.9150  0    0.1651    0.8349  0\n",
      "   2    0.4719    0.5281  0    0.1752    0.8248  0    0.3548    0.6452  0\n",
      "   3    0.4850    0.5150  0    0.2501    0.7499  0    0.4744    0.5256  0\n",
      "   4    0.3757    0.6243  0    0.2782    0.7218  0    0.3164    0.6836  0\n",
      "   5    0.2056    0.7944  0    0.1554    0.8446  0    0.2478    0.7522  0\n",
      "   6    0.4386    0.5614  0    0.2695    0.7305  0    0.3473    0.6527  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  146 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.8560   2.1392e-02   3.1477e-04    2.8777 |   0.61438   0.80859   0.80993   0.80850 |    9.2239   2.1392e-02   3.1477e-04     9.2456 |  11.2 |\n",
      "Previous best_epoch:   140   best iter: 21525,   best_value: 0.80808\n",
      "New      best_epoch:   146   best iter: 22680,   best_value: 0.80859\n",
      "  146 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.8611   1.9818e-02   2.3825e-04    2.8812 |   0.61178   0.80869   0.80991   0.80858 |    9.1666   1.9802e-02   2.0439e-04     9.1866 |  10.8 |\n",
      "Previous best_epoch:   146   best iter: 22680,   best_value: 0.80859\n",
      "New      best_epoch:   146   best iter: 22785,   best_value: 0.80869\n",
      "  147 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.6473   1.9802e-02   2.0439e-04    2.6673 |   0.61638   0.80534   0.80717   0.80525 |    9.2411   1.9802e-02   2.0439e-04     9.2611 |   9.6 |\n",
      "  147 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.4393   2.0012e-02   2.0909e-04    2.4595 |   0.61472   0.80792   0.80945   0.80782 |    9.2097   2.0022e-02   3.8661e-04     9.2301 |   9.8 |\n",
      "  148 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.7519   2.0022e-02   3.8661e-04    2.7723 |   0.61458   0.80793   0.80920   0.80781 |    9.2285   2.0022e-02   3.8661e-04     9.2489 |  10.0 |\n",
      "  148 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    3.2268   2.0195e-02   2.8480e-04    3.2473 |   0.61076   0.80662   0.80795   0.80653 |    9.1740   2.0181e-02   3.7284e-04     9.1945 |   9.5 |\n",
      "  149 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.3239   2.0181e-02   3.7284e-04    2.3445 |   0.61549   0.80770   0.80887   0.80761 |    9.2385   2.0181e-02   3.7284e-04     9.2591 |   9.7 |\n",
      "  149 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.5969   2.0675e-02   2.5289e-04    2.6178 |   0.61439   0.80744   0.80922   0.80733 |    9.2178   2.0703e-02   3.4572e-04     9.2389 |   9.7 |\n",
      "  150 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.3098   2.0703e-02   3.4572e-04    2.3309 |   0.61241   0.80872   0.81022   0.80863 |    9.1982   2.0703e-02   3.4572e-04     9.2193 |   9.8 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous best_epoch:   146   best iter: 22785,   best_value: 0.80869\n",
      "New      best_epoch:   150   best iter: 23520,   best_value: 0.80872\n",
      "  150 |   7.50e-04   7.50e-04   1.00e-02  3.469e+00 |    2.3762   2.1268e-02   2.2076e-04    2.3977 |   0.61335   0.80877   0.81016   0.80867 |    9.1929   2.1284e-02   3.7288e-04     9.2146 |   9.8 |\n",
      "Epoch    75: reducing learning rate of group 0 to 7.5000e-03.\n",
      "Previous best_epoch:   150   best iter: 23520,   best_value: 0.80872\n",
      "New      best_epoch:   150   best iter: 23625,   best_value: 0.80877\n",
      "\n",
      "[e] Policy training epoch:150  it:23625 -  Total Loss: 9.2146     \n",
      "Task: 9.1929   Sparsity: 2.12843e-02    Sharing: 3.72883e-04 \n",
      "\n",
      " epch: 150   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1039    0.8961  0    0.0881    0.9119  0    0.1435    0.8565  0\n",
      "   2    0.4165    0.5835  0    0.2178    0.7822  0    0.3710    0.6290  0\n",
      "   3    0.4779    0.5221  0    0.3378    0.6622  0    0.4807    0.5193  0\n",
      "   4    0.3823    0.6177  0    0.2930    0.7070  0    0.3219    0.6781  0\n",
      "   5    0.1883    0.8117  0    0.1897    0.8103  0    0.2193    0.7807  0\n",
      "   6    0.3963    0.6037  0    0.2974    0.7026  0    0.2753    0.7247  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  151 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    1.9313   2.1284e-02   3.7288e-04    1.9529 |   0.61288   0.80863   0.80974   0.80854 |    9.1987   2.1284e-02   3.7288e-04     9.2204 |  10.2 |\n",
      "  151 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.5604   2.0379e-02   2.9164e-04    2.5811 |   0.61350   0.80864   0.81016   0.80855 |    9.2024   2.0377e-02   2.8303e-04     9.2231 |   9.9 |\n",
      "  152 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.7029   2.0377e-02   2.8303e-04    2.7236 |   0.62101   0.80930   0.81055   0.80919 |    9.3317   2.0377e-02   2.8303e-04     9.3524 |  10.7 |\n",
      "Previous best_epoch:   150   best iter: 23625,   best_value: 0.80877\n",
      "New      best_epoch:   152   best iter: 23940,   best_value: 0.80930\n",
      "  152 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.0808   1.9404e-02   1.9578e-04    2.1004 |   0.60916   0.80853   0.80961   0.80845 |    9.1296   1.9379e-02   2.3614e-04     9.1492 |   9.7 |\n",
      "  153 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.4027   1.9379e-02   2.3614e-04    2.4223 |   0.61254   0.80976   0.81082   0.80967 |    9.2197   1.9379e-02   2.3614e-04     9.2393 |   9.7 |\n",
      "Previous best_epoch:   152   best iter: 23940,   best_value: 0.80930\n",
      "New      best_epoch:   153   best iter: 24150,   best_value: 0.80976\n",
      "  153 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    3.0665   1.9450e-02   2.4262e-04    3.0862 |   0.62221   0.80806   0.80962   0.80796 |    9.3189   1.9442e-02   2.9896e-04     9.3387 |   9.7 |\n",
      "  154 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.0557   1.9442e-02   2.9896e-04    2.0754 |   0.61162   0.80969   0.81067   0.80959 |    9.1863   1.9442e-02   2.9896e-04     9.2060 |   9.9 |\n",
      "  154 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.1712   1.9921e-02   2.8789e-04    2.1914 |   0.61351   0.81087   0.81214   0.81077 |    9.2160   1.9896e-02   2.5984e-04     9.2361 |   9.6 |\n",
      "Previous best_epoch:   153   best iter: 24150,   best_value: 0.80976\n",
      "New      best_epoch:   154   best iter: 24465,   best_value: 0.81087\n",
      "  155 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.3003   1.9896e-02   2.5984e-04    2.3204 |   0.61746   0.81015   0.81019   0.81005 |    9.2651   1.9896e-02   2.5984e-04     9.2852 |   9.9 |\n",
      "  155 |   7.50e-04   7.50e-04   7.50e-03  3.469e+00 |    2.3628   2.0134e-02   2.6726e-04    2.3832 |   0.61720   0.80995   0.81094   0.80986 |    9.2382   2.0162e-02   3.1956e-04     9.2587 |   9.8 |\n",
      " decay gumbel softmax to 3.3473148024124995\n",
      "\n",
      "[e] Policy training epoch:155  it:24675 -  Total Loss: 9.2587     \n",
      "Task: 9.2382   Sparsity: 2.01621e-02    Sharing: 3.19561e-04 \n",
      "\n",
      " epch: 155   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0982    0.9018  0    0.0912    0.9088  0    0.1620    0.8380  0\n",
      "   2    0.4387    0.5613  0    0.2001    0.7999  0    0.3350    0.6650  0\n",
      "   3    0.4745    0.5255  0    0.2511    0.7489  0    0.4464    0.5536  0\n",
      "   4    0.3611    0.6389  0    0.3165    0.6835  0    0.3023    0.6977  0\n",
      "   5    0.1757    0.8243  0    0.1543    0.8457  0    0.2359    0.7641  0\n",
      "   6    0.4220    0.5780  0    0.2309    0.7691  0    0.2676    0.7324  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  156 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.7128   2.0162e-02   3.1956e-04    2.7333 |   0.62182   0.81040   0.81084   0.81030 |    9.3169   2.0162e-02   3.1956e-04     9.3374 |   9.8 |\n",
      "  156 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.3431   1.9723e-02   2.9582e-04    2.3631 |   0.61849   0.81018   0.81068   0.81009 |    9.2885   1.9714e-02   3.6411e-04     9.3086 |  11.7 |\n",
      "  157 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.1808   1.9714e-02   3.6411e-04    2.2009 |   0.61588   0.81088   0.81217   0.81079 |    9.2131   1.9714e-02   3.6411e-04     9.2331 |  10.6 |\n",
      "Previous best_epoch:   154   best iter: 24465,   best_value: 0.81087\n",
      "New      best_epoch:   157   best iter: 24990,   best_value: 0.81088\n",
      "  157 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.3634   2.0721e-02   3.4141e-04    2.3844 |   0.61362   0.81131   0.81235   0.81122 |    9.2109   2.0722e-02   2.9050e-04     9.2319 |   9.9 |\n",
      "Previous best_epoch:   157   best iter: 24990,   best_value: 0.81088\n",
      "New      best_epoch:   157   best iter: 25095,   best_value: 0.81131\n",
      "  158 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.7637   2.0722e-02   2.9050e-04    2.7847 |   0.61933   0.81158   0.81254   0.81149 |    9.3059   2.0722e-02   2.9050e-04     9.3269 |   9.7 |\n",
      "Previous best_epoch:   157   best iter: 25095,   best_value: 0.81131\n",
      "New      best_epoch:   158   best iter: 25200,   best_value: 0.81158\n",
      "  158 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.5462   2.0097e-02   2.9847e-04    2.5666 |   0.61323   0.81141   0.81225   0.81133 |    9.2037   2.0111e-02   3.2460e-04     9.2242 |   9.6 |\n",
      "  159 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.6512   2.0111e-02   3.2460e-04    2.6717 |   0.62022   0.80929   0.81032   0.80921 |    9.3223   2.0111e-02   3.2460e-04     9.3428 |   9.7 |\n",
      "  159 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.3755   2.0051e-02   1.8544e-04    2.3957 |   0.62025   0.81136   0.81158   0.81126 |    9.2868   2.0035e-02   2.6819e-04     9.3071 |  10.6 |\n",
      "  160 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.7657   2.0035e-02   2.6819e-04    2.7860 |   0.62214   0.81082   0.81115   0.81074 |    9.3342   2.0035e-02   2.6819e-04     9.3545 |  10.8 |\n",
      "  160 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.5991   2.0388e-02   2.8175e-04    2.6198 |   0.61822   0.81165   0.81155   0.81157 |    9.2701   2.0390e-02   2.8215e-04     9.2908 |  10.0 |\n",
      "Previous best_epoch:   158   best iter: 25200,   best_value: 0.81158\n",
      "New      best_epoch:   160   best iter: 25725,   best_value: 0.81165\n",
      "\n",
      "[e] Policy training epoch:160  it:25725 -  Total Loss: 9.2908     \n",
      "Task: 9.2701   Sparsity: 2.03896e-02    Sharing: 2.82153e-04 \n",
      "\n",
      " epch: 160   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.1036    0.8964  0    0.0888    0.9112  0    0.1399    0.8601  0\n",
      "   2    0.4638    0.5362  0    0.2090    0.7910  0    0.3469    0.6531  0\n",
      "   3    0.5025    0.4975  1    0.2699    0.7301  0    0.4279    0.5721  0\n",
      "   4    0.3642    0.6358  0    0.2761    0.7239  0    0.2925    0.7075  0\n",
      "   5    0.1783    0.8217  0    0.1925    0.8075  0    0.1867    0.8133  0\n",
      "   6    0.4339    0.5661  0    0.2892    0.7108  0    0.2287    0.7713  0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  161 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    3.1422   2.0390e-02   2.8215e-04    3.1628 |   0.62259   0.81150   0.81272   0.81141 |    9.3140   2.0390e-02   2.8215e-04     9.3347 |   9.6 |\n",
      "  161 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.4292   2.0729e-02   1.8145e-04    2.4501 |   0.61919   0.81293   0.81317   0.81284 |    9.2816   2.0712e-02   2.4336e-04     9.3025 |   9.8 |\n",
      "Previous best_epoch:   160   best iter: 25725,   best_value: 0.81165\n",
      "New      best_epoch:   161   best iter: 25935,   best_value: 0.81293\n",
      "  162 |   7.50e-04   7.50e-04   7.50e-03  3.347e+00 |    2.3632   2.0712e-02   2.4336e-04    2.3842 |   0.62480   0.80964   0.81095   0.80955 |    9.3255   2.0712e-02   2.4336e-04     9.3465 |   9.8 |\n",
      "Epoch   162: reducing learning rate of group 0 to 5.6250e-04.\n",
      "Epoch   162: reducing learning rate of group 1 to 5.6250e-04.\n",
      "  162 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.1445   2.1446e-02   1.5365e-04    2.1661 |   0.62137   0.81047   0.81203   0.81036 |    9.3162   2.1476e-02   1.9532e-04     9.3379 |   9.8 |\n",
      "  163 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.3818   2.1476e-02   1.9532e-04    2.4035 |   0.61663   0.81272   0.81360   0.81263 |    9.2550   2.1476e-02   1.9532e-04     9.2767 |   9.7 |\n",
      "  163 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.6542   2.1465e-02   2.1709e-04    2.6759 |   0.62852   0.80954   0.81075   0.80944 |    9.4462   2.1452e-02   1.9167e-04     9.4679 |   9.9 |\n",
      "  164 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.0511   2.1452e-02   1.9167e-04    2.0727 |   0.62938   0.80937   0.81059   0.80928 |    9.4593   2.1452e-02   1.9167e-04     9.4809 |  10.7 |\n",
      "  164 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.7381   2.0687e-02   2.3464e-04    2.7590 |   0.62405   0.81157   0.81242   0.81148 |    9.3914   2.0667e-02   1.4190e-04     9.4122 |  12.8 |\n",
      "  165 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.5410   2.0667e-02   1.4190e-04    2.5618 |   0.62687   0.81030   0.81133   0.81020 |    9.3923   2.0667e-02   1.4190e-04     9.4131 |  12.7 |\n",
      "  165 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.2828   2.0233e-02   2.6601e-04    2.3033 |   0.62237   0.81153   0.81287   0.81144 |    9.3371   2.0239e-02   2.4990e-04     9.3576 |  10.2 |\n",
      "\n",
      "[e] Policy training epoch:165  it:26775 -  Total Loss: 9.3576     \n",
      "Task: 9.3371   Sparsity: 2.02393e-02    Sharing: 2.49902e-04 \n",
      "\n",
      " epch: 165   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0936    0.9064  0    0.0758    0.9242  0    0.1486    0.8514  0\n",
      "   2    0.4460    0.5540  0    0.2250    0.7750  0    0.3009    0.6991  0\n",
      "   3    0.4848    0.5152  0    0.2791    0.7209  0    0.4529    0.5471  0\n",
      "   4    0.3511    0.6489  0    0.2984    0.7016  0    0.2661    0.7339  0\n",
      "   5    0.1962    0.8038  0    0.1843    0.8157  0    0.2273    0.7727  0\n",
      "   6    0.4123    0.5877  0    0.2844    0.7156  0    0.2550    0.7450  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  166 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.3597   2.0239e-02   2.4990e-04    2.3802 |   0.61876   0.81289   0.81318   0.81281 |    9.2683   2.0239e-02   2.4990e-04     9.2888 |  10.1 |\n",
      "  166 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.2878   2.1299e-02   2.1385e-04    2.3093 |   0.62277   0.81021   0.81134   0.81011 |    9.3398   2.1289e-02   1.5507e-04     9.3613 |  11.5 |\n",
      "  167 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.1280   2.1289e-02   1.5507e-04    2.1494 |   0.62521   0.81212   0.81352   0.81203 |    9.4188   2.1289e-02   1.5507e-04     9.4402 |   9.8 |\n",
      "  167 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.4126   2.0904e-02   1.9807e-04    2.4337 |   0.62664   0.81196   0.81303   0.81188 |    9.3967   2.0895e-02   3.2345e-04     9.4179 |   9.7 |\n",
      "  168 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.6582   2.0895e-02   3.2345e-04    2.6794 |   0.61992   0.81101   0.81248   0.81092 |    9.2888   2.0895e-02   3.2345e-04     9.3100 |   9.8 |\n",
      "  168 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.0259   2.0553e-02   1.7321e-04    2.0466 |   0.62598   0.81081   0.81251   0.81071 |    9.3999   2.0540e-02   1.8600e-04     9.4206 |   9.4 |\n",
      "  169 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.2929   2.0540e-02   1.8600e-04    2.3136 |   0.62684   0.81287   0.81412   0.81279 |    9.4203   2.0540e-02   1.8600e-04     9.4410 |  10.0 |\n",
      "  169 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.2910   2.1229e-02   3.4756e-04    2.3126 |   0.62308   0.81376   0.81403   0.81368 |    9.3566   2.1240e-02   3.1458e-04     9.3781 |  11.5 |\n",
      "Previous best_epoch:   161   best iter: 25935,   best_value: 0.81293\n",
      "New      best_epoch:   169   best iter: 27615,   best_value: 0.81376\n",
      "  170 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.8608   2.1240e-02   3.1458e-04    2.8824 |   0.62466   0.81283   0.81271   0.81273 |    9.3936   2.1240e-02   3.1458e-04     9.4151 |  10.2 |\n",
      "  170 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.3188   2.0675e-02   1.2277e-04    2.3396 |   0.62419   0.81287   0.81426   0.81275 |    9.3760   2.0685e-02   2.0432e-04     9.3969 |  11.6 |\n",
      "\n",
      "[e] Policy training epoch:170  it:27825 -  Total Loss: 9.3969     \n",
      "Task: 9.3760   Sparsity: 2.06855e-02    Sharing: 2.04323e-04 \n",
      "\n",
      " epch: 170   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0919    0.9081  0    0.0914    0.9086  0    0.1551    0.8449  0\n",
      "   2    0.4297    0.5703  0    0.2314    0.7686  0    0.3197    0.6803  0\n",
      "   3    0.4942    0.5058  0    0.2981    0.7019  0    0.4752    0.5248  0\n",
      "   4    0.3447    0.6553  0    0.3255    0.6745  0    0.2993    0.7007  0\n",
      "   5    0.1779    0.8221  0    0.1829    0.8171  0    0.2179    0.7821  0\n",
      "   6    0.3896    0.6104  0    0.2627    0.7373  0    0.2887    0.7113  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  171 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.3169   2.0685e-02   2.0432e-04    2.3378 |   0.61898   0.81354   0.81408   0.81346 |    9.2979   2.0685e-02   2.0432e-04     9.3188 |   9.9 |\n",
      "  171 |   5.63e-04   5.63e-04   7.50e-03  3.347e+00 |    2.1184   2.0673e-02   2.8133e-04    2.1393 |   0.62128   0.81345   0.81419   0.81335 |    9.3069   2.0649e-02   3.0437e-04     9.3279 |  10.9 |\n",
      " decay gumbel softmax to 3.230158784328062\n",
      "  172 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.3558   2.0649e-02   3.0437e-04    2.3767 |   0.62272   0.81322   0.81384   0.81314 |    9.3386   2.0649e-02   3.0437e-04     9.3596 |  13.8 |\n",
      "  172 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.1955   2.1126e-02   2.2811e-04    2.2168 |   0.62887   0.81248   0.81359   0.81238 |    9.4296   2.1131e-02   2.2720e-04     9.4510 |  12.3 |\n",
      "  173 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.7856   2.1131e-02   2.2720e-04    1.8070 |   0.62952   0.81212   0.81290   0.81203 |    9.4422   2.1131e-02   2.2720e-04     9.4635 |  10.1 |\n",
      "  173 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.0305   2.0997e-02   2.3065e-04    2.0517 |   0.62658   0.81319   0.81433   0.81309 |    9.3862   2.1003e-02   1.7375e-04     9.4073 |  10.0 |\n",
      "  174 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.0230   2.1003e-02   1.7375e-04    2.0442 |   0.61953   0.81281   0.81372   0.81272 |    9.3062   2.1003e-02   1.7375e-04     9.3273 |  10.1 |\n",
      "  174 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.9133   2.1210e-02   2.1590e-04    1.9347 |   0.62899   0.81359   0.81423   0.81349 |    9.3972   2.1211e-02   2.2584e-04     9.4186 |  10.1 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  175 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.8526   2.1211e-02   2.2584e-04    1.8741 |   0.63113   0.81213   0.81266   0.81203 |    9.4735   2.1211e-02   2.2584e-04     9.4949 |  10.0 |\n",
      "  175 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.1207   2.1510e-02   3.0413e-04    2.1426 |   0.63684   0.81212   0.81323   0.81204 |    9.5529   2.1518e-02   1.8225e-04     9.5746 |  10.0 |\n",
      "\n",
      "[e] Policy training epoch:175  it:28875 -  Total Loss: 9.5746     \n",
      "Task: 9.5529   Sparsity: 2.15176e-02    Sharing: 1.82249e-04 \n",
      "\n",
      " epch: 175   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0994    0.9006  0    0.1079    0.8921  0    0.1741    0.8259  0\n",
      "   2    0.4610    0.5390  0    0.2280    0.7720  0    0.3740    0.6260  0\n",
      "   3    0.4710    0.5290  0    0.2848    0.7152  0    0.4795    0.5205  0\n",
      "   4    0.3593    0.6407  0    0.3138    0.6862  0    0.3230    0.6770  0\n",
      "   5    0.1904    0.8096  0    0.1677    0.8323  0    0.2207    0.7793  0\n",
      "   6    0.4331    0.5669  0    0.2501    0.7499  0    0.3048    0.6952  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  176 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.9043   2.1518e-02   1.8225e-04    1.9260 |   0.62600   0.81166   0.81277   0.81157 |    9.4164   2.1518e-02   1.8225e-04     9.4381 |   9.9 |\n",
      "  176 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.5579   2.0944e-02   2.2735e-04    1.5791 |   0.62884   0.81268   0.81406   0.81257 |    9.4188   2.0939e-02   1.8819e-04     9.4400 |   9.9 |\n",
      "  177 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.8083   2.0939e-02   1.8819e-04    1.8295 |   0.63281   0.81261   0.81313   0.81252 |    9.4698   2.0939e-02   1.8819e-04     9.4909 |   9.8 |\n",
      "  177 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.9554   2.1638e-02   1.6897e-04    1.9772 |   0.63017   0.81231   0.81414   0.81223 |    9.4830   2.1641e-02   1.6593e-04     9.5048 |  10.2 |\n",
      "  178 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.3733   2.1641e-02   1.6593e-04    2.3951 |   0.63155   0.81235   0.81379   0.81226 |    9.4562   2.1641e-02   1.6593e-04     9.4781 |   9.8 |\n",
      "  178 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.9241   2.1870e-02   1.7834e-04    1.9462 |   0.63193   0.81432   0.81458   0.81422 |    9.4951   2.1864e-02   2.0237e-04     9.5172 |  10.0 |\n",
      "Previous best_epoch:   169   best iter: 27615,   best_value: 0.81376\n",
      "New      best_epoch:   178   best iter: 29505,   best_value: 0.81432\n",
      "  179 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.1828   2.1864e-02   2.0237e-04    2.2048 |   0.63913   0.81237   0.81356   0.81228 |    9.5924   2.1864e-02   2.0237e-04     9.6145 |   9.7 |\n",
      "  179 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.4892   2.0632e-02   2.6245e-04    2.5101 |   0.63211   0.81251   0.81406   0.81242 |    9.4831   2.0628e-02   2.1086e-04     9.5039 |   9.8 |\n",
      "  180 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.2316   2.0628e-02   2.1086e-04    2.2524 |   0.63052   0.81373   0.81420   0.81365 |    9.4866   2.0628e-02   2.1086e-04     9.5075 |   9.8 |\n",
      "  180 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.2358   2.0305e-02   2.1359e-04    2.2563 |   0.62646   0.81462   0.81511   0.81454 |    9.3827   2.0297e-02   3.1104e-04     9.4033 |  11.2 |\n",
      "Previous best_epoch:   178   best iter: 29505,   best_value: 0.81432\n",
      "New      best_epoch:   180   best iter: 29925,   best_value: 0.81462\n",
      "\n",
      "[e] Policy training epoch:180  it:29925 -  Total Loss: 9.4033     \n",
      "Task: 9.3827   Sparsity: 2.02970e-02    Sharing: 3.11041e-04 \n",
      "\n",
      " epch: 180   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0906    0.9094  0    0.0855    0.9145  0    0.1543    0.8457  0\n",
      "   2    0.4712    0.5288  0    0.2201    0.7799  0    0.3584    0.6416  0\n",
      "   3    0.4722    0.5278  0    0.3070    0.6930  0    0.4738    0.5262  0\n",
      "   4    0.3215    0.6785  0    0.3071    0.6929  0    0.2660    0.7340  0\n",
      "   5    0.1942    0.8058  0    0.1538    0.8462  0    0.2346    0.7654  0\n",
      "   6    0.3947    0.6053  0    0.2228    0.7772  0    0.2527    0.7473  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  181 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.2203   2.0297e-02   3.1104e-04    2.2409 |   0.63346   0.81318   0.81416   0.81309 |    9.5104   2.0297e-02   3.1104e-04     9.5310 |  13.8 |\n",
      "  181 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.9691   2.0528e-02   1.6835e-04    1.9898 |   0.62594   0.81469   0.81497   0.81460 |    9.3801   2.0532e-02   2.1834e-04     9.4008 |  10.2 |\n",
      "Previous best_epoch:   180   best iter: 29925,   best_value: 0.81462\n",
      "New      best_epoch:   181   best iter: 30135,   best_value: 0.81469\n",
      "  182 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.8768   2.0532e-02   2.1834e-04    1.8976 |   0.63238   0.81462   0.81509   0.81454 |    9.5184   2.0532e-02   2.1834e-04     9.5391 |   9.8 |\n",
      "  182 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.2916   2.1037e-02   2.5654e-04    2.3129 |   0.63409   0.81301   0.81407   0.81291 |    9.5162   2.1043e-02   2.4172e-04     9.5375 |  10.3 |\n",
      "  183 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.9278   2.1043e-02   2.4172e-04    1.9491 |   0.62992   0.81399   0.81509   0.81389 |    9.4588   2.1043e-02   2.4172e-04     9.4801 |  10.0 |\n",
      "  183 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.0945   2.0909e-02   3.2312e-04    2.1158 |   0.63391   0.81235   0.81376   0.81225 |    9.5218   2.0916e-02   3.0862e-04     9.5431 |  10.5 |\n",
      "  184 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.0394   2.0916e-02   3.0862e-04    2.0606 |   0.63346   0.81337   0.81460   0.81326 |    9.5032   2.0916e-02   3.0862e-04     9.5244 |   9.9 |\n",
      "  184 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.1196   2.0829e-02   3.0217e-04    2.1407 |   0.63425   0.81266   0.81451   0.81258 |    9.4974   2.0809e-02   3.8013e-04     9.5186 |   9.9 |\n",
      "  185 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.5636   2.0809e-02   3.8013e-04    1.5848 |   0.62702   0.81510   0.81500   0.81502 |    9.3927   2.0809e-02   3.8013e-04     9.4138 |   9.8 |\n",
      "Previous best_epoch:   181   best iter: 30135,   best_value: 0.81469\n",
      "New      best_epoch:   185   best iter: 30870,   best_value: 0.81510\n",
      "  185 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.6367   2.1646e-02   2.4277e-04    1.6586 |   0.63865   0.81414   0.81483   0.81406 |    9.5854   2.1687e-02   2.5681e-04     9.6074 |  10.1 |\n",
      "\n",
      "[e] Policy training epoch:185  it:30975 -  Total Loss: 9.6074     \n",
      "Task: 9.5854   Sparsity: 2.16874e-02    Sharing: 2.56806e-04 \n",
      "\n",
      " epch: 185   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0873    0.9127  0    0.0819    0.9181  0    0.1615    0.8385  0\n",
      "   2    0.4845    0.5155  0    0.2892    0.7108  0    0.3840    0.6160  0\n",
      "   3    0.4928    0.5072  0    0.3094    0.6906  0    0.4426    0.5574  0\n",
      "   4    0.3756    0.6244  0    0.3420    0.6580  0    0.2964    0.7036  0\n",
      "   5    0.2070    0.7930  0    0.1875    0.8125  0    0.2379    0.7621  0\n",
      "   6    0.3677    0.6323  0    0.2686    0.7314  0    0.2649    0.7351  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  186 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.8862   2.1687e-02   2.5681e-04    1.9081 |   0.64131   0.81423   0.81460   0.81415 |    9.6270   2.1687e-02   2.5681e-04     9.6489 |   9.9 |\n",
      "  186 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.5823   2.0998e-02   2.1671e-04    2.6035 |   0.63937   0.81470   0.81625   0.81461 |    9.5826   2.1012e-02   1.8512e-04     9.6038 |   9.8 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  187 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    1.8670   2.1012e-02   1.8512e-04    1.8882 |   0.63437   0.81320   0.81440   0.81312 |    9.5422   2.1012e-02   1.8512e-04     9.5634 |   9.9 |\n",
      "  187 |   5.63e-04   5.63e-04   7.50e-03  3.230e+00 |    2.6360   2.0396e-02   3.1024e-04    2.6567 |   0.63270   0.81522   0.81592   0.81514 |    9.5057   2.0388e-02   4.3103e-04     9.5265 |  10.4 |\n",
      "Previous best_epoch:   185   best iter: 30870,   best_value: 0.81510\n",
      "New      best_epoch:   187   best iter: 31395,   best_value: 0.81522\n",
      " decay gumbel softmax to 3.1171032268765795\n",
      "  188 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.5606   2.0388e-02   4.3103e-04    1.5815 |   0.63866   0.81386   0.81480   0.81378 |    9.5584   2.0388e-02   4.3103e-04     9.5793 |  11.6 |\n",
      "  188 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.2469   2.0533e-02   2.3351e-04    2.2677 |   0.63179   0.81505   0.81525   0.81497 |    9.5105   2.0528e-02   2.4166e-04     9.5313 |  10.2 |\n",
      "  189 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.1470   2.0528e-02   2.4166e-04    2.1678 |   0.63527   0.81482   0.81542   0.81474 |    9.5291   2.0528e-02   2.4166e-04     9.5498 |   9.8 |\n",
      "  189 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.2620   1.9675e-02   3.4256e-04    2.2820 |   0.63454   0.81349   0.81451   0.81340 |    9.5316   1.9633e-02   2.8989e-04     9.5515 |   9.8 |\n",
      "  190 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.4166   1.9633e-02   2.8989e-04    2.4366 |   0.63122   0.81539   0.81638   0.81530 |    9.4903   1.9633e-02   2.8989e-04     9.5103 |   9.8 |\n",
      "Previous best_epoch:   187   best iter: 31395,   best_value: 0.81522\n",
      "New      best_epoch:   190   best iter: 31920,   best_value: 0.81539\n",
      "  190 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.0722   2.0215e-02   1.7599e-04    2.0926 |   0.63787   0.81570   0.81619   0.81562 |    9.5908   2.0249e-02   3.3592e-04     9.6113 |   9.9 |\n",
      "Previous best_epoch:   190   best iter: 31920,   best_value: 0.81539\n",
      "New      best_epoch:   190   best iter: 32025,   best_value: 0.81570\n",
      "\n",
      "[e] Policy training epoch:190  it:32025 -  Total Loss: 9.6113     \n",
      "Task: 9.5908   Sparsity: 2.02489e-02    Sharing: 3.35924e-04 \n",
      "\n",
      " epch: 190   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0866    0.9134  0    0.1003    0.8997  0    0.1660    0.8340  0\n",
      "   2    0.5018    0.4982  1    0.2286    0.7714  0    0.3512    0.6488  0\n",
      "   3    0.4512    0.5488  0    0.2647    0.7353  0    0.4443    0.5557  0\n",
      "   4    0.3236    0.6764  0    0.2780    0.7220  0    0.2816    0.7184  0\n",
      "   5    0.2090    0.7910  0    0.1916    0.8084  0    0.2158    0.7842  0\n",
      "   6    0.3861    0.6139  0    0.2416    0.7584  0    0.2671    0.7329  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  191 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.2369   2.0249e-02   3.3592e-04    2.2575 |   0.63345   0.81462   0.81490   0.81455 |    9.5110   2.0249e-02   3.3592e-04     9.5316 |  11.9 |\n",
      "  191 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.0917   2.0344e-02   1.7768e-04    2.1122 |   0.64024   0.81344   0.81399   0.81336 |    9.5863   2.0344e-02   1.6778e-04     9.6068 |  10.9 |\n",
      "  192 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.3341   2.0344e-02   1.6778e-04    2.3546 |   0.63372   0.81518   0.81603   0.81508 |    9.5028   2.0344e-02   1.6778e-04     9.5233 |  10.6 |\n",
      "  192 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.9730   2.0700e-02   2.5581e-04    1.9939 |   0.64073   0.81583   0.81586   0.81574 |    9.5941   2.0708e-02   2.4097e-04     9.6150 |  10.0 |\n",
      "Previous best_epoch:   190   best iter: 32025,   best_value: 0.81570\n",
      "New      best_epoch:   192   best iter: 32445,   best_value: 0.81583\n",
      "  193 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.9505   2.0708e-02   2.4097e-04    1.9714 |   0.64658   0.81395   0.81384   0.81387 |    9.6614   2.0708e-02   2.4097e-04     9.6824 |  10.4 |\n",
      "  193 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.6040   2.0849e-02   2.5407e-04    2.6251 |   0.64015   0.81574   0.81592   0.81566 |    9.6160   2.0841e-02   1.4464e-04     9.6370 |  11.2 |\n",
      "  194 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.1003   2.0841e-02   1.4464e-04    2.1212 |   0.63770   0.81482   0.81537   0.81473 |    9.5615   2.0841e-02   1.4464e-04     9.5824 |  10.1 |\n",
      "  194 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.8114   2.1316e-02   2.0563e-04    1.8329 |   0.64210   0.81528   0.81563   0.81519 |    9.6655   2.1314e-02   2.0684e-04     9.6870 |  10.9 |\n",
      "  195 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.7726   2.1314e-02   2.0684e-04    1.7941 |   0.64448   0.81551   0.81644   0.81542 |    9.6547   2.1314e-02   2.0684e-04     9.6762 |  10.2 |\n",
      "  195 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.8927   2.1130e-02   2.1291e-04    1.9141 |   0.63825   0.81456   0.81515   0.81446 |    9.5774   2.1130e-02   2.6878e-04     9.5988 |   9.9 |\n",
      "\n",
      "[e] Policy training epoch:195  it:33075 -  Total Loss: 9.5988     \n",
      "Task: 9.5774   Sparsity: 2.11302e-02    Sharing: 2.68785e-04 \n",
      "\n",
      " epch: 195   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0853    0.9147  0    0.1033    0.8967  0    0.1520    0.8480  0\n",
      "   2    0.4865    0.5135  0    0.2530    0.7470  0    0.3591    0.6409  0\n",
      "   3    0.4933    0.5067  0    0.3213    0.6787  0    0.4599    0.5401  0\n",
      "   4    0.3547    0.6453  0    0.2982    0.7018  0    0.2820    0.7180  0\n",
      "   5    0.1983    0.8017  0    0.1986    0.8014  0    0.1903    0.8097  0\n",
      "   6    0.4181    0.5819  0    0.2341    0.7659  0    0.2625    0.7375  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  196 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.2072   2.1130e-02   2.6878e-04    2.2286 |   0.64379   0.81498   0.81518   0.81489 |    9.6819   2.1130e-02   2.6878e-04     9.7033 |  12.6 |\n",
      "  196 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.0663   2.0995e-02   2.4147e-04    2.0875 |   0.64761   0.81631   0.81630   0.81624 |    9.7058   2.0990e-02   3.1052e-04     9.7271 |  12.3 |\n",
      "Previous best_epoch:   192   best iter: 32445,   best_value: 0.81583\n",
      "New      best_epoch:   196   best iter: 33285,   best_value: 0.81631\n",
      "  197 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.0205   2.0990e-02   3.1052e-04    2.0418 |   0.63915   0.81536   0.81580   0.81528 |    9.5869   2.0990e-02   3.1052e-04     9.6082 |  10.7 |\n",
      "  197 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.8784   2.1385e-02   2.4845e-04    1.9000 |   0.64085   0.81524   0.81621   0.81515 |    9.6287   2.1395e-02   2.5352e-04     9.6503 |  14.1 |\n",
      "  198 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.0856   2.1395e-02   2.5352e-04    2.1072 |   0.64681   0.81543   0.81542   0.81535 |    9.7075   2.1395e-02   2.5352e-04     9.7292 |  11.2 |\n",
      "  198 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.1257   2.1065e-02   2.5163e-04    2.1470 |   0.64378   0.81484   0.81516   0.81476 |    9.6730   2.1047e-02   2.3321e-04     9.6943 |  10.9 |\n",
      "  199 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.9953   2.1047e-02   2.3321e-04    2.0165 |   0.63350   0.81605   0.81611   0.81597 |    9.5122   2.1047e-02   2.3321e-04     9.5334 |   9.9 |\n",
      "  199 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.3889   2.0817e-02   2.8447e-04    1.4100 |   0.64190   0.81579   0.81645   0.81570 |    9.6213   2.0843e-02   2.6206e-04     9.6424 |   9.8 |\n",
      "  200 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.8572   2.0843e-02   2.6206e-04    1.8783 |   0.63834   0.81573   0.81672   0.81562 |    9.5864   2.0843e-02   2.6206e-04     9.6075 |   9.7 |\n",
      "  200 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.8232   1.9990e-02   2.7994e-04    1.8435 |   0.64126   0.81608   0.81658   0.81599 |    9.6176   1.9964e-02   3.4034e-04     9.6379 |  10.5 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[e] Policy training epoch:200  it:34125 -  Total Loss: 9.6379     \n",
      "Task: 9.6176   Sparsity: 1.99643e-02    Sharing: 3.40343e-04 \n",
      "\n",
      " epch: 200   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0740    0.9260  0    0.0896    0.9104  0    0.1366    0.8634  0\n",
      "   2    0.5002    0.4998  1    0.2363    0.7637  0    0.3429    0.6571  0\n",
      "   3    0.4651    0.5349  0    0.2946    0.7054  0    0.4153    0.5847  0\n",
      "   4    0.3362    0.6638  0    0.2907    0.7093  0    0.2768    0.7232  0\n",
      "   5    0.1853    0.8147  0    0.1724    0.8276  0    0.2279    0.7721  0\n",
      "   6    0.3862    0.6138  0    0.2104    0.7896  0    0.2762    0.7238  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  201 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.7275   1.9964e-02   3.4034e-04    1.7478 |   0.64233   0.81562   0.81618   0.81553 |    9.6308   1.9964e-02   3.4034e-04     9.6511 |   9.7 |\n",
      "  201 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.1026   2.0259e-02   1.9126e-04    2.1230 |   0.63638   0.81577   0.81603   0.81569 |    9.5715   2.0278e-02   2.5031e-04     9.5921 |   9.8 |\n",
      "  202 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.1769   2.0278e-02   2.5031e-04    2.1974 |   0.64338   0.81622   0.81663   0.81613 |    9.6748   2.0278e-02   2.5031e-04     9.6953 |  10.1 |\n",
      "  202 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.4612   2.1657e-02   3.7708e-04    2.4833 |   0.65404   0.81520   0.81572   0.81511 |    9.8362   2.1671e-02   3.3839e-04     9.8582 |  10.1 |\n",
      "  203 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    2.1243   2.1671e-02   3.3839e-04    2.1463 |   0.64395   0.81449   0.81542   0.81441 |    9.6692   2.1671e-02   3.3839e-04     9.6913 |  10.2 |\n",
      "  203 |   5.63e-04   5.63e-04   7.50e-03  3.117e+00 |    1.6901   2.2029e-02   3.0807e-04    1.7124 |   0.65000   0.81578   0.81617   0.81570 |    9.7760   2.2023e-02   3.2121e-04     9.7983 |   9.9 |\n",
      " decay gumbel softmax to 3.008004613935899\n",
      "  204 |   5.63e-04   5.63e-04   7.50e-03  3.008e+00 |    1.4781   2.2023e-02   3.2121e-04    1.5004 |   0.64101   0.81462   0.81618   0.81453 |    9.6147   2.2023e-02   3.2121e-04     9.6370 |  11.6 |\n",
      "  204 |   5.63e-04   5.63e-04   7.50e-03  3.008e+00 |    2.2750   2.1485e-02   2.8522e-04    2.2967 |   0.64994   0.81445   0.81526   0.81436 |    9.7345   2.1494e-02   2.9903e-04     9.7563 |  10.6 |\n",
      "  205 |   5.63e-04   5.63e-04   7.50e-03  3.008e+00 |    2.2655   2.1494e-02   2.9903e-04    2.2872 |   0.64504   0.81539   0.81654   0.81530 |    9.6571   2.1494e-02   2.9903e-04     9.6789 |   9.7 |\n",
      "  205 |   5.63e-04   5.63e-04   7.50e-03  3.008e+00 |    1.9296   2.2384e-02   1.8128e-04    1.9521 |   0.64894   0.81607   0.81689   0.81599 |    9.7190   2.2406e-02   2.0323e-04     9.7416 |  11.4 |\n",
      "\n",
      "[e] Policy training epoch:205  it:35175 -  Total Loss: 9.7416     \n",
      "Task: 9.7190   Sparsity: 2.24056e-02    Sharing: 2.03226e-04 \n",
      "\n",
      " epch: 205   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0859    0.9141  0    0.0970    0.9030  0    0.1460    0.8540  0\n",
      "   2    0.5317    0.4683  1    0.2538    0.7462  0    0.4004    0.5996  0\n",
      "   3    0.4727    0.5273  0    0.3046    0.6954  0    0.5066    0.4934  1\n",
      "   4    0.3611    0.6389  0    0.3348    0.6652  0    0.3034    0.6966  0\n",
      "   5    0.2023    0.7977  0    0.1729    0.8271  0    0.2635    0.7365  0\n",
      "   6    0.4210    0.5790  0    0.2288    0.7712  0    0.2982    0.7018  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  206 |   5.63e-04   5.63e-04   7.50e-03  3.008e+00 |    1.7922   2.2406e-02   2.0323e-04    1.8149 |   0.65064   0.81589   0.81647   0.81580 |    9.7654   2.2406e-02   2.0323e-04     9.7880 |  10.2 |\n",
      "  206 |   5.63e-04   5.63e-04   7.50e-03  3.008e+00 |    1.5044   2.2729e-02   1.6806e-04    1.5273 |   0.64252   0.81651   0.81723   0.81641 |    9.6505   2.2759e-02   1.5401e-04     9.6734 |  11.0 |\n",
      "Epoch   131: reducing learning rate of group 0 to 5.6250e-03.\n",
      "Previous best_epoch:   196   best iter: 33285,   best_value: 0.81631\n",
      "New      best_epoch:   206   best iter: 35385,   best_value: 0.81651\n",
      "  207 |   5.63e-04   5.63e-04   5.62e-03  3.008e+00 |    2.0438   2.2759e-02   1.5401e-04    2.0667 |   0.64582   0.81776   0.81823   0.81767 |    9.6896   2.2759e-02   1.5401e-04     9.7125 |  10.0 |\n",
      "Previous best_epoch:   206   best iter: 35385,   best_value: 0.81651\n",
      "New      best_epoch:   207   best iter: 35490,   best_value: 0.81776\n",
      "  207 |   5.63e-04   5.63e-04   5.62e-03  3.008e+00 |    2.0689   2.3509e-02   1.7696e-04    2.0926 |   0.65419   0.81641   0.81718   0.81630 |    9.8001   2.3498e-02   1.7206e-04     9.8237 |  10.2 |\n",
      "  208 |   5.63e-04   5.63e-04   5.62e-03  3.008e+00 |    1.7594   2.3498e-02   1.7206e-04    1.7830 |   0.64723   0.81616   0.81707   0.81607 |    9.7069   2.3498e-02   1.7206e-04     9.7305 |  12.4 |\n",
      "Epoch   208: reducing learning rate of group 0 to 4.2188e-04.\n",
      "Epoch   208: reducing learning rate of group 1 to 4.2188e-04.\n",
      "  208 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.6603   2.2678e-02   9.7019e-05    1.6831 |   0.65282   0.81453   0.81658   0.81442 |    9.8072   2.2674e-02   1.4138e-04     9.8300 |   9.8 |\n",
      "  209 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.8042   2.2674e-02   1.4138e-04    1.8270 |   0.65324   0.81530   0.81584   0.81522 |    9.8578   2.2674e-02   1.4138e-04     9.8806 |  10.8 |\n",
      "  209 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.0971   2.1956e-02   2.1054e-04    2.1192 |   0.65577   0.81571   0.81668   0.81563 |    9.8277   2.1962e-02   1.4996e-04     9.8498 |  11.2 |\n",
      "  210 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.3886   2.1962e-02   1.4996e-04    2.4107 |   0.65167   0.81609   0.81663   0.81601 |    9.7709   2.1962e-02   1.4996e-04     9.7931 |   9.6 |\n",
      "  210 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.5416   2.2564e-02   1.5678e-04    1.5643 |   0.65013   0.81437   0.81571   0.81428 |    9.7695   2.2578e-02   1.9965e-04     9.7923 |   9.7 |\n",
      "\n",
      "[e] Policy training epoch:210  it:36225 -  Total Loss: 9.7923     \n",
      "Task: 9.7695   Sparsity: 2.25778e-02    Sharing: 1.99647e-04 \n",
      "\n",
      " epch: 210   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0775    0.9225  0    0.0938    0.9062  0    0.1462    0.8538  0\n",
      "   2    0.5009    0.4991  1    0.2689    0.7311  0    0.3772    0.6228  0\n",
      "   3    0.5283    0.4717  1    0.3187    0.6813  0    0.4665    0.5335  0\n",
      "   4    0.4012    0.5988  0    0.3638    0.6362  0    0.3184    0.6816  0\n",
      "   5    0.1961    0.8039  0    0.1921    0.8079  0    0.2457    0.7543  0\n",
      "   6    0.4087    0.5913  0    0.2188    0.7812  0    0.3003    0.6997  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  211 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.0063   2.2578e-02   1.9965e-04    2.0291 |   0.65138   0.81621   0.81630   0.81613 |    9.7843   2.2578e-02   1.9965e-04     9.8071 |  12.2 |\n",
      "  211 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.8769   2.2267e-02   1.7984e-04    1.8994 |   0.65406   0.81706   0.81770   0.81698 |    9.8107   2.2276e-02   1.4955e-04     9.8331 |   9.7 |\n",
      "  212 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.8000   2.2276e-02   1.4955e-04    1.8224 |   0.65259   0.81640   0.81714   0.81631 |    9.8136   2.2276e-02   1.4955e-04     9.8360 |  11.0 |\n",
      "  212 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.0417   2.1623e-02   1.7312e-04    2.0635 |   0.65549   0.81594   0.81652   0.81586 |    9.8179   2.1596e-02   2.0855e-04     9.8397 |  10.4 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  213 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.0871   2.1596e-02   2.0855e-04    2.1089 |   0.65646   0.81526   0.81613   0.81517 |    9.8389   2.1596e-02   2.0855e-04     9.8607 |  10.2 |\n",
      "  213 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.7099   2.1633e-02   2.0884e-04    1.7318 |   0.65510   0.81596   0.81668   0.81587 |    9.8677   2.1638e-02   2.0339e-04     9.8896 |  11.2 |\n",
      "  214 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.1079   2.1638e-02   2.0339e-04    2.1297 |   0.65256   0.81659   0.81653   0.81651 |    9.7879   2.1638e-02   2.0339e-04     9.8097 |   9.9 |\n",
      "  214 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.7770   2.2495e-02   1.4329e-04    1.7996 |   0.64913   0.81695   0.81723   0.81686 |    9.7363   2.2500e-02   2.4983e-04     9.7591 |  10.0 |\n",
      "  215 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.1871   2.2500e-02   2.4983e-04    2.2099 |   0.65489   0.81518   0.81610   0.81509 |    9.8103   2.2500e-02   2.4983e-04     9.8331 |  10.9 |\n",
      "  215 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.9323   2.1901e-02   1.8556e-04    1.9544 |   0.65552   0.81680   0.81759   0.81669 |    9.8480   2.1899e-02   2.0305e-04     9.8701 |  10.0 |\n",
      "\n",
      "[e] Policy training epoch:215  it:37275 -  Total Loss: 9.8701     \n",
      "Task: 9.8480   Sparsity: 2.18991e-02    Sharing: 2.03046e-04 \n",
      "\n",
      " epch: 215   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0852    0.9148  0    0.0984    0.9016  0    0.1338    0.8662  0\n",
      "   2    0.4861    0.5139  0    0.2624    0.7376  0    0.3558    0.6442  0\n",
      "   3    0.5103    0.4897  1    0.3250    0.6750  0    0.4877    0.5123  0\n",
      "   4    0.3891    0.6109  0    0.3206    0.6794  0    0.3271    0.6729  0\n",
      "   5    0.1971    0.8029  0    0.1761    0.8239  0    0.2189    0.7811  0\n",
      "   6    0.4228    0.5772  0    0.2254    0.7746  0    0.2642    0.7358  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  216 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.9206   2.1899e-02   2.0305e-04    1.9427 |   0.65314   0.81573   0.81628   0.81564 |    9.8153   2.1899e-02   2.0305e-04     9.8374 |  11.0 |\n",
      "  216 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.6786   2.2067e-02   2.0765e-04    1.7009 |   0.65216   0.81649   0.81742   0.81640 |    9.7809   2.2064e-02   2.3455e-04     9.8032 |  11.3 |\n",
      "  217 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.9309   2.2064e-02   2.3455e-04    1.9532 |   0.66003   0.81524   0.81669   0.81514 |    9.8857   2.2064e-02   2.3455e-04     9.9080 |  10.2 |\n",
      "  217 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.7346   2.1934e-02   1.8634e-04    1.7568 |   0.65396   0.81623   0.81730   0.81614 |    9.8292   2.1929e-02   2.1663e-04     9.8514 |  11.2 |\n",
      "  218 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.9475   2.1929e-02   2.1663e-04    1.9697 |   0.65604   0.81659   0.81693   0.81650 |    9.8421   2.1929e-02   2.1663e-04     9.8642 |  10.5 |\n",
      "  218 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    2.0317   2.2140e-02   1.3838e-04    2.0540 |   0.65755   0.81770   0.81783   0.81762 |    9.8624   2.2135e-02   1.3884e-04     9.8847 |   9.8 |\n",
      "  219 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.5836   2.2135e-02   1.3884e-04    1.6059 |   0.65460   0.81707   0.81781   0.81698 |    9.8239   2.2135e-02   1.3884e-04     9.8462 |  11.0 |\n",
      "  219 |   4.22e-04   4.22e-04   5.62e-03  3.008e+00 |    1.7088   2.2048e-02   1.0289e-04    1.7310 |   0.65788   0.81760   0.81791   0.81752 |    9.8689   2.2063e-02   1.2219e-04     9.8911 |  10.4 |\n",
      " decay gumbel softmax to 2.9027244524481426\n",
      "  220 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.4611   2.2063e-02   1.2219e-04    1.4833 |   0.65891   0.81595   0.81649   0.81587 |    9.8563   2.2063e-02   1.2219e-04     9.8785 |  11.0 |\n",
      "  220 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    2.0843   2.2228e-02   1.9735e-04    2.1068 |   0.66259   0.81682   0.81802   0.81674 |    9.9475   2.2223e-02   1.7449e-04     9.9699 |  10.4 |\n",
      "\n",
      "[e] Policy training epoch:220  it:38325 -  Total Loss: 9.9699     \n",
      "Task: 9.9475   Sparsity: 2.22229e-02    Sharing: 1.74493e-04 \n",
      "\n",
      " epch: 220   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0937    0.9063  0    0.0955    0.9045  0    0.1469    0.8531  0\n",
      "   2    0.4974    0.5026  0    0.2638    0.7362  0    0.3553    0.6447  0\n",
      "   3    0.5333    0.4667  1    0.3259    0.6741  0    0.4769    0.5231  0\n",
      "   4    0.3954    0.6046  0    0.3450    0.6550  0    0.3253    0.6747  0\n",
      "   5    0.1948    0.8052  0    0.1905    0.8095  0    0.2181    0.7819  0\n",
      "   6    0.3995    0.6005  0    0.2289    0.7711  0    0.2648    0.7352  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  221 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.3913   2.2223e-02   1.7449e-04    1.4137 |   0.66046   0.81871   0.81873   0.81863 |    9.9154   2.2223e-02   1.7449e-04     9.9377 |  11.3 |\n",
      "Previous best_epoch:   207   best iter: 35490,   best_value: 0.81776\n",
      "New      best_epoch:   221   best iter: 38430,   best_value: 0.81871\n",
      "  221 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.7044   2.2484e-02   2.1334e-04    1.7271 |   0.65791   0.81700   0.81766   0.81691 |    9.8686   2.2481e-02   2.0046e-04     9.8912 |  10.1 |\n",
      "  222 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    2.2373   2.2481e-02   2.0046e-04    2.2600 |   0.65824   0.81644   0.81674   0.81636 |    9.8736   2.2481e-02   2.0046e-04     9.8963 |  12.0 |\n",
      "  222 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.6573   2.2287e-02   1.8315e-04    1.6797 |   0.65909   0.81646   0.81746   0.81636 |    9.9109   2.2286e-02   1.8115e-04     9.9334 |  10.3 |\n",
      "  223 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    2.0050   2.2286e-02   1.8115e-04    2.0275 |   0.66518   0.81620   0.81690   0.81609 |    9.9740   2.2286e-02   1.8115e-04     9.9965 |  10.1 |\n",
      "  223 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8646   2.3108e-02   1.9556e-04    1.8879 |   0.66390   0.81763   0.81754   0.81755 |    9.9635   2.3098e-02   1.9920e-04     9.9868 |  10.3 |\n",
      "  224 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.6995   2.3098e-02   1.9920e-04    1.7228 |   0.66261   0.81711   0.81766   0.81701 |    9.9471   2.3098e-02   1.9920e-04     9.9704 |  10.7 |\n",
      "  224 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.9974   2.2627e-02   1.2926e-04    2.0201 |   0.65445   0.81730   0.81752   0.81721 |    9.8144   2.2633e-02   1.3521e-04     9.8371 |  10.3 |\n",
      "  225 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.6392   2.2633e-02   1.3521e-04    1.6620 |   0.66206   0.81664   0.81706   0.81656 |    9.9657   2.2633e-02   1.3521e-04     9.9884 |  10.0 |\n",
      "  225 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8891   2.2574e-02   2.2280e-04    1.9119 |   0.66426   0.81605   0.81681   0.81595 |    9.9896   2.2572e-02   1.7242e-04    10.0124 |  11.3 |\n",
      "\n",
      "[e] Policy training epoch:225  it:39375 -  Total Loss: 10.0124     \n",
      "Task: 9.9896   Sparsity: 2.25722e-02    Sharing: 1.72416e-04 \n",
      "\n",
      " epch: 225   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0858    0.9142  0    0.1050    0.8950  0    0.1576    0.8424  0\n",
      "   2    0.5005    0.4995  1    0.2871    0.7129  0    0.3546    0.6454  0\n",
      "   3    0.5141    0.4859  1    0.3367    0.6633  0    0.4837    0.5163  0\n",
      "   4    0.3880    0.6120  0    0.3421    0.6579  0    0.3001    0.6999  0\n",
      "   5    0.2114    0.7886  0    0.2197    0.7803  0    0.2161    0.7839  0\n",
      "   6    0.4494    0.5506  0    0.2264    0.7736  0    0.2492    0.7508  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  226 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.9122   2.2572e-02   1.7242e-04    1.9349 |   0.66374   0.81726   0.81770   0.81716 |    9.9510   2.2572e-02   1.7242e-04     9.9738 |  10.0 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  226 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8139   2.2370e-02   2.4604e-04    1.8365 |   0.65995   0.81616   0.81575   0.81608 |    9.9191   2.2389e-02   1.9538e-04     9.9417 |   9.6 |\n",
      "  227 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    2.0844   2.2389e-02   1.9538e-04    2.1070 |   0.66328   0.81695   0.81779   0.81686 |    9.9447   2.2389e-02   1.9538e-04     9.9673 |   9.6 |\n",
      "  227 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.4519   2.2634e-02   1.2909e-04    1.4747 |   0.66769   0.81696   0.81715   0.81688 |    9.9867   2.2654e-02   2.0286e-04    10.0095 |  13.4 |\n",
      "  228 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.7738   2.2654e-02   2.0286e-04    1.7966 |   0.65937   0.81624   0.81662   0.81615 |    9.8768   2.2654e-02   2.0286e-04     9.8996 |  10.6 |\n",
      "  228 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8928   2.2763e-02   1.1791e-04    1.9157 |   0.66643   0.81646   0.81704   0.81637 |    9.9643   2.2748e-02   1.2073e-04     9.9872 |  10.3 |\n",
      "  229 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.7214   2.2748e-02   1.2073e-04    1.7443 |   0.65593   0.81663   0.81740   0.81654 |    9.8288   2.2748e-02   1.2073e-04     9.8516 |  10.8 |\n",
      "  229 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.5665   2.2418e-02   1.4207e-04    1.5891 |   0.65788   0.81769   0.81806   0.81760 |    9.8546   2.2418e-02   2.0403e-04     9.8772 |  11.1 |\n",
      "  230 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    2.0252   2.2418e-02   2.0403e-04    2.0478 |   0.66161   0.81680   0.81751   0.81672 |    9.9509   2.2418e-02   2.0403e-04     9.9735 |  10.6 |\n",
      "  230 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.6618   2.3122e-02   1.8534e-04    1.6851 |   0.66780   0.81730   0.81782   0.81721 |    9.9891   2.3140e-02   2.1075e-04    10.0125 |  13.2 |\n",
      "\n",
      "[e] Policy training epoch:230  it:40425 -  Total Loss: 10.0125     \n",
      "Task: 9.9891   Sparsity: 2.31398e-02    Sharing: 2.10755e-04 \n",
      "\n",
      " epch: 230   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0913    0.9087  0    0.1013    0.8987  0    0.1545    0.8455  0\n",
      "   2    0.4783    0.5217  0    0.2545    0.7455  0    0.3800    0.6200  0\n",
      "   3    0.5318    0.4682  1    0.3507    0.6493  0    0.5163    0.4837  1\n",
      "   4    0.4159    0.5841  0    0.3812    0.6188  0    0.3463    0.6537  0\n",
      "   5    0.1877    0.8123  0    0.1700    0.8300  0    0.2019    0.7981  0\n",
      "   6    0.4326    0.5674  0    0.2368    0.7632  0    0.2828    0.7172  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  231 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.2097   2.3140e-02   2.1075e-04    1.2331 |   0.66001   0.81658   0.81734   0.81649 |    9.9216   2.3140e-02   2.1075e-04     9.9449 |  10.6 |\n",
      "  231 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.6111   2.2822e-02   2.2836e-04    1.6342 |   0.66266   0.81637   0.81732   0.81628 |    9.9318   2.2829e-02   2.3510e-04     9.9549 |  10.7 |\n",
      "  232 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.7418   2.2829e-02   2.3510e-04    1.7649 |   0.66196   0.81707   0.81734   0.81699 |    9.9291   2.2829e-02   2.3510e-04     9.9521 |  10.6 |\n",
      "  232 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8208   2.2573e-02   1.6033e-04    1.8435 |   0.66858   0.81698   0.81754   0.81689 |   10.0570   2.2571e-02   1.3764e-04    10.0797 |  11.6 |\n",
      "  233 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8012   2.2571e-02   1.3764e-04    1.8239 |   0.66021   0.81750   0.81789   0.81742 |    9.9308   2.2571e-02   1.3764e-04     9.9535 |  13.4 |\n",
      "  233 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8440   2.3076e-02   3.1753e-04    1.8674 |   0.66140   0.81792   0.81851   0.81783 |    9.9101   2.3078e-02   3.3481e-04     9.9335 |  10.9 |\n",
      "  234 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    2.2648   2.3078e-02   3.3481e-04    2.2882 |   0.66112   0.81751   0.81775   0.81743 |    9.9127   2.3078e-02   3.3481e-04     9.9361 |  11.0 |\n",
      "  234 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.3577   2.3397e-02   1.3193e-04    1.3812 |   0.66894   0.81754   0.81849   0.81746 |   10.0396   2.3408e-02   2.2930e-04    10.0632 |  10.5 |\n",
      "  235 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.5360   2.3408e-02   2.2930e-04    1.5597 |   0.65856   0.81753   0.81802   0.81744 |    9.8708   2.3408e-02   2.2930e-04     9.8945 |  10.9 |\n",
      "  235 |   4.22e-04   4.22e-04   5.62e-03  2.903e+00 |    1.8324   2.3665e-02   1.9779e-04    1.8562 |   0.67047   0.81734   0.81799   0.81725 |   10.0244   2.3685e-02   1.9018e-04    10.0483 |  11.8 |\n",
      " decay gumbel softmax to 2.8011290966124576\n",
      "\n",
      "[e] Policy training epoch:235  it:41475 -  Total Loss: 10.0483     \n",
      "Task: 10.0244   Sparsity: 2.36846e-02    Sharing: 1.90182e-04 \n",
      "\n",
      " epch: 235   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0843    0.9157  0    0.1070    0.8930  0    0.1607    0.8393  0\n",
      "   2    0.5574    0.4426  1    0.2963    0.7037  0    0.3985    0.6015  0\n",
      "   3    0.5306    0.4694  1    0.3511    0.6489  0    0.5124    0.4876  1\n",
      "   4    0.3840    0.6160  0    0.3611    0.6389  0    0.3223    0.6777  0\n",
      "   5    0.1915    0.8085  0    0.1823    0.8177  0    0.2044    0.7956  0\n",
      "   6    0.4475    0.5525  0    0.2618    0.7382  0    0.2489    0.7511  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  236 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.5207   2.3685e-02   1.9018e-04    1.5445 |   0.66571   0.81632   0.81732   0.81623 |    9.9890   2.3685e-02   1.9018e-04    10.0129 |  11.0 |\n",
      "  236 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.6283   2.4602e-02   1.3744e-04    1.6530 |   0.66864   0.81711   0.81775   0.81702 |   10.0210   2.4620e-02   1.5109e-04    10.0458 |  10.8 |\n",
      "  237 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.4558   2.4620e-02   1.5109e-04    1.4806 |   0.66949   0.81795   0.81861   0.81786 |   10.0228   2.4620e-02   1.5109e-04    10.0475 |  10.1 |\n",
      "  237 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.5599   2.4700e-02   1.5833e-04    1.5847 |   0.67048   0.81727   0.81771   0.81718 |   10.0598   2.4708e-02   2.0736e-04    10.0848 |  10.6 |\n",
      "  238 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.7175   2.4708e-02   2.0736e-04    1.7424 |   0.67805   0.81643   0.81735   0.81632 |   10.1829   2.4708e-02   2.0736e-04    10.2078 |  10.3 |\n",
      "  238 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.6482   2.4905e-02   2.0836e-04    1.6733 |   0.66627   0.81877   0.81901   0.81868 |   10.0005   2.4906e-02   1.2588e-04    10.0256 |  10.4 |\n",
      "Previous best_epoch:   221   best iter: 38430,   best_value: 0.81871\n",
      "New      best_epoch:   238   best iter: 42105,   best_value: 0.81877\n",
      "  239 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.7674   2.4906e-02   1.2588e-04    1.7924 |   0.67058   0.81716   0.81774   0.81707 |   10.0634   2.4906e-02   1.2588e-04    10.0885 |  10.8 |\n",
      "  239 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.7409   2.4683e-02   1.9570e-04    1.7658 |   0.67051   0.81880   0.81930   0.81871 |   10.0659   2.4688e-02   2.1700e-04    10.0908 |  12.4 |\n",
      "Previous best_epoch:   238   best iter: 42105,   best_value: 0.81877\n",
      "New      best_epoch:   239   best iter: 42315,   best_value: 0.81880\n",
      "  240 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.4980   2.4688e-02   2.1700e-04    1.5229 |   0.66889   0.81778   0.81785   0.81769 |   10.0404   2.4688e-02   2.1700e-04    10.0653 |  11.9 |\n",
      "  240 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.6665   2.4709e-02   1.4514e-04    1.6913 |   0.66478   0.81705   0.81774   0.81696 |    9.9800   2.4702e-02   1.0726e-04    10.0048 |  10.8 |\n",
      "\n",
      "[e] Policy training epoch:240  it:42525 -  Total Loss: 10.0048     \n",
      "Task: 9.9800   Sparsity: 2.47021e-02    Sharing: 1.07255e-04 \n",
      "\n",
      " epch: 240   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0817    0.9183  0    0.0960    0.9040  0    0.1423    0.8577  0\n",
      "   2    0.5713    0.4287  1    0.3391    0.6609  0    0.4123    0.5877  0\n",
      "   3    0.4942    0.5058  0    0.3591    0.6409  0    0.5064    0.4936  1\n",
      "   4    0.3764    0.6236  0    0.3908    0.6092  0    0.3180    0.6820  0\n",
      "   5    0.2219    0.7781  0    0.2254    0.7746  0    0.2442    0.7558  0\n",
      "   6    0.4364    0.5636  0    0.3056    0.6944  0    0.3098    0.6902  0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  241 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.9927   2.4702e-02   1.0725e-04    2.0175 |   0.66951   0.81774   0.81854   0.81763 |   10.0822   2.4702e-02   1.0726e-04    10.1071 |  10.1 |\n",
      "  241 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8402   2.4578e-02   1.4312e-04    1.8649 |   0.67420   0.81733   0.81804   0.81724 |   10.1408   2.4555e-02   1.7894e-04    10.1655 |  10.3 |\n",
      "  242 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    2.0569   2.4555e-02   1.7894e-04    2.0816 |   0.67515   0.81789   0.81848   0.81779 |   10.1301   2.4555e-02   1.7894e-04    10.1548 |  11.5 |\n",
      "  242 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8128   2.5369e-02   1.8524e-04    1.8383 |   0.67585   0.81854   0.81801   0.81846 |   10.1242   2.5382e-02   2.5596e-04    10.1498 |  10.9 |\n",
      "  243 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.5606   2.5382e-02   2.5596e-04    1.5862 |   0.66848   0.81965   0.81950   0.81957 |   10.0170   2.5382e-02   2.5596e-04    10.0427 |   9.9 |\n",
      "Previous best_epoch:   239   best iter: 42315,   best_value: 0.81880\n",
      "New      best_epoch:   243   best iter: 43050,   best_value: 0.81965\n",
      "  243 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.4480   2.5625e-02   1.6794e-04    1.4738 |   0.67447   0.81886   0.81908   0.81878 |   10.1028   2.5628e-02   1.8249e-04    10.1286 |  10.7 |\n",
      "  244 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8506   2.5628e-02   1.8249e-04    1.8764 |   0.66500   0.81983   0.81971   0.81975 |    9.9632   2.5628e-02   1.8249e-04     9.9890 |  10.2 |\n",
      "Previous best_epoch:   243   best iter: 43050,   best_value: 0.81965\n",
      "New      best_epoch:   244   best iter: 43260,   best_value: 0.81983\n",
      "  244 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8647   2.4978e-02   2.0370e-04    1.8898 |   0.67022   0.81803   0.81883   0.81793 |   10.0371   2.4961e-02   2.3133e-04    10.0623 |   9.8 |\n",
      "  245 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8914   2.4961e-02   2.3133e-04    1.9166 |   0.67179   0.81825   0.81882   0.81817 |   10.0857   2.4961e-02   2.3133e-04    10.1109 |  10.8 |\n",
      "  245 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8546   2.5067e-02   1.8814e-04    1.8798 |   0.66983   0.81652   0.81750   0.81642 |   10.0299   2.5056e-02   2.3744e-04    10.0552 |  10.4 |\n",
      "\n",
      "[e] Policy training epoch:245  it:43575 -  Total Loss: 10.0552     \n",
      "Task: 10.0299   Sparsity: 2.50562e-02    Sharing: 2.37443e-04 \n",
      "\n",
      " epch: 245   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0751    0.9249  0    0.0903    0.9097  0    0.1475    0.8525  0\n",
      "   2    0.5873    0.4127  1    0.3170    0.6830  0    0.4057    0.5943  0\n",
      "   3    0.5426    0.4574  1    0.3606    0.6394  0    0.5339    0.4661  1\n",
      "   4    0.4033    0.5967  0    0.3739    0.6261  0    0.3279    0.6721  0\n",
      "   5    0.2151    0.7849  0    0.2116    0.7884  0    0.2480    0.7520  0\n",
      "   6    0.4373    0.5627  0    0.2641    0.7359  0    0.3138    0.6862  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  246 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.9635   2.5056e-02   2.3744e-04    1.9888 |   0.67789   0.81891   0.81862   0.81883 |   10.1676   2.5056e-02   2.3744e-04    10.1929 |   9.7 |\n",
      "  246 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.3944   2.3747e-02   2.0774e-04    1.4184 |   0.66882   0.81904   0.81896   0.81895 |   10.0436   2.3752e-02   2.2529e-04    10.0676 |  10.7 |\n",
      "  247 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8653   2.3752e-02   2.2529e-04    1.8893 |   0.67245   0.81710   0.81809   0.81701 |   10.1015   2.3752e-02   2.2529e-04    10.1254 |  12.6 |\n",
      "  247 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8154   2.3969e-02   1.8900e-04    1.8395 |   0.67102   0.81897   0.81904   0.81888 |   10.0748   2.3969e-02   2.1986e-04    10.0990 |  10.4 |\n",
      "  248 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.6029   2.3969e-02   2.1986e-04    1.6270 |   0.66439   0.81815   0.81837   0.81807 |    9.9582   2.3969e-02   2.1986e-04     9.9823 |  10.6 |\n",
      "  248 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.9807   2.4198e-02   1.9242e-04    2.0051 |   0.67113   0.81768   0.81836   0.81758 |   10.0730   2.4214e-02   1.4386e-04    10.0973 |  10.2 |\n",
      "  249 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8264   2.4214e-02   1.4386e-04    1.8508 |   0.67890   0.81841   0.81817   0.81833 |   10.2105   2.4214e-02   1.4386e-04    10.2348 |  10.6 |\n",
      "  249 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.5876   2.3617e-02   1.5585e-04    1.6114 |   0.67446   0.81761   0.81785   0.81752 |   10.1133   2.3606e-02   1.5704e-04    10.1371 |   9.9 |\n",
      "  250 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.4775   2.3606e-02   1.5704e-04    1.5012 |   0.68063   0.81854   0.81901   0.81847 |   10.2273   2.3606e-02   1.5704e-04    10.2511 |   9.4 |\n",
      "  250 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.8252   2.3534e-02   1.5383e-04    1.8489 |   0.67854   0.81809   0.81850   0.81800 |   10.1584   2.3526e-02   1.9918e-04    10.1821 |   9.8 |\n",
      "\n",
      "[e] Policy training epoch:250  it:44625 -  Total Loss: 10.1821     \n",
      "Task: 10.1584   Sparsity: 2.35261e-02    Sharing: 1.99184e-04 \n",
      "\n",
      " epch: 250   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0843    0.9157  0    0.1107    0.8893  0    0.1444    0.8556  0\n",
      "   2    0.5315    0.4685  1    0.3132    0.6868  0    0.4052    0.5948  0\n",
      "   3    0.5214    0.4786  1    0.3431    0.6569  0    0.5137    0.4863  1\n",
      "   4    0.3717    0.6283  0    0.3484    0.6516  0    0.3132    0.6868  0\n",
      "   5    0.2141    0.7859  0    0.2061    0.7940  0    0.2196    0.7804  0\n",
      "   6    0.4428    0.5572  0    0.2426    0.7574  0    0.2696    0.7304  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  251 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    2.0605   2.3526e-02   1.9918e-04    2.0843 |   0.68210   0.81835   0.81884   0.81825 |   10.1895   2.3526e-02   1.9918e-04    10.2132 |  10.4 |\n",
      "  251 |   4.22e-04   4.22e-04   5.62e-03  2.801e+00 |    1.3512   2.3582e-02   2.2063e-04    1.3750 |   0.68197   0.81812   0.81886   0.81804 |   10.2128   2.3580e-02   1.9247e-04    10.2366 |  14.4 |\n",
      " decay gumbel softmax to 2.7030895782310216\n",
      "  252 |   4.22e-04   4.22e-04   5.62e-03  2.703e+00 |    1.6474   2.3580e-02   1.9247e-04    1.6712 |   0.67419   0.81914   0.81892   0.81906 |   10.1017   2.3580e-02   1.9247e-04    10.1255 |  13.7 |\n",
      "  252 |   4.22e-04   4.22e-04   5.62e-03  2.703e+00 |    2.1293   2.3280e-02   2.3190e-04    2.1529 |   0.66986   0.81938   0.81930   0.81930 |   10.0473   2.3291e-02   2.6729e-04    10.0708 |  11.3 |\n",
      "  253 |   4.22e-04   4.22e-04   5.62e-03  2.703e+00 |    1.7238   2.3291e-02   2.6729e-04    1.7473 |   0.67683   0.81860   0.81842   0.81852 |   10.1440   2.3291e-02   2.6729e-04    10.1676 |  10.0 |\n",
      "  253 |   4.22e-04   4.22e-04   5.62e-03  2.703e+00 |    1.4715   2.3190e-02   1.8927e-04    1.4949 |   0.67470   0.81756   0.81790   0.81747 |   10.0997   2.3200e-02   1.6130e-04    10.1231 |  10.0 |\n",
      "  254 |   4.22e-04   4.22e-04   5.62e-03  2.703e+00 |    1.7816   2.3200e-02   1.6130e-04    1.8049 |   0.67473   0.81834   0.81842   0.81825 |   10.1231   2.3200e-02   1.6130e-04    10.1464 |  10.7 |\n",
      "Epoch   254: reducing learning rate of group 0 to 3.1641e-04.\n",
      "Epoch   254: reducing learning rate of group 1 to 3.1641e-04.\n",
      "  254 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    2.2440   2.3573e-02   2.3473e-04    2.2679 |   0.67691   0.81807   0.81878   0.81799 |   10.1799   2.3576e-02   1.3778e-04    10.2036 |  11.5 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  255 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.5965   2.3576e-02   1.3778e-04    1.6202 |   0.67252   0.81833   0.81910   0.81822 |   10.1007   2.3576e-02   1.3778e-04    10.1244 |  10.3 |\n",
      "  255 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.4490   2.3051e-02   1.6399e-04    1.4722 |   0.67322   0.81865   0.81917   0.81856 |   10.1061   2.3049e-02   1.5451e-04    10.1293 |  10.7 |\n",
      "\n",
      "[e] Policy training epoch:255  it:45675 -  Total Loss: 10.1293     \n",
      "Task: 10.1061   Sparsity: 2.30492e-02    Sharing: 1.54512e-04 \n",
      "\n",
      " epch: 255   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0759    0.9241  0    0.1035    0.8965  0    0.1496    0.8504  0\n",
      "   2    0.5153    0.4847  1    0.3102    0.6898  0    0.3539    0.6461  0\n",
      "   3    0.5037    0.4963  1    0.3233    0.6767  0    0.4983    0.5017  0\n",
      "   4    0.3876    0.6124  0    0.3742    0.6258  0    0.3009    0.6991  0\n",
      "   5    0.2106    0.7894  0    0.2099    0.7901  0    0.2207    0.7793  0\n",
      "   6    0.4352    0.5648  0    0.2875    0.7125  0    0.2609    0.7391  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  256 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.7845   2.3049e-02   1.5451e-04    1.8077 |   0.66598   0.81889   0.81893   0.81881 |   10.0122   2.3049e-02   1.5451e-04    10.0354 |  10.0 |\n",
      "  256 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.5840   2.3727e-02   1.3832e-04    1.6079 |   0.66995   0.81895   0.81932   0.81887 |   10.0768   2.3750e-02   1.1390e-04    10.1006 |  11.7 |\n",
      "  257 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.4428   2.3750e-02   1.1390e-04    1.4666 |   0.67704   0.81794   0.81874   0.81786 |   10.1906   2.3750e-02   1.1390e-04    10.2145 |  11.6 |\n",
      "  257 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.4072   2.4337e-02   2.0438e-04    1.4317 |   0.67177   0.81757   0.81834   0.81747 |   10.0987   2.4362e-02   1.7621e-04    10.1232 |  12.1 |\n",
      "  258 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.4155   2.4362e-02   1.7621e-04    1.4400 |   0.68017   0.81826   0.81904   0.81818 |   10.2086   2.4362e-02   1.7621e-04    10.2331 |  10.9 |\n",
      "  258 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.9782   2.4498e-02   2.6843e-04    2.0030 |   0.68268   0.81809   0.81869   0.81800 |   10.2516   2.4484e-02   1.7918e-04    10.2763 |  12.6 |\n",
      "  259 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.6601   2.4484e-02   1.7918e-04    1.6847 |   0.67913   0.81886   0.81905   0.81878 |   10.1781   2.4484e-02   1.7918e-04    10.2027 |  11.2 |\n",
      "  259 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.7144   2.4118e-02   1.6687e-04    1.7387 |   0.67478   0.81803   0.81894   0.81794 |   10.1118   2.4132e-02   2.4831e-04    10.1362 |  11.7 |\n",
      "  260 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.8325   2.4132e-02   2.4831e-04    1.8568 |   0.67170   0.81805   0.81883   0.81797 |   10.0744   2.4132e-02   2.4831e-04    10.0988 |  10.5 |\n",
      "  260 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.3913   2.4596e-02   2.2477e-04    1.4161 |   0.67702   0.81827   0.81890   0.81818 |   10.1409   2.4601e-02   1.8314e-04    10.1657 |  10.6 |\n",
      "\n",
      "[e] Policy training epoch:260  it:46725 -  Total Loss: 10.1657     \n",
      "Task: 10.1409   Sparsity: 2.46012e-02    Sharing: 1.83142e-04 \n",
      "\n",
      " epch: 260   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0827    0.9173  0    0.1051    0.8949  0    0.1474    0.8526  0\n",
      "   2    0.5173    0.4827  1    0.3469    0.6531  0    0.3713    0.6287  0\n",
      "   3    0.5574    0.4426  1    0.3805    0.6195  0    0.4886    0.5114  0\n",
      "   4    0.4214    0.5786  0    0.3972    0.6028  0    0.3453    0.6547  0\n",
      "   5    0.2140    0.7860  0    0.2113    0.7887  0    0.2036    0.7964  0\n",
      "   6    0.4548    0.5452  0    0.2821    0.7179  0    0.2736    0.7264  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  261 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    2.3557   2.4601e-02   1.8314e-04    2.3805 |   0.67334   0.81964   0.81952   0.81956 |   10.0914   2.4601e-02   1.8314e-04    10.1162 |   9.9 |\n",
      "  261 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.0161   2.5136e-02   2.2624e-04    1.0415 |   0.67796   0.81824   0.81883   0.81814 |   10.1610   2.5124e-02   2.1963e-04    10.1864 |  11.3 |\n",
      "  262 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.4024   2.5124e-02   2.1963e-04    1.4277 |   0.67690   0.81935   0.81890   0.81927 |   10.1638   2.5124e-02   2.1963e-04    10.1892 |  10.4 |\n",
      "  262 |   3.16e-04   3.16e-04   5.62e-03  2.703e+00 |    1.8980   2.5097e-02   1.9813e-04    1.9233 |   0.68113   0.81795   0.81876   0.81785 |   10.2279   2.5077e-02   1.9672e-04    10.2531 |  11.2 |\n",
      "Epoch   187: reducing learning rate of group 0 to 4.2188e-03.\n",
      "  263 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.5509   2.5077e-02   1.9672e-04    1.5762 |   0.67985   0.81865   0.81897   0.81857 |   10.1789   2.5077e-02   1.9672e-04    10.2042 |  11.1 |\n",
      "  263 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.4433   2.4901e-02   1.1754e-04    1.4683 |   0.69214   0.81784   0.81864   0.81776 |   10.3817   2.4900e-02   1.2920e-04    10.4067 |  10.1 |\n",
      "  264 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.7489   2.4900e-02   1.2920e-04    1.7739 |   0.68402   0.81864   0.81833   0.81857 |   10.2625   2.4900e-02   1.2920e-04    10.2875 |  10.0 |\n",
      "  264 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.6013   2.5161e-02   1.0392e-04    1.6266 |   0.68015   0.81862   0.81851   0.81854 |   10.2050   2.5152e-02   1.1548e-04    10.2302 |  10.4 |\n",
      "  265 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.5132   2.5152e-02   1.1548e-04    1.5384 |   0.68874   0.81911   0.81929   0.81903 |   10.3145   2.5152e-02   1.1548e-04    10.3398 |  10.3 |\n",
      "  265 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.7568   2.4600e-02   1.3682e-04    1.7816 |   0.68648   0.81887   0.81951   0.81877 |   10.2810   2.4599e-02   1.2326e-04    10.3057 |  10.7 |\n",
      "\n",
      "[e] Policy training epoch:265  it:47775 -  Total Loss: 10.3057     \n",
      "Task: 10.2810   Sparsity: 2.45990e-02    Sharing: 1.23261e-04 \n",
      "\n",
      " epch: 265   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0890    0.9110  0    0.1035    0.8965  0    0.1380    0.8620  0\n",
      "   2    0.5184    0.4816  1    0.2997    0.7003  0    0.3825    0.6175  0\n",
      "   3    0.5689    0.4311  1    0.3235    0.6765  0    0.5241    0.4759  1\n",
      "   4    0.4227    0.5773  0    0.3687    0.6313  0    0.3634    0.6366  0\n",
      "   5    0.2059    0.7941  0    0.1949    0.8051  0    0.2237    0.7763  0\n",
      "   6    0.4807    0.5193  0    0.2697    0.7303  0    0.2982    0.7018  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  266 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.4070   2.4599e-02   1.2326e-04    1.4317 |   0.67633   0.81905   0.81953   0.81896 |   10.1356   2.4599e-02   1.2326e-04    10.1603 |  10.2 |\n",
      "  266 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.6018   2.5329e-02   9.3872e-05    1.6272 |   0.68594   0.81863   0.81898   0.81853 |   10.2915   2.5344e-02   1.0367e-04    10.3170 |  10.7 |\n",
      "  267 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.5767   2.5344e-02   1.0367e-04    1.6021 |   0.69647   0.81859   0.81898   0.81848 |   10.4096   2.5344e-02   1.0367e-04    10.4350 |  10.9 |\n",
      "  267 |   3.16e-04   3.16e-04   4.22e-03  2.703e+00 |    1.4009   2.5637e-02   1.5357e-04    1.4267 |   0.68331   0.81947   0.81984   0.81939 |   10.2316   2.5633e-02   1.4790e-04    10.2573 |  12.8 |\n",
      " decay gumbel softmax to 2.608481442992936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  268 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.4447   2.5633e-02   1.4790e-04    1.4704 |   0.69385   0.81841   0.81901   0.81832 |   10.3712   2.5633e-02   1.4790e-04    10.3970 |  10.1 |\n",
      "  268 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.8079   2.4885e-02   1.0958e-04    1.8329 |   0.68839   0.81737   0.81806   0.81729 |   10.3546   2.4880e-02   1.1699e-04    10.3796 |  11.7 |\n",
      "  269 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.7942   2.4880e-02   1.1699e-04    1.8192 |   0.68589   0.81922   0.81954   0.81913 |   10.3024   2.4880e-02   1.1699e-04    10.3274 |  10.1 |\n",
      "  269 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6609   2.5054e-02   1.4099e-04    1.6861 |   0.68719   0.81836   0.81916   0.81827 |   10.2986   2.5056e-02   1.3554e-04    10.3238 |   9.9 |\n",
      "  270 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.1536   2.5056e-02   1.3554e-04    1.1788 |   0.68669   0.81927   0.81957   0.81918 |   10.2861   2.5056e-02   1.3554e-04    10.3113 |   9.9 |\n",
      "  270 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.4403   2.5179e-02   1.6213e-04    1.4657 |   0.69293   0.81830   0.81883   0.81822 |   10.3689   2.5171e-02   1.9214e-04    10.3943 |  10.1 |\n",
      "\n",
      "[e] Policy training epoch:270  it:48825 -  Total Loss: 10.3943     \n",
      "Task: 10.3689   Sparsity: 2.51709e-02    Sharing: 1.92140e-04 \n",
      "\n",
      " epch: 270   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0853    0.9147  0    0.1042    0.8958  0    0.1441    0.8559  0\n",
      "   2    0.5165    0.4835  1    0.3127    0.6873  0    0.3653    0.6347  0\n",
      "   3    0.5632    0.4368  1    0.3717    0.6283  0    0.5141    0.4859  1\n",
      "   4    0.4406    0.5594  0    0.3789    0.6211  0    0.3828    0.6172  0\n",
      "   5    0.2103    0.7897  0    0.2172    0.7828  0    0.2449    0.7551  0\n",
      "   6    0.4601    0.5399  0    0.2775    0.7225  0    0.3179    0.6821  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  271 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.4332   2.5171e-02   1.9214e-04    1.4586 |   0.68744   0.81895   0.81946   0.81886 |   10.3063   2.5171e-02   1.9214e-04    10.3317 |   9.7 |\n",
      "  271 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6473   2.5796e-02   1.0861e-04    1.6732 |   0.68364   0.81894   0.81963   0.81885 |   10.2617   2.5809e-02   1.0096e-04    10.2876 |  10.4 |\n",
      "  272 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.3410   2.5809e-02   1.0096e-04    1.3669 |   0.68951   0.81827   0.81884   0.81817 |   10.3626   2.5809e-02   1.0096e-04    10.3885 |  10.2 |\n",
      "  272 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.9794   2.5757e-02   1.9397e-04    2.0053 |   0.69226   0.81864   0.81929   0.81855 |   10.3914   2.5753e-02   2.0070e-04    10.4174 |  11.0 |\n",
      "  273 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.1951   2.5753e-02   2.0070e-04    1.2210 |   0.68847   0.81835   0.81897   0.81827 |   10.3042   2.5753e-02   2.0070e-04    10.3301 |  10.9 |\n",
      "  273 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.8313   2.5396e-02   1.4831e-04    1.8568 |   0.69454   0.81821   0.81893   0.81813 |   10.4049   2.5393e-02   2.0702e-04    10.4305 |  11.2 |\n",
      "  274 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.7662   2.5393e-02   2.0702e-04    1.7918 |   0.69182   0.81910   0.81921   0.81902 |   10.4067   2.5393e-02   2.0702e-04    10.4323 |  11.4 |\n",
      "  274 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.3430   2.5038e-02   2.2692e-04    1.3683 |   0.68633   0.81807   0.81898   0.81799 |   10.3151   2.5040e-02   1.8517e-04    10.3403 |  10.7 |\n",
      "  275 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.4794   2.5040e-02   1.8517e-04    1.5047 |   0.68057   0.81905   0.81931   0.81896 |   10.1990   2.5040e-02   1.8517e-04    10.2242 |  10.3 |\n",
      "  275 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.2547   2.4800e-02   9.8225e-05    1.2796 |   0.69287   0.81892   0.81878   0.81883 |   10.4265   2.4804e-02   1.7481e-04    10.4515 |  11.3 |\n",
      "\n",
      "[e] Policy training epoch:275  it:49875 -  Total Loss: 10.4515     \n",
      "Task: 10.4265   Sparsity: 2.48036e-02    Sharing: 1.74806e-04 \n",
      "\n",
      " epch: 275   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0816    0.9184  0    0.0993    0.9007  0    0.1402    0.8598  0\n",
      "   2    0.5322    0.4678  1    0.3081    0.6919  0    0.3737    0.6263  0\n",
      "   3    0.5617    0.4383  1    0.3619    0.6381  0    0.4892    0.5108  0\n",
      "   4    0.4364    0.5636  0    0.3694    0.6306  0    0.3660    0.6340  0\n",
      "   5    0.2182    0.7818  0    0.2120    0.7880  0    0.2314    0.7686  0\n",
      "   6    0.4793    0.5207  0    0.2722    0.7278  0    0.2955    0.7045  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  276 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6384   2.4804e-02   1.7481e-04    1.6633 |   0.69039   0.81856   0.81910   0.81848 |   10.3829   2.4804e-02   1.7481e-04    10.4079 |  12.0 |\n",
      "  276 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6497   2.5131e-02   8.1001e-05    1.6749 |   0.68785   0.81837   0.81936   0.81829 |   10.3219   2.5128e-02   1.2752e-04    10.3471 |  10.6 |\n",
      "  277 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.2864   2.5128e-02   1.2752e-04    1.3116 |   0.70049   0.81857   0.81950   0.81849 |   10.4910   2.5128e-02   1.2752e-04    10.5162 |  10.4 |\n",
      "  277 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6512   2.5088e-02   1.2142e-04    1.6765 |   0.67979   0.81949   0.81947   0.81941 |   10.1944   2.5071e-02   1.0041e-04    10.2196 |  10.1 |\n",
      "  278 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.7307   2.5071e-02   1.0041e-04    1.7558 |   0.69319   0.81893   0.81943   0.81883 |   10.4091   2.5071e-02   1.0041e-04    10.4343 |  10.6 |\n",
      "  278 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.5266   2.4961e-02   1.6325e-04    1.5517 |   0.68625   0.81797   0.81855   0.81789 |   10.2729   2.4960e-02   1.8830e-04    10.2981 |  10.4 |\n",
      "  279 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.2831   2.4960e-02   1.8830e-04    1.3082 |   0.68780   0.81844   0.81875   0.81837 |   10.3315   2.4960e-02   1.8830e-04    10.3567 |  11.2 |\n",
      "  279 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.7834   2.4793e-02   1.0317e-04    1.8083 |   0.68572   0.81973   0.81971   0.81964 |   10.2735   2.4792e-02   1.1775e-04    10.2984 |  11.5 |\n",
      "  280 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.7172   2.4792e-02   1.1775e-04    1.7421 |   0.68626   0.81844   0.81904   0.81836 |   10.2913   2.4792e-02   1.1775e-04    10.3162 |   9.6 |\n",
      "  280 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.7826   2.5143e-02   1.2222e-04    1.8078 |   0.69182   0.81872   0.81967   0.81864 |   10.3916   2.5139e-02   1.0166e-04    10.4169 |  10.3 |\n",
      "\n",
      "[e] Policy training epoch:280  it:50925 -  Total Loss: 10.4169     \n",
      "Task: 10.3916   Sparsity: 2.51393e-02    Sharing: 1.01659e-04 \n",
      "\n",
      " epch: 280   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0873    0.9127  0    0.1082    0.8918  0    0.1487    0.8513  0\n",
      "   2    0.5548    0.4452  1    0.3136    0.6864  0    0.4008    0.5992  0\n",
      "   3    0.5403    0.4597  1    0.3665    0.6335  0    0.5123    0.4877  1\n",
      "   4    0.4370    0.5630  0    0.3651    0.6349  0    0.3620    0.6380  0\n",
      "   5    0.1965    0.8035  0    0.1977    0.8023  0    0.2473    0.7527  0\n",
      "   6    0.4688    0.5312  0    0.2632    0.7368  0    0.3202    0.6798  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  281 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.3491   2.5139e-02   1.0166e-04    1.3744 |   0.68896   0.81781   0.81862   0.81773 |   10.3463   2.5139e-02   1.0166e-04    10.3715 |  11.3 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  281 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6160   2.5549e-02   1.3824e-04    1.6417 |   0.69113   0.81820   0.81852   0.81812 |   10.3530   2.5547e-02   1.2304e-04    10.3786 |  10.6 |\n",
      "  282 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.5907   2.5547e-02   1.2304e-04    1.6164 |   0.69476   0.81782   0.81922   0.81772 |   10.3832   2.5547e-02   1.2304e-04    10.4088 |  11.5 |\n",
      "  282 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.4821   2.5477e-02   1.8934e-04    1.5077 |   0.69681   0.81840   0.81860   0.81831 |   10.4439   2.5469e-02   1.9651e-04    10.4696 |   9.6 |\n",
      "  283 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.6314   2.5469e-02   1.9651e-04    1.6570 |   0.69415   0.81824   0.81936   0.81815 |   10.4171   2.5469e-02   1.9651e-04    10.4428 |  10.4 |\n",
      "  283 |   3.16e-04   3.16e-04   4.22e-03  2.608e+00 |    1.4545   2.5372e-02   1.7356e-04    1.4801 |   0.69677   0.81764   0.81886   0.81755 |   10.4520   2.5369e-02   1.7133e-04    10.4775 |   9.8 |\n",
      " decay gumbel softmax to 2.5171845924881833\n",
      "  284 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.4765   2.5369e-02   1.7133e-04    1.5020 |   0.69456   0.81809   0.81862   0.81800 |   10.4196   2.5369e-02   1.7133e-04    10.4451 |  10.0 |\n",
      "  284 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.5859   2.5581e-02   1.5809e-04    1.6116 |   0.69801   0.81783   0.81906   0.81774 |   10.4831   2.5586e-02   2.1246e-04    10.5089 |  10.2 |\n",
      "  285 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.6168   2.5586e-02   2.1246e-04    1.6426 |   0.69649   0.81986   0.82024   0.81978 |   10.4685   2.5586e-02   2.1246e-04    10.4943 |   9.4 |\n",
      "Previous best_epoch:   244   best iter: 43260,   best_value: 0.81983\n",
      "New      best_epoch:   285   best iter: 51870,   best_value: 0.81986\n",
      "  285 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.8308   2.5890e-02   1.2374e-04    1.8568 |   0.69257   0.81880   0.81914   0.81872 |   10.4067   2.5875e-02   1.0614e-04    10.4327 |  10.0 |\n",
      "\n",
      "[e] Policy training epoch:285  it:51975 -  Total Loss: 10.4327     \n",
      "Task: 10.4067   Sparsity: 2.58753e-02    Sharing: 1.06135e-04 \n",
      "\n",
      " epch: 285   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0908    0.9092  0    0.0956    0.9044  0    0.1525    0.8475  0\n",
      "   2    0.5693    0.4307  1    0.3176    0.6824  0    0.4105    0.5895  0\n",
      "   3    0.5654    0.4346  1    0.3645    0.6355  0    0.5026    0.4974  1\n",
      "   4    0.4361    0.5639  0    0.3802    0.6198  0    0.3510    0.6490  0\n",
      "   5    0.1972    0.8028  0    0.2081    0.7919  0    0.2764    0.7236  0\n",
      "   6    0.4983    0.5017  0    0.2793    0.7207  0    0.3205    0.6795  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  286 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.7913   2.5875e-02   1.0614e-04    1.8173 |   0.69888   0.81898   0.81972   0.81889 |   10.4928   2.5875e-02   1.0614e-04    10.5188 |  10.1 |\n",
      "  286 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.2643   2.5705e-02   2.5658e-04    1.2903 |   0.68243   0.81944   0.81962   0.81934 |   10.2113   2.5708e-02   1.6868e-04    10.2371 |   9.8 |\n",
      "  287 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.6816   2.5708e-02   1.6868e-04    1.7075 |   0.69464   0.81964   0.81993   0.81956 |   10.4156   2.5708e-02   1.6868e-04    10.4415 |   9.9 |\n",
      "  287 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.8007   2.5773e-02   1.4681e-04    1.8266 |   0.69792   0.81918   0.81947   0.81909 |   10.4677   2.5768e-02   1.2372e-04    10.4936 |  10.1 |\n",
      "  288 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.9239   2.5768e-02   1.2372e-04    1.9498 |   0.69503   0.81900   0.81919   0.81892 |   10.4194   2.5768e-02   1.2372e-04    10.4453 |   9.5 |\n",
      "  288 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.3998   2.5825e-02   1.3722e-04    1.4258 |   0.69571   0.81902   0.81939   0.81894 |   10.4525   2.5807e-02   1.4221e-04    10.4784 |   9.3 |\n",
      "  289 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.5362   2.5807e-02   1.4221e-04    1.5621 |   0.69371   0.81898   0.81952   0.81888 |   10.4107   2.5807e-02   1.4221e-04    10.4367 |   9.4 |\n",
      "  289 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.4611   2.6224e-02   1.5526e-04    1.4875 |   0.70144   0.81950   0.81981   0.81941 |   10.5195   2.6237e-02   1.7969e-04    10.5459 |   9.8 |\n",
      "  290 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.1083   2.6237e-02   1.7969e-04    1.1347 |   0.69332   0.81906   0.81899   0.81898 |   10.4183   2.6237e-02   1.7969e-04    10.4447 |  10.0 |\n",
      "  290 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.9527   2.6932e-02   1.8421e-04    1.9798 |   0.68908   0.81993   0.81975   0.81986 |   10.3381   2.6934e-02   1.6682e-04    10.3652 |  10.7 |\n",
      "Previous best_epoch:   285   best iter: 51870,   best_value: 0.81986\n",
      "New      best_epoch:   290   best iter: 53025,   best_value: 0.81993\n",
      "\n",
      "[e] Policy training epoch:290  it:53025 -  Total Loss: 10.3652     \n",
      "Task: 10.3381   Sparsity: 2.69345e-02    Sharing: 1.66819e-04 \n",
      "\n",
      " epch: 290   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0935    0.9065  0    0.1089    0.8911  0    0.1457    0.8543  0\n",
      "   2    0.5834    0.4166  1    0.3432    0.6568  0    0.4322    0.5678  0\n",
      "   3    0.5790    0.4210  1    0.3925    0.6075  0    0.5345    0.4655  1\n",
      "   4    0.4590    0.5410  0    0.3804    0.6196  0    0.3785    0.6215  0\n",
      "   5    0.2156    0.7844  0    0.2120    0.7880  0    0.2632    0.7368  0\n",
      "   6    0.4761    0.5239  0    0.2793    0.7207  0    0.3232    0.6768  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  291 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.4269   2.6934e-02   1.6682e-04    1.4540 |   0.70183   0.81887   0.81921   0.81879 |   10.5700   2.6934e-02   1.6682e-04    10.5971 |   9.7 |\n",
      "  291 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.1858   2.6868e-02   1.7970e-04    1.2128 |   0.70212   0.81853   0.81876   0.81845 |   10.5530   2.6869e-02   1.8119e-04    10.5800 |  10.6 |\n",
      "  292 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    2.0417   2.6869e-02   1.8119e-04    2.0688 |   0.69683   0.81870   0.81904   0.81861 |   10.4229   2.6869e-02   1.8119e-04    10.4500 |   9.6 |\n",
      "  292 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.5201   2.7202e-02   1.0948e-04    1.5474 |   0.70071   0.81947   0.82017   0.81937 |   10.4884   2.7202e-02   1.0794e-04    10.5157 |  10.1 |\n",
      "  293 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.6698   2.7202e-02   1.0794e-04    1.6971 |   0.69730   0.81843   0.81921   0.81834 |   10.4988   2.7202e-02   1.0794e-04    10.5262 |  10.8 |\n",
      "  293 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.2464   2.6287e-02   8.3187e-05    1.2728 |   0.69199   0.81993   0.82014   0.81984 |   10.3873   2.6270e-02   1.0344e-04    10.4137 |  10.7 |\n",
      "  294 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.0373   2.6270e-02   1.0344e-04    1.0637 |   0.69721   0.81979   0.82013   0.81971 |   10.4338   2.6270e-02   1.0344e-04    10.4602 |  10.0 |\n",
      "  294 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.3120   2.6240e-02   1.8109e-04    1.3385 |   0.69925   0.81901   0.81962   0.81893 |   10.4908   2.6254e-02   1.6569e-04    10.5172 |   9.9 |\n",
      "  295 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.4192   2.6254e-02   1.6569e-04    1.4456 |   0.69453   0.81913   0.81991   0.81905 |   10.3882   2.6254e-02   1.6569e-04    10.4146 |  10.0 |\n",
      "  295 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.7303   2.6335e-02   1.0833e-04    1.7567 |   0.70335   0.81880   0.81889   0.81873 |   10.5438   2.6338e-02   1.1780e-04    10.5703 |   9.9 |\n",
      "\n",
      "[e] Policy training epoch:295  it:54075 -  Total Loss: 10.5703     \n",
      "Task: 10.5438   Sparsity: 2.63379e-02    Sharing: 1.17803e-04 \n",
      "\n",
      " epch: 295   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0866    0.9134  0    0.1013    0.8987  0    0.1468    0.8532  0\n",
      "   2    0.5982    0.4018  1    0.3388    0.6612  0    0.4303    0.5697  0\n",
      "   3    0.5373    0.4627  1    0.3799    0.6201  0    0.5216    0.4784  1\n",
      "   4    0.4174    0.5826  0    0.3977    0.6023  0    0.3728    0.6272  0\n",
      "   5    0.2059    0.7941  0    0.2097    0.7903  0    0.2476    0.7524  0\n",
      "   6    0.4722    0.5278  0    0.2985    0.7015  0    0.3385    0.6615  0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  296 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.6578   2.6338e-02   1.1780e-04    1.6843 |   0.69464   0.81961   0.82009   0.81950 |   10.4632   2.6338e-02   1.1780e-04    10.4896 |  10.0 |\n",
      "  296 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.3425   2.5863e-02   1.3019e-04    1.3685 |   0.70317   0.81956   0.81973   0.81947 |   10.5229   2.5853e-02   1.2393e-04    10.5489 |  11.0 |\n",
      "  297 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    2.0685   2.5853e-02   1.2393e-04    2.0944 |   0.70218   0.81892   0.81906   0.81884 |   10.5635   2.5853e-02   1.2393e-04    10.5895 |  10.7 |\n",
      "  297 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.5901   2.5958e-02   1.2516e-04    1.6162 |   0.69340   0.81959   0.82000   0.81950 |   10.4199   2.5957e-02   1.1658e-04    10.4460 |   9.5 |\n",
      "  298 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.8206   2.5957e-02   1.1658e-04    1.8467 |   0.70305   0.81963   0.81981   0.81954 |   10.5330   2.5957e-02   1.1658e-04    10.5591 |   9.7 |\n",
      "  298 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.6496   2.5930e-02   1.4491e-04    1.6757 |   0.69537   0.81996   0.81955   0.81987 |   10.4463   2.5911e-02   1.5904e-04    10.4724 |  10.0 |\n",
      "Previous best_epoch:   290   best iter: 53025,   best_value: 0.81993\n",
      "New      best_epoch:   298   best iter: 54705,   best_value: 0.81996\n",
      "  299 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.7819   2.5911e-02   1.5904e-04    1.8080 |   0.70489   0.81938   0.81977   0.81930 |   10.5888   2.5911e-02   1.5904e-04    10.6148 |  10.5 |\n",
      "  299 |   3.16e-04   3.16e-04   4.22e-03  2.517e+00 |    1.2276   2.6485e-02   1.3799e-04    1.2542 |   0.70135   0.81947   0.82008   0.81938 |   10.5409   2.6489e-02   1.3119e-04    10.5676 |  10.6 |\n",
      " decay gumbel softmax to 2.4290831317510966\n",
      "  300 |   3.16e-04   3.16e-04   4.22e-03  2.429e+00 |    1.4901   2.6489e-02   1.3119e-04    1.5168 |   0.70155   0.81902   0.81983   0.81893 |   10.5161   2.6489e-02   1.3119e-04    10.5427 |  10.8 |\n",
      "Epoch   300: reducing learning rate of group 0 to 2.3730e-04.\n",
      "Epoch   300: reducing learning rate of group 1 to 2.3730e-04.\n",
      "  300 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.7954   2.6921e-02   1.2003e-04    1.8224 |   0.70436   0.81934   0.81935   0.81926 |   10.6457   2.6916e-02   1.3946e-04    10.6727 |  10.6 |\n",
      "\n",
      "[e] Policy training epoch:300  it:55125 -  Total Loss: 10.6727     \n",
      "Task: 10.6457   Sparsity: 2.69155e-02    Sharing: 1.39457e-04 \n",
      "\n",
      " epch: 300   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0913    0.9087  0    0.1116    0.8884  0    0.1534    0.8466  0\n",
      "   2    0.5776    0.4224  1    0.3192    0.6808  0    0.4073    0.5927  0\n",
      "   3    0.5685    0.4315  1    0.3898    0.6102  0    0.5224    0.4776  1\n",
      "   4    0.4598    0.5402  0    0.4085    0.5915  0    0.3799    0.6201  0\n",
      "   5    0.2206    0.7794  0    0.2070    0.7930  0    0.2513    0.7487  0\n",
      "   6    0.5099    0.4901  1    0.2932    0.7068  0    0.3313    0.6687  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  301 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.3439   2.6916e-02   1.3946e-04    1.3709 |   0.70156   0.81881   0.81910   0.81873 |   10.5538   2.6916e-02   1.3946e-04    10.5808 |   9.4 |\n",
      "  301 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.3421   2.6557e-02   7.8421e-05    1.3687 |   0.70178   0.81886   0.81938   0.81876 |   10.5078   2.6561e-02   1.4933e-04    10.5346 |   9.1 |\n",
      "  302 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2463   2.6561e-02   1.4933e-04    1.2731 |   0.70243   0.82006   0.81991   0.81997 |   10.5744   2.6561e-02   1.4933e-04    10.6011 |   9.4 |\n",
      "Previous best_epoch:   298   best iter: 54705,   best_value: 0.81996\n",
      "New      best_epoch:   302   best iter: 55440,   best_value: 0.82006\n",
      "  302 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.5023   2.6581e-02   1.2162e-04    1.5291 |   0.69806   0.81881   0.81904   0.81873 |   10.4879   2.6589e-02   1.4209e-04    10.5146 |   9.5 |\n",
      "  303 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.7722   2.6589e-02   1.4209e-04    1.7990 |   0.70459   0.81913   0.81981   0.81904 |   10.5421   2.6589e-02   1.4209e-04    10.5688 |   9.5 |\n",
      "  303 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.4198   2.7157e-02   1.2969e-04    1.4471 |   0.70471   0.81873   0.81917   0.81865 |   10.5379   2.7143e-02   1.0790e-04    10.5652 |   9.4 |\n",
      "  304 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.4079   2.7143e-02   1.0790e-04    1.4352 |   0.70035   0.81887   0.81937   0.81879 |   10.5326   2.7143e-02   1.0790e-04    10.5599 |   9.5 |\n",
      "  304 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2555   2.7214e-02   1.4580e-04    1.2828 |   0.70129   0.81853   0.81907   0.81843 |   10.4947   2.7211e-02   1.6460e-04    10.5221 |   9.6 |\n",
      "  305 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.4903   2.7211e-02   1.6460e-04    1.5176 |   0.70697   0.81972   0.81968   0.81963 |   10.6140   2.7211e-02   1.6460e-04    10.6414 |  10.1 |\n",
      "  305 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.6741   2.7248e-02   8.7401e-05    1.7015 |   0.69762   0.81882   0.81959   0.81873 |   10.4774   2.7262e-02   1.0458e-04    10.5048 |   9.8 |\n",
      "\n",
      "[e] Policy training epoch:305  it:56175 -  Total Loss: 10.5048     \n",
      "Task: 10.4774   Sparsity: 2.72617e-02    Sharing: 1.04583e-04 \n",
      "\n",
      " epch: 305   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0862    0.9138  0    0.1076    0.8924  0    0.1489    0.8511  0\n",
      "   2    0.5661    0.4339  1    0.3459    0.6541  0    0.3902    0.6098  0\n",
      "   3    0.5889    0.4111  1    0.3921    0.6079  0    0.5525    0.4475  1\n",
      "   4    0.4615    0.5385  0    0.4129    0.5871  0    0.3891    0.6109  0\n",
      "   5    0.2137    0.7863  0    0.2160    0.7840  0    0.2573    0.7427  0\n",
      "   6    0.4951    0.5049  0    0.3053    0.6947  0    0.3264    0.6736  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  306 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.1574   2.7262e-02   1.0458e-04    1.1847 |   0.70318   0.81915   0.81970   0.81905 |   10.5626   2.7262e-02   1.0458e-04    10.5900 |  10.7 |\n",
      "  306 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.5921   2.7060e-02   1.5009e-04    1.6193 |   0.70351   0.81913   0.81965   0.81905 |   10.5636   2.7063e-02   1.5324e-04    10.5908 |   9.9 |\n",
      "  307 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.7029   2.7063e-02   1.5324e-04    1.7301 |   0.70233   0.81923   0.81969   0.81915 |   10.5226   2.7063e-02   1.5324e-04    10.5498 |   9.5 |\n",
      "  307 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2770   2.7017e-02   1.8099e-04    1.3042 |   0.70262   0.81879   0.81960   0.81871 |   10.5176   2.7021e-02   1.1159e-04    10.5448 |  10.0 |\n",
      "  308 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.7663   2.7021e-02   1.1159e-04    1.7935 |   0.71472   0.81995   0.82027   0.81985 |   10.7407   2.7021e-02   1.1159e-04    10.7678 |   9.8 |\n",
      "  308 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.4519   2.7092e-02   1.3742e-04    1.4791 |   0.71146   0.81911   0.81914   0.81902 |   10.6600   2.7100e-02   1.5203e-04    10.6873 |   9.2 |\n",
      "  309 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.6539   2.7100e-02   1.5203e-04    1.6812 |   0.70870   0.81978   0.82058   0.81969 |   10.6346   2.7100e-02   1.5203e-04    10.6619 |   9.4 |\n",
      "  309 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2125   2.7004e-02   1.8614e-04    1.2397 |   0.70528   0.81930   0.81996   0.81922 |   10.5793   2.6993e-02   1.8714e-04    10.6065 |   9.4 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  310 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.4631   2.6993e-02   1.8714e-04    1.4903 |   0.70221   0.81987   0.81998   0.81979 |   10.5334   2.6993e-02   1.8714e-04    10.5606 |   9.3 |\n",
      "  310 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.4034   2.7087e-02   1.0104e-04    1.4306 |   0.70065   0.81996   0.82016   0.81987 |   10.5246   2.7083e-02   1.6353e-04    10.5519 |   9.7 |\n",
      "\n",
      "[e] Policy training epoch:310  it:57225 -  Total Loss: 10.5519     \n",
      "Task: 10.5246   Sparsity: 2.70835e-02    Sharing: 1.63534e-04 \n",
      "\n",
      " epch: 310   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0843    0.9157  0    0.1037    0.8963  0    0.1568    0.8432  0\n",
      "   2    0.5646    0.4354  1    0.3385    0.6615  0    0.4128    0.5872  0\n",
      "   3    0.5774    0.4226  1    0.4071    0.5929  0    0.5386    0.4614  1\n",
      "   4    0.4424    0.5576  0    0.4032    0.5968  0    0.3956    0.6044  0\n",
      "   5    0.2249    0.7751  0    0.2099    0.7901  0    0.2600    0.7400  0\n",
      "   6    0.5025    0.4975  1    0.2958    0.7042  0    0.3147    0.6853  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  311 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    2.0587   2.7083e-02   1.6353e-04    2.0859 |   0.71092   0.81942   0.81962   0.81933 |   10.6661   2.7083e-02   1.6353e-04    10.6934 |   9.5 |\n",
      "  311 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2831   2.6892e-02   1.3665e-04    1.3102 |   0.70346   0.81899   0.81962   0.81890 |   10.5437   2.6882e-02   1.5495e-04    10.5707 |   9.4 |\n",
      "  312 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.3138   2.6882e-02   1.5495e-04    1.3408 |   0.70386   0.81901   0.81987   0.81892 |   10.5705   2.6882e-02   1.5495e-04    10.5976 |   9.4 |\n",
      "  312 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.7498   2.7419e-02   1.2203e-04    1.7774 |   0.70349   0.81871   0.81965   0.81862 |   10.5332   2.7417e-02   9.4663e-05    10.5607 |   9.6 |\n",
      "  313 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2581   2.7417e-02   9.4663e-05    1.2856 |   0.70943   0.81948   0.81973   0.81939 |   10.6562   2.7417e-02   9.4663e-05    10.6838 |   9.6 |\n",
      "  313 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.3004   2.7466e-02   8.4285e-05    1.3279 |   0.71103   0.81861   0.81945   0.81852 |   10.6776   2.7466e-02   1.1202e-04    10.7051 |  10.4 |\n",
      "  314 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.3480   2.7466e-02   1.1202e-04    1.3756 |   0.70467   0.81971   0.82009   0.81962 |   10.6056   2.7466e-02   1.1202e-04    10.6332 |  10.1 |\n",
      "  314 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.7673   2.8003e-02   1.7944e-04    1.7955 |   0.71269   0.81937   0.81949   0.81927 |   10.6850   2.8016e-02   1.5567e-04    10.7132 |  10.0 |\n",
      "  315 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.6700   2.8016e-02   1.5567e-04    1.6982 |   0.70115   0.81968   0.81982   0.81960 |   10.5271   2.8016e-02   1.5567e-04    10.5552 |   9.5 |\n",
      "  315 |   2.37e-04   2.37e-04   4.22e-03  2.429e+00 |    1.2061   2.7912e-02   1.1198e-04    1.2341 |   0.70859   0.81930   0.82014   0.81921 |   10.6252   2.7918e-02   1.4179e-04    10.6533 |   9.8 |\n",
      " decay gumbel softmax to 2.344065222139808\n",
      "\n",
      "[e] Policy training epoch:315  it:58275 -  Total Loss: 10.6533     \n",
      "Task: 10.6252   Sparsity: 2.79177e-02    Sharing: 1.41795e-04 \n",
      "\n",
      " epch: 315   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0836    0.9164  0    0.1020    0.8980  0    0.1463    0.8537  0\n",
      "   2    0.5676    0.4324  1    0.3593    0.6407  0    0.4234    0.5766  0\n",
      "   3    0.5929    0.4071  1    0.4212    0.5788  0    0.5590    0.4410  1\n",
      "   4    0.4501    0.5499  0    0.4024    0.5976  0    0.3985    0.6015  0\n",
      "   5    0.2153    0.7847  0    0.2361    0.7639  0    0.2620    0.7380  0\n",
      "   6    0.4984    0.5016  0    0.3101    0.6899  0    0.3469    0.6531  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  316 |   2.37e-04   2.37e-04   4.22e-03  2.344e+00 |    1.4178   2.7918e-02   1.4179e-04    1.4458 |   0.70960   0.81966   0.81964   0.81958 |   10.6332   2.7918e-02   1.4179e-04    10.6612 |  12.8 |\n",
      "  316 |   2.37e-04   2.37e-04   4.22e-03  2.344e+00 |    1.6789   2.7752e-02   1.1590e-04    1.7068 |   0.70501   0.81952   0.82026   0.81943 |   10.5904   2.7748e-02   7.2538e-05    10.6182 |  10.5 |\n",
      "  317 |   2.37e-04   2.37e-04   4.22e-03  2.344e+00 |    1.8516   2.7748e-02   7.2538e-05    1.8794 |   0.71017   0.81873   0.81966   0.81864 |   10.6109   2.7748e-02   7.2538e-05    10.6387 |  11.0 |\n",
      "  317 |   2.37e-04   2.37e-04   4.22e-03  2.344e+00 |    1.1584   2.7864e-02   1.8054e-04    1.1865 |   0.71200   0.81969   0.81951   0.81962 |   10.6956   2.7860e-02   1.5911e-04    10.7236 |   9.8 |\n",
      "  318 |   2.37e-04   2.37e-04   4.22e-03  2.344e+00 |    1.3612   2.7860e-02   1.5911e-04    1.3892 |   0.70695   0.82024   0.82063   0.82014 |   10.6213   2.7860e-02   1.5911e-04    10.6493 |  10.1 |\n",
      "Previous best_epoch:   302   best iter: 55440,   best_value: 0.82006\n",
      "New      best_epoch:   318   best iter: 58800,   best_value: 0.82024\n",
      "  318 |   2.37e-04   2.37e-04   4.22e-03  2.344e+00 |    1.7308   2.7628e-02   1.2567e-04    1.7586 |   0.70380   0.82084   0.82080   0.82075 |   10.5678   2.7634e-02   1.1721e-04    10.5955 |  12.3 |\n",
      "Epoch   243: reducing learning rate of group 0 to 3.1641e-03.\n",
      "Previous best_epoch:   318   best iter: 58800,   best_value: 0.82024\n",
      "New      best_epoch:   318   best iter: 58905,   best_value: 0.82084\n",
      "  319 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.8179   2.7634e-02   1.1721e-04    1.8456 |   0.71492   0.81911   0.81965   0.81903 |   10.7338   2.7634e-02   1.1721e-04    10.7616 |   9.8 |\n",
      "  319 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.2679   2.7240e-02   1.4484e-04    1.2953 |   0.71185   0.81921   0.81980   0.81913 |   10.6624   2.7221e-02   1.4000e-04    10.6898 |  10.4 |\n",
      "  320 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.2541   2.7221e-02   1.4000e-04    1.2814 |   0.70797   0.81910   0.81919   0.81902 |   10.6376   2.7221e-02   1.4000e-04    10.6649 |   9.8 |\n",
      "  320 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.2896   2.7201e-02   9.4892e-05    1.3169 |   0.71134   0.81959   0.81998   0.81950 |   10.7172   2.7194e-02   7.0558e-05    10.7445 |   9.3 |\n",
      "\n",
      "[e] Policy training epoch:320  it:59325 -  Total Loss: 10.7445     \n",
      "Task: 10.7172   Sparsity: 2.71938e-02    Sharing: 7.05581e-05 \n",
      "\n",
      " epch: 320   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0855    0.9145  0    0.1119    0.8881  0    0.1444    0.8556  0\n",
      "   2    0.5789    0.4211  1    0.3646    0.6354  0    0.4215    0.5785  0\n",
      "   3    0.5793    0.4207  1    0.3941    0.6059  0    0.5403    0.4597  1\n",
      "   4    0.4504    0.5496  0    0.3924    0.6076  0    0.3886    0.6114  0\n",
      "   5    0.2157    0.7843  0    0.2263    0.7737  0    0.2547    0.7453  0\n",
      "   6    0.4850    0.5150  0    0.2819    0.7181  0    0.3346    0.6654  0\n",
      "\n",
      "\n",
      "Epoch | BckBone LR   Heads LR  Policy LR Gumbl Temp |  trn loss     trn spar     trn shar   trn ttl |   bceloss  avg prec    aucroc     aucpr |  val loss     val spar     val shar    val ttl |  time |\n",
      "  321 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.2377   2.7194e-02   7.0558e-05    1.2650 |   0.70550   0.81916   0.81959   0.81908 |   10.5482   2.7194e-02   7.0558e-05    10.5754 |   9.2 |\n",
      "  321 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.1859   2.7699e-02   8.8710e-05    1.2137 |   0.70993   0.81945   0.82015   0.81935 |   10.7060   2.7717e-02   8.7736e-05    10.7338 |   9.2 |\n",
      "  322 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.4101   2.7717e-02   8.7737e-05    1.4379 |   0.71081   0.81951   0.81960   0.81943 |   10.6701   2.7717e-02   8.7736e-05    10.6980 |   9.3 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  322 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.5799   2.7602e-02   8.6529e-05    1.6076 |   0.71142   0.81909   0.81969   0.81900 |   10.6466   2.7608e-02   8.9878e-05    10.6743 |   9.3 |\n",
      "  323 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.3504   2.7608e-02   8.9878e-05    1.3781 |   0.70003   0.81906   0.81919   0.81898 |   10.4638   2.7608e-02   8.9878e-05    10.4915 |   9.3 |\n",
      "  323 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.6109   2.8326e-02   1.0537e-04    1.6393 |   0.71689   0.81959   0.82013   0.81950 |   10.7855   2.8331e-02   9.9723e-05    10.8139 |   9.2 |\n",
      "  324 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.2638   2.8331e-02   9.9723e-05    1.2922 |   0.71093   0.81873   0.81908   0.81865 |   10.6603   2.8331e-02   9.9723e-05    10.6887 |   9.8 |\n",
      "  324 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.4794   2.8049e-02   9.2703e-05    1.5075 |   0.71406   0.81962   0.81989   0.81954 |   10.6592   2.8040e-02   1.0270e-04    10.6874 |  10.8 |\n",
      "  325 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.4711   2.8040e-02   1.0270e-04    1.4993 |   0.71923   0.81950   0.81999   0.81942 |   10.7537   2.8040e-02   1.0270e-04    10.7819 |  10.7 |\n",
      "  325 |   2.37e-04   2.37e-04   3.16e-03  2.344e+00 |    1.4112   2.7945e-02   1.0820e-04    1.4392 |   0.70210   0.81986   0.82026   0.81978 |   10.5773   2.7953e-02   1.4025e-04    10.6054 |  10.7 |\n",
      "\n",
      "[e] Policy training epoch:325  it:60375 -  Total Loss: 10.6054     \n",
      "Task: 10.5773   Sparsity: 2.79527e-02    Sharing: 1.40253e-04 \n",
      "\n",
      " epch: 325   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0859    0.9141  0    0.1184    0.8816  0    0.1488    0.8512  0\n",
      "   2    0.5766    0.4234  1    0.3816    0.6184  0    0.4154    0.5846  0\n",
      "   3    0.5966    0.4034  1    0.4167    0.5833  0    0.5561    0.4439  1\n",
      "   4    0.4482    0.5518  0    0.3895    0.6105  0    0.3862    0.6138  0\n",
      "   5    0.2319    0.7681  0    0.2431    0.7569  0    0.2767    0.7233  0\n",
      "   6    0.4775    0.5225  0    0.2908    0.7092  0    0.3548    0.6452  0\n",
      "\n",
      "\n",
      "[Final] ep:325  it:60375 -  Total Loss: 10.6054     \n",
      "Task: 10.5773   Sparsity: 2.79527e-02    Sharing: 1.40253e-04 \n",
      "\n",
      " epch: 325   softmax      s        softmax       s        softmax       s\n",
      " -----  ----------------- -    ----------------- -    ----------------- - \n",
      "   1    0.0859    0.9141  0    0.1184    0.8816  0    0.1488    0.8512  0\n",
      "   2    0.5766    0.4234  1    0.3816    0.6184  0    0.4154    0.5846  0\n",
      "   3    0.5966    0.4034  1    0.4167    0.5833  0    0.5561    0.4439  1\n",
      "   4    0.4482    0.5518  0    0.3895    0.6105  0    0.3862    0.6138  0\n",
      "   5    0.2319    0.7681  0    0.2431    0.7569  0    0.2767    0.7233  0\n",
      "   6    0.4775    0.5225  0    0.2908    0.7092  0    0.3548    0.6452  0\n",
      "\n",
      "\n",
      "\n",
      " epch: 325   logits       s          logits      s         logits       s\n",
      " -----  ----------------- -    ----------------  -    ----------------  - \n",
      "   1   -1.0373    1.3273  0   -1.0366    0.9709  0   -1.0372    0.7071  0\n",
      "   2   -0.1304   -0.4393  1   -0.1311    0.3518  0   -0.1321    0.2098  0\n",
      "   3   -0.0022   -0.3934  1   -0.0020    0.3343  0   -0.0015   -0.2267  1\n",
      "   4   -0.2101   -0.0021  0   -0.2107    0.2387  0   -0.2105    0.2529  0\n",
      "   5   -0.6051    0.5926  0   -0.6043    0.5317  0   -0.6041    0.3569  0\n",
      "   6   -0.2911   -0.2010  0   -0.2915    0.6000  0   -0.2891    0.3089  0\n",
      "\n",
      "\n",
      " save train val_metrics to :  model_train_ep_325\n",
      " save train checkpoint  to :  model_train_ep_325\n"
     ]
    }
   ],
   "source": [
    "# weight_policy_training(ns, opt, environ, dldrs, epochs = 100)\n",
    "weight_policy_training(ns, opt, environ, dldrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27779c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:52:52.798202Z",
     "start_time": "2022-03-25T13:52:52.706053Z"
    }
   },
   "outputs": [],
   "source": [
    "ns.best_epoch, ns.best_iter, ns.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be1060",
   "metadata": {},
   "source": [
    "### Close WandB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f6dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-25T13:52:58.301350Z",
     "start_time": "2022-03-25T13:52:54.265574Z"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49cc3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T21:40:38.885929Z",
     "start_time": "2022-03-10T21:40:38.783808Z"
    }
   },
   "outputs": [],
   "source": [
    "# ns.best_epoch = 0\n",
    "# from utils.notebook_modules import wrapup_phase\n",
    "# wrapup_phase(ns, opt, environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1656da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T01:52:34.096090Z",
     "start_time": "2022-03-10T01:52:34.003457Z"
    }
   },
   "outputs": [],
   "source": [
    "print( f\" Backbone Learning Rate      : {environ.opt['train']['backbone_lr']}\\n\"\n",
    "       f\" Tasks    Learning Rate      : {environ.opt['train']['task_lr']}\\n\"\n",
    "       f\" Policy   Learning Rate      : {environ.opt['train']['policy_lr']}\\n\")\n",
    "\n",
    "\n",
    "print( f\" Sparsity regularization     : {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization     : {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization     : {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                 : {environ.gumbel_temperature:.4f}         \\n\" #\n",
    "       f\" Gumbel Temp decay           : {environ.opt['train']['decay_temp_freq']}\") #\n",
    "print(opt['train']['decay_temp_freq'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570db82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T23:15:53.792025Z",
     "start_time": "2022-03-09T23:15:53.736492Z"
    }
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['policy_lr']       = 0.002\n",
    "# environ.opt['train']['lambda_sparsity'] = 0.05\n",
    "# environ.opt['train']['lambda_sharing']  = 0.01\n",
    "# environ.opt['train']['lambda_tasks']    = 1.0\n",
    "# # environ.opt['train']['decay_temp_freq'] = 2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26efa07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T03:10:08.772444Z",
     "start_time": "2022-03-10T03:10:08.706432Z"
    }
   },
   "outputs": [],
   "source": [
    "print( f\" Backbone Learning Rate      : {environ.opt['train']['backbone_lr']}\\n\"\n",
    "       f\" Tasks    Learning Rate      : {environ.opt['train']['task_lr']}\\n\"\n",
    "       f\" Policy   Learning Rate      : {environ.opt['train']['policy_lr']}\\n\")\n",
    "\n",
    "\n",
    "print( f\" Sparsity regularization     : {environ.opt['train']['lambda_sparsity']}\\n\"\n",
    "       f\" Sharing  regularization     : {environ.opt['train']['lambda_sharing']} \\n\\n\"\n",
    "       f\" Tasks    regularization     : {environ.opt['train']['lambda_tasks']}   \\n\"\n",
    "       f\" Gumbel Temp                 : {environ.gumbel_temperature:.4f}         \\n\" #\n",
    "       f\" Gumbel Temp decay           : {environ.opt['train']['decay_temp_freq']}\") #\n",
    "\n",
    "print()\n",
    "print( f\" current_iters               : {ns.current_iter}\")  \n",
    "print( f\" current_epochs              : {ns.current_epoch}\") \n",
    "print( f\" train_total_epochs          : {ns.training_epochs}\") \n",
    "print( f\" stop_epoch_training         : {ns.stop_epoch_training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac6b6a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Post Training Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de4040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-13T22:17:44.833671Z",
     "start_time": "2022-03-13T22:17:44.799394Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.losses)\n",
    "# pp.pprint(environ.val_metrics)\n",
    "environ.num_layers, environ.networks['mtl-net'].num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ca92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T23:23:43.744498Z",
     "start_time": "2022-03-10T23:23:43.696990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.val_metrics['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e5cec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T17:44:36.218784Z",
     "start_time": "2022-02-21T17:44:36.063411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_loss(environ.val_metrics, title = f\"[Final] ep:{current_epoch}  it:{current_iter}\",)\n",
    "# environ.display_trained_policy(current_epoch)\n",
    "# environ.display_trained_logits(current_epoch)\n",
    "# environ.log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c1c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T00:32:52.580865Z",
     "start_time": "2022-03-06T00:32:52.554112Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_label   = 'model_train_ep_%d_seed_%04d' % (current_epoch, opt['random_seed'])\n",
    "# metrics_label = 'metrics_train_ep_%d_seed_%04d.pickle' % (current_epoch, opt['random_seed'])\n",
    "# environ.save_checkpoint(model_label, current_iter, current_epoch) \n",
    "# save_to_pickle(environ.val_metrics, environ.opt['paths']['checkpoint_dir'], metrics_label)\n",
    "# print_loss(environ.val_metrics, title = f\"[Final] ep:{current_epoch}  it:{current_iter}\",)\n",
    "# environ.display_trained_policy(current_epoch,out=[sys.stdout, environ.log_file])\n",
    "# environ.display_trained_logits(current_epoch)\n",
    "# environ.log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad3a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T22:48:27.014120Z",
     "start_time": "2022-02-20T22:48:26.982535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_loss(current_iter, environ.losses, title = f\"[e] Policy training epoch:{current_epoch}    iter:\")\n",
    "# print()\n",
    "# print_loss(current_iter, trn_losses, title = f\"[e] Policy training epoch:{current_epoch}    iter:\")\n",
    "# print()\n",
    "# print_loss(current_iter, environ.val_metrics, title = f\"[e] Policy training epoch:{current_epoch}    iter:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d5db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T20:31:45.254334Z",
     "start_time": "2022-03-01T20:31:45.116895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.losses\n",
    "# environ.val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4dd81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T20:47:29.582501Z",
     "start_time": "2022-03-01T20:47:29.492581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.batch_data\n",
    "# print_metrics_cr(current_epoch, time.time() - start_time, trn_losses, environ.val_metrics, 0, out=[sys.stdout])\n",
    "# environ.display_parameters()\n",
    "\n",
    "# with np.printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', precision=7, formatter={'float': lambda x: f\"{x:12.5e}\"}):\n",
    "#     environ.print_logit_grads('gradients')\n",
    "\n",
    "# environ_params = environ.get_task_specific_parameters()\n",
    "# environ_params = environ.get_arch_parameters()\n",
    "# environ_params = environ.get_backbone_parameters()\n",
    "# print(environ_params)\n",
    "# for param in environ_params:\n",
    "#     print(param.grad.shape, '\\n', param.grad)\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c80c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T21:12:46.806056Z",
     "start_time": "2022-03-11T21:12:46.471801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.display_trained_logits(ns.current_epoch)\n",
    "environ.display_trained_policy(ns.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d47dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T21:13:19.578964Z",
     "start_time": "2022-03-11T21:13:19.242252Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.display_test_sample_policy(ns.current_epoch, hard_sampling = True)\n",
    "environ.display_train_sample_policy(ns.current_epoch, hard_sampling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754b317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-06T00:33:19.474125Z",
     "start_time": "2022-03-06T00:33:19.447847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.define_optimizer(policy_learning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e89541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T00:07:08.097708Z",
     "start_time": "2022-03-09T00:07:08.070721Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.optimizers['alphas'])\n",
    "print(environ.optimizers['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecc91e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T00:07:50.026992Z",
     "start_time": "2022-03-09T00:07:49.986101Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Policy  initial_lr : ', environ.optimizers['alphas'].param_groups[0]['initial_lr'], 'lr : ',environ.optimizers['alphas'].param_groups[0]['lr'])\n",
    "print('Weights initial_lr : ', environ.optimizers['weights'].param_groups[0]['initial_lr'], 'lr : ',environ.optimizers['weights'].param_groups[0]['lr'])\n",
    "print('Weights initial_lr : ', environ.optimizers['weights'].param_groups[1]['initial_lr'], 'lr : ',environ.optimizers['weights'].param_groups[1]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1306e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T22:31:50.425696Z",
     "start_time": "2022-03-10T22:31:50.396531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wandb.run is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b8e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T23:10:03.751132Z",
     "start_time": "2022-03-05T23:10:03.724538Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:44:13.145647Z",
     "iopub.status.busy": "2022-01-07T22:44:13.145313Z",
     "iopub.status.idle": "2022-01-07T22:44:13.193262Z",
     "shell.execute_reply": "2022-01-07T22:44:13.192140Z",
     "shell.execute_reply.started": "2022-01-07T22:44:13.145622Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# opt['exp_instance'] = '0218_1358'     \n",
    "# folder_name=  f\"{opt['exp_instance']}_bs{opt['train']['batch_size']:03d}_{opt['train']['decay_lr_rate']:3.2f}_{opt['train']['decay_lr_freq']}\"\n",
    "# print()\n",
    "# opt['exp_instance'] = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "# opt['exp_description'] = f\"No Alternating Weight/Policy - training all done with both weights and policy\"\n",
    "# folder_name=  f\"{opt['exp_instance']}_bs{opt['train']['batch_size']:03d}_{opt['train']['decay_lr_rate']:3.2f}_{opt['train']['decay_lr_freq']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2affee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T21:12:20.322227Z",
     "start_time": "2022-03-11T21:12:20.285961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527bd00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T21:25:11.319751Z",
     "start_time": "2022-02-20T21:25:11.210062Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "p = environ.get_current_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919068f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T21:25:26.324030Z",
     "start_time": "2022-02-20T21:25:26.112782Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82a453",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Post Warm-up Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb74c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:45:43.999959Z",
     "start_time": "2022-02-01T12:45:43.862475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b47da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:46:36.700361Z",
     "start_time": "2022-02-01T12:46:36.367037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.networks['mtl-net'].arch_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f0235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:37:10.158440Z",
     "start_time": "2022-01-28T16:37:09.742327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = environ.get_sample_policy(hard_sampling = False)\n",
    "print(p)\n",
    "p = environ.get_policy_prob()\n",
    "print(p)\n",
    "p = environ.get_policy_logits()\n",
    "print(p)\n",
    "\n",
    "# p = environ.get_current_policy()\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bddd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:40:37.799917Z",
     "start_time": "2022-01-28T16:40:37.773177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = softmax([0.0, 1])\n",
    "print(a)\n",
    "sampled = np.random.choice((1, 0), p=a)\n",
    "print(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8f376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T16:13:16.205889Z",
     "start_time": "2022-01-28T16:13:16.179303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.optimizers['weights'])\n",
    "print(environ.schedulers['weights'].get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf9c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T12:46:50.411465Z",
     "start_time": "2022-02-01T12:46:50.020540Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('losses.keys      : ', environ.losses.keys())\n",
    "print('losses[task]keys : ', environ.losses['task1'].keys())\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T22:57:02.151169Z",
     "start_time": "2022-01-15T22:57:02.056562Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( environ.val_metrics.keys())\n",
    "# pp.pprint(val_metrics)\n",
    "print(type(environ.val_metrics['aggregated']))\n",
    "print()\n",
    "print(type(environ.val_metrics['task1']['classification_agg']))\n",
    "print()\n",
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6e311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.266303Z",
     "start_time": "2022-01-14T18:57:26.166878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"val_metrics.pkl\", mode= 'wb') as f:\n",
    "#         pickle.dump(val_metrics, f)\n",
    "    \n",
    "# with open('val_metrics.pkl', 'rb') as f:    \n",
    "#     tst_val_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00a8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.297444Z",
     "start_time": "2022-01-14T18:57:26.269323Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(environ.input.shape) \n",
    "# a = getattr(environ, 'task1_pred')\n",
    "# yc_data = environ.batch['task1_data']\n",
    "# print(yc_data.shape)\n",
    "# yc_ind = environ.batch['task1_ind']\n",
    "# print(yc_ind.shape)\n",
    "# yc_hat_all = getattr(environ, 'task1_pred')\n",
    "# print(yc_hat_all.shape)\n",
    "# yc_hat  = yc_hat_all[yc_ind[0], yc_ind[1]]\n",
    "# print(yc_hat_all.shape, yc_hat.shape)\n",
    "\n",
    "# \n",
    "# environ.losses\n",
    "# loss = {}\n",
    "# for key in environ.losses.keys():\n",
    "#     loss[key] = {}\n",
    "#     for subkey, v in environ.losses[key].items():\n",
    "#         print(f\" key:  {key}   subkey: {subkey} \")\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             loss[key][subkey] = v.data\n",
    "#             print(f\" Tensor  -  key:  {key}   subkey: {subkey}           value type: {type(v)}  value: {v:.4f}\")\n",
    "#         else:\n",
    "#             loss[key][subkey] = v\n",
    "#             print(f\" integer -  key:  {key}   subkey: {subkey}           value type: {type(v)}  value: {v:.4f}\")\n",
    "# pp.pprint(tst_val_metrics)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987f89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.340792Z",
     "start_time": "2022-01-14T18:57:26.302528Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-07T22:49:07.606120Z",
     "iopub.status.busy": "2022-01-07T22:49:07.604909Z",
     "iopub.status.idle": "2022-01-07T22:49:08.025886Z",
     "shell.execute_reply": "2022-01-07T22:49:08.024798Z",
     "shell.execute_reply.started": "2022-01-07T22:49:07.606065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print('metrics.keys: ', environ.metrics.keys())\n",
    "# print('metrics[task].keys: ', environ.metrics['task1'].keys())\n",
    "# pp.pprint(environ.metrics['task1'])\n",
    "# pp.pprint(environ.losses['task1']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ae417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:57:26.373399Z",
     "start_time": "2022-01-14T18:57:26.345065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# title='Iteration'\n",
    "# for t_id, _ in enumerate(environ.tasks):\n",
    "#     task_key = f\"task{t_id+1}\"\n",
    "# #     print_heading(f\"{title}  {current_iter}  {task_key} : {val_metrics[task_key]['classification_agg']}\", verbose = True)\n",
    "\n",
    "#     for key, _  in val_metrics[task_key]['classification_agg'].items():\n",
    "#         print('%s/%-20s'%(task_key, key), val_metrics[task_key]['classification_agg'][key], current_iter)\n",
    "#         print(f\"{task_key:s}/{key:20s}\", val_metrics[task_key]['classification_agg'][key], current_iter)\n",
    "#         print()\n",
    "#             # print_current_errors(os.path.join(self.log_dir, 'loss.txt'), current_iter,key, loss[key], time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b73b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:52:48.162261Z",
     "start_time": "2022-01-07T22:52:48.140423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.print_loss(current_iter, start_time, metrics = val_metrics['loss'], verbose=True)\n",
    "# print(opt['lambdas'])\n",
    "# p = (opt['lambdas'][0] * environ.losses['tasks']['task1'])\n",
    "# print(p)\n",
    "\n",
    "# environ.print_val_metrics(current_iter, start_time, val_metrics , title='validation', verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850378b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:52:47.661019Z",
     "start_time": "2022-01-07T22:52:47.639094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(current_iter)\n",
    "# print_metrics_cr(current_iter, t1 - t0, None, val_metrics , True)\n",
    "# environ.print_val_metrics(current_iter, start_time, val_metrics, title='validation', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a80b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T23:14:58.034384Z",
     "start_time": "2022-01-07T23:14:58.004850Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\" val_metric keys               : {val_metrics.keys()}\")\n",
    "print(f\" loss keys                     : {val_metrics['loss'].keys()}\")\n",
    "print(f\" task1 keys                    : {val_metrics['task1'].keys()}\")\n",
    "print(f\" task1 classification keys     : {val_metrics['task1']['classification'].keys()}\")\n",
    "print(f\" task1 classification_agg keys : {val_metrics['task1']['classification_agg'].keys()}\")\n",
    "print()\n",
    "print(f\" task1                       : {val_metrics['task1']['classification_agg']['loss']:5f}\")\n",
    "print(f\" task2                       : {val_metrics['task2']['classification_agg']['loss']:5f}\")\n",
    "print(f\" task3                       : {val_metrics['task3']['classification_agg']['loss']:5f}\")\n",
    "print(f\" loss                        : {val_metrics['loss']['total']:5f}\")\n",
    "print(f\" train_time                  : {val_metrics['train_time']:2f}\")\n",
    "print(f\" epoch                       : {val_metrics['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc43a6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Post Weight + Policy Training Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65640cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:22:20.634818Z",
     "start_time": "2022-01-27T00:22:20.444566Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.networks['mtl-net'].backbone.layer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4374287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:30:31.940280Z",
     "start_time": "2022-01-26T19:30:31.910058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_blocks = 6\n",
    "num_policy_layers = 6\n",
    "gt =  torch.ones((num_blocks)).long()\n",
    "gt0 =  torch.zeros((num_blocks)).long()\n",
    "print(gt)\n",
    "print(gt0)\n",
    "\n",
    "loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651bc43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T19:42:31.300891Z",
     "start_time": "2022-01-26T19:42:31.257774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if environ.opt['diff_sparsity_weights'] and not environ.opt['is_sharing']:\n",
    "    print(' cond 1')\n",
    "    ## Assign higher weights to higher layers \n",
    "    loss_weights = ((torch.arange(0, num_policy_layers, 1) + 1).float() / num_policy_layers)\n",
    "    print(f\"{task_key} sparsity error:  {2 * (loss_weights[-num_blocks:] * environ.cross_entropy2(logits[-num_blocks:], gt)).mean()})\")\n",
    "    print_dbg(f\" loss_weights :  {loss_weights}\", verbose = True)\n",
    "    print_dbg(f\" cross_entropy:  {environ.cross_entropy2(logits[-num_blocks:], gt)}  \", verbose = True)\n",
    "    print_dbg(f\" loss[sparsity][{task_key}]: {self.losses['sparsity'][task_key] } \", verbose = True)\n",
    "\n",
    "else:\n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between \\n Logits   : \\n{logits[-num_blocks:]} \\n and gt: \\n{gt} \\n\", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-num_blocks:], gt)}\")\n",
    "    \n",
    "    print('\\n cond 2')\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt[-1:])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits      : {logits[-1:]}  and gt: {gt0[-1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[-1:], gt0[-1:])} \\n\")\n",
    "    \n",
    "    print('\\n cond 3')    \n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt[0:1])} \\n\")\n",
    "    print_dbg(f\"Compute CrossEntropyLoss between Logits   : {logits[0:1]}  and gt: {gt0[0:1]} \", verbose = True)\n",
    "    print(f\"{task_key} sparsity error:  {environ.cross_entropy_sparsity(logits[0:1], gt0[0:1])} \\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686cd05",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ee1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:14:30.155045Z",
     "start_time": "2022-01-26T00:14:30.107095Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# flag = 'update_w'\n",
    "# environ.fix_alpha\n",
    "# environ.free_w(opt['fix_BN'])\n",
    "\n",
    "flag = 'update_alpha'\n",
    "environ.fix_weights()\n",
    "environ.free_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7996b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:31.184285Z",
     "start_time": "2022-01-25T23:43:31.159229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.networks['mtl-net'].num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436ee6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T00:14:34.993711Z",
     "start_time": "2022-01-26T00:14:34.968623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"current_epochs           : {current_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") \n",
    "\n",
    "train_total_epochs += 5\n",
    "\n",
    "print(f\"current_iters         : {current_iter}\")  \n",
    "print(f\"current_epochs           : {current_epoch}\") \n",
    "print(f\"train_total_epochs    : {train_total_epochs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334a0b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T17:15:02.216665Z",
     "start_time": "2022-01-25T17:15:01.848081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_metrics_cr(current_epoch, time.time() - t0, None, environ.val_metrics , num_prints)      \n",
    "\n",
    "# num_prints += 1\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Take check point\n",
    "# environ.save_checkpoint('latest', current_iter)\n",
    "# environ.train()\n",
    "# #-------------------------------------------------------\n",
    "# # END validation process\n",
    "# #-------------------------------------------------------       \n",
    "# flag = 'update_alpha'\n",
    "# environ.fix_w()\n",
    "# environ.free_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c4f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:08:57.366231Z",
     "start_time": "2022-01-08T01:08:57.295445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dilation = 2\n",
    "# kernel_size = np.asarray((3, 3))\n",
    "# upsampled_kernel_size = (kernel_size - 1) * (dilation - 1) + kernel_size\n",
    "# print(upsampled_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500c390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:39.763599Z",
     "start_time": "2022-01-25T23:43:39.728402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.optimizers['weights'].param_groups[0]\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb71bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T23:43:41.628847Z",
     "start_time": "2022-01-25T23:43:41.602238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.schedulers['weights'].get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7934862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:36:17.129535Z",
     "start_time": "2022-01-08T01:36:16.006144Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_state = {}\n",
    "for k, v in environ.optimizers.items():\n",
    "    print(f'state dict for {k} = {v}')\n",
    "    current_state[k] = v.state_dict()\n",
    "pp.pprint(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5faf7f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:40:22.032247Z",
     "start_time": "2022-01-08T01:40:22.006953Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_state = {}\n",
    "for k, v in environ.schedulers.items():\n",
    "    print(f'state dict for {k} = {v}')\n",
    "    print(v.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd497e72",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66169a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T16:34:44.555617Z",
     "start_time": "2022-02-09T16:34:44.507417Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn_losses = environ.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb8234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:35:11.758429Z",
     "start_time": "2022-02-10T03:35:11.278211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_metrics_cr(current_epoch, time.time() - start_time, trn_losses, environ.val_metrics , num_prints)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0301f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:21:49.107793Z",
     "start_time": "2022-02-10T03:21:49.084484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print_metrics_cr(current_epoch, time.time() - start_time, trn_losses, environ.val_metrics , num_prints)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498b6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:20:32.866010Z",
     "start_time": "2022-02-10T03:20:32.442919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pp.pprint(environ.losses)\n",
    "pp.pprint(trn_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe30724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T03:34:21.945701Z",
     "start_time": "2022-02-10T03:34:21.411234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pp.pprint(environ.val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d81167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:49.364883Z",
     "start_time": "2022-01-27T00:58:49.342931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# environ.opt['train']['Lambda_sharing'] = 0.5\n",
    "# opt['train']['Lambda_sharing'] = 0.5\n",
    "\n",
    "# environ.opt['train']['policy_lr'] = 0.001\n",
    "# opt['train']['policy_lr'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73aa06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T01:00:41.996410Z",
     "start_time": "2022-01-26T01:00:41.559006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "environ.losses.keys()\n",
    "pp.pprint(environ.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ece74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T01:19:43.683550Z",
     "start_time": "2022-01-08T01:19:43.571450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = environ.get_loss_dict()\n",
    "print(tmp.keys())\n",
    "pp.pprint(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac0256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T00:58:50.458223Z",
     "start_time": "2022-01-27T00:58:50.430889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(opt['diff_sparsity_weights'])\n",
    "print(opt['is_sharing'])\n",
    "print(opt['diff_sparsity_weights'] and not opt['is_sharing'])\n",
    "print(environ.opt['train']['Lambda_sharing'])\n",
    "print(opt['train']['Lambda_sharing'])\n",
    "print(environ.opt['train']['Lambda_sparsity'])\n",
    "print(opt['train']['Lambda_sparsity'])\n",
    "print(environ.opt['train']['policy_lr'])\n",
    "print(opt['train']['policy_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2d510",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Policy / Logit stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb628497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:35:14.041577Z",
     "start_time": "2022-02-08T20:35:14.018303Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.special          import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eed454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:30.103364Z",
     "start_time": "2022-02-08T20:00:30.068021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b5cc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_task_logits(n)` Get logits for task group n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed8b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:56:35.652087Z",
     "start_time": "2022-02-08T20:56:35.327406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task_logits = environ.get_task_logits(1)\n",
    "print(task_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66fa5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_arch_parameters()`: Get last used logits from network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85521e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:00:31.101960Z",
     "start_time": "2022-02-08T20:00:30.757064Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b0bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:06.054699Z",
     "start_time": "2022-02-09T15:43:05.689327Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "arch_parameters      = environ.get_arch_parameters()\n",
    "print(arch_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea1743",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_logits()`:  Get Policy Logits - returns same as `get_arch_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb40c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:24.972390Z",
     "start_time": "2022-02-09T15:43:24.636629Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = environ.get_policy_logits()\n",
    "for i in logs:\n",
    "    print(i, '\\n')\n",
    "# probs = softmax(logs, axis= -1)\n",
    "# for i in probs:\n",
    "#     print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080a364",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `get_policy_prob()` : Gets the softmax of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c75af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T15:43:29.733732Z",
     "start_time": "2022-02-09T15:43:29.699600Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "for i in policy_softmaxs:\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3160d9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = False)` : Calls test_sample_policy of network with random choices based on softmax of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:21:15.636722Z",
     "start_time": "2022-02-08T22:21:15.165456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "policies,logits = environ.get_sample_policy(hard_sampling = False)\n",
    "\n",
    "for l, p, s in zip(logits, policies, policy_softmaxs) :\n",
    "    for  l_row, p_row, s_row in zip(l, p, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802664ec",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_sample_policy( hard_sampling = True)` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea65bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:59:40.790899Z",
     "start_time": "2022-02-08T20:59:40.726657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "policy_softmaxs = environ.get_policy_prob()\n",
    "hard_policies, logits = environ.get_sample_policy(hard_sampling = True)\n",
    "\n",
    "for p,l,s in zip(hard_policies, logits, policy_softmaxs) :\n",
    "    for  p_row, l_row, s_row in zip(p, l, s):\n",
    "        print( l_row,'\\t', p_row, '\\t', s_row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c39cf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:00:47.452220Z",
     "start_time": "2022-02-08T21:00:47.422902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\" Layer    task 1      task 2      task 3\")\n",
    "print(f\" -----    ------      ------      ------\")\n",
    "for idx, (l1, l2, l3) in enumerate(zip(hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1}       {l2}       {l3}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade0ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:39:39.936555Z",
     "start_time": "2022-02-08T22:39:39.911591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_trained_policy(iter):\n",
    "\n",
    "    policy_softmaxs = environ.get_policy_prob()\n",
    "    policy_argmaxs = 1-np.argmax(policy_softmaxs, axis = -1)\n",
    "    print(f\"  Trained polcies at iteration: {iter} \")\n",
    "    print(f\"                   task 1                           task 2                         task 3        \")\n",
    "    print(f\" Layer       softmax        select          softmax        select          softmax        select   \")\n",
    "    print(f\" -----    ---------------   ------       ---------------   ------       ---------------   ------   \")\n",
    "    for idx, (l1,l2,l3,  p1,p2,p3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2], policy_argmaxs[0], policy_argmaxs[1], policy_argmaxs[2]),1):\n",
    "        print(f\"   {idx}      {l1[0]:.4f}   {l1[1]:.4f}   {p1:4d}    {l2[0]:11.4f}   {l2[1]:.4f}   {p2:4d}    {l3[0]:11.4f}   {l3[1]:.4f}   {p3:4d}\")\n",
    "\n",
    "    print()\n",
    "# print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec208dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:42:12.650813Z",
     "start_time": "2022-02-08T22:42:12.330169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_trained_policy(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec517e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:07:30.836214Z",
     "start_time": "2022-02-08T22:07:30.804575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"                        POLICIES (SOFTMAX)                                       task 3          \")\n",
    "print(f\" Layer    task1              task2            task3 softmax         softmax         argmax         softmax         argmax   \")\n",
    "print(f\" -----    -------------     -------------     -------------   ------   \")\n",
    "for idx, (l1,l2,l3, h1,h2,h3) in enumerate(zip(policy_softmaxs[0], policy_softmaxs[1], policy_softmaxs[2],hard_policies[0], hard_policies[1], hard_policies[2]),1):\n",
    "    print(f\"   {idx}      {l1[0]:.4f} {l1[1]:.4f}     {l2[0]:.4f} {l2[1]:.4f}     {l3[0]:.4f} {l3[1]:.4f}    {h3}\")\n",
    "    \n",
    "print(f\"\\n\\n where [p1  p2]:  p1: layer is selected    p2: layer is not selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbeacb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T22:04:50.757406Z",
     "start_time": "2022-02-08T22:04:50.731736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(policy_softmaxs[2], np.argmax(1-policy_softmaxs[2], axis = -1))\n",
    "print(policy_softmaxs, np.argmax(policy_softmaxs, axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0240",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_current_logits()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb7240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:19:06.155425Z",
     "start_time": "2022-02-08T21:19:06.118640Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits  = (environ.get_current_logits())\n",
    "for i in logits:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e84662",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### `get_current_policy()` : Calls test_sample_policy of network using ARGMAX of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cfa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T20:40:06.543376Z",
     "start_time": "2022-02-08T20:40:06.230711Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pols  = (environ.get_current_policy())\n",
    "\n",
    "for i in pols:\n",
    "    print(i ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f556a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T18:27:33.593255Z",
     "start_time": "2022-01-27T18:27:33.553141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792710e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `gumbel_softmax()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265490e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:28:17.107529Z",
     "start_time": "2022-02-08T21:28:17.084910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8,edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', floatmode = 'maxprec_equal')\n",
    "torch.set_printoptions(precision=8,linewidth=132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb0087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:35:11.617269Z",
     "start_time": "2022-02-08T21:35:11.569599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(environ.temp)\n",
    "# tau = environ.temp\n",
    "tau = 1\n",
    "for i in range(3): \n",
    "    logits_tensor = torch.tensor(logits[0])\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=tau, hard=False).cpu().numpy() \n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=tau, hard=True).cpu().numpy()\n",
    "    \n",
    "    for l, gs, gh in zip(lgts, gumbel_soft, gumbel_hard):\n",
    "        print(f\"   {l}   \\t {gs}            \\t {gh}\")\n",
    "#     print(lgts)\n",
    "#     print(gumbel_soft)\n",
    "#     print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5ef7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e0e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:21:35.524957Z",
     "start_time": "2022-02-08T21:21:35.488812Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for lgts in logits:\n",
    "    logits_tensor = torch.tensor(lgts)\n",
    "    print(lgts)\n",
    "    # Sample soft categorical using reparametrization trick:\n",
    "    gumbel_soft = F.gumbel_softmax(logits_tensor, tau=1, hard=False)\n",
    "    print(gumbel_soft)\n",
    "\n",
    "    # Sample hard categorical using \"Straight-through\" trick:\n",
    "    gumbel_hard  = F.gumbel_softmax(logits_tensor, tau=1, hard=True)\n",
    "    print(gumbel_hard)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34a06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T20:49:37.643349Z",
     "start_time": "2022-02-03T20:49:37.580786Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "smax = scipy.special.softmax(logs, axis =1)\n",
    "# smax = np.array( \n",
    "# [[0.46973792, 0.530262  ],\n",
    "#  [0.45025694, 0.549743  ],\n",
    "#  [0.4443086 , 0.5556915 ],\n",
    "#  [0.4138397 , 0.58616036],\n",
    "#  [0.4140113 , 0.5859887 ],\n",
    "#  [0.42114905, 0.57885087]])\n",
    "\n",
    "print(smax.shape)\n",
    "print(smax)\n",
    "print(smax[0])\n",
    "print(smax[0].sum())\n",
    "print(np.random.choice((1,0), p =smax[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7de25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T21:15:19.893888Z",
     "start_time": "2022-02-08T21:15:19.870899Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logs = np.array(\n",
    "[[0.33064184, 0.42053092],\n",
    " [0.3532089 , 0.52056104],\n",
    " [0.3888512 , 0.5680909 ],\n",
    " [0.42039296, 0.694217  ],\n",
    " [0.4519742 , 0.73311865],\n",
    " [0.48401102, 0.7522658 ]],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyt-gpu]",
   "language": "python",
   "name": "conda-env-pyt-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "399px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
